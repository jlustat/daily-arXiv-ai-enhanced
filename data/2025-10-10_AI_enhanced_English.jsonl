{"id": "2510.07501", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07501", "abs": "https://arxiv.org/abs/2510.07501", "authors": ["Sihyung Park", "Wenbin Lu", "Shu Yang"], "title": "Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death", "comment": "30 pages, 5 figures, 6 tables, The Thirty-Ninth Annual Conference on\n  Neural Information Processing Systems", "summary": "Truncation by death, a prevalent challenge in critical care, renders\ntraditional dynamic treatment regime (DTR) evaluation inapplicable due to\nill-defined potential outcomes. We introduce a principal stratification-based\nmethod, focusing on the always-survivor value function. We derive a\nsemiparametrically efficient, multiply robust estimator for multi-stage DTRs,\ndemonstrating its robustness and efficiency. Empirical validation and an\napplication to electronic health records showcase its utility for personalized\ntreatment optimization.", "AI": {"tldr": "A principal stratification method for evaluating dynamic treatment regimes in critical care settings where truncation by death makes traditional methods inapplicable, with a semiparametrically efficient, multiply robust estimator for multi-stage DTRs.", "motivation": "Truncation by death is a common problem in critical care that makes traditional dynamic treatment regime evaluation methods inapplicable because potential outcomes become ill-defined when patients die before treatment completion.", "method": "Principal stratification-based approach focusing on the always-survivor value function, with derivation of a semiparametrically efficient, multiply robust estimator for multi-stage dynamic treatment regimes.", "result": "The proposed estimator demonstrates robustness and efficiency, with empirical validation and application to electronic health records showing its utility for personalized treatment optimization.", "conclusion": "The method provides a viable solution for evaluating dynamic treatment regimes in critical care settings affected by truncation by death, enabling personalized treatment optimization through robust statistical estimation."}}
{"id": "2510.07624", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07624", "abs": "https://arxiv.org/abs/2510.07624", "authors": ["Abdelhakim Benechehab", "Gabriel Singer", "Corentin L\u00e9ger", "Youssef Attia El Hili", "Giuseppe Paolo", "Albert Thomas", "Maurizio Filippone", "Bal\u00e1zs K\u00e9gl"], "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation", "comment": null, "summary": "Generative models form the backbone of modern machine learning, underpinning\nstate-of-the-art systems in text, vision, and multimodal applications. While\nMaximum Likelihood Estimation has traditionally served as the dominant training\nparadigm, recent work have highlighted its limitations, particularly in\ngeneralization and susceptibility to catastrophic forgetting compared to\nReinforcement Learning techniques, such as Policy Gradient methods. However,\nthese approaches depend on explicit reward signals, which are often unavailable\nin practice, leaving open the fundamental problem of how to align generative\nmodels when only high-quality datasets are accessible. In this work, we address\nthis challenge via a Bilevel Optimization framework, where the reward function\nis treated as the optimization variable of an outer-level problem, while a\npolicy gradient objective defines the inner-level. We then conduct a\ntheoretical analysis of this optimization problem in a tractable setting and\nextract insights that, as we demonstrate, generalize to applications such as\ntabular classification and model-based reinforcement learning. We release the\ncode at https://github.com/abenechehab/nll_to_po .", "AI": {"tldr": "This paper proposes a bilevel optimization framework to align generative models when only high-quality datasets are available, addressing limitations of Maximum Likelihood Estimation and the dependency on explicit reward signals in Reinforcement Learning methods.", "motivation": "Traditional Maximum Likelihood Estimation has limitations in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning, but RL methods require explicit reward signals which are often unavailable in practice. The paper aims to solve the fundamental problem of aligning generative models using only high-quality datasets.", "method": "The authors introduce a Bilevel Optimization framework where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. They conduct theoretical analysis in a tractable setting.", "result": "The extracted insights from the theoretical analysis are demonstrated to generalize to applications such as tabular classification and model-based reinforcement learning.", "conclusion": "The proposed bilevel optimization framework provides a solution for aligning generative models when only high-quality datasets are accessible, bridging the gap between Maximum Likelihood Estimation and Reinforcement Learning approaches."}}
{"id": "2510.07649", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07649", "abs": "https://arxiv.org/abs/2510.07649", "authors": ["Tianyu Pan", "Vincent Z. Yu", "Viswanath Devanarayan", "Lu Tian"], "title": "A Honest Cross-Validation Estimator for Prediction Performance", "comment": null, "summary": "Cross-validation is a standard tool for obtaining a honest assessment of the\nperformance of a prediction model. The commonly used version repeatedly splits\ndata, trains the prediction model on the training set, evaluates the model\nperformance on the test set, and averages the model performance across\ndifferent data splits. A well-known criticism is that such cross-validation\nprocedure does not directly estimate the performance of the particular model\nrecommended for future use. In this paper, we propose a new method to estimate\nthe performance of a model trained on a specific (random) training set. A naive\nestimator can be obtained by applying the model to a disjoint testing set.\nSurprisingly, cross-validation estimators computed from other random splits can\nbe used to improve this naive estimator within a random-effects model\nframework. We develop two estimators -- a hierarchical Bayesian estimator and\nan empirical Bayes estimator -- that perform similarly to or better than both\nthe conventional cross-validation estimator and the naive single-split\nestimator. Simulations and a real-data example demonstrate the superior\nperformance of the proposed method.", "AI": {"tldr": "The paper proposes a new method to estimate the performance of a model trained on a specific training set, improving upon conventional cross-validation by using information from other random splits within a random-effects model framework.", "motivation": "Standard cross-validation doesn't directly estimate the performance of the particular model recommended for future use, as it averages performance across different data splits rather than focusing on the specific model trained on the available data.", "method": "Developed two estimators - a hierarchical Bayesian estimator and an empirical Bayes estimator - that use cross-validation estimators from other random splits to improve the naive single-split estimator within a random-effects model framework.", "result": "The proposed estimators perform similarly to or better than both conventional cross-validation and naive single-split estimators in simulations and real-data examples.", "conclusion": "The new method provides superior performance for estimating the performance of models trained on specific training sets, offering an improvement over standard cross-validation approaches."}}
{"id": "2510.07750", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07750", "abs": "https://arxiv.org/abs/2510.07750", "authors": ["Wenbin Zhou", "Shixiang Zhu"], "title": "When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making", "comment": null, "summary": "Robust optimization safeguards decisions against uncertainty by optimizing\nagainst worst-case scenarios, yet their effectiveness hinges on a prespecified\nrobustness level that is often chosen ad hoc, leading to either insufficient\nprotection or overly conservative and costly solutions. Recent approaches using\nconformal prediction construct data-driven uncertainty sets with finite-sample\ncoverage guarantees, but they still fix coverage targets a priori and offer\nlittle guidance for selecting robustness levels. We propose a new framework\nthat provides distribution-free, finite-sample guarantees on both miscoverage\nand regret for any family of robust predict-then-optimize policies. Our method\nconstructs valid estimators that trace out the miscoverage-regret Pareto\nfrontier, enabling decision-makers to reliably evaluate and calibrate\nrobustness levels according to their cost-risk preferences. The framework is\nsimple to implement, broadly applicable across classical optimization\nformulations, and achieves sharper finite-sample performance than existing\napproaches. These results offer the first principled data-driven methodology\nfor guiding robustness selection and empower practitioners to balance\nrobustness and conservativeness in high-stakes decision-making.", "AI": {"tldr": "A new framework for robust optimization that provides finite-sample guarantees on both miscoverage and regret, enabling decision-makers to calibrate robustness levels based on cost-risk preferences without presetting coverage targets.", "motivation": "Traditional robust optimization relies on ad hoc robustness levels that lead to either insufficient protection or overly conservative solutions, while existing conformal prediction methods still fix coverage targets a priori without guidance for selecting appropriate robustness levels.", "method": "Constructs valid estimators that trace out the miscoverage-regret Pareto frontier, providing distribution-free finite-sample guarantees for any family of robust predict-then-optimize policies.", "result": "The framework achieves sharper finite-sample performance than existing approaches, is simple to implement, broadly applicable across classical optimization formulations, and enables reliable evaluation and calibration of robustness levels.", "conclusion": "This offers the first principled data-driven methodology for guiding robustness selection, empowering practitioners to balance robustness and conservativeness in high-stakes decision-making."}}
{"id": "2510.07518", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.07518", "abs": "https://arxiv.org/abs/2510.07518", "authors": ["Blake Hansen", "Dafne Zorzetto", "Valeria Edefonti", "Roberta De Vito"], "title": "Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization", "comment": null, "summary": "Understanding the association between dietary patterns and health outcomes,\nsuch as the cancer risk, is crucial to inform public health guidelines and\nshaping future dietary interventions. However, dietary intake data present\nseveral statistical challenges: they are high-dimensional, often sparse with\nexcess zeros, and exhibit heterogeneity driven by individual-level covariates.\nNon-Negative Matrix Factorization (NMF), commonly used to estimate patterns in\nhigh-dimensional count data, typically relies on Poisson assumptions and lacks\nthe flexibility to fully address these complexities. Additionally, integrating\ndata across multiple studies, such as case-control studies on cancer risk,\nrequires models that can share information across sources while preserving\nstudy-specific structure.\n  In this paper, we introduce a novel Bayesian NMF model that (i) jointly\nmodels multi-study count data to enable cross-study information sharing, (ii)\nincorporate a mixture component to account for zero inflation, and (iii)\nleverage flexible Bayesian non-parametric priors for characterizing the\nheterogeneity in pattern scores induced by the individual covariates. This\nstructure allows for clustering of individuals based on dietary profiles,\nenabling downstream association analyses with health outcomes. Through\nextensive simulation studies, we demonstrate that our model significantly\nimproves estimation accuracy compared to existing Bayesian NMF methods.\n  We further illustrate its utility through an application to multiple\ncase-control studies on diet and upper aero-digestive tract cancers,\nidentifying nutritionally meaningful dietary patterns. An R package\nimplementing our approach is available at\nhttps://github.com/blhansen/ZIMultiStudyNMF.", "AI": {"tldr": "A novel Bayesian Non-Negative Matrix Factorization model that handles multi-study dietary data with zero inflation and covariate-driven heterogeneity, enabling better dietary pattern estimation and cancer risk analysis.", "motivation": "Dietary intake data presents statistical challenges including high dimensionality, sparsity with excess zeros, and heterogeneity from individual covariates. Existing NMF methods using Poisson assumptions lack flexibility to address these complexities and cannot effectively integrate data across multiple studies.", "method": "Developed a Bayesian NMF model that jointly models multi-study count data for cross-study information sharing, incorporates a mixture component for zero inflation, and uses flexible Bayesian non-parametric priors to characterize heterogeneity in pattern scores from individual covariates.", "result": "Simulation studies show significant improvement in estimation accuracy compared to existing Bayesian NMF methods. Application to case-control studies on diet and upper aero-digestive tract cancers successfully identified nutritionally meaningful dietary patterns.", "conclusion": "The proposed Bayesian NMF model effectively addresses key challenges in dietary data analysis, enabling better dietary pattern estimation and facilitating downstream association analyses with health outcomes like cancer risk."}}
{"id": "2510.07832", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07832", "abs": "https://arxiv.org/abs/2510.07832", "authors": ["Yuta Shikuri", "Hironori Fujisawa"], "title": "Surrogate Graph Partitioning for Spatial Prediction", "comment": "18 pages, 5 figures, 2 tables", "summary": "Spatial prediction refers to the estimation of unobserved values from\nspatially distributed observations. Although recent advances have improved the\ncapacity to model diverse observation types, adoption in practice remains\nlimited in industries that demand interpretability. To mitigate this gap,\nsurrogate models that explain black-box predictors provide a promising path\ntoward interpretable decision making. In this study, we propose a graph\npartitioning problem to construct spatial segments that minimize the sum of\nwithin-segment variances of individual predictions. The assignment of data\npoints to segments can be formulated as a mixed-integer quadratic programming\nproblem. While this formulation potentially enables the identification of exact\nsegments, its computational complexity becomes prohibitive as the number of\ndata points increases. Motivated by this challenge, we develop an approximation\nscheme that leverages the structural properties of graph partitioning.\nExperimental results demonstrate the computational efficiency of this\napproximation in identifying spatial segments.", "AI": {"tldr": "Proposes a graph partitioning approach for interpretable spatial prediction by creating spatial segments that minimize within-segment variance, with an efficient approximation scheme for large datasets.", "motivation": "Address the gap between advanced spatial prediction models and practical adoption in industries requiring interpretability, by developing surrogate models that explain black-box predictors.", "method": "Formulates spatial segmentation as a mixed-integer quadratic programming problem using graph partitioning to minimize within-segment variances, and develops an approximation scheme for computational efficiency with large datasets.", "result": "Experimental results show the approximation scheme achieves computational efficiency while effectively identifying spatial segments.", "conclusion": "The proposed graph partitioning approach provides an interpretable spatial prediction method with practical computational efficiency through approximation, bridging the gap between advanced models and industry adoption requirements."}}
{"id": "2510.07521", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.07521", "abs": "https://arxiv.org/abs/2510.07521", "authors": ["Danielle Mccool", "Peter Lugtig", "Bella Struminskaya"], "title": "Integrating smart surveys with traditional surveys", "comment": null, "summary": "Smart surveys are surveys that make use of sensors and machine intelligence\nto reduce respondent burden and increase data quality. Smart surveys have been\ntests as a way to improve diary surveys in official statistics, where data are\ncollected on topics such as travel, time use and household expenditures. There\nare often inherent differences both in measurement and representation between\nsmart surveys and traditional diaries, which makes it difficult to integrate\nboth data sources in producing statistics over time, or within a mixed- or\nmulti-source context. This paper distinguishes two different approaches to\nintegration: the mixed-mode approach, which prioritizes outcome alignment and\nminimizes measurement differences for straightforward data merging, and the\nmultisource approach, which maintains inherent mode differences and integrates\ndata at the modeling stage, allowing exploitation of the strengths of each\nsource. Using travel surveys as an illustrative example, we explore the\nbenefits and drawbacks of each approach, and propose a decision framework to\nguide researchers in selecting the appropriate integration strategy.", "AI": {"tldr": "This paper analyzes two approaches for integrating smart surveys with traditional diaries in official statistics: mixed-mode (aligning outcomes for direct merging) and multisource (preserving mode differences and integrating through modeling).", "motivation": "Smart surveys using sensors and AI can reduce respondent burden and improve data quality, but integrating them with traditional diaries is challenging due to measurement and representation differences.", "method": "The paper distinguishes between mixed-mode and multisource integration approaches, using travel surveys as an illustrative example to explore benefits and drawbacks of each method.", "result": "The analysis reveals trade-offs between the two approaches - mixed-mode enables straightforward data merging but may lose unique strengths of each source, while multisource preserves mode differences but requires more complex modeling.", "conclusion": "The authors propose a decision framework to help researchers select the appropriate integration strategy based on their specific needs and context."}}
{"id": "2510.07862", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07862", "abs": "https://arxiv.org/abs/2510.07862", "authors": ["Sanghwa Kim", "Dohyun Ahn", "Seungki Min"], "title": "On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses", "comment": null, "summary": "We study the problem of estimating a continuous ability parameter from\nsequential binary responses by actively asking questions with varying\ndifficulties, a setting that arises naturally in adaptive testing and online\npreference learning. Our goal is to certify that the estimate lies within a\ndesired margin of error, using as few queries as possible. We propose a simple\nalgorithm that adaptively selects questions to maximize Fisher information and\nupdates the estimate using a method-of-moments approach, paired with a novel\ntest statistic to decide when the estimate is accurate enough. We prove that\nthis Fisher-tracking strategy achieves optimal performance in both\nfixed-confidence and fixed-budget regimes, which are commonly invested in the\nbest-arm identification literature. Our analysis overcomes a key technical\nchallenge in the fixed-budget setting -- handling the dependence between the\nevolving estimate and the query distribution -- by exploiting a structural\nsymmetry in the model and combining large deviation tools with Ville's\ninequality. Our results provide rigorous theoretical support for simple and\nefficient adaptive testing procedures.", "AI": {"tldr": "The paper proposes an adaptive testing algorithm that uses Fisher information to select questions and a method-of-moments approach to estimate ability parameters, achieving optimal performance in both fixed-confidence and fixed-budget settings.", "motivation": "To develop efficient adaptive testing procedures that can certify estimation accuracy with minimal queries, addressing challenges in adaptive testing and online preference learning where sequential binary responses are used to estimate continuous ability parameters.", "method": "Proposes a Fisher-tracking algorithm that adaptively selects questions to maximize Fisher information, updates estimates using method-of-moments, and uses a novel test statistic to determine when estimates are sufficiently accurate.", "result": "The algorithm achieves optimal performance in both fixed-confidence and fixed-budget regimes, overcoming technical challenges in handling dependence between evolving estimates and query distributions through structural symmetry and large deviation tools.", "conclusion": "The Fisher-tracking strategy provides rigorous theoretical support for simple and efficient adaptive testing procedures, with proven optimality in common experimental settings."}}
{"id": "2510.07608", "categories": ["stat.ME", "62G07, 62H30"], "pdf": "https://arxiv.org/pdf/2510.07608", "abs": "https://arxiv.org/abs/2510.07608", "authors": ["Jiajin Xie", "Yong Wang", "Eduardo Garc\u00eda-Portugu\u00e9s"], "title": "Density estimation for compositional data using nonparametric mixtures", "comment": "26 pages, 12 figures, 11 tables", "summary": "Compositional data, representing proportions constrained to the simplex,\narise in diverse fields such as geosciences, ecology, genomics, and microbiome\nresearch. Existing nonparametric density estimation methods often rely on\ntransformations, which may induce substantial bias near the simplex boundary.\nWe propose a nonparametric mixture-based framework for density estimation on\ncompositions. Nonparametric Dirichlet mixtures are employed to naturally\naccommodate boundary values, thereby avoiding the transformation or\nzero-replacement, while also identifying components supported on the boundary,\nproviding reliable estimates for data with zero or near-zero values. Bandwidth\nselection and initialization schemes are addressed. For comparison,\nnonparametric Gaussian mixtures, coupled with log-ratio transformations, are\nalso considered. Extensive simulations show that the proposed estimators\noutperform existing approaches. Three real data applications, including GDP\ndata analysis, handwritten digit recognition, and skin detection, demonstrate\nthe usefulness of nonparametric Dirichlet mixtures in practice.", "AI": {"tldr": "Proposes nonparametric Dirichlet mixtures for compositional data density estimation to handle boundary values without transformations, outperforming existing methods in simulations and real applications.", "motivation": "Existing density estimation methods for compositional data often use transformations that cause bias near simplex boundaries, especially for data with zero or near-zero values.", "method": "Uses nonparametric Dirichlet mixtures that naturally accommodate boundary values without transformations or zero-replacement, with bandwidth selection and initialization schemes.", "result": "Extensive simulations show proposed estimators outperform existing approaches. Real applications in GDP analysis, digit recognition, and skin detection demonstrate practical usefulness.", "conclusion": "Nonparametric Dirichlet mixtures provide reliable density estimation for compositional data with boundary values, avoiding transformation-induced bias and handling zero/near-zero values effectively."}}
{"id": "2510.07867", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07867", "abs": "https://arxiv.org/abs/2510.07867", "authors": ["Xabier de Juan", "Santiago Mazuelas"], "title": "On the Optimality of the Median-of-Means Estimator under Adversarial Contamination", "comment": null, "summary": "The Median-of-Means (MoM) is a robust estimator widely used in machine\nlearning that is known to be (minimax) optimal in scenarios where samples are\ni.i.d. In more grave scenarios, samples are contaminated by an adversary that\ncan inspect and modify the data. Previous work has theoretically shown the\nsuitability of the MoM estimator in certain contaminated settings. However, the\n(minimax) optimality of MoM and its limitations under adversarial contamination\nremain unknown beyond the Gaussian case. In this paper, we present upper and\nlower bounds for the error of MoM under adversarial contamination for multiple\nclasses of distributions. In particular, we show that MoM is (minimax) optimal\nin the class of distributions with finite variance, as well as in the class of\ndistributions with infinite variance and finite absolute $(1+r)$-th moment. We\nalso provide lower bounds for MoM's error that match the order of the presented\nupper bounds, and show that MoM is sub-optimal for light-tailed distributions.", "AI": {"tldr": "This paper analyzes the Median-of-Means (MoM) estimator's performance under adversarial contamination, establishing its minimax optimality for distributions with finite variance and infinite variance with finite absolute (1+r)-th moment, while showing sub-optimality for light-tailed distributions.", "motivation": "While MoM is known to be optimal for i.i.d. samples, its performance and optimality under adversarial contamination scenarios where samples can be inspected and modified by adversaries remains unknown beyond Gaussian cases.", "method": "The authors present theoretical analysis with upper and lower bounds for MoM's error under adversarial contamination across multiple distribution classes, including finite variance distributions and infinite variance distributions with finite absolute (1+r)-th moment.", "result": "MoM is proven to be minimax optimal for distributions with finite variance and for infinite variance distributions with finite absolute (1+r)-th moment. Lower bounds matching the order of upper bounds are provided, and MoM is shown to be sub-optimal for light-tailed distributions.", "conclusion": "The study establishes the theoretical foundations of MoM's robustness under adversarial contamination, characterizing its optimality conditions across different distribution classes and revealing its limitations for light-tailed distributions."}}
{"id": "2510.07770", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.07770", "abs": "https://arxiv.org/abs/2510.07770", "authors": ["Zhi Yang Tho", "Raymond Chambers", "A. H. Welsh"], "title": "Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data", "comment": null, "summary": "Clustered data arise naturally in many scientific and applied research\nsettings where units are grouped within clusters. They are commonly analyzed\nusing linear mixed models to account for within-cluster correlations. This\narticle focuses on the scenario in which cluster sizes might be highly\nunbalanced and proposes a proportional random effect block bootstrap and a\nmodified random effect block bootstrap, which are applicable in such cases and\naccommodate general distributions of random effects and error terms. These\nmethods generalize the random effect block bootstrap, originally designed for\nthe balanced case, and can be used for inference on parameters of linear mixed\nmodels or functions thereof. Both proposed bootstraps are shown to enjoy Fisher\nconsistency under general cluster sizes, while the original random effect block\nbootstrap is consistent only for balanced clusters. Simulations demonstrate\nstrong finite sample inferential performance of the proposed bootstraps\nrelative to the random effect block bootstrap and other existing bootstrap\nmethods for clustered data. Application to the Oman rainfall enhancement trial\ndataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap\nconfidence intervals using the proposed bootstraps over the random effect block\nbootstrap and a statistically significant effect of the ionization technology\non rainfall.", "AI": {"tldr": "Proposes two new bootstrap methods (proportional and modified random effect block bootstraps) for linear mixed models with highly unbalanced cluster sizes, showing improved performance over existing methods.", "motivation": "Clustered data with highly unbalanced cluster sizes are common in research, but existing bootstrap methods like the random effect block bootstrap were designed for balanced cases and perform poorly with unbalanced clusters.", "method": "Developed proportional random effect block bootstrap and modified random effect block bootstrap that accommodate general distributions of random effects and error terms, generalizing the original balanced-case method to handle highly unbalanced cluster sizes.", "result": "Both proposed methods achieve Fisher consistency under general cluster sizes, while the original method only works for balanced clusters. Simulations show superior finite sample performance compared to existing bootstrap methods. Application to Oman rainfall data (cluster sizes 1-58) shows improved confidence intervals and statistically significant effect of ionization technology on rainfall.", "conclusion": "The proposed bootstrap methods provide reliable inference for linear mixed models with highly unbalanced cluster sizes, outperforming existing methods and enabling proper statistical analysis in real-world applications with unbalanced clustered data."}}
{"id": "2510.07965", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07965", "abs": "https://arxiv.org/abs/2510.07965", "authors": ["Seungsu Han", "Juyoung Hwang", "Won Chang"], "title": "Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference", "comment": null, "summary": "Normalizing flows with a Gaussian base provide a computationally efficient\nway to approximate posterior distributions in Bayesian inference, but they\noften struggle to capture complex posteriors with multimodality and heavy\ntails. We propose a stick-breaking mixture base with component-wise tail\nadaptation (StiCTAF) for posterior approximation. The method first learns a\nflexible mixture base to mitigate the mode-seeking bias of reverse KL\ndivergence through a weighted average of component-wise ELBOs. It then\nestimates local tail indices of unnormalized densities and finally refines each\nmixture component using a shared backbone combined with component-specific tail\ntransforms calibrated by the estimated indices. This design enables accurate\nmode coverage and anisotropic tail modeling while retaining exact density\nevaluation and stable optimization. Experiments on synthetic posteriors\ndemonstrate improved tail recovery and better coverage of multiple modes\ncompared to benchmark models. We also present a real-data analysis illustrating\nthe practical benefits of our approach for posterior inference.", "AI": {"tldr": "The paper proposes StiCTAF, a stick-breaking mixture base with component-wise tail adaptation for posterior approximation, addressing limitations of Gaussian-based normalizing flows in capturing multimodal and heavy-tailed posteriors.", "motivation": "Normalizing flows with Gaussian bases struggle to capture complex posterior distributions with multimodality and heavy tails, which limits their effectiveness in Bayesian inference applications.", "method": "The method learns a flexible mixture base using weighted average of component-wise ELBOs, estimates local tail indices of unnormalized densities, and refines mixture components using shared backbone with component-specific tail transforms calibrated by estimated indices.", "result": "Experiments on synthetic posteriors show improved tail recovery and better coverage of multiple modes compared to benchmark models, with practical benefits demonstrated in real-data analysis.", "conclusion": "StiCTAF enables accurate mode coverage and anisotropic tail modeling while maintaining exact density evaluation and stable optimization, providing superior posterior approximation for complex distributions."}}
{"id": "2510.07854", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.07854", "abs": "https://arxiv.org/abs/2510.07854", "authors": ["\u0160\u00e1rka Hudecov\u00e1", "Claudia Kirch"], "title": "Detection of mean changes in partially observed functional data", "comment": null, "summary": "We propose a test for a change in the mean for a sequence of functional\nobservations that are only partially observed on subsets of the domain, with no\ninformation available on the complement. The framework accommodates important\nscenarios, including both abrupt and gradual changes. The significance of the\ntest statistic is assessed via a permutation test. In addition to the classical\npermutation approach with a fixed number of permutation samples, we also\ndiscuss a variant with controlled resampling risk that relies on a random\n(data-driven) number of permutation samples. The small sample performance of\nthe proposed methodology is illustrated in a Monte Carlo simulation study and\nan application to real data.", "AI": {"tldr": "A test for detecting mean changes in partially observed functional data using permutation methods with both fixed and data-driven sample sizes.", "motivation": "To address the challenge of detecting mean changes in functional observations that are only partially observed on subsets of the domain, with no information available on the complement, accommodating both abrupt and gradual changes.", "method": "The test uses a permutation approach with two variants: classical permutation with fixed samples and a controlled resampling risk variant with random, data-driven permutation samples.", "result": "The methodology's small sample performance is evaluated through Monte Carlo simulations and real data applications.", "conclusion": "The proposed test effectively detects mean changes in partially observed functional data using permutation methods, with variants for different sampling approaches."}}
{"id": "2510.08095", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08095", "abs": "https://arxiv.org/abs/2510.08095", "authors": ["Amitis Shidani", "Tyler Farghly", "Yang Sun", "Habib Ganjgahi", "George Deligiannidis"], "title": "Beyond Real Data: Synthetic Data through the Lens of Regularization", "comment": null, "summary": "Synthetic data can improve generalization when real data is scarce, but\nexcessive reliance may introduce distributional mismatches that degrade\nperformance. In this paper, we present a learning-theoretic framework to\nquantify the trade-off between synthetic and real data. Our approach leverages\nalgorithmic stability to derive generalization error bounds, characterizing the\noptimal synthetic-to-real data ratio that minimizes expected test error as a\nfunction of the Wasserstein distance between the real and synthetic\ndistributions. We motivate our framework in the setting of kernel ridge\nregression with mixed data, offering a detailed analysis that may be of\nindependent interest. Our theory predicts the existence of an optimal ratio,\nleading to a U-shaped behavior of test error with respect to the proportion of\nsynthetic data. Empirically, we validate this prediction on CIFAR-10 and a\nclinical brain MRI dataset. Our theory extends to the important scenario of\ndomain adaptation, showing that carefully blending synthetic target data with\nlimited source data can mitigate domain shift and enhance generalization. We\nconclude with practical guidance for applying our results to both in-domain and\nout-of-domain scenarios.", "AI": {"tldr": "A learning-theoretic framework that quantifies the optimal synthetic-to-real data ratio using algorithmic stability and Wasserstein distance, predicting U-shaped test error behavior with empirical validation on CIFAR-10 and clinical MRI data.", "motivation": "Synthetic data can improve generalization when real data is scarce, but excessive reliance may introduce distributional mismatches that degrade performance. The paper aims to quantify the trade-off between synthetic and real data.", "method": "Leverages algorithmic stability to derive generalization error bounds, characterizing the optimal synthetic-to-real data ratio as a function of Wasserstein distance between distributions. Analyzes kernel ridge regression with mixed data.", "result": "Theory predicts existence of an optimal ratio leading to U-shaped test error behavior. Empirical validation on CIFAR-10 and clinical brain MRI dataset confirms predictions. Framework extends to domain adaptation scenarios.", "conclusion": "Provides practical guidance for applying optimal data blending in both in-domain and out-of-domain scenarios, showing that carefully blending synthetic target data with limited source data can mitigate domain shift and enhance generalization."}}
{"id": "2510.08204", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.08204", "abs": "https://arxiv.org/abs/2510.08204", "authors": ["Soham Ghosh", "Saloni Bhogale", "Sameer K. Deshpande"], "title": "Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles", "comment": null, "summary": "By allowing the effects of $p$ covariates in a linear regression model to\nvary as functions of $R$ additional effect modifiers, varying-coefficient\nmodels (VCMs) strike a compelling balance between interpretable-but-rigid\nparametric models popular in classical statistics and flexible-but-opaque\nmethods popular in machine learning. But in high-dimensional settings where $p$\nand/or $R$ exceed the number of observations, existing approaches to fitting\nVCMs fail to identify which covariates have a non-zero effect and which effect\nmodifiers drive these effects. We propose sparseVCBART, a fully Bayesian model\nthat approximates each coefficient function in a VCM with a regression tree\nensemble and encourages sparsity with a global--local shrinkage prior on the\nregression tree leaf outputs and a hierarchical prior on the splitting\nprobabilities of each tree. We show that the sparseVCBART posterior contracts\nat a near-minimax optimal rate, automatically adapting to the unknown sparsity\nstructure and smoothness of the true coefficient functions. Compared to\nexisting state-of-the-art methods, sparseVCBART achieved competitive predictive\naccuracy and substantially narrower and better-calibrated uncertainty\nintervals, especially for null covariate effects. We use sparseVCBART to\ninvestigate how the effects of interpersonal conversations on prejudice could\nvary according to the political and demographic characteristics of the\nrespondents.", "AI": {"tldr": "SparseVCBART is a Bayesian varying-coefficient model that uses regression tree ensembles and sparsity-inducing priors to handle high-dimensional settings where the number of covariates and effect modifiers exceeds observations.", "motivation": "Existing varying-coefficient models fail in high-dimensional settings to identify which covariates have non-zero effects and which effect modifiers drive these effects, creating a need for methods that balance interpretability and flexibility.", "method": "The method uses regression tree ensembles to approximate coefficient functions, with global-local shrinkage priors on leaf outputs and hierarchical priors on tree splitting probabilities to encourage sparsity.", "result": "SparseVCBART achieved competitive predictive accuracy with substantially narrower and better-calibrated uncertainty intervals compared to state-of-the-art methods, especially for null covariate effects.", "conclusion": "The posterior contracts at near-minimax optimal rate, automatically adapting to unknown sparsity and smoothness, making sparseVCBART effective for high-dimensional varying-coefficient modeling with demonstrated application in social science research."}}
{"id": "2510.08123", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08123", "abs": "https://arxiv.org/abs/2510.08123", "authors": ["Parham Rezaei", "Filip Kovacevic", "Francesco Locatello", "Marco Mondelli"], "title": "High-dimensional Analysis of Synthetic Data Selection", "comment": null, "summary": "Despite the progress in the development of generative models, their\nusefulness in creating synthetic data that improve prediction performance of\nclassifiers has been put into question. Besides heuristic principles such as\n\"synthetic data should be close to the real data distribution\", it is actually\nnot clear which specific properties affect the generalization error. Our paper\naddresses this question through the lens of high-dimensional regression.\nTheoretically, we show that, for linear models, the covariance shift between\nthe target distribution and the distribution of the synthetic data affects the\ngeneralization error but, surprisingly, the mean shift does not. Furthermore we\nprove that, in some settings, matching the covariance of the target\ndistribution is optimal. Remarkably, the theoretical insights from linear\nmodels carry over to deep neural networks and generative models. We empirically\ndemonstrate that the covariance matching procedure (matching the covariance of\nthe synthetic data with that of the data coming from the target distribution)\nperforms well against several recent approaches for synthetic data selection,\nacross training paradigms, architectures, datasets and generative models used\nfor augmentation.", "AI": {"tldr": "The paper shows that matching the covariance of synthetic data to the target distribution improves classifier performance, while mean shift has no effect on generalization error.", "motivation": "To understand which specific properties of synthetic data affect generalization error, beyond heuristic principles like closeness to real data distribution.", "method": "Theoretical analysis through high-dimensional regression and linear models, then empirical validation with deep neural networks and generative models using covariance matching procedure.", "result": "Covariance shift affects generalization error but mean shift does not; covariance matching performs better than recent synthetic data selection approaches across various training paradigms, architectures, datasets and generative models.", "conclusion": "Matching the covariance of synthetic data with the target distribution is optimal for improving classifier performance, and theoretical insights from linear models generalize to deep neural networks and generative models."}}
{"id": "2510.08304", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08304", "abs": "https://arxiv.org/abs/2510.08304", "authors": ["Matteo Amestoy", "Mark van de Wiel", "Jeroen Lakerveld", "Wessel van Wieringen"], "title": "Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data", "comment": null, "summary": "Exposure to diverse non-genetic factors, known as the exposome, is a critical\ndeterminant of health outcomes. However, analyzing the exposome presents\nsignificant methodological challenges, including: high collinearity among\nexposures, the longitudinal nature of repeated measurements, and potential\ncomplex interactions with individual characteristics. In this paper, we address\nthese challenges by proposing a novel statistical framework that extends\nBayesian profile regression. Our method integrates profile regression, which\nhandles collinearity by clustering exposures into latent profiles, into a\nlinear mixed model (LMM), a framework for longitudinal data analysis. This\nprofile-LMM approach effectively accounts for within-person variability over\ntime while also incorporating interactions between the latent exposure clusters\nand individual characteristics. We validate our method using simulated data,\ndemonstrating its ability to accurately identify model parameters and recover\nthe true latent exposure cluster structure. Finally, we apply this approach to\na large longitudinal data set from the Lifelines cohort to identify\ncombinations of exposures that are significantly associated with diastolic\nblood pressure.", "AI": {"tldr": "A novel Bayesian statistical framework combining profile regression with linear mixed models to analyze longitudinal exposome data, addressing collinearity, repeated measurements, and interactions.", "motivation": "To overcome methodological challenges in exposome analysis including high collinearity among exposures, longitudinal nature of repeated measurements, and complex interactions with individual characteristics.", "method": "Extends Bayesian profile regression by integrating it into a linear mixed model framework, creating a profile-LMM approach that clusters exposures into latent profiles while accounting for within-person variability over time.", "result": "Validated on simulated data showing accurate parameter identification and recovery of true latent exposure cluster structure. Applied to Lifelines cohort data to identify exposure combinations significantly associated with diastolic blood pressure.", "conclusion": "The proposed profile-LMM framework effectively addresses key challenges in exposome analysis and provides a robust statistical approach for identifying exposure-health outcome relationships in longitudinal studies."}}
{"id": "2510.08335", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08335", "abs": "https://arxiv.org/abs/2510.08335", "authors": ["Ivan Kirev", "Lyuben Baltadzhiev", "Nikola Konstantinov"], "title": "PAC Learnability in the Presence of Performativity", "comment": "21 pages, 3 figures", "summary": "Following the wide-spread adoption of machine learning models in real-world\napplications, the phenomenon of performativity, i.e. model-dependent shifts in\nthe test distribution, becomes increasingly prevalent. Unfortunately, since\nmodels are usually trained solely based on samples from the original\n(unshifted) distribution, this performative shift may lead to decreased\ntest-time performance. In this paper, we study the question of whether and when\nperformative binary classification problems are learnable, via the lens of the\nclassic PAC (Probably Approximately Correct) learning framework. We motivate\nseveral performative scenarios, accounting in particular for linear shifts in\nthe label distribution, as well as for more general changes in both the labels\nand the features. We construct a performative empirical risk function, which\ndepends only on data from the original distribution and on the type\nperformative effect, and is yet an unbiased estimate of the true risk of a\nclassifier on the shifted distribution. Minimizing this notion of performative\nrisk allows us to show that any PAC-learnable hypothesis space in the standard\nbinary classification setting remains PAC-learnable for the considered\nperformative scenarios. We also conduct an extensive experimental evaluation of\nour performative risk minimization method and showcase benefits on synthetic\nand real data.", "AI": {"tldr": "The paper studies PAC learnability of performative binary classification, where model predictions shift the test distribution. It proposes performative empirical risk minimization using only original distribution data and proves PAC learnability remains for linear label shifts and general feature/label changes.", "motivation": "Machine learning models deployed in real-world applications cause performative shifts where predictions change the test distribution, leading to decreased performance. Models are typically trained on unshifted data, creating a need to understand when such problems remain learnable.", "method": "Constructs a performative empirical risk function using only original distribution data and knowledge of the performative effect type. This provides an unbiased estimate of true risk on shifted distribution. Proves PAC learnability through performative risk minimization.", "result": "Shows that any PAC-learnable hypothesis space in standard binary classification remains PAC-learnable for performative scenarios with linear label shifts and general feature/label changes. Experimental evaluation demonstrates benefits on synthetic and real data.", "conclusion": "Performative binary classification problems remain PAC-learnable under various shift scenarios. The proposed performative risk minimization approach enables learning from original distribution data while accounting for performative effects, maintaining theoretical guarantees."}}
{"id": "2510.08359", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.08359", "abs": "https://arxiv.org/abs/2510.08359", "authors": ["Jinho Cha", "Eunchan Cha"], "title": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials", "comment": "32 pages, 7 figures, planned to submit to Biostatistics", "summary": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings.", "AI": {"tldr": "Proposes a doubly robust estimated mean excursion effect (DR-EMEE) estimator with stabilized weights for micro-randomized trials, combining inverse probability weighting and outcome regression to improve efficiency and stability over existing methods.", "motivation": "Standard IPW estimators are unbiased but unstable in small samples or under extreme randomization, while EMEE improves efficiency but lacks double robustness, creating a need for more robust estimators in micro-randomized trials.", "method": "Developed DR-EMEE with stabilized and truncated weights that combines per-decision inverse probability weighting and outcome regression, with extensions to machine learning nuisance estimators and finite-sample variance corrections.", "result": "Simulations show DR-EMEE reduces root mean squared error, improves coverage, achieves up to twofold efficiency gains over IPW and 5-10% over EMEE, with stable performance across HeartSteps, PAMAP2, and mHealth datasets.", "conclusion": "DR-EMEE provides a doubly robust, asymptotically efficient estimator that maintains stability and efficiency across both randomized and observational settings in micro-randomized trials."}}
{"id": "2510.08409", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08409", "abs": "https://arxiv.org/abs/2510.08409", "authors": ["Yu-Han Wu", "Quentin Berthet", "G\u00e9rard Biau", "Claire Boyer", "Romuald Elie", "Pierre Marion"], "title": "Optimal Stopping in Latent Diffusion Models", "comment": null, "summary": "We identify and analyze a surprising phenomenon of Latent Diffusion Models\n(LDMs) where the final steps of the diffusion can degrade sample quality. In\ncontrast to conventional arguments that justify early stopping for numerical\nstability, this phenomenon is intrinsic to the dimensionality reduction in\nLDMs. We provide a principled explanation by analyzing the interaction between\nlatent dimension and stopping time. Under a Gaussian framework with linear\nautoencoders, we characterize the conditions under which early stopping is\nneeded to minimize the distance between generated and target distributions.\nMore precisely, we show that lower-dimensional representations benefit from\nearlier termination, whereas higher-dimensional latent spaces require later\nstopping time. We further establish that the latent dimension interplays with\nother hyperparameters of the problem such as constraints in the parameters of\nscore matching. Experiments on synthetic and real datasets illustrate these\nproperties, underlining that early stopping can improve generative quality.\nTogether, our results offer a theoretical foundation for understanding how the\nlatent dimension influences the sample quality, and highlight stopping time as\na key hyperparameter in LDMs.", "AI": {"tldr": "Latent Diffusion Models (LDMs) exhibit a phenomenon where final diffusion steps degrade sample quality, which is intrinsic to dimensionality reduction rather than numerical stability. The study shows that lower-dimensional latent spaces benefit from earlier stopping, while higher-dimensional ones require later stopping.", "motivation": "To understand why early stopping improves sample quality in LDMs, challenging conventional explanations based on numerical stability and revealing the fundamental role of latent dimension in this phenomenon.", "method": "Theoretical analysis using Gaussian framework with linear autoencoders to characterize conditions for optimal stopping time, examining interaction between latent dimension and stopping time, and experiments on synthetic and real datasets.", "result": "Lower-dimensional latent representations require earlier termination to minimize distribution distance, while higher-dimensional spaces need later stopping. Latent dimension interacts with other hyperparameters like score matching constraints.", "conclusion": "Early stopping is a crucial hyperparameter in LDMs that significantly impacts generative quality, with optimal stopping time depending on latent dimension. This provides theoretical foundation for understanding how dimensionality affects sample quality."}}
{"id": "2510.08438", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.08438", "abs": "https://arxiv.org/abs/2510.08438", "authors": ["Xi Fang", "Bingkai Wang", "Liangyuan Hu", "Fan Li"], "title": "Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes", "comment": "1 figure", "summary": "Cluster-randomized trials (CRTs) are experimental designs where groups or\nclusters of participants, rather than the individual participants themselves,\nare randomized to intervention groups. Analyzing CRT requires distinguishing\nbetween treatment effects at the cluster level and the individual level, which\nrequires a clear definition of the estimands under the potential outcomes\nframework. For analyzing survival outcomes, it is common to assess the\ntreatment effect by comparing survival functions or restricted mean survival\ntimes between treatment groups. In this article, we formally characterize\ncluster-level and individual-level treatment effect estimands with\nright-censored survival outcomes in CRTs and propose doubly robust estimators\nfor targeting such estimands. Under covariate-dependent censoring, our\nestimators ensure consistency when either the censoring model or the outcome\nmodel is correctly specified, but not necessarily both. We explore different\nmodeling options for the censoring and outcome models to estimate the censoring\nand survival distributions, and investigate a deletion-based jackknife method\nfor variance and interval estimation. Extensive simulations demonstrate that\nthe proposed methods perform adequately in finite samples. Finally, we\nillustrate our method by analyzing a completed CRT with survival endpoints.", "AI": {"tldr": "This paper proposes doubly robust estimators for cluster-level and individual-level treatment effects in cluster-randomized trials with survival outcomes, addressing right-censoring through flexible modeling approaches.", "motivation": "Cluster-randomized trials require distinguishing between cluster-level and individual-level treatment effects, particularly for survival outcomes with right-censoring, where existing methods may not adequately handle covariate-dependent censoring.", "method": "The authors develop doubly robust estimators that ensure consistency when either the censoring model or outcome model is correctly specified, using various modeling approaches for censoring and survival distributions, along with a deletion-based jackknife method for variance estimation.", "result": "Extensive simulations show the proposed methods perform well in finite samples, demonstrating adequate performance for estimating treatment effects in cluster-randomized trials with survival endpoints.", "conclusion": "The proposed doubly robust estimators provide reliable estimation of treatment effects in cluster-randomized trials with survival outcomes, effectively handling right-censoring and offering practical utility as demonstrated through application to a completed trial."}}
{"id": "2510.08465", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08465", "abs": "https://arxiv.org/abs/2510.08465", "authors": ["Chih-Yu Chang", "Ming-Chung Chang"], "title": "Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models", "comment": null, "summary": "Recent advances in supervised learning have driven growing interest in\nexplaining black-box models, particularly by estimating the effects of input\nvariables on model predictions. However, existing approaches often face key\nlimitations, including poor scalability, sensitivity to out-of-distribution\nsampling, and instability under correlated features. To address these issues,\nwe propose A2D2E, an $\\textbf{E}$stimator based on $\\textbf{A}$ccelerated\n$\\textbf{A}$ggregated $\\textbf{D}$-Optimal $\\textbf{D}$esigns. Our method\nleverages principled experimental design to improve efficiency and robustness\nin main effect estimation. We establish theoretical guarantees, including\nconvergence and variance reduction, and validate A2D2E through extensive\nsimulations. We further provide the potential of the proposed method with a\ncase study on real data and applications in language models. The code to\nreproduce the results can be found at https://github.com/cchihyu/A2D2E.", "AI": {"tldr": "A2D2E is an estimator using Accelerated Aggregated D-Optimal Designs to address limitations in explaining black-box models, improving efficiency and robustness in main effect estimation.", "motivation": "Existing methods for explaining black-box models suffer from poor scalability, sensitivity to out-of-distribution sampling, and instability under correlated features.", "method": "Leverages principled experimental design through Accelerated Aggregated D-Optimal Designs to improve efficiency and robustness.", "result": "Established theoretical guarantees including convergence and variance reduction, validated through extensive simulations and a real data case study.", "conclusion": "A2D2E shows potential for improving model explanation methods, with applications demonstrated in language models."}}
{"id": "2510.08535", "categories": ["stat.ML", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.08535", "abs": "https://arxiv.org/abs/2510.08535", "authors": ["Tassilo Schwarz", "Cai Dieball", "Constantin Kogler", "Kevin Lam", "Renaud Lambiotte", "Arnaud Doucet", "Alja\u017e Godec", "George Deligiannidis"], "title": "Permutation-Invariant Spectral Learning via Dyson Diffusion", "comment": null, "summary": "Diffusion models are central to generative modeling and have been adapted to\ngraphs by diffusing adjacency matrix representations. The challenge of having\nup to $n!$ such representations for graphs with $n$ nodes is only partially\nmitigated by using permutation-equivariant learning architectures. Despite\ntheir computational efficiency, existing graph diffusion models struggle to\ndistinguish certain graph families, unless graph data are augmented with ad hoc\nfeatures. This shortcoming stems from enforcing the inductive bias within the\nlearning architecture. In this work, we leverage random matrix theory to\nanalytically extract the spectral properties of the diffusion process, allowing\nus to push the inductive bias from the architecture into the dynamics. Building\non this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian\nMotion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the\nadjacency matrix while retaining all non-spectral information. We demonstrate\nthat the Dyson Diffusion Model learns graph spectra accurately and outperforms\nexisting graph diffusion models.", "AI": {"tldr": "The paper introduces Dyson Diffusion Model, a novel graph diffusion approach that uses Dyson's Brownian Motion to capture spectral dynamics, overcoming limitations of existing methods that struggle with graph family discrimination and require ad hoc feature augmentation.", "motivation": "Existing graph diffusion models face challenges in distinguishing certain graph families and often require additional feature augmentation, primarily due to enforcing inductive bias within the learning architecture rather than the diffusion dynamics.", "method": "Leverages random matrix theory to analytically extract spectral properties of diffusion process, pushing inductive bias from architecture to dynamics. Uses Dyson's Brownian Motion to capture spectral dynamics of Ornstein-Uhlenbeck process on adjacency matrix while preserving non-spectral information.", "result": "The Dyson Diffusion Model learns graph spectra accurately and outperforms existing graph diffusion models in performance.", "conclusion": "By shifting the inductive bias from the learning architecture to the diffusion dynamics through spectral analysis, the proposed Dyson Diffusion Model provides a more effective approach for graph generation that better captures graph structural properties."}}
