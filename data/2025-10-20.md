<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 9]
- [cs.LG](#cs.LG) [Total: 84]
- [stat.ML](#stat.ML) [Total: 19]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Asymptotic distribution of the global clustering coefficient in a random annulus graph](https://arxiv.org/abs/2510.15003)
*Mingao Yuan*

Main category: stat.ME

TL;DR: The paper proves that the standardized global clustering coefficient in random annulus graphs converges to a standard normal distribution using degenerate U-statistics theory.


<details>
  <summary>Details</summary>
Motivation: To analyze the asymptotic distribution of global clustering coefficient in random annulus graphs, which are used for modeling network communities, and to develop new methods for deriving asymptotic distributions of network statistics.

Method: Used the asymptotic theory of degenerate U-statistics with a sample-size dependent kernel, which differs from established approaches for deriving asymptotic distributions of network statistics.

Result: Demonstrated that the standardized global clustering coefficient converges in law to the standard normal distribution, and obtained the explicit expression of the limit of the global clustering coefficient.

Conclusion: The global clustering coefficient in random annulus graphs follows a normal distribution asymptotically, and the method using degenerate U-statistics provides a novel approach for analyzing network statistics.

Abstract: The global clustering coefficient is an effective measure for analyzing and
comparing the structures of complex networks. The random annulus graph is a
modified version of the well-known Erd\H{o}s-R\'{e}nyi random graph. It has
been recently proposed in modeling network communities. This paper investigates
the asymptotic distribution of the global clustering coefficient in a random
annulus graph. It is demonstrated that the standardized global clustering
coefficient converges in law to the standard normal distribution. The result is
established using the asymptotic theory of degenerate U-statistics with a
sample-size dependent kernel. As far as we know, this method is different from
established approaches for deriving asymptotic distributions of network
statistics. Moreover, we get the explicit expression of the limit of the global
clustering coefficient.

</details>


### [2] [Estimand framework and intercurrent events handling for clinical trials with time-to-event outcomes](https://arxiv.org/abs/2510.15000)
*Yixin Fang,Man Jin*

Main category: stat.ME

TL;DR: This paper extends the ICH E9(R1) estimand framework to time-to-event outcomes, proposing six strategies for handling intercurrent events including a new competing-risk strategy, with innovations in potential outcomes definition, time-dependent covariate utilization, and efficient estimators.


<details>
  <summary>Details</summary>
Motivation: The ICH E9(R1) guideline provides comprehensive estimand framework for clinical trials but lacks discussion for time-to-event outcomes, creating a gap that this paper aims to address.

Method: The authors define estimands using potential outcomes framework, propose six strategies for handling intercurrent events (including five from ICH E9(R1) plus a new competing-risk strategy), and discuss methods that can utilize time-dependent covariates straightforwardly.

Result: The paper provides a comprehensive framework for defining estimands and handling intercurrent events specifically for time-to-event outcomes in clinical trials, extending beyond the ICH E9(R1) guideline.

Conclusion: This work fills the gap in ICH E9(R1) for time-to-event outcomes by providing novel approaches for estimand definition and intercurrent event handling, with practical applications for clinical trial design and analysis.

Abstract: The ICH E9(R1) guideline presents a framework of estimand for clinical
trials, proposes five strategies for handling intercurrent events (ICEs), and
provides a comprehensive discussion and many real-life clinical examples for
quantitative outcomes and categorical outcomes. However, in ICH E9(R1) the
discussion is lacking for time-to-event (TTE) outcomes. In this paper, we
discuss how to define estimands and how to handle ICEs for clinical trials with
TTE outcomes. Specifically, we discuss six ICE handling strategies, including
those five strategies proposed by ICH E9(R1) and a new strategy, the
competing-risk strategy. Compared with ICH E9(R1), the novelty of this paper is
three-fold: (1) the estimands are defined in terms of potential outcomes, (2)
the methods can utilize time-dependent covariates straightforwardly, and (3)
the efficient estimators are discussed accordingly.

</details>


### [3] [Conditional GLMMs for reaction times in choice tasks](https://arxiv.org/abs/2510.15203)
*Mauricio Tejo,Cristian Meza,Fernando Marmolejo-Ramos*

Main category: stat.ME

TL;DR: This paper connects diffusion models and GLMMs for reaction time analysis by showing how Inverse Gaussian and Gamma distributions from diffusion models can be used in GLMMs to infer cognitive processes.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between two established methods for modeling reaction times: diffusion models that describe cognitive processes and Generalized Linear Mixed Models (GLMMs) that are commonly used in statistical analysis.

Method: Analyzing RT distributions conditioned on response alternatives, leveraging that certain diffusion model variants yield Inverse Gaussian and Gamma distributions for first-hitting times, and incorporating these distributions into GLMMs.

Result: The approach successfully connects diffusion models and GLMMs, demonstrated through simulations and application to real-world data, allowing inference of underlying cognitive processes from GLMMs.

Conclusion: The proposed method provides a unified framework for reaction time analysis that combines the cognitive interpretability of diffusion models with the statistical flexibility of GLMMs, with potential for further extensions.

Abstract: This study connects two methods for modeling reaction times (RTs) in choice
tasks: (1) the first-hitting time of a simple diffusion model with a single
barrier, representing the cognitive process leading to a response, and (2)
Generalized Linear Mixed Models (GLMMs). We achieve this by analyzing RT
distributions conditioned on each response alternative. Because certain
diffusion model variants yield Inverse Gaussian (IG) and Gamma distributions
for first-hitting times, we can justify using these distributions in RT models.
Conversely, employing IG and Gamma distributions within GLMMs allows us to
infer the underlying cognitive processes. We demonstrate this concept through
simulations and apply it to previously published real-world data. Finally, we
discuss the scope and potential extensions of our approach.

</details>


### [4] [Bayesian Sequential Modeling of Time-to-Urination for Dynamic ED Triage](https://arxiv.org/abs/2510.15272)
*Atsushi Senda,Yuki Takatsu,Ryokan Ikebe,Hiroshi Suginaka,Koji Morishita,Akira Endo*

Main category: stat.ME

TL;DR: A Bayesian framework that dynamically updates risk predictions using real-time behavioral cues like time to first urination (TTU) to improve emergency triage accuracy.


<details>
  <summary>Details</summary>
Motivation: Current triage tools are static and miss valuable behavioral cues that clinicians observe in real-time, limiting their effectiveness in emergency care settings.

Method: Developed a Bayesian sequentially updating framework that integrates incoming cues, tested using a prospective single-center cohort of ambulance arrivals in Japan (n=2,221) with time to first urination as a proof-of-concept cue.

Result: Excellent population-level fit to admission curve (integrated squared error 0.002), improved patient-level performance with age/sex adjustment (AUC 0.70 vs 0.50 unadjusted), better calibration, and favorable net benefit at common thresholds.

Conclusion: The framework effectively integrates real-time behavioral cues to enhance triage predictions, is extensible to multimodal inputs, and designed to complement rather than replace existing triage systems.

Abstract: Triage tools in routine emergency care are largely static, failing to exploit
simple behavioral cues clinicians notice in real time. Here, we developed a
Bayesian, sequentially updating framework that integrates incoming cues to
produce calibrated, time-consistent risk. Using a prospective single-center
cohort of ambulance arrivals in Japan (February-August 2025; n=2,221), we
evaluated time to first urination (TTU) as a proof-of-concept bedside cue for
predicting hospital admission. Population-level fit to the cumulative admission
curve was excellent (integrated squared error 0.002; RMSE 0.003;
Kolmogorov-Smirnov 0.008; coverage 0.98). At the patient level, performance
improved markedly with age/sex adjustment (AUC[t] 0.70 vs. 0.50 unadjusted),
with lower Brier scores and positive calibration slopes. Platt recalibration
refined probability scaling without altering discrimination, and decision-curve
analysis showed small, favorable net benefit at common thresholds. This
framework is readily extensible to multimodal inputs and external validation
and is designed to complement, not replace, existing triage systems.

</details>


### [5] [Nonparametric Testing of Spatial Dependence in 2D and 3D Random Fields](https://arxiv.org/abs/2510.15381)
*Christian H. Weiß,Philipp Adämmer*

Main category: stat.ME

TL;DR: A nonparametric framework for testing spatial dependence in 2D/3D random fields using Hilbert curves to convert spatial data into time series, then applying ordinal pattern tests for serial dependence.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and robust alternative to existing spatial dependence testing methods that are typically limited to two-dimensional settings.

Method: Convert spatial data into 1D time series using space-filling Hilbert curves, then apply ordinal pattern-based tests for serial dependence to the transformed sequence.

Result: The approach preserves spatial locality through Hilbert curves, making spatial dependence manifest as serial dependence in the transformed sequence.

Conclusion: This method provides a practical and general framework that accommodates arbitrary grid sizes, extends beyond three dimensions, and is easy to implement.

Abstract: We propose a flexible and robust nonparametric framework for testing spatial
dependence in two- and three-dimensional random fields. Our approach involves
converting spatial data into one-dimensional time series using space-filling
Hilbert curves. We then apply ordinal pattern-based tests for serial dependence
to this series. Because Hilbert curves preserve spatial locality, spatial
dependence in the original field manifests as serial dependence in the
transformed sequence. The approach is easy to implement, accommodates arbitrary
grid sizes through generalized Hilbert (``gilbert'') curves, and naturally
extends beyond three dimensions. This provides a practical and general
alternative to existing methods based on spatial ordinal patterns, which are
typically limited to two-dimensional settings.

</details>


### [6] [Adaptive Influence Diagnostics in High-Dimensional Regression](https://arxiv.org/abs/2510.15618)
*Abdul-Nasah Soale,Adewale Lukman*

Main category: stat.ME

TL;DR: Proposes an adaptive Cook's distance (ACD) method for detecting influential observations in high-dimensional single-index models, addressing multicollinearity and outlier contamination through sparse local linear gradients.


<details>
  <summary>Details</summary>
Motivation: To develop a model-free diagnostic tool that can effectively identify influential observations in high-dimensional settings with multicollinearity and outliers, overcoming limitations of classical methods like Cook's distance and local influence.

Method: ACD uses sparse local linear gradients to mitigate leverage effects, with implementations based on LASSO (ACD-LASSO) and SCAD (ACD-SCAD) penalties. The method is tested in simulations with varying dimensional designs and strong correlation.

Result: ACD-LASSO and ACD-SCAD significantly reduced masking and swamping effects compared to classical Cook's distance, local influence, DF-Model, and Case-Weight adjusted LASSO. Trimming points identified by ACD stabilized variable selection while maintaining core signals.

Conclusion: The proposed ACD method provides consistent improvements in selection stability and interpretability, as demonstrated in applications to US cities pollution data and riboflavin genomics experiments, making it a robust diagnostic tool for high-dimensional models with multicollinearity and outliers.

Abstract: An adaptive Cook's distance (ACD) for diagnosing influential observations in
high-dimensional single-index models with multicollinearity and outlier
contamination is proposed. ACD is a model-free technique built on sparse local
linear gradients to temper leverage effects. In simulations spanning low- and
high-dimensional design settings with strong correlation, ACD based on LASSO
(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative
to classical Cook's distance and local influence as well as the DF-Model and
Case-Weight adjusted solution for LASSO. Trimming points flagged by ACD
stabilizes variable selection while preserving core signals. Applications to
two datasets--the 1960 US cities pollution study and a high-dimensional
riboflavin genomics experiment show consistent gains in selection stability and
interpretability.

</details>


### [7] [Robust Estimation of Polyserial Correlation](https://arxiv.org/abs/2510.15632)
*Max Welz*

Main category: stat.ME

TL;DR: The paper proposes a robust estimator for polyserial correlation models to handle outliers and model misspecification, maintaining high efficiency while providing outlier detection capabilities.


<details>
  <summary>Details</summary>
Motivation: Standard maximum likelihood estimation for polyserial correlation is highly sensitive to outliers and model misspecification, where even one problematic observation can severely degrade estimates.

Method: A novel robust estimator that implicitly downweights observations discrepant from the partially-latent normality assumption, generalizing maximum likelihood while providing outlier detection through weight analysis.

Result: The estimator is consistent, asymptotically Gaussian, and achieves substantial robustness while maintaining over 98% of ML efficiency. Simulation and empirical applications demonstrate effective outlier identification.

Conclusion: The proposed robust estimator provides a practical solution for polyserial correlation modeling in the presence of outliers, with implementation available in open-source software.

Abstract: The association between a continuous and an ordinal variable is commonly
modeled through the polyserial correlation model. However, this model, which is
based on a partially-latent normality assumption, may be misspecified in
practice, due to, for example (but not limited to), outliers or careless
responses. We demonstrate that the typically used maximum likelihood (ML)
estimator is highly susceptible to such misspecification: One single
observation not generated by partially-latent normality can suffice to produce
arbitrarily poor estimates. As a remedy, we propose a novel estimator of the
polyserial correlation model designed to be robust against the adverse effects
of observations discrepant to that model. The estimator achieves robustness by
implicitly downweighting such observations; the ensuing weights constitute a
useful tool for pinpointing potential sources of model misspecification. We
show that the proposed estimator generalizes ML and is consistent as well as
asymptotically Gaussian. As price for robustness, some efficiency must be
sacrificed, but substantial robustness can be gained while maintaining more
than 98% of ML efficiency. We demonstrate our estimator's robustness and
practical usefulness in simulation experiments and an empirical application in
personality psychology where our estimator helps identify outliers. Finally,
the proposed methodology is implemented in free open-source software.

</details>


### [8] [Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss](https://arxiv.org/abs/2510.15664)
*Lucas Amoudruz,Sergey Litvinov,Costas Papadimitriou,Petros Koumoutsakos*

Main category: stat.ME

TL;DR: B-ODIL is a Bayesian extension of the ODIL method that integrates PDE loss as prior knowledge with data likelihood for solving inverse problems with quantified uncertainties.


<details>
  <summary>Details</summary>
Motivation: Inverse problems require additional knowledge when measurements are incomplete or indirect. PDE-based models can close this gap, and extending ODIL to Bayesian framework enables uncertainty quantification.

Method: B-ODIL combines PDE loss from ODIL as prior knowledge with data likelihood in a Bayesian formulation, using variational inference to solve PDE-based inverse problems.

Result: The method successfully demonstrated capabilities in synthetic benchmarks across 1D, 2D, and 3D PDEs, and applied to estimate tumor concentration and uncertainty in brain MRI using 3D tumor growth model.

Conclusion: B-ODIL provides an effective Bayesian framework for PDE-based inverse problems that quantifies uncertainties while maintaining the computational advantages of ODIL.

Abstract: Inverse problems are crucial for many applications in science, engineering
and medicine that involve data assimilation, design, and imaging. Their
solution infers the parameters or latent states of a complex system from noisy
data and partially observable processes. When measurements are an incomplete or
indirect view of the system, additional knowledge is required to accurately
solve the inverse problem. Adopting a physical model of the system in the form
of partial differential equations (PDEs) is a potent method to close this gap.
In particular, the method of optimizing a discrete loss (ODIL) has shown great
potential in terms of robustness and computational cost. In this work, we
introduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of
ODIL as prior knowledge and combines it with a likelihood describing the data.
B-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer
solutions with quantified uncertainties. We demonstrate the capabilities of
B-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and
three dimensions. We showcase the application of B-ODIL in estimating tumor
concentration and its uncertainty in a patient's brain from MRI scans using a
three-dimensional tumor growth model.

</details>


### [9] [A Multiclass ROC Curve](https://arxiv.org/abs/2510.15670)
*Paolo Giudici,Rosa C. Rosciano,Johanna Schrader,Delf-Magnus Kummerfeld*

Main category: stat.ME

TL;DR: A novel method for constructing multiclass ROC curves using the multidimensional Gini index, extending the Gini-ROC relationship to multiclass settings with validation in healthcare and finance applications.


<details>
  <summary>Details</summary>
Motivation: To address the need for theoretically grounded multiclass performance evaluation methods, particularly for imbalanced datasets where assessment should prioritize prudence over class frequency.

Method: Leverages the relationship between Gini coefficient and ROC curve, extending it to multiclass settings through multidimensional Gini index. Validated with two comprehensive case studies in healthcare and finance.

Result: Successfully developed a framework for multiclass ROC curve construction using multidimensional Gini index, providing a theoretically sound approach for performance evaluation in multiclass classification problems.

Conclusion: The proposed methodology offers a valuable solution for multiclass performance evaluation, especially beneficial for imbalanced datasets where careful assessment is more important than considering class frequencies.

Abstract: This paper introduces a novel methodology for constructing multiclass ROC
curves using the multidimensional Gini index. The proposed methodology
leverages the established relationship between the Gini coefficient and the ROC
Curve and extends it to multiclass settings through the multidimensional Gini
index. The framework is validated by means of two comprehensive case studies in
health care and finance. The paper provides a theoretically grounded solution
to multiclass performance evaluation, particularly valuable for imbalanced
datasets, for which a prudential assessment should take precedence over class
frequency considerations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators](https://arxiv.org/abs/2510.14983)
*Oskar Triebe,Fletcher Passow,Simon Wittner,Leonie Wagner,Julio Arend,Tao Sun,Chad Zanocco,Marek Miltner,Arezou Ghesmati,Chen-Hao Tsai,Christoph Bergmeir,Ram Rajagopal*

Main category: cs.LG

TL;DR: A multi-level forecasting system for power grid operators that improves nodal load forecast accuracy and interpretability while maintaining scalability.


<details>
  <summary>Details</summary>
Motivation: Sustainable energy developments increase electric load uncertainty, requiring higher spatial resolution forecasts from zonal aggregates to individual nodes, but nodal loads are harder to forecast accurately and manage in daily operations.

Method: Developed an interpretable and scalable forecasting model using extensive zonal and nodal net load data, with fully parallelized single-model workflow and solutions addressing nodal load heterogeneity and volatility.

Result: Showed accuracy and interpretability improvements for zonal forecasts, substantial improvements for nodal forecasts, and enabled operators to adjust forecasts with unprecedented confidence and diagnose errors precisely.

Conclusion: The multi-level forecasting system successfully addresses the challenges of nodal load forecasting, allowing transmission system operators to extend operations from zonal to nodal level with improved manageability and reliability.

Abstract: The reliability of local power grid infrastructure is challenged by
sustainable energy developments increasing electric load uncertainty.
Transmission System Operators (TSOs) need load forecasts of higher spatial
resolution, extending current forecasting operations from zonal aggregates to
individual nodes. However, nodal loads are less accurate to forecast and
require a large number of individual forecasts, which are hard to manage for
the human experts assessing risks in the control room's daily operations
(operator). In collaboration with a TSO, we design a multi-level system that
meets the needs of operators for hourly day-ahead load forecasting. Utilizing a
uniquely extensive dataset of zonal and nodal net loads, we experimentally
evaluate our system components. First, we develop an interpretable and scalable
forecasting model that allows for TSOs to gradually extend zonal operations to
include nodal forecasts. Second, we evaluate solutions to address the
heterogeneity and volatility of nodal load, subject to a trade-off. Third, our
system is manageable with a fully parallelized single-model forecasting
workflow. Our results show accuracy and interpretability improvements for zonal
forecasts, and substantial improvements for nodal forecasts. In practice, our
multi-level forecasting system allows operators to adjust forecasts with
unprecedented confidence and accuracy, and to diagnose otherwise opaque errors
precisely.

</details>


### [11] [TangledFeatures: Robust Feature Selection in Highly Correlated Spaces](https://arxiv.org/abs/2510.15005)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: TangledFeatures is a feature selection framework that identifies representative features from correlated predictor groups, reducing redundancy while maintaining explanatory power for more interpretable and stable analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional feature selection methods focus on predictive accuracy but degrade with correlated predictors, creating a need for approaches that handle feature entanglement while maintaining interpretability.

Method: The framework identifies representative features from groups of entangled predictors, selecting features that reduce redundancy while retaining explanatory power for downstream modeling.

Result: Applied to Alanine Dipeptide, TangledFeatures successfully identified structurally meaningful intra-atomic distances that explain variation in backbone torsional angles.

Conclusion: TangledFeatures provides a more interpretable and stable feature selection approach for correlated feature spaces compared to traditional techniques, with demonstrated effectiveness in molecular analysis applications.

Abstract: Feature selection is a fundamental step in model development, shaping both
predictive performance and interpretability. Yet, most widely used methods
focus on predictive accuracy, and their performance degrades in the presence of
correlated predictors. To address this gap, we introduce TangledFeatures, a
framework for feature selection in correlated feature spaces. It identifies
representative features from groups of entangled predictors, reducing
redundancy while retaining explanatory power. The resulting feature subset can
be directly applied in downstream models, offering a more interpretable and
stable basis for analysis compared to traditional selection techniques. We
demonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying
it to the prediction of backbone torsional angles and show that the selected
features correspond to structurally meaningful intra-atomic distances that
explain variation in these angles.

</details>


### [12] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: This paper presents ES-C51, a modified version of the C51 distributional RL algorithm that replaces greedy Q-learning updates with Expected Sarsa updates using softmax, improving stability and performance when actions have similar expected rewards.


<details>
  <summary>Details</summary>
Motivation: Standard C51 uses greedy Q-learning updates that can cause instability when multiple actions have similar expected rewards but different distributions, preventing stable distribution learning.

Method: ES-C51 replaces the greedy Bellman update in C51 with an Expected Sarsa update that uses softmax to combine information from all possible actions rather than relying on a single best action.

Result: ES-C51 outperforms the modified QL-C51 (which uses softmax exploration instead of e-greedy) across many classic control environments from Gym and Atari-10 games.

Conclusion: Using Expected Sarsa updates in distributional RL improves stability and performance compared to greedy Q-learning approaches when actions have similar expected rewards.

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [13] [Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines](https://arxiv.org/abs/2510.15010)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Balamurugan Balusamy,Wathiq Mansoor*

Main category: cs.LG

TL;DR: Novel ensemble deep learning framework for unsupervised anomaly detection in wind turbines using VAE, LSTM Autoencoders, and Transformers on SCADA data, achieving 0.947 AUC-ROC and 48-hour early fault detection.


<details>
  <summary>Details</summary>
Motivation: Wind turbine reliability is critical for renewable energy sector growth, where early fault detection reduces downtime and maintenance costs significantly.

Method: Ensemble framework integrating VAE, LSTM Autoencoders, and Transformers with feature engineering pipeline extracting temporal, statistical, and frequency-domain indicators from SCADA data, using ensemble scoring and adaptive thresholding.

Result: Achieved 0.947 AUC-ROC on CARE dataset with 89 years of real-world turbine data, detecting faults up to 48 hours before failure.

Conclusion: The approach enables predictive maintenance, reduces turbine failures, and enhances operational efficiency in large-scale wind energy deployments.

Abstract: Wind turbine reliability is critical to the growing renewable energy sector,
where early fault detection significantly reduces downtime and maintenance
costs. This paper introduces a novel ensemble-based deep learning framework for
unsupervised anomaly detection in wind turbines. The method integrates
Variational Autoencoders (VAE), LSTM Autoencoders, and Transformer
architectures, each capturing different temporal and contextual patterns from
high-dimensional SCADA data. A unique feature engineering pipeline extracts
temporal, statistical, and frequency-domain indicators, which are then
processed by the deep models. Ensemble scoring combines model predictions,
followed by adaptive thresholding to detect operational anomalies without
requiring labeled fault data. Evaluated on the CARE dataset containing 89 years
of real-world turbine data across three wind farms, the proposed method
achieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to
failure. This approach offers significant societal value by enabling predictive
maintenance, reducing turbine failures, and enhancing operational efficiency in
large-scale wind energy deployments.

</details>


### [14] [AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.15038)
*Lingkai Kong,Molei Tao,Yang Liu,Bryan Wang,Jinmiao Fu,Chien-Chih Wang,Huidong Liu*

Main category: cs.LG

TL;DR: AlignFlow introduces Semi-Discrete Optimal Transport (SDOT) to improve Flow-based Generative Models by creating optimal noise-data pairings through Laguerre cell partitioning, enabling better scalability and performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing OT-based methods in FGMs use mini-batch sampling for optimal transport estimation, which limits scalability to large and high-dimensional datasets. There's a need for more efficient and scalable approaches.

Method: Leverages Semi-Discrete Optimal Transport (SDOT) to establish explicit optimal alignment between noise distribution and data points. SDOT partitions noise space into Laguerre cells, each mapped to a data point, creating optimal pairings during training.

Result: AlignFlow scales well to large datasets and model architectures with negligible computational overhead. It improves performance of various state-of-the-art FGM algorithms and can be integrated as a plug-and-play component.

Conclusion: AlignFlow provides an effective and scalable solution for enhancing FGM training through SDOT-based noise-data alignment, offering guaranteed convergence and improved performance across different model architectures.

Abstract: Flow-based Generative Models (FGMs) effectively transform noise into complex
data distributions. Incorporating Optimal Transport (OT) to couple noise and
data during FGM training has been shown to improve the straightness of flow
trajectories, enabling more effective inference. However, existing OT-based
methods estimate the OT plan using (mini-)batches of sampled noise and data
points, which limits their scalability to large and high-dimensional datasets
in FGMs. This paper introduces AlignFlow, a novel approach that leverages
Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by
establishing an explicit, optimal alignment between noise distribution and data
points with guaranteed convergence. SDOT computes a transport map by
partitioning the noise space into Laguerre cells, each mapped to a
corresponding data point. During FGM training, i.i.d. noise samples are paired
with data points via the SDOT map. AlignFlow scales well to large datasets and
model architectures with negligible computational overhead. Experimental
results show that AlignFlow improves the performance of a wide range of
state-of-the-art FGM algorithms and can be integrated as a plug-and-play
component. Code is available at: https://github.com/konglk1203/AlignFlow.

</details>


### [15] [IQNN-CS: Interpretable Quantum Neural Network for Credit Scoring](https://arxiv.org/abs/2510.15044)
*Abdul Samad Khan,Nouhaila Innan,Aeysha Khalique,Muhammad Shafique*

Main category: cs.LG

TL;DR: IQNN-CS is an interpretable quantum neural network framework for multiclass credit risk classification that combines variational QNN with post-hoc explanation techniques and introduces Inter-Class Attribution Alignment (ICAA) metric to enhance transparency in financial decision-making.


<details>
  <summary>Details</summary>
Motivation: Credit scoring requires transparency and trust due to regulatory scrutiny and impact on individuals' access to credit, but current Quantum Machine Learning approaches are black-box models that lack interpretability needed for financial services.

Method: Developed IQNN-CS framework combining variational quantum neural network with post-hoc explanation techniques for structured data, and introduced Inter-Class Attribution Alignment (ICAA) metric to quantify attribution divergence across predicted classes.

Result: IQNN-CS demonstrated stable training dynamics, competitive predictive performance, and enhanced interpretability when evaluated on two real-world credit datasets.

Conclusion: The framework provides a practical path toward transparent and accountable QML models for financial decision-making, addressing the interpretability gap in quantum machine learning for high-stakes applications.

Abstract: Credit scoring is a high-stakes task in financial services, where model
decisions directly impact individuals' access to credit and are subject to
strict regulatory scrutiny. While Quantum Machine Learning (QML) offers new
computational capabilities, its black-box nature poses challenges for adoption
in domains that demand transparency and trust. In this work, we present
IQNN-CS, an interpretable quantum neural network framework designed for
multiclass credit risk classification. The architecture combines a variational
QNN with a suite of post-hoc explanation techniques tailored for structured
data. To address the lack of structured interpretability in QML, we introduce
Inter-Class Attribution Alignment (ICAA), a novel metric that quantifies
attribution divergence across predicted classes, revealing how the model
distinguishes between credit risk categories. Evaluated on two real-world
credit datasets, IQNN-CS demonstrates stable training dynamics, competitive
predictive performance, and enhanced interpretability. Our results highlight a
practical path toward transparent and accountable QML models for financial
decision-making.

</details>


### [16] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: This paper proposes entropy-regularized trust-decay for sequential decision-making under distribution drift, using stress-aware exponential tilting in belief updates and mirror-descent decisions. It establishes robustness measures, proves sensitivity bounds, and achieves O(√T) dynamic regret under KL-drift path length.


<details>
  <summary>Details</summary>
Motivation: To address sequential decision-making problems where the underlying data distribution changes over time (distribution drift), requiring adaptive methods that can handle uncertainty and maintain performance under changing conditions.

Method: Proposes entropy-regularized trust-decay framework with stress-aware exponential tilting applied to both belief updates and mirror-descent decisions. Uses Fenchel-dual equivalence on simplex, formalizes robustness via fragility measures, and develops parameter-free hedge adaptation.

Result: Achieves dynamic-regret guarantees of Õ(√T) under KL-drift path length, O(1) per-switch regret, high-probability sensitivity bounds, and extensions to various settings including bandit feedback, distributed optimization, and KL-drift estimation.

Conclusion: The framework unifies dynamic-regret analysis, distributionally robust objectives, and KL-regularized control within a single stress-adaptive update, providing a comprehensive approach to handling distribution drift in sequential decision-making.

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [17] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: SPA framework improves LLM agent performance in out-of-distribution scenarios by incorporating an internal world model through self-play supervised finetuning before reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle in out-of-distribution scenarios due to difficulty grounding internal knowledge in complex environmental dynamics, leading to brittle exploration and limited generalization.

Method: SPA framework decomposes world model into state representation and transition modeling, uses self-play supervised finetuning to learn the world model, then applies it to simulate future states before policy optimization.

Result: Significant performance improvements across diverse environments: Sokoban success rate increased from 25.6% to 59.8%, FrozenLake score from 22.1% to 70.9% for Qwen2.5-1.5B-Instruct model.

Conclusion: Equipping LLM agents with an internal world model through SPA framework better aligns reasoning with environmental dynamics and substantially improves decision-making in out-of-distribution scenarios.

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [18] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: The paper introduces MCTVMDP, a framework where agents can actively modify their environment's dynamics through model-changing actions, rather than just adapting to a fixed environment.


<details>
  <summary>Details</summary>
Motivation: Traditional RL assumes passive adaptation to fixed environments, but agents could potentially increase rewards by actively reconfiguring the environment dynamics themselves.

Method: Proposes multi-layer configurable time-varying Markov decision process (MCTVMDP) with upper-level model-changing actions that configure the non-stationary transition function of the lower-level MDP.

Result: A framework that enables joint optimization of both configuration policies (upper-level) and primitive action policies (lower-level) to maximize long-term reward.

Conclusion: Active environment modification through model-changing actions provides a new dimension for RL agents to improve performance beyond traditional passive adaptation approaches.

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [19] [Small Ensemble-based Data Assimilation: A Machine Learning-Enhanced Data Assimilation Method with Limited Ensemble Size](https://arxiv.org/abs/2510.15284)
*Zhilin Li,Yao Zhou,Xianglong Li,Zeng Liu,Zhaokuan Lu,Shanlin Xu,Seungnam Kim,Guangyao Wang*

Main category: cs.LG

TL;DR: A novel machine learning-based data assimilation method combining ensemble Kalman filter with fully connected neural network to improve accuracy without significant computational cost increase.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between analysis accuracy and computational efficiency in ensemble-based data assimilation methods, where larger ensemble sizes needed for accuracy lead to higher computational costs.

Method: Combine traditional ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN), using small ensemble size for preliminary analysis via EnKF, then FCNN learns and predicts correction terms to compensate for limited ensemble size effects.

Result: Numerical experiments with Lorenz systems and nonlinear ocean wave field simulations show EnKF-FCNN achieves higher accuracy than traditional EnKF with same ensemble size, with negligible additional computational cost.

Conclusion: The EnKF-FCNN method effectively improves data assimilation accuracy while maintaining computational efficiency, and is adaptable to diverse applications through coupling with different models and alternative ensemble-based DA methods.

Abstract: Ensemble-based data assimilation (DA) methods have become increasingly
popular due to their inherent ability to address nonlinear dynamic problems.
However, these methods often face a trade-off between analysis accuracy and
computational efficiency, as larger ensemble sizes required for higher accuracy
also lead to greater computational cost. In this study, we propose a novel
machine learning-based data assimilation approach that combines the traditional
ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN).
Specifically, our method uses a relatively small ensemble size to generate
preliminary yet suboptimal analysis states via EnKF. A FCNN is then employed to
learn and predict correction terms for these states, thereby mitigating the
performance degradation induced by the limited ensemble size. We evaluate the
performance of our proposed EnKF-FCNN method through numerical experiments
involving Lorenz systems and nonlinear ocean wave field simulations. The
results consistently demonstrate that the new method achieves higher accuracy
than traditional EnKF with the same ensemble size, while incurring negligible
additional computational cost. Moreover, the EnKF-FCNN method is adaptable to
diverse applications through coupling with different models and the use of
alternative ensemble-based DA methods.

</details>


### [20] [Physics-informed data-driven machine health monitoring for two-photon lithography](https://arxiv.org/abs/2510.15075)
*Sixian Jia,Zhiqiao Dong,Chenhui Shao*

Main category: cs.LG

TL;DR: This paper presents three methods for monitoring two-photon lithography (TPL) machine health using physics-informed data-driven predictive models combined with statistical approaches to enable timely maintenance and prevent unnecessary downtime.


<details>
  <summary>Details</summary>
Motivation: Current TPL maintenance practices rely on experience rather than informed monitoring, leading to either untimely maintenance causing machine downtime and poor fabrication quality, or unnecessary maintenance resulting in inefficiencies and avoidable downtime.

Method: Three methods integrating physics-informed data-driven predictive models for structure dimensions with statistical approaches, designed to handle increasingly complex scenarios with different levels of generalizability.

Result: The approaches achieved high accuracies across all test scenarios using a comprehensive experimental dataset with six process parameter combinations and six structure dimensions under two machine health conditions, demonstrating excellent effectiveness, robustness, and generalizability.

Conclusion: The results represent a significant step toward condition-based maintenance for TPL systems, enabling more informed and timely maintenance decisions.

Abstract: Two-photon lithography (TPL) is a sophisticated additive manufacturing
technology for creating three-dimensional (3D) micro- and nano-structures.
Maintaining the health of TPL systems is critical for ensuring consistent
fabrication quality. Current maintenance practices often rely on experience
rather than informed monitoring of machine health, resulting in either untimely
maintenance that causes machine downtime and poor-quality fabrication, or
unnecessary maintenance that leads to inefficiencies and avoidable downtime. To
address this gap, this paper presents three methods for accurate and timely
monitoring of TPL machine health. Through integrating physics-informed
data-driven predictive models for structure dimensions with statistical
approaches, the proposed methods are able to handle increasingly complex
scenarios featuring different levels of generalizability. A comprehensive
experimental dataset that encompasses six process parameter combinations and
six structure dimensions under two machine health conditions was collected to
evaluate the effectiveness of the proposed approaches. Across all test
scenarios, the approaches are shown to achieve high accuracies, demonstrating
excellent effectiveness, robustness, and generalizability. These results
represent a significant step toward condition-based maintenance for TPL
systems.

</details>


### [21] [Online Correlation Clustering: Simultaneously Optimizing All $\ell_p$-norms](https://arxiv.org/abs/2510.15076)
*Sami Davies,Benjamin Moseley,Heather Newman*

Main category: cs.LG

TL;DR: This paper presents the first online algorithm that simultaneously approximates all ℓp-norms for correlation clustering using a small sample, achieving O(log⁴ n) competitiveness for all norms, O(log n) for ℓ∞-norm, and O(1) for ℓ₁-norm.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the fundamental trade-off between minimizing total disagreements (ℓ₁-norm) and ensuring fairness to individual nodes (ℓ∞-norm) in correlation clustering. While offline algorithms can simultaneously approximate all ℓp-norms, it was unknown if this was possible in online settings. The authors also discovered a fundamental separation between these objectives in standard online models.

Method: The authors develop a single algorithm for the online-with-a-sample (AOS) model that uses a small constant fraction of the input as a sample. This algorithm produces one clustering that achieves competitive guarantees for all ℓp-norms simultaneously.

Result: The algorithm achieves: O(log⁴ n)-competitive for all ℓp-norms with high probability, O(log n)-competitive for ℓ∞-norm with high probability, and O(1)-competitive for ℓ₁-norm in expectation. The paper also proves a hardness result showing that in the standard random-order online model, any algorithm for ℓ∞-norm must have competitive ratio at least Ω(n¹/³).

Conclusion: This work successfully translates the offline "all-norms" guarantee to the online world using the AOS model, demonstrating that simultaneous approximation of all ℓp-norms is possible in online settings with a small sample. The results show that the AOS model is necessary to overcome the fundamental limitations of standard online models for fairness objectives.

Abstract: The $\ell_p$-norm objectives for correlation clustering present a fundamental
trade-off between minimizing total disagreements (the $\ell_1$-norm) and
ensuring fairness to individual nodes (the $\ell_\infty$-norm). Surprisingly,
in the offline setting it is possible to simultaneously approximate all
$\ell_p$-norms with a single clustering. Can this powerful guarantee be
achieved in an online setting? This paper provides the first affirmative
answer. We present a single algorithm for the online-with-a-sample (AOS) model
that, given a small constant fraction of the input as a sample, produces one
clustering that is simultaneously $O(\log^4 n)$-competitive for all
$\ell_p$-norms with high probability, $O(\log n)$-competitive for the
$\ell_\infty$-norm with high probability, and $O(1)$-competitive for the
$\ell_1$-norm in expectation. This work successfully translates the offline
"all-norms" guarantee to the online world.
  Our setting is motivated by a new hardness result that demonstrates a
fundamental separation between these objectives in the standard random-order
(RO) online model. Namely, while the $\ell_1$-norm is trivially
$O(1)$-approximable in the RO model, we prove that any algorithm in the RO
model for the fairness-promoting $\ell_\infty$-norm must have a competitive
ratio of at least $\Omega(n^{1/3})$. This highlights the necessity of a
different beyond-worst-case model. We complement our algorithm with lower
bounds, showing our competitive ratios for the $\ell_1$- and $\ell_\infty$-
norms are nearly tight in the AOS model.

</details>


### [22] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: TempO is a latent flow matching model for forecasting high-dimensional PDE-governed dynamics that uses sparse conditioning with channel folding and time-conditioned Fourier layers to efficiently capture multi-scale modes, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive and diffusion-based approaches for forecasting high-dimensional PDE dynamics suffer from cumulative errors and discretization artifacts that limit long, physically consistent forecasts. Flow matching offers a natural alternative for efficient deterministic sampling.

Method: TempO uses latent flow matching with sparse conditioning and channel folding to efficiently process 3D spatiotemporal fields. It employs time-conditioned Fourier layers to capture multi-scale modes with high fidelity, providing a parameter- and memory-light design compared to attention-based or convolutional regressors.

Result: TempO outperforms state-of-the-art baselines across three benchmark PDE datasets. Spectral analysis demonstrates superior recovery of multi-scale dynamics, and efficiency studies highlight its parameter- and memory-light design.

Conclusion: Flow matching with TempO's architecture provides an effective solution for high-dimensional PDE forecasting, addressing limitations of existing approaches while maintaining efficiency and physical consistency in long-term forecasts.

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [23] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: DLER is a reinforcement learning training method that uses simple truncation penalties to reduce reasoning model output length by over 70% while maintaining or improving accuracy, addressing optimization challenges like bias in advantage estimation, entropy collapse, and sparse rewards.


<details>
  <summary>Details</summary>
Motivation: Current reasoning language models generate unnecessarily long outputs, wasting computational resources and reducing efficiency. The goal is to maximize intelligence per token - achieving high accuracy relative to response length - which remains an unsolved problem.

Method: DLER combines batch-wise reward normalization, higher clipping, dynamic sampling, and simple truncation length penalty to address three key RL optimization challenges: large bias in advantage estimation, entropy collapse, and sparse reward signals. Also introduces Difficulty-Aware DLER for adaptive truncation and update-selective merging for preserving baseline accuracy.

Result: DLER achieves state-of-the-art accuracy-efficiency trade-offs, cutting output length by over 70% while surpassing all previous baseline accuracy. DLER-7B generates multiple concise responses with 28% higher accuracy and lower latency compared to DeepSeek-R1-7B. Difficulty-Aware DLER provides additional efficiency gains.

Conclusion: Simple truncation penalties can be highly effective when combined with proper RL optimization techniques. DLER demonstrates that sophisticated length penalties are not necessary - the key lies in addressing fundamental RL optimization challenges to achieve optimal accuracy-efficiency balance in reasoning models.

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [24] [Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework](https://arxiv.org/abs/2510.15127)
*David J. Albers,Tell D. Bennett,Jana de Wiljes,Bradford J. Smith,Peter D. Sottile,J. N. Stroh*

Main category: cs.LG

TL;DR: A framework using evolutionary game theory to analyze mechanical ventilation data from ICU patients, aiming to optimize and personalize ventilation strategies by understanding patient-ventilator-care system interactions.


<details>
  <summary>Details</summary>
Motivation: To understand the effects of mechanical ventilation strategies on patient outcomes by analyzing heterogeneous patient-ventilator systems in critical care environments, enabling hypothesis generation for improved respiratory management.

Method: Uses evolutionary game theory (EGT) to analyze breath behaviors in mechanical ventilation data, creating quantitative precursors for deeper probabilistic analysis. The approach is validated on synthetic data before applying to real ICU data from joint patient-ventilator-care systems (J6).

Result: Developed a scalable analytical method for complex patient-ventilator systems that can identify advantageous care variations. The EGT-based process successfully handles the complexities of real-world ICU data and provides foundations for state transition modeling.

Conclusion: The framework represents a significant step toward mechanical ventilation optimization and personalization, with potential for developing state transition models that simulate MV decision effects using empirical and game-theoretic approaches.

Abstract: Identifying the effects of mechanical ventilation strategies and protocols in
critical care requires analyzing data from heterogeneous patient-ventilator
systems within the context of the clinical decision-making environment. This
research develops a framework to help understand the consequences of mechanical
ventilation (MV) and adjunct care decisions on patient outcome from
observations of critical care patients receiving MV. Developing an
understanding of and improving critical care respiratory management requires
the analysis of existing secondary-use clinical data to generate hypotheses
about advantageous variations and adaptations of current care. This work
introduces a perspective of the joint patient-ventilator-care systems
(so-called J6) to develop a scalable method for analyzing data and trajectories
of these complex systems. To that end, breath behaviors are analyzed using
evolutionary game theory (EGT), which generates the necessary quantitative
precursors for deeper analysis through probabilistic and stochastic machinery
such as reinforcement learning. This result is one step along the pathway
toward MV optimization and personalization. The EGT-based process is
analytically validated on synthetic data to reveal potential caveats before
proceeding to real-world ICU data applications that expose complexities of the
data-generating process J6. The discussion includes potential developments
toward a state transition model for the simulating effects of MV decision using
empirical and game-theoretic elements.

</details>


### [25] [A Simple Method for PMF Estimation on Large Supports](https://arxiv.org/abs/2510.15132)
*Alex Shtoff*

Main category: cs.LG

TL;DR: A nonparametric method for estimating multi-modal, heavy-tailed PMFs using graph Laplacian filtering and eigenvector projection, with automated dimension selection.


<details>
  <summary>Details</summary>
Motivation: To estimate probability mass functions on large discrete supports when the PMF is multi-modal and heavy-tailed, preserving coarse structure while suppressing sampling noise.

Method: Treat empirical PMF as signal on line graph, apply data-dependent low-pass filter using path graph Laplacian perturbed with empirical PMF diagonal matrix, project onto smallest eigenvector subspace, then clip and re-normalize.

Result: Method preserves coarse structure while suppressing noise, compares favorably to logspline and Gaussian-KDE baselines in intended regimes, but has known failure modes like abrupt discontinuities.

Conclusion: The approach is short to implement, robust across sample sizes, and suitable for automated pipelines and exploratory analysis due to reliability and speed, with practical data-driven dimension selection.

Abstract: We study nonparametric estimation of a probability mass function (PMF) on a
large discrete support, where the PMF is multi-modal and heavy-tailed. The core
idea is to treat the empirical PMF as a signal on a line graph and apply a
data-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal
operator, the path graph Laplacian perturbed with a diagonal matrix built from
the empirical PMF, then compute the eigenvectors, corresponding to the smallest
feq eigenvalues. Projecting the empirical PMF onto this low dimensional
subspace produces a smooth, multi-modal estimate that preserves coarse
structure while suppressing noise. A light post-processing step of clipping and
re-normalizing yields a valid PMF.
  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the
computation is reliable and runs time and memory proportional to the support
times the dimension of the desired low-dimensional supspace. We also provide a
practical, data-driven rule for selecting the dimension based on an
orthogonal-series risk estimate, so the method "just works" with minimal
tuning. On synthetic and real heavy-tailed examples, the approach preserves
coarse structure while suppressing sampling noise, compares favorably to
logspline and Gaussian-KDE baselines in the intended regimes. However, it has
known failure modes (e.g., abrupt discontinuities). The method is short to
implement, robust across sample sizes, and suitable for automated pipelines and
exploratory analysis at scale because of its reliability and speed.

</details>


### [26] [Predicting the Unpredictable: Reproducible BiLSTM Forecasting of Incident Counts in the Global Terrorism Database (GTD)](https://arxiv.org/abs/2510.15136)
*Oluwasegun Adegoke*

Main category: cs.LG

TL;DR: A BiLSTM model outperforms classical and deep learning baselines for weekly terrorism incident forecasting using GTD data, with key findings on optimal training history, lookback windows, and feature importance.


<details>
  <summary>Details</summary>
Motivation: To develop a reproducible pipeline for short-horizon forecasting of weekly terrorism incident counts and establish transparent baseline-beating references for GTD incident forecasting.

Method: Built a reproducible pipeline with fixed time-based splits, evaluated BiLSTM against seasonal-naive, linear/ARIMA baselines and LSTM-Attention, with ablations on temporal memory, training-history length, spatial grain, lookback size, and feature groups.

Result: BiLSTM achieved RMSE 6.38 on held-out test set, outperforming LSTM-Attention (9.19; +30.6%) and linear lag-regression baseline (+35.4% RMSE gain), with parallel improvements in MAE and MAPE.

Conclusion: Models trained on long historical data generalize best, moderate lookback (20-30 weeks) provides strong context, bidirectional encoding is critical for capturing patterns, and short-horizon structure features contribute most to forecasting performance.

Abstract: We study short-horizon forecasting of weekly terrorism incident counts using
the Global Terrorism Database (GTD, 1970--2016). We build a reproducible
pipeline with fixed time-based splits and evaluate a Bidirectional LSTM
(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a
deep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE
6.38, outperforming LSTM-Attention (9.19; +30.6\%) and a linear lag-regression
baseline (+35.4\% RMSE gain), with parallel improvements in MAE and MAPE.
Ablations varying temporal memory, training-history length, spatial grain,
lookback size, and feature groups show that models trained on long historical
data generalize best; a moderate lookback (20--30 weeks) provides strong
context; and bidirectional encoding is critical for capturing both build-up and
aftermath patterns within the window. Feature-group analysis indicates that
short-horizon structure (lagged counts and rolling statistics) contributes
most, with geographic and casualty features adding incremental lift. We release
code, configs, and compact result tables, and provide a data/ethics statement
documenting GTD licensing and research-only use. Overall, the study offers a
transparent, baseline-beating reference for GTD incident forecasting.

</details>


### [27] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: This paper provides the first theoretical proof of policy transfer for continuous-time RL, showing that optimal policies from one LQR can serve as near-optimal initializations for related LQRs while maintaining convergence rates. It also introduces a new policy learning algorithm with global linear and local super-linear convergence.


<details>
  <summary>Details</summary>
Motivation: Training RL agents from scratch on complex tasks is inefficient. Transfer learning, successful in LLMs, offers potential to enhance RL efficiency by leveraging pre-trained models, but theoretical foundations for continuous-time RL were lacking.

Method: The study investigates policy transfer in continuous-time linear quadratic regulators (LQRs) with entropy regularization. It provides theoretical analysis and introduces a novel policy learning algorithm for continuous-time LQRs.

Result: The paper proves that a policy optimal for one LQR serves as near-optimal initialization for related LQRs while preserving original convergence rates. The new algorithm achieves global linear and local super-linear convergence.

Conclusion: The work demonstrates theoretical guarantees and algorithmic benefits of transfer learning in continuous-time RL, bridging a gap in literature and extending prior work from discrete to continuous time. As a byproduct, it derives stability for continuous-time score-based diffusion models via LQR connections.

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [28] [A simple mean field model of feature learning](https://arxiv.org/abs/2510.15174)
*Niclas Göring,Chris Mingard,Yoonsoo Nam,Ard Louis*

Main category: cs.LG

TL;DR: The paper develops a mean-field theory for feature learning in finite-width neural networks, revealing a symmetry breaking phase transition where networks align with target functions, and identifies self-reinforcing input feature selection as a key mechanism missing from basic mean-field descriptions.


<details>
  <summary>Details</summary>
Motivation: To better understand feature learning in neural networks, which remains poorly understood despite being fundamental to their success, by developing a theoretical framework that can capture the emergence of feature learning in finite-width networks.

Method: Using statistical physics methods to derive a self-consistent mean-field theory for the Bayesian posterior of two-layer non-linear networks trained with stochastic gradient Langevin dynamics (SGLD), and extending it to incorporate self-reinforcing input feature selection.

Result: The basic mean-field theory predicts a symmetry breaking phase transition where networks abruptly align with target functions and semi-quantitatively predicts the onset of feature learning with noise or sample size, but underestimates generalization improvements. The extended theory with self-reinforcing feature selection quantitatively matches SGLD learning curves.

Conclusion: The mean-field framework provides mechanistic insight into feature learning, with self-reinforcing input feature selection being a crucial mechanism that explains the gap between basic mean-field predictions and actual network performance, offering a quantitative understanding of feature learning dynamics.

Abstract: Feature learning (FL), where neural networks adapt their internal
representations during training, remains poorly understood. Using methods from
statistical physics, we derive a tractable, self-consistent mean-field (MF)
theory for the Bayesian posterior of two-layer non-linear networks trained with
stochastic gradient Langevin dynamics (SGLD). At infinite width, this theory
reduces to kernel ridge regression, but at finite width it predicts a symmetry
breaking phase transition where networks abruptly align with target functions.
While the basic MF theory provides theoretical insight into the emergence of FL
in the finite-width regime, semi-quantitatively predicting the onset of FL with
noise or sample size, it substantially underestimates the improvements in
generalisation after the transition. We trace this discrepancy to a key
mechanism absent from the plain MF description: \textit{self-reinforcing input
feature selection}. Incorporating this mechanism into the MF theory allows us
to quantitatively match the learning curves of SGLD-trained networks and
provides mechanistic insight into FL.

</details>


### [29] [Finding geodesics with the Deep Ritz method](https://arxiv.org/abs/2510.15177)
*Conor Rowan*

Main category: cs.LG

TL;DR: The paper argues that geodesic problems are well-suited for the Deep Ritz method due to their simple geometry, variational structure, and natural nonlinearity, and demonstrates this with three numerical examples from path planning, optics, and solid mechanics.


<details>
  <summary>Details</summary>
Motivation: Geodesic problems are ubiquitous in physics and engineering but have received little attention from the scientific machine learning community. The authors aim to identify a promising application for the Deep Ritz method in this domain.

Method: The authors apply the Deep Ritz method to geodesic problems, leveraging its suitability for problems with variational structure. They demonstrate the approach through three numerical examples from different domains.

Result: The paper presents successful applications of the Deep Ritz method to geodesic problems in path planning, optics, and solid mechanics, showing its effectiveness for this class of problems.

Conclusion: Geodesic problems represent a promising application area for the Deep Ritz method and a fruitful direction for future scientific machine learning research, given their favorable characteristics for this approach.

Abstract: Geodesic problems involve computing trajectories between prescribed initial
and final states to minimize a user-defined measure of distance, cost, or
energy. They arise throughout physics and engineering -- for instance, in
determining optimal paths through complex environments, modeling light
propagation in refractive media, and the study of spacetime trajectories in
control theory and general relativity. Despite their ubiquity, the scientific
machine learning (SciML) community has given relatively little attention to
investigating its methods in the context of these problems. In this work, we
argue that given their simple geometry, variational structure, and natural
nonlinearity, geodesic problems are particularly well-suited for the Deep Ritz
method. We substantiate this claim with three numerical examples drawn from
path planning, optics, and solid mechanics. Our goal is not to provide an
exhaustive study of geodesic problems, but rather to identify a promising
application of the Deep Ritz method and a fruitful direction for future SciML
research.

</details>


### [30] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: A two-stage model combining clinical data and DXA imaging improves hip fracture risk prediction, outperforming traditional methods like T-score and FRAX in sensitivity and reducing missed high-risk cases.


<details>
  <summary>Details</summary>
Motivation: Current hip fracture risk assessment tools (DXA T-score and FRAX) lack sensitivity and miss high-risk individuals, especially those without prior fractures or with osteopenia, creating a need for more accurate prediction methods.

Method: Sequential two-stage model: Stage 1 uses clinical, demographic, and functional variables for baseline risk screening; Stage 2 incorporates DXA-derived imaging features for refinement. Validated using data from MrOS, SOF, and UK Biobank cohorts.

Result: The two-stage framework achieved higher sensitivity and reduced missed cases compared to T-score and FRAX, with consistent performance across different cohorts through internal and external validation.

Conclusion: The proposed two-stage model provides a cost-effective and personalized approach for early hip fracture risk assessment, offering improved accuracy over traditional methods.

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [31] [Automotive Crash Dynamics Modeling Accelerated with Machine Learning](https://arxiv.org/abs/2510.15201)
*Mohammad Amin Nabian,Sudeep Chavare,Deepak Akhare,Rishikesh Ranade,Ram Cherukuri,Srinivas Tadepalli*

Main category: cs.LG

TL;DR: This paper presents a comparative study of machine learning surrogate models for predicting structural deformation in automotive crash scenarios using NVIDIA PhysicsNeMo framework, demonstrating feasibility and computational efficiency gains over traditional finite element simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional finite element simulations for crashworthiness assessment are computationally expensive and time-consuming, creating a need for faster alternatives to enable rapid design exploration and optimization.

Method: The study investigates two neural network architectures (MeshGraphNet and Transolver) and three transient dynamics modeling strategies (time-conditional, standard Autoregressive, and stability-enhanced Autoregressive with rollout-based training) on a comprehensive Body-in-White crash dataset of 150 detailed FE simulations.

Result: The models captured overall deformation trends with reasonable fidelity and achieved orders-of-magnitude reductions in computational cost compared to full FE simulations, though they did not yet match full FE accuracy.

Conclusion: Machine learning approaches show feasibility for structural crash dynamics prediction, enabling rapid design exploration and early-stage optimization in crashworthiness evaluation despite not yet achieving full FE simulation accuracy.

Abstract: Crashworthiness assessment is a critical aspect of automotive design,
traditionally relying on high-fidelity finite element (FE) simulations that are
computationally expensive and time-consuming. This work presents an exploratory
comparative study on developing machine learning-based surrogate models for
efficient prediction of structural deformation in crash scenarios using the
NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine
learning to structural crash dynamics, the primary contribution lies in
demonstrating the feasibility and engineering utility of the various modeling
approaches explored in this work. We investigate two state-of-the-art neural
network architectures for modeling crash dynamics: MeshGraphNet, and
Transolver. Additionally, we examine three strategies for modeling transient
dynamics: time-conditional, the standard Autoregressive approach, and a
stability-enhanced Autoregressive scheme incorporating rollout-based training.
The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset
comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a
structurally rich vehicle assembly with over 200 components, including 38 key
components featuring variable thickness distributions to capture realistic
manufacturing variability. Each model utilizes the undeformed mesh geometry and
component characteristics as inputs to predict the spatiotemporal evolution of
the deformed mesh during the crash sequence. Evaluation results show that the
models capture the overall deformation trends with reasonable fidelity,
demonstrating the feasibility of applying machine learning to structural crash
dynamics. Although not yet matching full FE accuracy, the models achieve
orders-of-magnitude reductions in computational cost, enabling rapid design
exploration and early-stage optimization in crashworthiness evaluation.

</details>


### [32] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: This paper analyzes how representation geometry and normalization affect Mahalanobis distance-based OOD detection, showing these methods aren't universally reliable and proposing radially scaled ℓ₂ normalization to improve performance by controlling feature space geometry.


<details>
  <summary>Details</summary>
Motivation: The impact of representation geometry and normalization on Mahalanobis distance methods for OOD detection is not fully understood, limiting their practical application. The authors aim to bridge this gap through empirical analysis.

Method: Conducted comprehensive empirical study across diverse image foundation models, datasets, and distance normalization schemes. Analyzed spectral and intrinsic-dimensionality metrics, and proposed radially scaled ℓ₂ normalization with a tunable parameter to control feature space geometry.

Result: Mahalanobis-based methods aren't universally reliable. Spectral and intrinsic-dimensionality metrics can predict OOD performance. Radially scaled ℓ₂ normalization significantly improves OOD detection by systematically contracting or expanding representations.

Conclusion: The study bridges representation geometry, normalization, and OOD performance, offering insights for designing more effective and reliable deep learning models through controlled feature space geometry.

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [33] [ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning](https://arxiv.org/abs/2510.15211)
*Yongchan Kwon,Shang Zhu,Federico Bianchi,Kaitlyn Zhou,James Zou*

Main category: cs.LG

TL;DR: This paper introduces ReasonIF, a benchmark to evaluate how well large reasoning models (LRMs) follow user instructions during their reasoning process, not just in final responses. The study finds significant failures in instruction adherence across major LRMs and proposes two improvement strategies.


<details>
  <summary>Details</summary>
Motivation: While prior work focused on instruction following in final responses, this paper argues it's critical for LRMs to follow instructions throughout their reasoning process to improve controllability, transparency, and reduce risks like hallucinations and reward hacking.

Method: The authors developed ReasonIF benchmark with six categories of instruction prompts covering multilingual reasoning, formatting, and length control. They evaluated multiple open-source LRMs and explored two enhancement strategies: multi-turn reasoning and Reasoning Instruction Finetuning (RIF) using synthetic data.

Result: Substantial failures in reasoning instruction adherence were found - the highest instruction following score (IFS) remained below 0.25 across models. Performance degraded further with increased task difficulty. RIF improved GPT-OSS-20B's IFS from 0.11 to 0.27.

Conclusion: Current LRMs have significant room for improvement in reasoning instruction following. The proposed RIF method shows measurable progress but substantial challenges remain in ensuring LRMs properly follow instructions throughout their reasoning processes.

Abstract: The ability of large language models (LLMs) to follow user instructions is
central to their reliability, safety, and usefulness. While prior studies
assess instruction adherence in the model's main responses, we argue that it is
also critical for large reasoning models (LRMs) to follow user instructions
throughout their reasoning process. Reasoning instruction following makes LRMs
more controllable and transparent, while reducing risks of undesirable
shortcuts, hallucinations, or reward hacking within reasoning traces. To
evaluate this dimension, we introduce ReasonIF, a systematic benchmark for
assessing reasoning instruction following. ReasonIF includes six categories of
instruction prompts, spanning multilingual reasoning, formatting and length
control. Across many open-source LRMs including GPT-OSS, Qwen3, and
DeepSeek-R1, we find substantial failures in reasoning instruction adherence:
the highest instruction following score (IFS) remains below 0.25, meaning that
fewer than $25\%$ of reasoning traces comply with the given instructions.
Notably, as task difficulty increases, reasoning instruction following degrades
further. We also explore two strategies to enhance reasoning instruction
fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning
(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to
0.27, indicating measurable progress but leaving ample room for improvement.

</details>


### [34] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: The paper investigates why different base models show varying performance after reinforcement learning with verifiable rewards (RLVR). It discovers that high-performing models inherently distinguish between sound and unsound reasoning rules, while weaker models don't, and introduces a metric called Soundness-Aware Level (SAL) to quantify this property.


<details>
  <summary>Details</summary>
Motivation: To understand why RLVR performance varies dramatically across different base models and identify the microscopic properties of pre-trained models that lead to this variation.

Method: Formalize reasoning as chains of Horn clauses built from features extracted via cross-layer sparse autoencoders (SAEs), estimate transition probabilities between features, categorize rules by semantic soundness levels, and introduce SAL metric using Jensen-Shannon Divergence to measure distribution separation.

Result: High-potential models are soundness-aware with distinct internal probability distributions for different soundness levels, while weaker models are soundness-agnostic. SAL predicts post-RLVR reasoning performance with high accuracy (R^2=0.87) across diverse model families and scales.

Conclusion: A model's reasoning potential is tied to its intrinsic, pre-trained ability to distinguish sound knowledge from unsound ones, highlighting the critical role of model pre-training in shaping reasoning capabilities and providing a practical metric for selecting stronger base models.

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [35] [Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025](https://arxiv.org/abs/2510.15217)
*Emily Alsentzer,Marie-Laure Charpignon,Bill Chen,Niharika D'Souza,Jason Fries,Yixing Jiang,Aparajita Kashyap,Chanwoo Kim,Simon Lee,Aishwarya Mandyam,Ashery Christopher Mbilinyi,Nikita Mehandru,Nitish Nagesh,Brighton Nuwagira,Emma Pierson,Arvind Pillai,Akane Sano,Tanveer Syeda-Mahmood,Shashank Yadav,Elias Adhanom,Muhammad Umar Afza,Amelia Archer,Suhana Bedi,Vasiliki Bikia,Trenton Chang,George H. Chen,Winston Chen,Erica Chiang,Edward Choi,Octavia Ciora,Paz Dozie-Nnamah,Shaza Elsharief,Matthew Engelhard,Ali Eshragh,Jean Feng,Josh Fessel,Scott Fleming,Kei Sen Fong,Thomas Frost,Soham Gadgil,Judy Gichoya,Leeor Hershkovich,Sujeong Im,Bhavya Jain,Vincent Jeanselme,Furong Jia,Qixuan,Jin,Yuxuan Jin,Daniel Kapash,Geetika Kapoor,Behdokht Kiafar,Matthias Kleiner,Stefan Kraft,Annika Kumar,Daeun Kyung,Zhongyuan Liang,Joanna Lin,Qianchu,Liu,Chang Liu,Hongzhou Luan,Chris Lunt,Leopoldo Julían Lechuga López,Matthew B. A. McDermott,Shahriar Noroozizadeh,Connor O'Brien,YongKyung Oh,Mixail Ota,Stephen Pfohl,Meagan Pi,Tanmoy Sarkar Pias,Emma Rocheteau,Avishaan Sethi,Toru Shirakawa,Anita Silver,Neha Simha,Kamile Stankeviciute,Max Sunog,Peter Szolovits,Shengpu Tang,Jialu Tang,Aaron Tierney,John Valdovinos,Byron Wallace,Will Ke Wang,Peter Washington,Jeremy Weiss,Daniel Wolfe,Emily Wong,Hye Sun Yun,Xiaoman Zhang,Xiao Yu Cindy Zhang,Hayoung Jeong,Kaveri A. Thakoor*

Main category: cs.LG

TL;DR: CHIL 2025 conference featured Research Roundtables with 8 sessions on key ML-healthcare topics, fostering collaborative dialogue among 19 chairs.


<details>
  <summary>Details</summary>
Motivation: To catalyze collaborative discussions on critical topics at the intersection of machine learning and healthcare through small-group dialogue.

Method: Hosted Research Roundtables moderated by senior and junior chairs, emphasizing open exchange, intellectual curiosity, and inclusive engagement in small-group settings.

Result: Successfully conducted 8 roundtables on topics including Explainability, Uncertainty/Bias/Fairness, Causality, Domain Adaptation, Foundation Models, Learning from Small Medical Data, Multimodal Methods, and Scalable Healthcare Solutions.

Conclusion: The roundtables effectively fostered rigorous discussion of key challenges and collective ideation toward actionable directions in ML-healthcare research.

Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),
hosted by the Association for Health Learning and Inference (AHLI), was held in
person on June 25-27, 2025, at the University of California, Berkeley, in
Berkeley, California, USA. As part of this year's program, we hosted Research
Roundtables to catalyze collaborative, small-group dialogue around critical,
timely topics at the intersection of machine learning and healthcare. Each
roundtable was moderated by a team of senior and junior chairs who fostered
open exchange, intellectual curiosity, and inclusive engagement. The sessions
emphasized rigorous discussion of key challenges, exploration of emerging
opportunities, and collective ideation toward actionable directions in the
field. In total, eight roundtables were held by 19 roundtable chairs on topics
of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,
and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning
from Small Medical Data," "Multimodal Methods," and "Scalable, Translational
Healthcare Solutions."

</details>


### [36] [Machine Learning for Early Detection of Meningitis: Stacked Ensemble Learning with EHR data](https://arxiv.org/abs/2510.15218)
*Han Ouyang,Jesse Hamilton,Saeed Amal*

Main category: cs.LG

TL;DR: This paper develops an ensemble learning approach using Random Forest, LightGBM, and DNN models to diagnose meningitis, achieving high AUC scores (0.9637 and 0.9472) on test sets by simulating real-world ER scenarios.


<details>
  <summary>Details</summary>
Motivation: To create an AI-driven diagnostic tool for meningitis that can be used in real-world emergency room settings, addressing the challenge of accurate and timely diagnosis.

Method: Used MIMIC-III database with 214 meningitis and 46,303 non-meningitis patients. Applied extensive preprocessing including ICD-based cohort selection, one-hot encoding, and two-stage feature selection. Trained three base models (Random Forest, LightGBM, DNN) and aggregated their outputs through ensemble learning with logistic regression as meta model.

Result: Achieved excellent performance with AUC of 0.9637 on Testing Set 1 and 0.9472 on Testing Set 2. Selected clinically relevant features including gender, subarachnoid hemorrhage, secondary malignant neoplasm of the brain, and generalized epilepsy.

Conclusion: The ensemble learning approach provides a promising foundation for future AI-driven diagnostic tools for meningitis, though direct clinical deployment remains challenging. The study successfully simulated real-world ER scenarios to enhance clinical applicability.

Abstract: We utilized a cohort of 214 meningitis patients and 46,303 non-meningitis
patients from the MIMIC-III database. After extensive data preprocessing, which
included ICD-based cohort selection, one-hot encoding of coding, and a
two-stage feature selection process (for both the training set and the testing
sets), clinically relevant features such as gender and high-risk ICD codes
(including subarachnoid hemorrhage, secondary malignant neoplasm of the brain,
and generalized epilepsy) are selected. Overall, these clinically reasonable
and temporally adherent features provided excellent modeling performance. Three
models (Random Forest, LightGBM, and Deep Neural Networks (DNN) are trained as
base models for Ensemble Learning. Base model outputs are aggregated and
stacked into a meta model (Logistic Regression) that uses the base model
outputs as input values in training. Ultimately, soldier outputs (AUC of
Testing Set 1: 0.9637, AUC of Testing Set 2: 0.9472) are obtained through
ensemble learning.
  We created a challenging condition for diagnosing meningitis, simulating a
real-world ER (Emergency Room) scenario to enhance clinical use in real-world
applications. While directly deploying a diagnostic tool that clinicians can
use is challenging, this paper paves the way for a potential future AI-driven
diagnostic approach for meningitis using Ensemble Learning.

</details>


### [37] [Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)](https://arxiv.org/abs/2510.15219)
*Patricia Medina,Rasika Karkare*

Main category: cs.LG

TL;DR: This paper extends previous work by combining product coefficients with autoencoder representations and KNN classifiers, showing consistent performance improvements over PCA baselines and earlier frameworks.


<details>
  <summary>Details</summary>
Motivation: To enhance 3D LiDAR point-cloud classification by integrating product coefficients with autoencoder representations, building on previous research that showed product coefficients complement original spatial LiDAR features.

Method: Combined product coefficients with autoencoder representation and KNN classifier, investigated adding product coefficients level by level to analyze their impact on performance.

Result: The combination delivered consistent performance gains over PCA-based baselines and earlier frameworks, with richer sets of coefficients systematically improving class separability and overall accuracy.

Conclusion: Hierarchical product-coefficient features combined with autoencoders significantly enhance LiDAR classification performance, demonstrating the value of this integrated approach.

Abstract: This work extends our previous study on enhancing 3D LiDAR point-cloud
classification with product coefficients
\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic
descriptors that complement the original spatial Lidar features. Here, we show
that combining product coefficients with an autoencoder representation and a
KNN classifier delivers consistent performance gains over both PCA-based
baselines and our earlier framework. We also investigate the effect of adding
product coefficients level by level, revealing a clear trend: richer sets of
coefficients systematically improve class separability and overall accuracy.
The results highlight the value of combining hierarchical product-coefficient
features with autoencoders to push LiDAR classification performance further.

</details>


### [38] [Robust Layerwise Scaling Rules by Proper Weight Decay Tuning](https://arxiv.org/abs/2510.15262)
*Zhiyuan Fan,Yifeng Liu,Qingyue Zhao,Angela Yuan,Quanquan Gu*

Main category: cs.LG

TL;DR: The paper introduces a weight-decay scaling rule for AdamW that enables zero-shot transfer of learning rates and weight decay across different model widths, extending μP beyond the initial training phase by controlling optimizer-governed steady-state scales.


<details>
  <summary>Details</summary>
Motivation: Modern scale-invariant architectures quickly enter an optimizer-governed steady state where normalization layers create backward scale sensitivity, making effective learning rate width-dependent and degrading μP transfer performance.

Method: Proposes a weight-decay scaling rule λ₂ ∝ √d for matrix-like parameters combined with μP learning-rate rule η₂ ∝ d⁻¹, while keeping vector-like parameters at η₁ = Θ_d(1) and λ₁ = 0, to maintain sublayer gain invariance across widths.

Result: Validated on LLaMA-style Transformers and minimal synthetic settings, the method enables zero-shot hyperparameter transfer without per-width sweeps and provides a diagnostic for checking sublayer-gain invariance.

Conclusion: The approach extends μP beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW.

Abstract: Empirical scaling laws prescribe how to allocate parameters, data, and
compute, while maximal-update parameterization ($\mu$P) enables learning-rate
transfer across widths by equalizing early-time update magnitudes. However, in
modern scale-invariant architectures, training quickly enters an
optimizer-governed steady state where normalization layers create backward
scale sensitivity and the effective learning rate becomes width dependent,
degrading $\mu$P transfer. We address this by introducing a weight-decay
scaling rule for AdamW that preserves sublayer gain across widths. Empirically,
the singular-value spectrum of each matrix parameter scales in norm as
$\sqrt{\eta/\lambda}$ with an approximately invariant shape; under width
scaling $d$, we observe that the top singular value scales approximately as
$\sqrt{\eta/\lambda}\cdot d^{0.75}$. Combining this observation with the $\mu$P
learning-rate rule $\eta_2\propto d^{-1}$ for matrix-like parameters implies an
empirical weight-decay scaling rule $\lambda_2\propto \sqrt{d}$ that
approximately keeps sublayer gains width invariant. Together with vector-like
parameters trained at $\eta_1=\Theta_d(1)$ and $\lambda_1=0$, this yields
\emph{zero-shot} transfer of both learning rate and weight decay from proxy to
target widths, removing per-width sweeps. We validate the rule on LLaMA-style
Transformers and in a minimal synthetic setting, and we provide a simple
diagnostic, matching top singular values, to check sublayer-gain invariance.
Our results extend $\mu$P beyond the near-init regime by explicitly controlling
steady-state scales set by the optimizer, offering a practical recipe for
width-robust hyperparameter transfer under AdamW.

</details>


### [39] [Particle Dynamics for Latent-Variable Energy-Based Models](https://arxiv.org/abs/2510.15447)
*Shiqin Tang,Shuxin Zhuang,Rong Feng,Runsheng Yu,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: The paper presents a novel training method for latent-variable energy-based models (LVEBMs) by reformulating maximum-likelihood training as a saddle-point problem over distributions, using coupled Wasserstein gradient flows without requiring discriminators or auxiliary networks.


<details>
  <summary>Details</summary>
Motivation: To develop an expressive generative modeling approach for latent-variable energy-based models that captures hidden structure while avoiding the need for discriminators or auxiliary networks, providing theoretical guarantees and improved performance.

Method: Recast maximum-likelihood training as a saddle problem over distributions on latent and joint manifolds, using coupled Wasserstein gradient flows with alternating overdamped Langevin updates for joint negative pool and conditional latent particles with stochastic parameter ascent.

Result: The method achieves competitive performance on numerical approximations of physical systems, with proven existence and convergence under standard assumptions, and provides an ELBO strictly tighter than bounds from restricted amortized posteriors.

Conclusion: The saddle-point formulation enables effective training of LVEBMs with theoretical guarantees, demonstrating competitive performance while eliminating the need for discriminators or auxiliary networks.

Abstract: Latent-variable energy-based models (LVEBMs) assign a single normalized
energy to joint pairs of observed data and latent variables, offering
expressive generative modeling while capturing hidden structure. We recast
maximum-likelihood training as a saddle problem over distributions on the
latent and joint manifolds and view the inner updates as coupled Wasserstein
gradient flows. The resulting algorithm alternates overdamped Langevin updates
for a joint negative pool and for conditional latent particles with stochastic
parameter ascent, requiring no discriminator or auxiliary networks. We prove
existence and convergence under standard smoothness and dissipativity
assumptions, with decay rates in KL divergence and Wasserstein-2 distance. The
saddle-point view further yields an ELBO strictly tighter than bounds obtained
with restricted amortized posteriors. Our method is evaluated on numerical
approximations of physical systems and performs competitively against
comparable approaches.

</details>


### [40] [FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain](https://arxiv.org/abs/2510.15232)
*Tiansheng Hu,Tongyan Hu,Liuyang Bai,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.LG

TL;DR: FinTrust is a comprehensive benchmark for evaluating LLM trustworthiness in finance applications, assessing 11 LLMs across various alignment dimensions and finding proprietary models generally outperform but open-source models excel in specific areas like fairness, while all models struggle with legal awareness tasks.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs in real-world finance applications is challenging due to high risks and stakes, necessitating a specialized benchmark to evaluate trustworthiness across practical contexts and alignment issues.

Method: Developed FinTrust benchmark with fine-grained tasks across multiple trustworthiness dimensions, evaluated 11 LLMs including proprietary (o4-mini) and open-source (DeepSeek-V3) models on various finance-related trustworthiness metrics.

Result: Proprietary models like o4-mini outperformed in most tasks (e.g., safety), open-source models like DeepSeek-V3 had advantages in specific areas (e.g., industry-level fairness), and all LLMs performed poorly on challenging tasks like fiduciary alignment and disclosure, showing significant legal awareness gaps.

Conclusion: FinTrust serves as a valuable benchmark for evaluating LLM trustworthiness in finance, revealing current limitations in legal awareness and highlighting the need for continued improvement in financial domain alignment.

Abstract: Recent LLMs have demonstrated promising ability in solving finance related
problems. However, applying LLMs in real-world finance application remains
challenging due to its high risk and high stakes property. This paper
introduces FinTrust, a comprehensive benchmark specifically designed for
evaluating the trustworthiness of LLMs in finance applications. Our benchmark
focuses on a wide range of alignment issues based on practical context and
features fine-grained tasks for each dimension of trustworthiness evaluation.
We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini
outperforms in most tasks such as safety while open-source models like
DeepSeek-V3 have advantage in specific areas like industry-level fairness. For
challenging task like fiduciary alignment and disclosure, all LLMs fall short,
showing a significant gap in legal awareness. We believe that FinTrust can be a
valuable benchmark for LLMs' trustworthiness evaluation in finance domain.

</details>


### [41] [Learning to Answer from Correct Demonstrations](https://arxiv.org/abs/2510.15464)
*Nirmit Joshi,Gene Li,Siddharth Bhandari,Shiva Prasad Kasiviswanathan,Cong Ma,Nathan Srebro*

Main category: cs.LG

TL;DR: The paper addresses learning to generate answers from correct demonstrations when multiple valid answers exist, proposing an alternative to maximum likelihood estimation that works with low-cardinality reward models.


<details>
  <summary>Details</summary>
Motivation: Traditional methods assume low-complexity policy classes, but this paper argues that assuming low-cardinality reward classes is a weaker and more realistic assumption for learning from correct demonstrations.

Method: The authors formalize the problem as offline imitation learning in contextual bandits and devise a novel approach that learns with logarithmic sample complexity in the reward class cardinality, avoiding likelihood maximization failures.

Result: The proposed method achieves sample complexity logarithmic in the cardinality of the reward class, outperforming maximum likelihood estimation approaches which can fail in this setting.

Conclusion: The work motivates moving beyond likelihood maximization for learning from correct demonstrations and demonstrates the effectiveness of their alternative approach with provable guarantees.

Abstract: We study the problem of learning to generate an answer (or completion) to a
question (or prompt), where there could be multiple correct answers, any one of
which is acceptable at test time. Learning is based on demonstrations of some
correct answer to each training question, as in Supervised Fine Tuning (SFT).
We formalize the problem as offline imitation learning in contextual bandits,
with demonstrations from some optimal policy, without explicitly observed
rewards. Prior work assumes that the demonstrator belongs to a low-complexity
policy class, which motivates maximum likelihood estimation (i.e., log-loss
minimization). In contrast, we propose relying only on the reward model
(specifying which answers are correct) being in a low-cardinality class, which
we argue is a weaker assumption. We show that likelihood maximization methods
can fail in this case, and instead devise an alternative novel approach that
learns with sample complexity logarithmic in the cardinality of the reward
class. Our work motivates looking beyond likelihood maximization when learning
from correct demonstrations.

</details>


### [42] [Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction](https://arxiv.org/abs/2510.15233)
*Amitesh Badkul,Lei Xie*

Main category: cs.LG

TL;DR: TESSERA is a novel uncertainty quantification method that provides reliable coverage guarantees, informative prediction intervals, and adaptive interval widths that track actual error, particularly addressing challenges in protein-ligand affinity prediction under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Current ML methods lack reliable uncertainty quantification, hindering AI/ML application in risk-sensitive domains like drug discovery. Protein-ligand affinity prediction faces challenges with heterogeneous assay noise, imbalanced chemical space, and distribution shifts.

Method: TESSERA combines Mixture of Expert (MoE) diversity with conformal calibration to provide per-sample uncertainty with reliable coverage guarantees, adaptive prediction intervals, and scaled estimation for efficient reliable adaptive intervals.

Result: TESSERA achieves near-nominal coverage and the best coverage-width trade-off (measured by CWC), maintains competitive adaptivity (lowest AUSE), and demonstrates right-sized intervals through Size-Stratified Coverage analysis.

Conclusion: By unifying MoE diversity with conformal calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties suitable for selective prediction and downstream decision-making in drug discovery and other applications.

Abstract: Reliable, informative, and individual uncertainty quantification (UQ) remains
missing in current ML community. This hinders the effective application of
AI/ML to risk-sensitive domains. Most methods either fail to provide coverage
on new data, inflate intervals so broadly that they are not actionable, or
assign uncertainties that do not track actual error, especially under a
distribution shift. In high-stakes drug discovery, protein-ligand affinity
(PLI) prediction is especially challenging as assay noise is heterogeneous,
chemical space is imbalanced and large, and practical evaluations routinely
involve distribution shift. In this work, we introduce a novel uncertainty
quantification method, Trustworthy Expert Split-conformal with Scaled
Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides
per-sample uncertainty with reliable coverage guarantee, informative and
adaptive prediction interval widths that track the absolute error. We evaluate
on protein-ligand binding affinity prediction under both independent and
identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)
splits, comparing against strong UQ baselines. TESSERA attains near-nominal
coverage and the best coverage-width trade-off as measured by the
Coverage-Width Criterion (CWC), while maintaining competitive adaptivity
(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage
(SSC) further confirms that intervals are right-sized, indicating width
increases when data are scarce or noisy, and remain tight when predictions are
reliable. By unifying Mixture of Expert (MoE) diversity with conformal
calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties
that are well-suited to selective prediction and downstream decision-making in
the drug-discovery pipeline and other applications.

</details>


### [43] [Adversary-Free Counterfactual Prediction via Information-Regularized Representations](https://arxiv.org/abs/2510.15479)
*Shiqin Tang,Rong Feng,Shuxin Zhuang,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: The paper proposes an information-theoretic approach for counterfactual prediction under assignment bias, using mutual information minimization to remove treatment-covariate dependence without adversarial training.


<details>
  <summary>Details</summary>
Motivation: To address assignment bias in counterfactual prediction while avoiding the training instabilities and tuning difficulties of adversarial methods.

Method: Learn a stochastic representation Z that predicts outcomes while minimizing I(Z; T) using a tractable variational objective that upper-bounds the information term and couples with a supervised decoder.

Result: The method performs favorably on controlled simulations and real-world clinical data across likelihood, counterfactual error, and policy evaluation metrics, outperforming balancing, reweighting, and adversarial baselines.

Conclusion: The information-theoretic framework provides stable, provably motivated training for counterfactual prediction that avoids adversarial training instabilities while maintaining strong performance.

Abstract: We study counterfactual prediction under assignment bias and propose a
mathematically grounded, information-theoretic approach that removes
treatment-covariate dependence without adversarial training. Starting from a
bound that links the counterfactual-factual risk gap to mutual information, we
learn a stochastic representation Z that is predictive of outcomes while
minimizing I(Z; T). We derive a tractable variational objective that
upper-bounds the information term and couples it with a supervised decoder,
yielding a stable, provably motivated training criterion. The framework extends
naturally to dynamic settings by applying the information penalty to sequential
representations at each decision time. We evaluate the method on controlled
numerical simulations and a real-world clinical dataset, comparing against
recent state-of-the-art balancing, reweighting, and adversarial baselines.
Across metrics of likelihood, counterfactual error, and policy evaluation, our
approach performs favorably while avoiding the training instabilities and
tuning burden of adversarial schemes.

</details>


### [44] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: KME-CLIP enhances CLIP by using kernel methods to better approximate pointwise mutual information (PMI) for similarity computation, improving performance on retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current CLIP implementations don't fully utilize the linear structure of PMI, which theory shows should be the optimal similarity metric between paired modalities.

Method: Proposed KME-CLIP uses inner product in reproducing kernel Hilbert space to leverage PMI's linear structure and approximate it with arbitrary accuracy.

Result: Empirical results show KME-CLIP outperforms standard CLIP across several retrieval and classification tasks.

Conclusion: The kernel-based approach effectively captures the optimal PMI structure and improves multi-modal contrastive learning performance.

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [45] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: Transformer-based framework predicts disease risk at migratory bird trajectory endpoints using GPS tracking, outbreak records, and geospatial data, achieving high accuracy (0.9821) and AUC (0.9803).


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting of avian disease outbreaks is critical for wildlife conservation and public health, requiring early-warning systems for timely intervention.

Method: Integrates multi-source datasets (GPS tracking from Movebank, WOAH outbreak records, geospatial context) with H3 hierarchical geospatial encoding and Transformer architecture to learn spatiotemporal dependencies from bird movement sequences.

Result: Strong predictive performance on test set: accuracy 0.9821, AUC 0.9803, average precision 0.9299, F1-score 0.8836 at optimal threshold.

Conclusion: Transformer architectures show potential for supporting early-warning systems in avian disease surveillance, enabling timely intervention and prevention strategies.

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [46] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero introduces robust Bayesian optimization for zero-shot prompt optimization, using f-divergence balls to handle distribution shifts and improve reliability across tasks like formality rewriting and code debugging.


<details>
  <summary>Details</summary>
Motivation: Current prompt search methods like InstructZero often fail under distribution shift and adversarial evaluation because they optimize for expected performance under a single evaluation distribution, leading to poor transferability.

Method: Formulates zero-shot prompt optimization as robust Bayesian optimization with an f-divergence ball defining an ambiguity set around the evaluation distribution, using a robust acquisition rule to maximize worst-case expected utility while maintaining query efficiency.

Result: Significant improvements across tasks: BIG-Bench informative-to-formal rewriting accuracy increased from 61.3% to 85-90% (+25-30 points), auto-debugging showed +25-point gains under domain shift, while stable tasks maintained above 96% accuracy with no in-distribution performance loss.

Conclusion: DRO-InstructZero connects distributionally robust optimization with prompt learning, providing a plug-and-play approach for reliable and transferable prompt alignment under real-world uncertainty, with consistent improvements across divergence choices and decoding temperatures.

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [47] [Chronos-2: From Univariate to Universal Forecasting](https://arxiv.org/abs/2510.15821)
*Abdul Fatir Ansari,Oleksandr Shchur,Jaris Küken,Andreas Auer,Boran Han,Pedro Mercado,Syama Sundar Rangapuram,Huibin Shen,Lorenzo Stella,Xiyuan Zhang,Mononito Goswami,Shubham Kapoor,Danielle C. Maddix,Pablo Guerron,Tony Hu,Junming Yin,Nick Erickson,Prateek Mutalik Desai,Hao Wang,Huzefa Rangwala,George Karypis,Yuyang Wang,Michael Bohlke-Schneider*

Main category: cs.LG

TL;DR: Chronos-2 is a pretrained time series model that handles univariate, multivariate, and covariate-informed forecasting tasks in zero-shot manner using group attention mechanism for in-context learning across related time series.


<details>
  <summary>Details</summary>
Motivation: Existing pretrained time series models focus mainly on univariate forecasting, limiting their real-world applicability where multivariate data and covariates are crucial for accurate predictions.

Method: Chronos-2 employs group attention mechanism for in-context learning across multiple time series within groups, trained on synthetic datasets that impose diverse multivariate structures on univariate series.

Result: Achieves state-of-the-art performance across three benchmarks (fev-bench, GIFT-Eval, Chronos Benchmark II), with substantial improvements in multivariate and covariate-informed forecasting, consistently outperforming baselines by wide margins.

Conclusion: Chronos-2's in-context learning capabilities establish it as a general-purpose forecasting model that can be used 'as is' in real-world forecasting pipelines without task-specific training.

Abstract: Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate forecasting, limiting their
applicability in real-world scenarios where multivariate data and covariates
play a crucial role. We present Chronos-2, a pretrained model capable of
handling univariate, multivariate, and covariate-informed forecasting tasks in
a zero-shot manner. Chronos-2 employs a group attention mechanism that
facilitates in-context learning (ICL) through efficient information sharing
across multiple time series within a group, which may represent sets of related
series, variates of a multivariate series, or targets and covariates in a
forecasting task. These general capabilities are achieved through training on
synthetic datasets that impose diverse multivariate structures on univariate
series. Chronos-2 delivers state-of-the-art performance across three
comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On
fev-bench, which emphasizes multivariate and covariate-informed forecasting,
Chronos-2's universal ICL capabilities lead to substantial improvements over
existing models. On tasks involving covariates, it consistently outperforms
baselines by a wide margin. Case studies in the energy and retail domains
further highlight its practical advantages. The in-context learning
capabilities of Chronos-2 establish it as a general-purpose forecasting model
that can be used "as is" in real-world forecasting pipelines.

</details>


### [48] [Learning Correlated Reward Models: Statistical Barriers and Opportunities](https://arxiv.org/abs/2510.15839)
*Yeshwanth Cherapanamjeri,Constantinos Daskalakis,Gabriele Farina,Sobhan Mohammadpour*

Main category: cs.LG

TL;DR: The paper shows that pairwise preference data is insufficient for learning correlated Random Utility Models, but best-of-three preference data enables efficient learning of correlated utilities with improved personalization.


<details>
  <summary>Details</summary>
Motivation: Random Utility Models (RUMs) are crucial for RLHF but suffer from the Independence of Irrelevant Alternatives (IIA) assumption, which oversimplifies human preferences. Existing methods avoiding IIA lack statistical and computational guarantees.

Method: The authors investigate learning correlated probit models, showing pairwise data is fundamentally insufficient. They propose using best-of-three preference data and develop an efficient estimator with near-optimal performance.

Result: Theoretical analysis proves best-of-three data overcomes limitations of pairwise data. The proposed estimator achieves statistically and computationally efficient learning of correlated utilities, validated on real-world datasets with improved personalization.

Conclusion: Higher-order preference data (specifically best-of-three) enables effective learning of correlated utilities, providing more fine-grained modeling of human preferences and better personalization compared to traditional pairwise approaches.

Abstract: Random Utility Models (RUMs) are a classical framework for modeling user
preferences and play a key role in reward modeling for Reinforcement Learning
from Human Feedback (RLHF). However, a crucial shortcoming of many of these
techniques is the Independence of Irrelevant Alternatives (IIA) assumption,
which collapses \emph{all} human preferences to a universal underlying utility
function, yielding a coarse approximation of the range of human preferences. On
the other hand, statistical and computational guarantees for models avoiding
this assumption are scarce. In this paper, we investigate the statistical and
computational challenges of learning a \emph{correlated} probit model, a
fundamental RUM that avoids the IIA assumption. First, we establish that the
classical data collection paradigm of pairwise preference data is
\emph{fundamentally insufficient} to learn correlational information,
explaining the lack of statistical and computational guarantees in this
setting. Next, we demonstrate that \emph{best-of-three} preference data
provably overcomes these shortcomings, and devise a statistically and
computationally efficient estimator with near-optimal performance. These
results highlight the benefits of higher-order preference data in learning
correlated utilities, allowing for more fine-grained modeling of human
preferences. Finally, we validate these theoretical guarantees on several
real-world datasets, demonstrating improved personalization of human
preferences.

</details>


### [49] [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
*Emam Hossain,Muhammad Hasan Ferdous,Devon Dunmire,Aneesh Subramanian,Md Osman Gani*

Main category: cs.LG

TL;DR: RIC-TSC is a causal time-series classification framework that embeds lag-aware causal discovery into sequence modeling for spatiotemporal Earth observation, achieving 12.59% higher accuracy than correlation-based methods under out-of-distribution evaluation.


<details>
  <summary>Details</summary>
Motivation: Current spatiotemporal Earth observation models rely on correlational features that fail to transfer across heterogeneous domains, while causal modeling offers principled foundations for uncovering stable, invariant relationships that improve robustness and generalization under distribution shifts.

Method: Proposed RIC-TSC framework uses Joint PCMCI+ (J-PCMCI+) for region-specific and invariant causal discovery of supraglacial lake evolution predictors from multi-modal satellite data (Sentinel-1, Sentinel-2, Landsat-8, CARRA meteorological variables), with validated predictors and time lags supplied to lightweight classifiers.

Result: On a balanced benchmark of 1000 manually labeled lakes from two contrasting melt seasons (2018-2019), causal models achieved up to 12.59% higher accuracy than correlation-based baselines under out-of-distribution evaluation.

Conclusion: Causal discovery serves not only as feature selection but also enables generalizable and mechanistically grounded models of dynamic Earth surface processes, demonstrating superior performance over purely correlational approaches.

Abstract: Causal modeling offers a principled foundation for uncovering stable,
invariant relationships in time-series data, thereby improving robustness and
generalization under distribution shifts. Yet its potential is underutilized in
spatiotemporal Earth observation, where models often depend on purely
correlational features that fail to transfer across heterogeneous domains. We
propose RIC-TSC, a regionally-informed causal time-series classification
framework that embeds lag-aware causal discovery directly into sequence
modeling, enabling both predictive accuracy and scientific interpretability.
Using multi-modal satellite and reanalysis data-including Sentinel-1 microwave
backscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA
meteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify
region-specific and invariant predictors of supraglacial lake evolution in
Greenland. Causal graphs are estimated globally and per basin, with validated
predictors and their time lags supplied to lightweight classifiers. On a
balanced benchmark of 1000 manually labeled lakes from two contrasting melt
seasons (2018-2019), causal models achieve up to 12.59% higher accuracy than
correlation-based baselines under out-of-distribution evaluation. These results
show that causal discovery is not only a means of feature selection but also a
pathway to generalizable and mechanistically grounded models of dynamic Earth
surface processes.

</details>


### [50] [Semi-Supervised Regression with Heteroscedastic Pseudo-Labels](https://arxiv.org/abs/2510.15266)
*Xueqing Sun,Renzhen Wang,Quanziang Wang,Yichen Wu,Xixi Jia,Deyu Meng*

Main category: cs.LG

TL;DR: Proposes an uncertainty-aware pseudo-labeling framework for semi-supervised regression that dynamically adjusts pseudo-label influence through bi-level optimization to handle heteroscedastic noise and prevent error accumulation.


<details>
  <summary>Details</summary>
Motivation: Pseudo-labeling is under-explored in semi-supervised regression due to continuous outputs with heteroscedastic noise, making reliability assessment challenging and leading to error accumulation and overfitting to incorrect labels.

Method: Uncertainty-aware pseudo-labeling framework using bi-level optimization that jointly minimizes empirical risk over all data while optimizing uncertainty estimates to enhance generalization on labeled data.

Result: Extensive experiments on benchmark SSR datasets demonstrate superior robustness and performance compared to existing methods.

Conclusion: The proposed framework effectively mitigates the impact of unreliable pseudo-labels in semi-supervised regression through uncertainty-aware dynamic adjustment of pseudo-label influence.

Abstract: Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet
its application to semi-supervised regression (SSR) remains relatively
under-explored. Unlike classification, where pseudo-labels are discrete and
confidence-based filtering is effective, SSR involves continuous outputs with
heteroscedastic noise, making it challenging to assess pseudo-label
reliability. As a result, naive pseudo-labeling can lead to error accumulation
and overfitting to incorrect labels. To address this, we propose an
uncertainty-aware pseudo-labeling framework that dynamically adjusts
pseudo-label influence from a bi-level optimization perspective. By jointly
minimizing empirical risk over all data and optimizing uncertainty estimates to
enhance generalization on labeled data, our method effectively mitigates the
impact of unreliable pseudo-labels. We provide theoretical insights and
extensive experiments to validate our approach across various benchmark SSR
datasets, and the results demonstrate superior robustness and performance
compared to existing methods. Our code is available at
https://github.com/sxq/Heteroscedastic-Pseudo-Labels.

</details>


### [51] [Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition](https://arxiv.org/abs/2510.15280)
*Fan Liu,Jindong Han,Tengfei Lyu,Weijia Zhang,Zhe-Rui Yang,Lu Dai,Cancheng Liu,Hao Liu*

Main category: cs.LG

TL;DR: Foundation models are transforming scientific research through a three-stage evolution: from enhancing existing workflows to human-AI collaboration and eventually autonomous scientific discovery.


<details>
  <summary>Details</summary>
Motivation: To examine whether foundation models are merely improving existing scientific methods or fundamentally redefining how science is conducted, and to provide a framework for understanding this transformation.

Method: Proposes a three-stage framework: (1) Meta-Scientific Integration (enhancing traditional workflows), (2) Hybrid Human-AI Co-Creation (active collaboration in problem-solving), and (3) Autonomous Scientific Discovery (independent knowledge generation). Reviews current applications and emerging capabilities.

Result: Identifies that foundation models are catalyzing a transition toward a new scientific paradigm, moving beyond mere acceleration of tasks to fundamentally changing how science is conducted.

Conclusion: Foundation models represent a transformative force in scientific discovery, requiring the community to understand their evolving role and reflect on the future of scientific methodology. The framework helps navigate this transition from enhancement to autonomous discovery.

Abstract: Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the
landscape of scientific research. Beyond accelerating tasks such as hypothesis
generation, experimental design, and result interpretation, they prompt a more
fundamental question: Are FMs merely enhancing existing scientific
methodologies, or are they redefining the way science is conducted? In this
paper, we argue that FMs are catalyzing a transition toward a new scientific
paradigm. We introduce a three-stage framework to describe this evolution: (1)
Meta-Scientific Integration, where FMs enhance workflows within traditional
paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active
collaborators in problem formulation, reasoning, and discovery; and (3)
Autonomous Scientific Discovery, where FMs operate as independent agents
capable of generating new scientific knowledge with minimal human intervention.
Through this lens, we review current applications and emerging capabilities of
FMs across existing scientific paradigms. We further identify risks and future
directions for FM-enabled scientific discovery. This position paper aims to
support the scientific community in understanding the transformative role of
FMs and to foster reflection on the future of scientific discovery. Our project
is available at
https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.

</details>


### [52] [Identifying internal patterns in (1+1)-dimensional directed percolation using neural networks](https://arxiv.org/abs/2510.15294)
*Danil Parkhomenko,Pavel Ovchinnikov,Konstantin Soldatov,Vitalii Kapitan,Gennady Y. Chitov*

Main category: cs.LG

TL;DR: Neural network method combining CNN, TCN and GRU for automatic detection of phase transitions and classification of hidden percolation patterns in (1+1)-dimensional replication processes.


<details>
  <summary>Details</summary>
Motivation: To develop an automated approach for detecting phase transitions and classifying hidden percolation patterns without manual feature extraction, leveraging deep learning capabilities.

Method: Combination of CNN, TCN and GRU networks trained directly on raw configurations without manual feature extraction, applied to (1+1)-dimensional replication processes.

Result: The network successfully reproduces the phase diagram and assigns phase labels to configurations, demonstrating its capability to extract hierarchical structures from raw numerical experiment data.

Conclusion: Deep neural architectures are effective for automatically extracting hierarchical structures and detecting phase transitions from raw data in complex physical systems.

Abstract: In this paper we present a neural network-based method for the automatic
detection of phase transitions and classification of hidden percolation
patterns in a (1+1)-dimensional replication process. The proposed network model
is based on the combination of CNN, TCN and GRU networks, which are trained
directly on raw configurations without any manual feature extraction. The
network reproduces the phase diagram and assigns phase labels to
configurations. It shows that deep architectures are capable of extracting
hierarchical structures from the raw data of numerical experiments.

</details>


### [53] [DFCA: Decentralized Federated Clustering Algorithm](https://arxiv.org/abs/2510.15300)
*Jonas Kirch,Sebastian Becker,Tiago Koketsu Rodrigues,Stefan Harmeling*

Main category: cs.LG

TL;DR: DFCA is a fully decentralized clustered federated learning algorithm that eliminates the need for a central server, enabling clients to collaboratively train cluster-specific models through sequential running average aggregation from neighbors.


<details>
  <summary>Details</summary>
Motivation: Existing clustered federated learning methods like IFCA rely on central servers, creating bottlenecks and single points of failure, limiting their applicability in realistic decentralized settings.

Method: DFCA uses sequential running average to aggregate models from neighbors as updates arrive, providing communication-efficient decentralized clustering without central coordination.

Result: Experiments show DFCA outperforms other decentralized algorithms and performs comparably to centralized IFCA, even under sparse connectivity conditions.

Conclusion: DFCA demonstrates robustness and practicality for dynamic real-world decentralized networks by providing effective decentralized clustered federated learning without central coordination bottlenecks.

Abstract: Clustered Federated Learning has emerged as an effective approach for
handling heterogeneous data across clients by partitioning them into clusters
with similar or identical data distributions. However, most existing methods,
including the Iterative Federated Clustering Algorithm (IFCA), rely on a
central server to coordinate model updates, which creates a bottleneck and a
single point of failure, limiting their applicability in more realistic
decentralized learning settings. In this work, we introduce DFCA, a fully
decentralized clustered FL algorithm that enables clients to collaboratively
train cluster-specific models without central coordination. DFCA uses a
sequential running average to aggregate models from neighbors as updates
arrive, providing a communication-efficient alternative to batch aggregation
while maintaining clustering performance. Our experiments on various datasets
demonstrate that DFCA outperforms other decentralized algorithms and performs
comparably to centralized IFCA, even under sparse connectivity, highlighting
its robustness and practicality for dynamic real-world decentralized networks.

</details>


### [54] [On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions](https://arxiv.org/abs/2510.15327)
*Zailin Ma,Jiansheng Yang,Yaodong Yang*

Main category: cs.LG

TL;DR: This paper provides sharp generalization bounds for Random Feature models with Learnable Activation Functions (RFLAF) using data-dependent sampling, significantly reducing the required number of features for learning in regression and classification tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization properties and reduce the computational complexity of kernel methods by developing sharper bounds on the required number of features through data-dependent sampling schemes.

Method: The authors apply data-dependent sampling schemes (plain sampling and leverage weighted sampling) for generating features in RFLAF models, and propose an algorithm to find approximate kernels for weighted RFLAF learning.

Result: Through weighted sampling, the bound on required features s improves from Ω(1/ε²) to Ω̃((1/ε)^{1/t}) for MSE loss, and to Ω(1) when the Gram matrix has finite rank. For Lipschitz loss, it improves from Ω(1/ε²) to Ω̃((1/ε²)^{1/t}). Empirical results show weighted RFLAF achieves same performance with significantly fewer features.

Conclusion: Data-dependent sampling schemes, particularly leverage weighted sampling, provide substantially sharper generalization bounds for RFLAF models, enabling efficient learning with fewer features while maintaining performance.

Abstract: This paper studies the generalization properties of a recently proposed
kernel method, the Random Feature models with Learnable Activation Functions
(RFLAF). By applying a data-dependent sampling scheme for generating features,
we provide by far the sharpest bounds on the required number of features for
learning RFLAF in both the regression and classification tasks. We provide a
unified theorem that describes the complexity of the feature number $s$, and
discuss the results for the plain sampling scheme and the data-dependent
leverage weighted scheme. Through weighted sampling, the bound on $s$ in the
MSE loss case is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon)^{1/t})$ in general $(t\geq 1)$, and even to
$\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss
case, the bound is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also
propose an algorithm to find an approximate kernel and then apply the leverage
weighted sampling. Empirical results show that the weighted RFLAF achieves the
same performances with a significantly fewer number of features compared to the
plainly sampled RFLAF, validating our theories and the effectiveness of this
method.

</details>


### [55] [Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks](https://arxiv.org/abs/2510.15333)
*Yuyuan Feng,Bin Ma,Enyan Dai*

Main category: cs.LG

TL;DR: A unified defense framework using Mixture of Experts (MoE) architecture to protect graph neural networks against multiple adversarial attacks including backdoor, edge manipulation, and node injection attacks.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural network defenses focus on single attack types, lacking comprehensive protection against multiple simultaneous threats like backdoor, manipulation, and injection attacks.

Method: Proposes MI-based logic diversity loss to make experts focus on different neighborhood structures, and a robustness-aware router that detects perturbations and routes nodes to appropriate robust experts.

Result: Extensive experiments show superior robustness across various adversarial settings compared to existing methods.

Conclusion: The MoE-based unified framework effectively defends against multiple graph adversarial attacks through expert diversity and adaptive routing.

Abstract: Extensive research has highlighted the vulnerability of graph neural networks
(GNNs) to adversarial attacks, including manipulation, node injection, and the
recently emerging threat of backdoor attacks. However, existing defenses
typically focus on a single type of attack, lacking a unified approach to
simultaneously defend against multiple threats. In this work, we leverage the
flexibility of the Mixture of Experts (MoE) architecture to design a scalable
and unified framework for defending against backdoor, edge manipulation, and
node injection attacks. Specifically, we propose an MI-based logic diversity
loss to encourage individual experts to focus on distinct neighborhood
structures in their decision processes, thus ensuring a sufficient subset of
experts remains unaffected under perturbations in local structures. Moreover,
we introduce a robustness-aware router that identifies perturbation patterns
and adaptively routes perturbed nodes to corresponding robust experts.
Extensive experiments conducted under various adversarial settings demonstrate
that our method consistently achieves superior robustness against multiple
graph adversarial attacks.

</details>


### [56] [Sequence Modeling with Spectral Mean Flows](https://arxiv.org/abs/2510.15366)
*Jinwoo Kim,Max Beier,Petar Bevanda,Nayun Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: The paper proposes Spectral Mean Flows, a novel sequence modeling approach that combines operator theory with maximum mean discrepancy gradient flows to learn nonlinear probabilistic state dynamics through tensor network decompositions and simulation-free learning.


<details>
  <summary>Details</summary>
Motivation: Current sequence modeling approaches struggle with representing and learning highly nonlinear and probabilistic state dynamics. Operator theory offers an appealing perspective by viewing dynamics as linear maps on Hilbert spaces, but this perspective has been overlooked in practical implementations.

Method: The method uses an operator-theoretic view of hidden Markov models, embedding full sequence distributions as tensors in product Hilbert spaces. It introduces spectral mean flows with two core components: (1) a neural architecture using spectral decomposition for scalable tensor network decomposition of sequence mean embeddings, and (2) extension of MMD gradient flows to time-dependent Hilbert spaces connected to flow matching via continuity equations.

Result: The approach demonstrates competitive results on various time-series modeling datasets, showing effectiveness in handling nonlinear probabilistic dynamics while overcoming challenges with large tensors and slow sampling convergence.

Conclusion: Spectral Mean Flows provide a tractable and effective framework for sequence modeling by integrating operator theory with modern gradient flow techniques, enabling scalable representation of complex state dynamics and faster sampling through simulation-free learning.

Abstract: A key question in sequence modeling with neural networks is how to represent
and learn highly nonlinear and probabilistic state dynamics. Operator theory
views such dynamics as linear maps on Hilbert spaces containing mean embedding
vectors of distributions, offering an appealing but currently overlooked
perspective. We propose a new approach to sequence modeling based on an
operator-theoretic view of a hidden Markov model (HMM). Instead of
materializing stochastic recurrence, we embed the full sequence distribution as
a tensor in the product Hilbert space. A generative process is then defined as
maximum mean discrepancy (MMD) gradient flow in the space of sequences. To
overcome challenges with large tensors and slow sampling convergence, we
introduce spectral mean flows, a novel tractable algorithm integrating two core
concepts. First, we propose a new neural architecture by leveraging spectral
decomposition of linear operators to derive a scalable tensor network
decomposition of sequence mean embeddings. Second, we extend MMD gradient flows
to time-dependent Hilbert spaces and connect them to flow matching via the
continuity equation, enabling simulation-free learning and faster sampling. We
demonstrate competitive results on a range of time-series modeling datasets.
Code is available at https://github.com/jw9730/spectral-mean-flow.

</details>


### [57] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: BREEZE is an improved zero-shot RL framework that addresses expressivity limitations and OOD issues in Forward-Backward representations through behavioral regularization, diffusion-based policy extraction, and attention-based architectures.


<details>
  <summary>Details</summary>
Motivation: Existing Forward-Backward methods in zero-shot RL suffer from limited expressivity and biased representations due to OOD actions during offline learning, leading to suboptimal performance.

Method: BREEZE introduces behavioral regularization for stable in-sample learning, uses task-conditioned diffusion models for policy extraction, and employs attention-based architectures for expressive representation modeling.

Result: Extensive experiments on ExORL and D4RL Kitchen show BREEZE achieves best or near-best performance with superior robustness compared to prior offline zero-shot RL methods.

Conclusion: BREEZE successfully enhances learning stability, policy extraction capability, and representation quality in zero-shot RL, demonstrating improved performance and robustness over existing methods.

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [58] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: SWFP framework discretizes flow matching inference to align with JKO principle, enabling stable fine-tuning of pre-trained flow policies through incremental transformations with entropic regularization.


<details>
  <summary>Details</summary>
Motivation: Behavior cloning with flow/diffusion policies is vulnerable to distributional shift, and standard RL methods struggle to fine-tune these models due to iterative inference processes and limitations of existing workarounds.

Method: SWFP decomposes global flow into sequence of small incremental transformations between proximate distributions using fixed-step Euler scheme, with each step corresponding to a JKO update regularized by entropic regularization.

Result: Comprehensive experiments demonstrate SWFP's enhanced stability, efficiency, and superior adaptation performance across diverse robotic control benchmarks.

Conclusion: SWFP provides an efficient algorithm for fine-tuning pre-trained flows via cascade of small flow blocks, offering simpler training, reduced computational costs, and provable stability grounded in Wasserstein trust regions.

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [59] [Geometric Mixture Models for Electrolyte Conductivity Prediction](https://arxiv.org/abs/2510.15403)
*Anyi Li,Jiacheng Cen,Songyou Li,Mingze Li,Yang Yu,Wenbing Huang*

Main category: cs.LG

TL;DR: GeoMix is a geometry-aware framework for predicting ionic conductivity in electrolyte systems that addresses challenges in standardized benchmarks and geometric modeling of mixture systems through equivariant message passing.


<details>
  <summary>Details</summary>
Motivation: Current research faces two key challenges: lack of high-quality standardized benchmarks and inadequate modeling of geometric structure and intermolecular interactions in mixture systems.

Method: Reorganized CALiSol and DiffMix datasets with geometric graph representations, then proposed GeoMix framework with Geometric Interaction Network (GIN) for equivariant intermolecular geometric message passing that preserves Set-SE(3) equivariance.

Result: GeoMix consistently outperforms diverse baselines (MLPs, GNNs, and geometric GNNs) across both datasets, validating the importance of cross-molecular geometric interactions and equivariant message passing.

Conclusion: This work establishes new benchmarks for electrolyte research and provides a general geometric learning framework that advances modeling of mixture systems in energy materials, pharmaceutical development, and beyond.

Abstract: Accurate prediction of ionic conductivity in electrolyte systems is crucial
for advancing numerous scientific and technological applications. While
significant progress has been made, current research faces two fundamental
challenges: (1) the lack of high-quality standardized benchmarks, and (2)
inadequate modeling of geometric structure and intermolecular interactions in
mixture systems. To address these limitations, we first reorganize and enhance
the CALiSol and DiffMix electrolyte datasets by incorporating geometric graph
representations of molecules. We then propose GeoMix, a novel geometry-aware
framework that preserves Set-SE(3) equivariance-an essential but challenging
property for mixture systems. At the heart of GeoMix lies the Geometric
Interaction Network (GIN), an equivariant module specifically designed for
intermolecular geometric message passing. Comprehensive experiments demonstrate
that GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,
and geometric GNNs) across both datasets, validating the importance of
cross-molecular geometric interactions and equivariant message passing for
accurate property prediction. This work not only establishes new benchmarks for
electrolyte research but also provides a general geometric learning framework
that advances modeling of mixture systems in energy materials, pharmaceutical
development, and beyond.

</details>


### [60] [Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing](https://arxiv.org/abs/2510.15404)
*Christopher Salazar,Krithika Manohar,Ashis G. Banerjee*

Main category: cs.LG

TL;DR: WORK-DMD is an online forecasting method that combines Random Fourier Features with Dynamic Mode Decomposition to handle non-stationary streaming data with fixed computational cost and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Address challenges in real-time forecasting from streaming data: handling non-stationary dynamics, operating under strict computational limits, and adapting rapidly without catastrophic forgetting, while overcoming trade-offs between accuracy, adaptability, and efficiency in constrained computing environments.

Method: Combines Random Fourier Features with online Dynamic Mode Decomposition to capture nonlinear dynamics through explicit feature mapping. Uses Sherman-Morrison updates within rolling windows for continuous adaptation from current data only, eliminating need for lengthy training or large historical data storage.

Result: Achieves higher accuracy than several state-of-the-art online forecasting methods across benchmark datasets, requires only single pass through data, and demonstrates particularly strong performance in short-term forecasting.

Conclusion: Combining kernel evaluations with adaptive matrix updates achieves strong predictive performance with minimal data requirements, offering a practical alternative to deep learning for streaming forecasting applications.

Abstract: Real-time forecasting from streaming data poses critical challenges: handling
non-stationary dynamics, operating under strict computational limits, and
adapting rapidly without catastrophic forgetting. However, many existing
approaches face trade-offs between accuracy, adaptability, and efficiency,
particularly when deployed in constrained computing environments. We introduce
WORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method
that combines Random Fourier Features with online Dynamic Mode Decomposition to
capture nonlinear dynamics through explicit feature mapping, while preserving
fixed computational cost and competitive predictive accuracy across evolving
data. WORK-DMD employs Sherman-Morrison updates within rolling windows,
enabling continuous adaptation to evolving dynamics from only current data,
eliminating the need for lengthy training or large storage requirements for
historical data. Experiments on benchmark datasets across several domains show
that WORK-DMD achieves higher accuracy than several state-of-the-art online
forecasting methods, while requiring only a single pass through the data and
demonstrating particularly strong performance in short-term forecasting. Our
results show that combining kernel evaluations with adaptive matrix updates
achieves strong predictive performance with minimal data requirements. This
sample efficiency offers a practical alternative to deep learning for streaming
forecasting applications.

</details>


### [61] [ParaFormer: Shallow Parallel Transformers with Progressive Approximation](https://arxiv.org/abs/2510.15425)
*Wei Wang,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: ParaFormer is a shallow Transformer architecture that achieves parallelism by organizing layers into parallel branches with algorithmic inter-layer collaboration, enabling faster convergence, model compression, and improved performance compared to standard Transformers.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of deep Transformers including long training times, high inference latency, and impracticality on resource-constrained devices, while challenging the 'deeper is better' philosophy.

Method: Formulate Transformers as function approximators in closed-form based on Universal Approximation Theorem, then organize layers into parallel branches with progressive approximation that ensures each new branch reduces loss from preceding branches.

Result: Outperforms standard Transformers like ViT, supports up to 15.07x model compression, enables 3.30x faster parallel deployment than FairScale, and facilitates adaptive continuous learning through model expansion.

Conclusion: The closed-form formulation of Transformers explains the 'depth belief' and enables efficient parallel architectures like ParaFormer, opening new avenues for designing efficient Transformer models without depth dependency.

Abstract: The widespread 'deeper is better' philosophy has driven the creation of
architectures like ResNet and Transformer, which achieve high performance by
stacking numerous layers. However, increasing model depth comes with challenges
such as longer training times, higher inference latency, and impracticality on
resource-constrained devices. To address these issues, we propose ParaFormer, a
shallow Transformer architecture designed for true parallelism in both
structure and computation. By formulating standard Transformers as function
approximators in closed-form, our theoretical analysis shows that their
performance relies on inter-layer collaboration for progressive approximation,
rather than depth itself. While deep Transformers enforce this collaboration
through sequential designs, we demonstrate that such collaboration is not
inherently tied to sequential structures. ParaFormer removes the sequential
constraint by organizing layers into parallel branches, enforcing inter-layer
collaboration algorithmically. Specifically, we implement progressive
approximation, ensuring that each new branch further reduces the loss from
preceding branches, enabling faster convergence. Extensive experiments validate
ParaFormer's effectiveness, outperforming standard Transformers like ViT.
Moreover, ParaFormer supports up to 15.07x model compression and facilitates
model expansion for adaptive continuous learning. Experimental results on
multi-GPU deployment demonstrate that ParaFormer is 3.30x faster than widely
used parallelism solutions such as FairScale. These advancements stem from our
closed-form formulation of Transformers based on the Universal Approximation
Theorem, which not only explains the ``depth belief'' but also opens new
avenues for designing efficient Transformer architectures. Source code:
https://(open-upon-acceptance)

</details>


### [62] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: This dissertation develops safe, sample-efficient RL methods for ranking systems and text-to-image diffusion models, featuring exposure-based safety guarantees, optimal baseline corrections, and the LOOP algorithm for improved generative alignment.


<details>
  <summary>Details</summary>
Motivation: To address the need for reinforcement learning methods that are simultaneously safe, sample-efficient, and robust across different application domains including ranking systems and generative models.

Method: Developed contextual-bandit RL framework with: 1) Exposure-based generalization bounds and counterfactual risk minimization for safe ranking deployment; 2) Baseline-correction framework unifying off-policy estimators with optimal baseline; 3) LOOP algorithm combining PPO and REINFORCE for diffusion models.

Result: Achieved safety guarantees preventing underperformance of logging policies even with sparse feedback, reduced variance in off-policy learning through optimal baselines, and improved text-to-image alignment with PPO-level efficiency via LOOP algorithm.

Conclusion: The dissertation successfully demonstrates how RL can be made safe and efficient across diverse applications, providing theoretical guarantees and practical algorithms for real-world deployment in ranking systems and generative modeling.

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [63] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: This paper provides the first theoretical framework for analyzing sampling-based test-time scaling methods in LLMs, identifies limitations in existing approaches (self-consistency and perplexity), and introduces RPC - a hybrid method that combines Perplexity Consistency and Reasoning Pruning to improve reasoning performance while reducing sampling costs.


<details>
  <summary>Details</summary>
Motivation: Despite the practical success of sampling-based test-time scaling methods that generate multiple reasoning paths during inference, their theoretical foundations remain underexplored. The paper aims to establish a theoretical framework to understand and improve these methods.

Method: The authors introduce RPC (Reasoning Pruning with Perplexity Consistency), a hybrid method with two key components: 1) Perplexity Consistency that combines strengths of self-consistency and perplexity to boost estimation error convergence rate from linear to exponential, and 2) Reasoning Pruning that eliminates low-probability reasoning paths to prevent degradation.

Result: Empirical results across seven benchmark datasets demonstrate that RPC achieves reasoning performance comparable to self-consistency while enhancing confidence reliability and reducing sampling costs by 50%. The method shows strong potential for reducing reasoning error.

Conclusion: RPC provides an effective solution to the limitations of existing test-time scaling methods by leveraging theoretical insights to create a hybrid approach that improves reasoning performance while being more computationally efficient.

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [64] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: The paper proposes a method to incorporate causal information via Temporal Logic-based Causal Diagrams into probabilistic reward machines to improve reinforcement learning in sparse-reward environments and aid task transfer.


<details>
  <summary>Details</summary>
Motivation: RL algorithms struggle with sparse rewards and complex temporal dependencies. Probabilistic reward machines help but are difficult to design manually and transfer across domains with different causal structures.

Method: Incorporates causal information using Temporal Logic-based Causal Diagrams into the reward formalism to expedite policy learning and facilitate task specification transfer.

Result: The method provides theoretical convergence guarantees to optimal policy and demonstrates empirical strengths in improving learning efficiency.

Conclusion: The proposed approach successfully integrates causal knowledge into reward machines, enhancing RL performance in sparse-reward environments and enabling better transfer of task specifications across domains.

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [65] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: OffSim is a model-based offline inverse reinforcement learning framework that learns environmental dynamics and reward functions from expert trajectories, enabling offline policy training without real environment interaction.


<details>
  <summary>Details</summary>
Motivation: Traditional RL requires time-consuming simulator development and manual reward function design. OffSim addresses this by learning both dynamics and rewards directly from expert data.

Method: Jointly optimizes high-entropy transition model and IRL-based reward function from state-action trajectories. OffSim+ extends this with marginal reward for multi-dataset settings.

Result: Extensive MuJoCo experiments show substantial performance gains over existing offline IRL methods, demonstrating efficacy and robustness.

Conclusion: OffSim provides an effective framework for learning simulators and rewards from expert data, enabling offline policy training without environment interaction.

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [66] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: SESA is a sequential sampling framework that addresses exploration limitations in RL-trained LLMs by generating diverse solution sketches sequentially, preventing entropy collapse and improving performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL methods for LLMs suffer from limited exploration and entropy collapse, where models exploit narrow solution sets, reducing sampling diversity and preventing further performance improvements, especially in parallel sampling scenarios.

Method: Proposed SESA framework generates diverse solution sketches sequentially before expanding them into full reasoning paths, conditioning each new output on previous ones to ensure broader exploration and prevent policy collapse.

Result: Experiments show sequential sampling outperforms traditional RL in path diversity and recovery from collapse. On three agent benchmarks, SESA improves success rates by +0.25, +0.42, and +0.07 absolute over base model (up to 211% relative improvement over baseline RL).

Conclusion: SESA introduces a structured approach to exploration that enables more effective and diverse reasoning in RL-trained LLMs, demonstrating significant improvements in both exploration and overall performance.

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [67] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: Transformers are proven to be injective (lossless) despite non-linear components, enabling exact input recovery from hidden representations using the SipIt algorithm.


<details>
  <summary>Details</summary>
Motivation: Challenge the view that transformer non-linearities prevent exact input recovery, aiming to establish injectivity as a fundamental property for transparency and interpretability.

Method: Mathematical proof of injectivity at initialization and during training, empirical collision tests on six language models, and development of SipIt algorithm for exact input reconstruction.

Result: No collisions observed in billions of tests, SipIt achieves linear-time exact input recovery from hidden activations in practice.

Conclusion: Injectivity is a fundamental property of language models with implications for transparency, interpretability, and safe deployment.

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


### [68] [Revisiting Knowledge Distillation: The Hidden Role of Dataset Size](https://arxiv.org/abs/2510.15516)
*Giulia Lanzillotta,Felix Sarnthein,Gil Kur,Thomas Hofmann,Bobby He*

Main category: cs.LG

TL;DR: Knowledge distillation's effectiveness increases in low-data regimes, disproving the label smoothing hypothesis and supporting dark knowledge theory.


<details>
  <summary>Details</summary>
Motivation: To understand how knowledge distillation works by examining its relationship with dataset size, as previous studies focused mainly on model size and generalization.

Method: Conducted extensive experiments across various datasets, tasks, and neural architectures while systematically varying dataset sizes to analyze distillation effects.

Result: Distillation's benefits are amplified in low-data settings, disproving the label smoothing hypothesis and providing evidence for dark knowledge theory.

Conclusion: Dataset size is a fundamental but overlooked variable in understanding knowledge distillation mechanisms, with distillation showing enhanced data efficiency in limited-data scenarios.

Abstract: The concept of knowledge distillation (KD) describes the training of a
student model from a teacher model and is a widely adopted technique in deep
learning. However, it is still not clear how and why distillation works.
Previous studies focus on two central aspects of distillation: model size, and
generalisation. In this work we study distillation in a third dimension:
dataset size. We present a suite of experiments across a wide range of
datasets, tasks and neural architectures, demonstrating that the effect of
distillation is not only preserved but amplified in low-data regimes. We call
this newly discovered property the data efficiency of distillation. Equipped
with this new perspective, we test the predictive power of existing theories of
KD as we vary the dataset size. Our results disprove the hypothesis that
distillation can be understood as label smoothing, and provide further evidence
in support of the dark knowledge hypothesis. Finally, we analyse the impact of
modelling factors such as the objective, scale and relative number of samples
on the observed phenomenon. Ultimately, this work reveals that the dataset size
may be a fundamental but overlooked variable in the mechanisms underpinning
distillation.

</details>


### [69] [Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation](https://arxiv.org/abs/2510.15535)
*Abhay Kumar Dwivedi,Shanu Saklani,Soumya Dutta*

Main category: cs.LG

TL;DR: The paper develops compressed neural representations for multivariate datasets using a single network with parameter sharing to achieve state-of-the-art data compression.


<details>
  <summary>Details</summary>
Motivation: The extensive adoption of Deep Neural Networks in scientific visualization tasks and recent successes in compressed data models using implicit neural representations for spatiotemporal volume visualization and super-resolution inspired the development of neural representations for multivariate datasets.

Method: Utilizes a single network to learn representations for all data variables simultaneously through parameter sharing, enabling compressed neural representations for datasets containing tens to hundreds of variables.

Result: Achieves state-of-the-art data compression with superior performance in reconstructed data quality, rendering and visualization quality, preservation of dependency information among variables, and storage efficiency.

Conclusion: The approach demonstrates effective compressed neural representations for multivariate datasets through comprehensive evaluations showing excellent performance across multiple metrics.

Abstract: The extensive adoption of Deep Neural Networks has led to their increased
utilization in challenging scientific visualization tasks. Recent advancements
in building compressed data models using implicit neural representations have
shown promising results for tasks like spatiotemporal volume visualization and
super-resolution. Inspired by these successes, we develop compressed neural
representations for multivariate datasets containing tens to hundreds of
variables. Our approach utilizes a single network to learn representations for
all data variables simultaneously through parameter sharing. This allows us to
achieve state-of-the-art data compression. Through comprehensive evaluations,
we demonstrate superior performance in terms of reconstructed data quality,
rendering and visualization quality, preservation of dependency information
among variables, and storage efficiency.

</details>


### [70] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: MC Dropout uncertainty shows weak correlation with segmentation errors in brain tumor MRI, especially near boundaries, suggesting limited utility for error localization in medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of MC Dropout-based uncertainty in identifying segmentation errors, particularly near tumor boundaries, for brain tumor MRI segmentation.

Method: Used U-Net trained under four augmentation settings (none, horizontal flip, rotation, scaling), computed uncertainty from 50 stochastic forward passes, and correlated with pixel-wise errors using Pearson and Spearman coefficients.

Result: Weak global correlations (r ≈ 0.30-0.38) and negligible boundary correlations (|r| < 0.05). Differences across augmentations were statistically significant but lacked practical relevance.

Conclusion: MC Dropout uncertainty provides limited cues for boundary error localization, highlighting the need for alternative or hybrid uncertainty estimation methods in medical image segmentation.

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [71] [Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems](https://arxiv.org/abs/2510.15555)
*Sibo Xiao*

Main category: cs.LG

TL;DR: SDR is a novel causal inference framework combining strategic equilibrium modeling with doubly robust estimation to handle endogenous treatment from strategic agent behavior, achieving significant bias reduction.


<details>
  <summary>Details</summary>
Motivation: Address endogenous treatment assignment caused by strategic agent behavior in causal inference, where traditional methods fail to account for agents' strategic responses to interventions.

Method: Integrates strategic equilibrium modeling with doubly robust estimation, maintaining double robustness while incorporating strategic considerations through strategic unconfoundedness assumptions.

Result: Achieves 7.6%-29.3% bias reduction over baseline methods across varying strategic strengths, maintains robust scalability with agent populations, and demonstrates superior performance in empirical evaluations.

Conclusion: SDR provides a principled framework for reliable causal inference in strategic environments where agents respond strategically to interventions, with proven theoretical properties and empirical effectiveness.

Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework
that integrates strategic equilibrium modeling with doubly robust estimation
for causal inference in strategic environments. SDR addresses endogenous
treatment assignment arising from strategic agent behavior, maintaining double
robustness while incorporating strategic considerations. Theoretical analysis
confirms SDR's consistency and asymptotic normality under strategic
unconfoundedness. Empirical evaluations demonstrate SDR's superior performance
over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying
strategic strengths and maintaining robust scalability with agent populations.
The framework provides a principled approach for reliable causal inference when
agents respond strategically to interventions.

</details>


### [72] [On the Neural Feature Ansatz for Deep Neural Networks](https://arxiv.org/abs/2510.15563)
*Edward Tansley,Estelle Massart,Coralia Cartis*

Main category: cs.LG

TL;DR: The paper extends the Neural Feature Ansatz (NFA) to deep linear networks, showing a depth-dependent exponent α=1/L, and proves asymptotic NFA for unbalanced initialization with weight decay. It also provides counterexamples for nonlinear networks.


<details>
  <summary>Details</summary>
Motivation: To understand feature learning in deep neural networks by extending the Neural Feature Ansatz to multi-layer architectures and investigating its depth dependency and conditions for validity.

Method: Theoretical analysis using gradient flow dynamics with balanced/unbalanced weight initialization, mathematical proofs for linear networks, counterexamples for nonlinear architectures, and numerical validation across various optimization settings.

Result: NFA holds with exponent α=1/L for L-layer linear networks, holds asymptotically for unbalanced initialization with weight decay, but fails for some nonlinear networks even with perfect training fit.

Conclusion: The NFA exhibits depth dependency in linear networks and requires specific conditions (weight decay) for unbalanced initialization, but may not generalize to nonlinear architectures despite good training performance.

Abstract: Understanding feature learning is an important open question in establishing
a mathematical foundation for deep neural networks. The Neural Feature Ansatz
(NFA) states that after training, the Gram matrix of the first-layer weights of
a deep neural network is proportional to some power $\alpha>0$ of the average
gradient outer product (AGOP) of this network with respect to its inputs.
Assuming gradient flow dynamics with balanced weight initialization, the NFA
was proven to hold throughout training for two-layer linear networks with
exponent $\alpha = 1/2$ (Radhakrishnan et al., 2024). We extend this result to
networks with $L \geq 2$ layers, showing that the NFA holds with exponent
$\alpha = 1/L$, thus demonstrating a depth dependency of the NFA. Furthermore,
we prove that for unbalanced initialization, the NFA holds asymptotically
through training if weight decay is applied. We also provide counterexamples
showing that the NFA does not hold for some network architectures with
nonlinear activations, even when these networks fit arbitrarily well the
training data. We thoroughly validate our theoretical results through numerical
experiments across a variety of optimization algorithms, weight decay rates and
initialization schemes.

</details>


### [73] [Attn-JGNN: Attention Enhanced Join-Graph Neural Networks](https://arxiv.org/abs/2510.15583)
*Jixin Zhang,Yong Lai*

Main category: cs.LG

TL;DR: Attn-JGNN is an attention-enhanced join-graph neural network model that improves #SAT solving accuracy by using tree decomposition, iterative message passing, and attention mechanisms to focus on key variables and clusters.


<details>
  <summary>Details</summary>
Motivation: To improve the solving accuracy of #SAT problems by leveraging neural networks and attention mechanisms to enhance probabilistic inference in constraint satisfaction problems.

Method: Uses tree decomposition to encode CNF formulas into join-graphs, performs iterative message passing on the graphs, applies attention mechanisms within and between clusters to focus on key variables, and learns partition functions to approximate model counts.

Result: Attn-JGNN achieves better results than other neural network methods for #SAT problem solving, demonstrating improved accuracy.

Conclusion: The attention-enhanced join-graph neural network approach effectively improves #SAT solving accuracy by focusing computational resources on critical variables and clusters while reducing redundant calculations.

Abstract: We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) model
for solving #SAT problems, which significantly improves the solving accuracy.
Inspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNN
uses tree decomposition to encode the CNF formula into a join-graph, then
performs iterative message passing on the join-graph, and finally approximates
the model number by learning partition functions. In order to further improve
the accuracy of the solution, we apply the attention mechanism in and between
clusters of the join-graphs, which makes Attn-JGNN pay more attention to the
key variables and clusters in probabilistic inference, and reduces the
redundant calculation. Finally, our experiments show that our Attn-JGNN model
achieves better results than other neural network methods.

</details>


### [74] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: GRATING is a training-free inference system that accelerates semantic top-K selection by exploiting sequence-level sparsity and progressive cluster pruning, achieving up to 89% latency reduction and 95% memory savings without precision loss.


<details>
  <summary>Details</summary>
Motivation: Semantic top-K selection with cross-encoder rerankers dominates latency and memory budgets on edge hardware for on-device AI services like retrieval-augmented generation and personalized recommendation.

Method: Monolithic forwarding with progressive cluster pruning that leverages sequence-level sparsity (relative rankings stabilize early in intermediate layers), plus dual-layer sliding window and chunked execution for memory management.

Result: GRATING reduces latency by up to 89.0% and peak memory by up to 94.9% in microbenchmarks, and 11.6%-51.0% latency reduction and 18.6%-77.8% memory savings across real-world applications, with no precision loss.

Conclusion: GRATING demonstrates substantial efficiency improvements for on-device AI services by exploiting relative ranking properties and sequence-level sparsity, enabling more deployable edge AI systems.

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [75] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: CQD-SHAP is a novel framework that uses Shapley values to explain the contribution of each query part in complex query answering over knowledge graphs, addressing the black-box nature of existing methods.


<details>
  <summary>Details</summary>
Motivation: Current neural and neurosymbolic complex query answering methods are mostly black-box models, raising trust concerns. Even neurosymbolic approaches like CQD lack explanations for the importance of different query parts.

Method: Proposed CQD-SHAP framework based on Shapley values from cooperative game theory to compute the contribution of each query part to answer ranking, satisfying all fundamental Shapley axioms.

Result: Automated evaluation shows CQD-SHAP's effectiveness for most query types in terms of necessary and sufficient explanations, outperforming various baselines.

Conclusion: CQD-SHAP successfully provides interpretable explanations for complex query answering by quantifying the value of neural predictors in inferring new knowledge from incomplete knowledge graphs.

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [76] [Decentralized Parameter-Free Online Learning](https://arxiv.org/abs/2510.15644)
*Tomas Ortega,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: First parameter-free decentralized online learning algorithms with sublinear network regret guarantees, connecting multi-agent coin-betting and decentralized learning via gossip steps.


<details>
  <summary>Details</summary>
Motivation: To develop decentralized online learning algorithms that achieve sublinear regret without requiring hyperparameter tuning, addressing the need for practical distributed learning systems.

Method: Proposes a family of algorithms combining multi-agent coin-betting with gossip steps, introducing a novel "betting function" formulation to simplify multi-agent regret analysis.

Result: Achieves sublinear network regret bounds, validated through experiments on synthetic and real datasets.

Conclusion: The parameter-free decentralized algorithms are applicable to distributed sensing, decentralized optimization, and collaborative machine learning applications.

Abstract: We propose the first parameter-free decentralized online learning algorithms
with network regret guarantees, which achieve sublinear regret without
requiring hyperparameter tuning. This family of algorithms connects multi-agent
coin-betting and decentralized online learning via gossip steps. To enable our
decentralized analysis, we introduce a novel "betting function" formulation for
coin-betting that simplifies the multi-agent regret analysis. Our analysis
shows sublinear network regret bounds and is validated through experiments on
synthetic and real datasets. This family of algorithms is applicable to
distributed sensing, decentralized optimization, and collaborative ML
applications.

</details>


### [77] [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)
*Ziqian Li,Kang Liu,Yongcun Song,Hangrui Yue,Enrique Zuazua*

Main category: cs.LG

TL;DR: NODE-ONet is a neural ODE operator network that incorporates PDE physics to improve temporal dynamics modeling and generalization beyond training time frames.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning approaches often ignore domain knowledge from PDEs, leading to poor temporal dynamics capture and generalization issues beyond training periods.

Method: Encoder-decoder architecture with three components: spatial encoder, neural ODE for latent temporal dynamics, and decoder for physical space reconstruction. Uses physics-encoded neural ODEs to incorporate PDE-specific properties.

Result: Demonstrates high accuracy, computational efficiency, and prediction capabilities beyond training time frames on nonlinear diffusion-reaction and Navier-Stokes equations. Reduces framework complexity while enhancing numerical efficiency and generalization.

Conclusion: NODE-ONet provides a scalable, physics-encoded framework for scientific machine learning with flexibility for diverse encoders/decoders and generalization across related PDE families.

Abstract: Operator learning has emerged as a promising paradigm for developing
efficient surrogate models to solve partial differential equations (PDEs).
However, existing approaches often overlook the domain knowledge inherent in
the underlying PDEs and hence suffer from challenges in capturing temporal
dynamics and generalization issues beyond training time frames. This paper
introduces a deep neural ordinary differential equation (ODE) operator network
framework, termed NODE-ONet, to alleviate these limitations. The framework
adopts an encoder-decoder architecture comprising three core components: an
encoder that spatially discretizes input functions, a neural ODE capturing
latent temporal dynamics, and a decoder reconstructing solutions in physical
spaces. Theoretically, error analysis for the encoder-decoder architecture is
investigated. Computationally, we propose novel physics-encoded neural ODEs to
incorporate PDE-specific physical properties. Such well-designed neural ODEs
significantly reduce the framework's complexity while enhancing numerical
efficiency, robustness, applicability, and generalization capacity. Numerical
experiments on nonlinear diffusion-reaction and Navier-Stokes equations
demonstrate high accuracy, computational efficiency, and prediction
capabilities beyond training time frames. Additionally, the framework's
flexibility to accommodate diverse encoders/decoders and its ability to
generalize across related PDE families further underscore its potential as a
scalable, physics-encoded tool for scientific machine learning.

</details>


### [78] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: The paper proposes an optimized Tsetlin Machine implementation using bitwise operations and early exit mechanisms to achieve up to 96.71% inference time reduction on ARM processors while maintaining code density.


<details>
  <summary>Details</summary>
Motivation: To leverage the Tsetlin Machine's logic-driven operations for high-speed inference on resource-constrained devices like CPUs, particularly taking advantage of modern CPU architectures' parallel execution capabilities.

Method: Uses instruction-level bitwise operations for compact model representation and accelerated processing, introduces an early exit mechanism to avoid unnecessary computations, and implements a literal reorder strategy applied during post-training through statistical analysis of literals and Tsetlin Automata actions.

Result: Experimental results using gem5 simulator with ARM processor show inference time reduction of up to 96.71% compared to conventional integer-based TM implementations while maintaining comparable code density.

Conclusion: The proposed optimization techniques significantly improve Tsetlin Machine inference speed on CPU architectures through bitwise operations and early exit mechanisms, making it highly suitable for resource-constrained devices.

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [79] [WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables](https://arxiv.org/abs/2510.15655)
*Lino Gerlach,Liv Våge,Thore Gerlach,Elliott Kauffman*

Main category: cs.LG

TL;DR: WARP-LUTs is a novel gradient-based method that efficiently learns combinations of logic gates with fewer parameters, achieving faster convergence on CIFAR-10 compared to DLGNs while maintaining comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing multiplication-free models like DLGNs suffer from high computational cost during training and poor generalization to logic blocks with more inputs, creating a need for more efficient training methods.

Method: WARP-LUTs uses Walsh-Assisted Relaxation for Probabilistic Look-Up Tables, a gradient-based approach that learns optimal combinations of logic gates with substantially fewer trainable parameters.

Result: WARP-LUTs achieve significantly faster convergence on CIFAR-10 compared to DLGNs while maintaining comparable accuracy, suggesting potential for extension to higher-input logic blocks.

Conclusion: The approach shows promise for extremely efficient deployment on modern FPGAs and real-time science applications, motivating future research in this direction.

Abstract: Fast and efficient machine learning is of growing interest to the scientific
community and has spurred significant research into novel model architectures
and hardware-aware design. Recent hard? and software co-design approaches have
demonstrated impressive results with entirely multiplication-free models.
Differentiable Logic Gate Networks (DLGNs), for instance, provide a
gradient-based framework for learning optimal combinations of low-level logic
gates, setting state-of-the-art trade-offs between accuracy, resource usage,
and latency. However, these models suffer from high computational cost during
training and do not generalize well to logic blocks with more inputs. In this
work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables
(WARP-LUTs) - a novel gradient-based method that efficiently learns
combinations of logic gates with substantially fewer trainable parameters. We
demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10
compared to DLGNs, while maintaining comparable accuracy. Furthermore, our
approach suggests potential for extension to higher-input logic blocks,
motivating future research on extremely efficient deployment on modern FPGAs
and its real-time science applications.

</details>


### [80] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: CarBoN is a test-time calibration framework that improves reasoning efficiency by adaptively guiding language model generation toward high-reward paths using input-specific temperature and shift parameters, achieving up to 4x fewer rollouts for the same accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling methods like Best-of-N sampling show diminishing returns as N increases, leading to inefficient computation during inference for reasoning tasks.

Method: A two-phase method that first explores the solution space, then learns calibration of logits via input-specific temperature T and additive shift vector δ to guide generation toward reliable reasoning paths.

Result: Experiments on MATH-500 and AIME-2024 show CarBoN improves efficiency with up to 4x fewer rollouts to reach same accuracy, often achieving higher accuracy under fixed budgets.

Conclusion: The framework provides theoretical guarantees for improving expected reward lower bounds without LLM retraining, and generalizes to step-level sampling strategies like beam search.

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [81] [Constrained Adversarial Perturbation](https://arxiv.org/abs/2510.15699)
*Virendra Nishad,Bhaskar Mukhoty,Hilal AlQuabeh,Sandeep K. Shukla,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: This paper proposes CAP (Constrained Adversarial Perturbation), a method for generating universal adversarial perturbations that respect domain-specific feature constraints, achieving higher attack success rates with reduced runtime across finance, IT networks, and cyber-physical systems.


<details>
  <summary>Details</summary>
Motivation: Current universal adversarial perturbation methods ignore domain-specific constraints that govern feature relationships, making adversarial examples implausible or easily detectable in real-world applications like credit scoring and network communication.

Method: The authors formulate an augmented Lagrangian-based min-max optimization problem to enforce multiple complex constraints and propose CAP, a gradient-based alternating optimization algorithm that solves this problem efficiently.

Result: CAP achieves higher attack success rates while significantly reducing runtime compared to existing baselines across diverse domains including finance, IT networks, and cyber-physical systems. It also generalizes well to individual adversarial perturbations.

Conclusion: The proposed CAP method effectively generates constrained adversarial perturbations, and the authors introduce a principled procedure for learning feature constraints directly from data, enabling broad applicability across domains with structured input spaces.

Abstract: Deep neural networks have achieved remarkable success in a wide range of
classification tasks. However, they remain highly susceptible to adversarial
examples - inputs that are subtly perturbed to induce misclassification while
appearing unchanged to humans. Among various attack strategies, Universal
Adversarial Perturbations (UAPs) have emerged as a powerful tool for both
stress testing model robustness and facilitating scalable adversarial training.
Despite their effectiveness, most existing UAP methods neglect domain specific
constraints that govern feature relationships. Violating such constraints, such
as debt to income ratios in credit scoring or packet flow invariants in network
communication, can render adversarial examples implausible or easily
detectable, thereby limiting their real world applicability.
  In this work, we advance universal adversarial attacks to constrained feature
spaces by formulating an augmented Lagrangian based min max optimization
problem that enforces multiple, potentially complex constraints of varying
importance. We propose Constrained Adversarial Perturbation (CAP), an efficient
algorithm that solves this problem using a gradient based alternating
optimization strategy. We evaluate CAP across diverse domains including
finance, IT networks, and cyber physical systems, and demonstrate that it
achieves higher attack success rates while significantly reducing runtime
compared to existing baselines. Our approach also generalizes seamlessly to
individual adversarial perturbations, where we observe similar strong
performance gains. Finally, we introduce a principled procedure for learning
feature constraints directly from data, enabling broad applicability across
domains with structured input spaces.

</details>


### [82] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer is a language model trained via expert iteration and RL to simplify Lean proofs without human supervision, achieving 49-87% length reduction on benchmarks while maintaining correctness.


<details>
  <summary>Details</summary>
Motivation: Neural theorem provers generate excessively long proofs that are mechanically verified but difficult for humans to comprehend, limiting mathematical insight. Proof simplification is a critical bottleneck with scarce training data.

Method: Trained via expert iteration and reinforcement learning using Lean to verify simplifications and provide training signal. Operates within an iterative proof-shortening workflow at inference time.

Result: Substantially compresses proofs: 87% reduction on miniF2F, 57% on PutnamBench, 49% on Seed-Prover's IMO 2025 proofs. Simplified proofs check faster in Lean and improve downstream prover performance when reused as training data.

Conclusion: ProofOptimizer effectively addresses the proof simplification bottleneck, enabling more comprehensible and efficient formal proofs while improving downstream theorem proving performance.

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [83] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: ProSh is a model-free safe RL algorithm that uses risk augmentation and policy shielding to ensure safety under cost constraints while preserving optimality in deterministic environments.


<details>
  <summary>Details</summary>
Motivation: Safety is critical for deploying RL systems, requiring formal guarantees about safety while maintaining optimal performance.

Method: Augments Constrained MDP state space with risk budget and applies shield to policy distribution using learned cost critic to ensure sampled actions remain safe in expectation.

Result: Provides tight upper-bound on expected cost depending on backup-critic accuracy, and guarantees safety during training under practical assumptions.

Conclusion: ProSh enables safe reinforcement learning with formal safety guarantees while preserving optimality in deterministic settings.

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [84] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: A novel RL approach using automaton-based preferences instead of explicit reward functions, with static and dynamic methods for learning policies in environments with complex temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Traditional RL struggles with complex, history-dependent reward structures that require manual reward engineering. The paper aims to eliminate this need by using automaton-based feedback.

Method: Leverages deterministic finite automaton (DFA) to generate preferences over trajectories, learning reward functions automatically. Two approaches: static (direct policy optimization) and dynamic (iterative reward and policy refinement).

Result: Outperforms traditional reward engineering and automaton-based baselines in both discrete and continuous environments, effectively handling temporal dependencies and non-Markovian rewards.

Conclusion: Automaton-based preferences provide a scalable, efficient, human-independent alternative to traditional reward modeling, with proven convergence guarantees for near-optimal policies.

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


### [85] [A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis](https://arxiv.org/abs/2510.15750)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) outperform 3D U-Nets as FEA surrogates for parametric I-beams, with Physics-Informed Neural Networks (PINN) enhancing generalization. MPNN PINN offers the best balance of accuracy, speed, and practicality.


<details>
  <summary>Details</summary>
Motivation: FEA is computationally expensive for design optimization, and deep learning models can provide efficient surrogates, but selecting the right architecture is challenging.

Method: Comprehensive evaluation of GNNs and 3D U-Nets with Physics-Informed Neural Network (PINN) framework using Navier-Cauchy equations, employing curriculum learning with pretraining followed by physics-informed fine-tuning.

Result: GNNs significantly outperform U-Nets, with MPNN and Graph Transformers achieving lowest errors (3.5% and 2.6% L2). PINN improved generalization by up to 11.3%. MPNN PINN provides best practical balance between performance and speed.

Conclusion: GNNs are superior FEA surrogates over U-Nets, with PINN enhancing generalization. MPNN PINN offers the most practical solution combining good accuracy, model size, and inference speed.

Abstract: Although Finite Element Analysis (FEA) is an integral part of the product
design lifecycle, the analysis is computationally expensive, making it
unsuitable for many design optimization problems. The deep learning models can
be a great solution. However, selecting the architecture that emulates the FEA
with great accuracy is a challenge. This paper presents a comprehensive
evaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEA
of parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)
framework, governed by the Navier Cauchy equations, to enforce physical laws.
Crucially, we demonstrate that a curriculum learning strategy, pretraining on
data followed by physics informed fine tuning, is essential for stabilizing
training. Our results show that GNNs fundamentally outperform the U-Net. Even
the worst performer among GNNs, the GCN framework, achieved a relative L2 error
of 8.7% while the best framework among U Net, U Net with attention mechanism
trained on high resolution data, achieved 13.0% score. Among the graph-based
architectures, the Message Passing Neural Networks (MPNN) and Graph
Transformers achieved the highest accuracy, achieving a relative L2 score of
3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)
significantly improved the generalization, reducing error by up to 11.3% on
high-signal tasks. While the Graph Transformer is the most accurate model, it
is more 37.5% slower during inference when compared to second best model, MPNN
PINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.
It offers a good compromise between predictive performance, model size, and
inference speed.

</details>


### [86] [SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive Mixup and Neural Collapse](https://arxiv.org/abs/2510.15751)
*Trung-Anh Dang,Vincent Nguyen,Ngoc-Son Vu,Christel Vrain*

Main category: cs.LG

TL;DR: SAMix is a novel adaptive mixup strategy that enhances neural collapse-based continual learning by improving model calibration, reducing overconfidence, mitigating forgetting, and increasing accuracy through geometric-aware feature space regularization.


<details>
  <summary>Details</summary>
Motivation: Most continual learning methods focus on mitigating forgetting and improving accuracy but overlook network calibration, which is crucial for reliable predictions. Neural collapse has shown benefits in continual learning, but few works address calibration improvement.

Method: Proposed Sphere-Adaptive Mixup (SAMix) - an adaptive mixup strategy specifically designed for neural collapse-based methods. SAMix adapts the mixing process to the geometric properties of feature spaces under neural collapse to ensure robust regularization and alignment.

Result: Experiments demonstrate that SAMix significantly boosts performance, surpassing state-of-the-art methods in continual learning while also improving model calibration. It enhances both across-task accuracy and prediction reliability.

Conclusion: SAMix represents a promising advancement for robust continual learning systems by simultaneously improving performance and calibration through geometric-aware feature space adaptation.

Abstract: While most continual learning methods focus on mitigating forgetting and
improving accuracy, they often overlook the critical aspect of network
calibration, despite its importance. Neural collapse, a phenomenon where
last-layer features collapse to their class means, has demonstrated advantages
in continual learning by reducing feature-classifier misalignment. Few works
aim to improve the calibration of continual models for more reliable
predictions. Our work goes a step further by proposing a novel method that not
only enhances calibration but also improves performance by reducing
overconfidence, mitigating forgetting, and increasing accuracy. We introduce
Sphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural
collapse-based methods. SAMix adapts the mixing process to the geometric
properties of feature spaces under neural collapse, ensuring more robust
regularization and alignment. Experiments show that SAMix significantly boosts
performance, surpassing SOTA methods in continual learning while also improving
model calibration. SAMix enhances both across-task accuracy and the broader
reliability of predictions, making it a promising advancement for robust
continual learning systems.

</details>


### [87] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI is a modular AI platform that integrates six modules for continuous poultry farm monitoring, including camera placement optimization, audio-visual welfare monitoring, real-time egg counting, production forecasting, and operational recommendations.


<details>
  <summary>Details</summary>
Motivation: Small and medium-sized poultry farms lack affordable, integrated tools for continuous monitoring and decision-making, relying on manual inspections instead of proactive management.

Method: The system uses evolutionary algorithms for camera placement optimization, synchronized audio-visual data analysis for welfare monitoring, edge vision models for real-time egg counting, forecasting models for production prediction, and integrates weather data for operational recommendations.

Result: Field trials achieved 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting up to 10 days in advance.

Conclusion: PoultryFI bridges the gap between isolated pilot tools and scalable farm-wide intelligence, enabling proactive management of animal welfare and profitability through integrated AI-powered monitoring and decision support.

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


### [88] [Cavity Duplexer Tuning with 1d Resnet-like Neural Networks](https://arxiv.org/abs/2510.15796)
*Anton Raskovalov*

Main category: cs.LG

TL;DR: Machine learning method for tuning cavity duplexers with many adjustment screws using supervised learning with a neural network architecture that includes 1D ResNet-like backbone and processes S-parameter curve characteristics.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for tuning cavity duplexers with a large number of adjustment screws, as conventional reinforcement learning approaches were found inadequate for this task.

Method: Used supervised learning with a neural network architecture featuring a 1D ResNet-like backbone that processes S-parameter characteristics including curve shape, peak positions, and amplitudes, combined with an external control algorithm.

Result: The neural network with external control algorithm can achieve nearly tuned state of the duplexer within 4-5 rotations per screw.

Conclusion: The supervised learning approach with specialized neural network architecture is effective for cavity duplexer tuning, significantly outperforming conventional reinforcement learning methods.

Abstract: This paper presents machine learning method for tuning of cavity duplexer
with a large amount of adjustment screws. After testing we declined
conventional reinforcement learning approach and reformulated our task in the
supervised learning setup. The suggested neural network architecture includes
1d ResNet-like backbone and processing of some additional information about
S-parameters, like the shape of curve and peaks positions and amplitudes. This
neural network with external control algorithm is capable to reach almost the
tuned state of the duplexer within 4-5 rotations per screw.

</details>


### [89] [AB-UPT for Automotive and Aerospace Applications](https://arxiv.org/abs/2510.15808)
*Benedikt Alkin,Richard Kurle,Louis Serrano,Dennis Just,Johannes Brandstetter*

Main category: cs.LG

TL;DR: AB-UPT neural networks show strong performance in automotive and aircraft CFD simulations, achieving near-perfect aerodynamic force predictions with significantly less computation than traditional solvers.


<details>
  <summary>Details</summary>
Motivation: To expand the empirical evaluation of AB-UPT by adding two new automotive and aircraft datasets, demonstrating its capabilities for industry-scale computational fluid dynamics applications.

Method: Used the Luminary Cloud platform to generate high-quality datasets (SHIFT-SUV for automotive and SHIFT-Wing for aircraft), then trained AB-UPT models on these datasets and compared against previous transformer-based baselines.

Result: AB-UPT achieved near-perfect prediction of integrated aerodynamic forces within seconds using simple geometry representations, outperforming previous state-of-the-art transformer models on both datasets.

Conclusion: AB-UPT provides highly efficient CFD simulations that are trainable within a day on a single GPU, making it suitable for industry-scale applications requiring fast and accurate aerodynamic predictions.

Abstract: The recently proposed Anchored-Branched Universal Physics Transformers
(AB-UPT) shows strong capabilities to replicate automotive computational fluid
dynamics simulations requiring orders of magnitudes less compute than
traditional numerical solvers. In this technical report, we add two new
datasets to the body of empirically evaluated use-cases of AB-UPT, combining
high-quality data generation with state-of-the-art neural surrogates. Both
datasets were generated with the Luminary Cloud platform containing automotives
(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data
generation. Next, we show favorable performances of AB-UPT against previous
state-of-the-art transformer-based baselines on both datasets, followed by
extensive qualitative and quantitative evaluations of our best AB-UPT model.
AB-UPT shows strong performances across the board. Notably, it obtains near
perfect prediction of integrated aerodynamic forces within seconds from a
simple isotopically tesselate geometry representation and is trainable within a
day on a single GPU, paving the way for industry-scale applications.

</details>


### [90] [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)
*Dominik Kallusky,Vinay Rao,Vishal Nandavanam,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: The paper shows that DiLoCo's effectiveness comes from applying Nesterov momentum to pseudo-gradients, and introduces SNOO (Step-K Nesterov Outer Optimizer) which achieves 1.5-2.5× compute gains in non-distributed settings with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To understand why DiLoCo optimizer performs well in non-distributed settings and develop a more efficient optimization technique for large language models.

Method: Proposed SNOO (Step-K Nesterov Outer Optimizer) which applies Nesterov momentum to pseudo-gradients in a two-loop Lookahead framework, with minimal compute/memory overhead and compatibility with model sharding.

Result: SNOO achieves compute factor gains of 1.5-2.5× in non-distributed settings up to 1e23 training FLOPs, with improvements increasing with model size.

Conclusion: SNOO is a practical enhancement for various inner optimizers (AdamW, Muon) that provides significant compute efficiency gains with minimal overhead.

Abstract: The rapid development of large language models (LLMs) has driven the demand
for more efficient optimization techniques. Among these, the Lookahead family
of optimizers employs a two-loop framework, maintaining fast and slow sets of
model weights. Multiple inner optimizer steps on the fast weights produce a
trajectory - the pseudo-gradient - that is used to update the slow weights.
DiLoCo, a notable example originally designed for distributed training, applies
Nesterov momentum to the averaged pseudo-gradient from multiple workers,
claiming to even outperform AdamW in a non-distributed setup. In this paper, we
empirically show that DiLoCo's surprising effectiveness stems primarily from
applying Nesterov momentum to the pseudo-gradient, which improves training in a
non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov
Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains
of 1.5 - 2.5$\times$ in a non-distributed setting up to a scale of 1e23
training FLOPs, with improvements that increase with model size. Because of its
minimal compute and memory overhead and compatibility with model sharding, SNOO
is a practical enhancement for a variety of inner optimizers, including AdamW
and Muon.

</details>


### [91] [FIDDLE: Reinforcement Learning for Quantum Fidelity Enhancement](https://arxiv.org/abs/2510.15833)
*Hoang M. Ngo,Tamer Kahveci,My T. Thai*

Main category: cs.LG

TL;DR: FIDDLE is a novel learning framework that directly maximizes quantum circuit process fidelity during routing by combining Gaussian Process-based fidelity estimation with reinforcement learning, outperforming traditional indirect metric approaches.


<details>
  <summary>Details</summary>
Motivation: Current quantum devices suffer from noise that reduces reliability, and existing transpilation methods optimize indirect metrics like circuit depth rather than directly maximizing process fidelity, which is crucial for quantum computing applications.

Method: FIDDLE uses a two-module approach: a Gaussian Process surrogate model for accurate process fidelity estimation with limited samples, and a reinforcement learning module to optimize routing decisions that directly maximize fidelity.

Result: The surrogate model provides better fidelity estimation than existing learning techniques, and the end-to-end framework significantly improves process fidelity across various noise models compared to state-of-the-art methods.

Conclusion: FIDDLE successfully addresses the Fidelity Maximization in Routing Stage problem by directly optimizing process fidelity, demonstrating superior performance over traditional approaches that rely on indirect circuit metrics.

Abstract: Quantum computing has the potential to revolutionize fields like quantum
optimization and quantum machine learning. However, current quantum devices are
hindered by noise, reducing their reliability. A key challenge in gate-based
quantum computing is improving the reliability of quantum circuits, measured by
process fidelity, during the transpilation process, particularly in the routing
stage. In this paper, we address the Fidelity Maximization in Routing Stage
(FMRS) problem by introducing FIDDLE, a novel learning framework comprising two
modules: a Gaussian Process-based surrogate model to estimate process fidelity
with limited training samples and a reinforcement learning module to optimize
routing. Our approach is the first to directly maximize process fidelity,
outperforming traditional methods that rely on indirect metrics such as circuit
depth or gate count. We rigorously evaluate FIDDLE by comparing it with
state-of-the-art fidelity estimation techniques and routing optimization
methods. The results demonstrate that our proposed surrogate model is able to
provide a better estimation on the process fidelity compared to existing
learning techniques, and our end-to-end framework significantly improves the
process fidelity of quantum circuits across various noise models.

</details>


### [92] [Transfer Orthology Networks](https://arxiv.org/abs/2510.15837)
*Vikash Singh*

Main category: cs.LG

TL;DR: TRON is a neural network architecture for cross-species transfer learning that uses orthologous relationships to guide knowledge transfer between species through a species conversion layer.


<details>
  <summary>Details</summary>
Motivation: To enable effective cross-species transfer learning by leveraging biological orthology relationships and provide interpretable insights into functional orthology across species.

Method: Uses a bipartite graph of orthologous relationships between species, with a learned species conversion layer whose weights are masked by the biadjacency matrix, prepended to a pre-trained feedforward neural network for phenotype prediction.

Result: The architecture allows efficient knowledge transfer by learning linear transformations that map gene expression from source to target species, with learned weights providing interpretable insights into functional orthology.

Conclusion: TRON provides a biologically grounded and interpretable approach to cross-species transfer learning, enabling more effective utilization of transcriptomic data across species.

Abstract: We present Transfer Orthology Networks (TRON), a novel neural network
architecture designed for cross-species transfer learning. TRON leverages
orthologous relationships, represented as a bipartite graph between species, to
guide knowledge transfer. Specifically, we prepend a learned species conversion
layer, whose weights are masked by the biadjacency matrix of this bipartite
graph, to a pre-trained feedforward neural network that predicts a phenotype
from gene expression data in a source species. This allows for efficient
transfer of knowledge to a target species by learning a linear transformation
that maps gene expression from the source to the target species' gene space.
The learned weights of this conversion layer offer a potential avenue for
interpreting functional orthology, providing insights into how genes across
species contribute to the phenotype of interest. TRON offers a biologically
grounded and interpretable approach to cross-species transfer learning, paving
the way for more effective utilization of available transcriptomic data. We are
in the process of collecting cross-species transcriptomic/phenotypic data to
gain experimental validation of the TRON architecture.

</details>


### [93] [Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch](https://arxiv.org/abs/2510.15850)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: A hybrid solver combining optimization proxies with classical solvers to guarantee optimality gaps while achieving significant speedups, using duality theory for certification.


<details>
  <summary>Details</summary>
Motivation: Optimization proxies achieve high average performance but have unreliable worst-case gaps, making them untrustworthy for practical deployment where optimality guarantees are needed.

Method: Proposes a hybrid solver that uses duality theory to bound optimality gaps of proxy predictions, falling back to classical solvers when optimality cannot be certified. Also introduces combined primal-dual proxy training to improve speedup.

Result: Achieves over 1000x speedup compared to parallelized simplex-based solver while guaranteeing maximum 2% optimality gap on large-scale transmission systems.

Conclusion: The hybrid approach enables trustworthy deployment of optimization proxies with interpretable speed-optimality tradeoffs based on user-defined thresholds, balancing the benefits of proxies and classical solvers.

Abstract: Recent research has shown that optimization proxies can be trained to high
fidelity, achieving average optimality gaps under 1% for large-scale problems.
However, worst-case analyses show that there exist in-distribution queries that
result in orders of magnitude higher optimality gap, making it difficult to
trust the predictions in practice. This paper aims at striking a balance
between classical solvers and optimization proxies in order to enable
trustworthy deployments with interpretable speed-optimality tradeoffs based on
a user-defined optimality threshold. To this end, the paper proposes a hybrid
solver that leverages duality theory to efficiently bound the optimality gap of
predictions, falling back to a classical solver for queries where optimality
cannot be certified. To improve the achieved speedup of the hybrid solver, the
paper proposes an alternative training procedure that combines the primal and
dual proxy training. Experiments on large-scale transmission systems show that
the hybrid solver is highly scalable. The proposed hybrid solver achieves
speedups of over 1000x compared to a parallelized simplex-based solver while
guaranteeing a maximum optimality gap of 2%.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [94] [From Universal Approximation Theorem to Tropical Geometry of Multi-Layer Perceptrons](https://arxiv.org/abs/2510.15012)
*Yi-Shan Chu,Yueh-Cheng Kuo*

Main category: stat.ML

TL;DR: This paper introduces a geometry-aware initialization method for sigmoidal MLPs using tropical geometry, enabling decision boundaries to align with prescribed shapes at initialization without needing ReLU networks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between tropical geometry insights (which show ReLU networks have combinatorial decision functions) and smooth sigmoidal MLPs, enabling interpretable, shape-driven initialization while maintaining the finite-sum format of the Universal Approximation Theorem.

Method: Using tropical geometry principles to design purely sigmoidal MLPs that adhere to the finite-sum format of UAT, creating decision boundaries that match prescribed shapes at initialization through constructive, geometry-aware initialization.

Result: The method produces sigmoidal MLPs with decision boundaries that already align with desired shapes at initialization, which can then be refined through standard training if needed.

Conclusion: This work provides a practical connection between tropical geometry and smooth MLPs, enabling interpretable initialization without requiring ReLU architectures, though currently limited to 2D with theoretical extensions left for future work.

Abstract: We revisit the Universal Approximation Theorem(UAT) through the lens of the
tropical geometry of neural networks and introduce a constructive,
geometry-aware initialization for sigmoidal multi-layer perceptrons (MLPs).
Tropical geometry shows that Rectified Linear Unit (ReLU) networks admit
decision functions with a combinatorial structure often described as a tropical
rational, namely a difference of tropical polynomials. Focusing on planar
binary classification, we design purely sigmoidal MLPs that adhere to the
finite-sum format of UAT: a finite linear combination of shifted and scaled
sigmoids of affine functions. The resulting models yield decision boundaries
that already align with prescribed shapes at initialization and can be refined
by standard training if desired. This provides a practical bridge between the
tropical perspective and smooth MLPs, enabling interpretable, shape-driven
initialization without resorting to ReLU architectures. We focus on the
construction and empirical demonstrations in two dimensions; theoretical
analysis and higher-dimensional extensions are left for future work.

</details>


### [95] [Reliable data clustering with Bayesian community detection](https://arxiv.org/abs/2510.15013)
*Magnus Neuman,Jelena Smiljanić,Martin Rosvall*

Main category: stat.ML

TL;DR: The paper introduces Bayesian community detection methods as a principled alternative to traditional clustering approaches, showing superior performance in detecting reliable clusters under high-noise conditions and with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering methods lack principled model selection and are susceptible to noise, while common workarounds like correlation matrix sparsification introduce arbitrary thresholds that distort structure and produce unreliable results.

Method: The authors test two Bayesian community detection methods: Degree-Corrected Stochastic Block Model and Regularized Map Equation, both based on Minimum Description Length principle for model selection. These methods unite sparsification and clustering with principled model selection.

Result: In synthetic data, the Bayesian methods outperform traditional approaches, detecting planted clusters under high-noise conditions and with fewer samples. On gene co-expression data, Regularized Map Equation identifies more robust and functionally coherent gene modules compared to WGCNA.

Conclusion: Bayesian community detection provides a principled and noise-resistant framework for uncovering modular structure in high-dimensional data across various scientific fields.

Abstract: From neuroscience and genomics to systems biology and ecology, researchers
rely on clustering similarity data to uncover modular structure. Yet widely
used clustering methods, such as hierarchical clustering, k-means, and WGCNA,
lack principled model selection, leaving them susceptible to noise. A common
workaround sparsifies a correlation matrix representation to remove noise
before clustering, but this extra step introduces arbitrary thresholds that can
distort the structure and lead to unreliable results. To detect reliable
clusters, we capitalize on recent advances in network science to unite
sparsification and clustering with principled model selection. We test two
Bayesian community detection methods, the Degree-Corrected Stochastic Block
Model and the Regularized Map Equation, both grounded in the Minimum
Description Length principle for model selection. In synthetic data, they
outperform traditional approaches, detecting planted clusters under high-noise
conditions and with fewer samples. Compared to WGCNA on gene co-expression
data, the Regularized Map Equation identifies more robust and functionally
coherent gene modules. Our results establish Bayesian community detection as a
principled and noise-resistant framework for uncovering modular structure in
high-dimensional data across fields.

</details>


### [96] [The Coverage Principle: How Pre-training Enables Post-Training](https://arxiv.org/abs/2510.15020)
*Fan Chen,Audrey Huang,Noah Golowich,Sadhika Malladi,Adam Block,Jordan T. Ash,Akshay Krishnamurthy,Dylan J. Foster*

Main category: stat.ML

TL;DR: The paper introduces 'coverage' as a better predictor of downstream performance than cross-entropy loss, showing how next-token prediction implicitly optimizes for coverage, which generalizes faster and avoids spurious dependencies on sequence length.


<details>
  <summary>Details</summary>
Motivation: Current understanding of why pre-training works is limited, as cross-entropy loss often fails to predict downstream performance. The authors aim to provide theoretical insights into this relationship through the concept of coverage.

Method: Theoretical analysis of coverage principle, studying how next-token prediction implicitly optimizes for coverage. Also examines practical interventions: model selection procedures, gradient normalization schemes, and test-time decoding strategies with provable benefits.

Result: Coverage generalizes faster than cross-entropy and avoids spurious dependence on problem-dependent parameters like sequence length. Coverage is necessary and sufficient for post-training methods like Best-of-N to succeed.

Conclusion: Coverage provides a more reliable predictor of downstream performance than cross-entropy loss, with theoretical mechanisms explaining its effectiveness and practical methods for improving coverage.

Abstract: Language models demonstrate remarkable abilities when pre-trained on large
text corpora and fine-tuned for specific tasks, but how and why pre-training
shapes the success of the final model remains poorly understood. Notably,
although pre-training success is often quantified by cross entropy loss,
cross-entropy can be a poor predictor of downstream performance. Instead, we
provide a theoretical perspective on this relationship through the lens of
\emph{coverage}, which quantifies the probability mass the pre-trained model
places on high-quality responses and which is necessary and sufficient for
post-training and test-time scaling methods such as Best-of-N to succeed. Our
main results develop an understanding of \emph{the coverage principle}, a
phenomenon whereby next-token prediction implicitly optimizes toward a model
with good coverage. In particular, we uncover a mechanism that explains the
power of coverage in predicting downstream performance: \emph{coverage
generalizes faster than cross entropy}, avoiding spurious dependence on
problem-dependent parameters such as the sequence length. We also study
practical algorithmic interventions with provable benefits for improving
coverage, including (i) model/checkpoint selection procedures, (ii) gradient
normalization schemes, and (iii) test-time decoding strategies.

</details>


### [97] [The Tree-SNE Tree Exists](https://arxiv.org/abs/2510.15014)
*Jack Kendrick*

Main category: stat.ML

TL;DR: The paper introduces tree-SNE, a (2+1)-dimensional extension of t-SNE that addresses the scale-problem in clustering by incorporating an additional scaling parameter to handle different levels of detail in high-dimensional data visualization.


<details>
  <summary>Details</summary>
Motivation: To solve the 'scale-problem' in clustering where traditional methods like t-SNE and UMAP struggle to determine whether to distinguish between different categories (e.g., digits) or different variations within categories (e.g., writing styles), which depends on the task and scale.

Method: Extends t-SNE by exploiting its underlying scaling symmetry to create (2+1)-dimensional embeddings where the additional parameter accounts for scale, resulting in tree-SNE. Proves continuous dependence of optimal embeddings on scaling parameter for almost all initial conditions.

Result: Demonstrates that tree-SNE trees exist and can handle different clustering scales effectively. The method is illustrated on several examples and potentially extends to other attraction-repulsion dimensionality reduction techniques.

Conclusion: Tree-SNE provides a principled solution to the scale-problem in clustering by incorporating scale as an explicit parameter, enabling continuous exploration of clustering at different levels of granularity while maintaining mathematical guarantees of continuity.

Abstract: The clustering and visualisation of high-dimensional data is a ubiquitous
task in modern data science. Popular techniques include nonlinear
dimensionality reduction methods like t-SNE or UMAP. These methods face the
`scale-problem' of clustering: when dealing with the MNIST dataset, do we want
to distinguish different digits or do we want to distinguish different ways of
writing the digits? The answer is task dependent and depends on scale. We
revisit an idea of Robinson & Pierce-Hoffman that exploits an underlying
scaling symmetry in t-SNE to replace 2-dimensional with (2+1)-dimensional
embeddings where the additional parameter accounts for scale. This gives rise
to the t-SNE tree (short: tree-SNE). We prove that the optimal embedding
depends continuously on the scaling parameter for all initial conditions
outside a set of measure 0: the tree-SNE tree exists. This idea conceivably
extends to other attraction-repulsion methods and is illustrated on several
examples.

</details>


### [98] [The Minimax Lower Bound of Kernel Stein Discrepancy Estimation](https://arxiv.org/abs/2510.15058)
*Jose Cribeiro-Ramallo,Agnideep Aich,Florian Kalinke,Ashit Baran Aich,Zoltán Szabó*

Main category: stat.ML

TL;DR: The paper establishes that the minimax lower bound for Kernel Stein Discrepancy (KSD) estimation is n^{-1/2}, confirming the optimality of existing estimators that achieve √n-convergence.


<details>
  <summary>Details</summary>
Motivation: Kernel Stein discrepancies have become important for goodness-of-fit testing, but it was unknown whether existing estimators achieving √n-convergence were optimal or if faster rates were possible.

Method: The authors present two complementary proof strategies: one for KSD estimation on ℝ^d with Langevin-Stein operator and Gaussian kernel, and another for general domains.

Result: The minimax lower bound for KSD estimation is proven to be n^{-1/2}, with the Gaussian kernel case showing that estimation difficulty may increase exponentially with dimensionality d.

Conclusion: Existing KSD estimators with √n-convergence are indeed minimax optimal, settling the fundamental limits of KSD estimation rates.

Abstract: Kernel Stein discrepancies (KSDs) have emerged as a powerful tool for
quantifying goodness-of-fit over the last decade, featuring numerous successful
applications. To the best of our knowledge, all existing KSD estimators with
known rate achieve $\sqrt n$-convergence. In this work, we present two
complementary results (with different proof strategies), establishing that the
minimax lower bound of KSD estimation is $n^{-1/2}$ and settling the optimality
of these estimators. Our first result focuses on KSD estimation on $\mathbb
R^d$ with the Langevin-Stein operator; our explicit constant for the Gaussian
kernel indicates that the difficulty of KSD estimation may increase
exponentially with the dimensionality $d$. Our second result settles the
minimax lower bound for KSD estimation on general domains.

</details>


### [99] [Foresighted Online Policy Optimization with Interference](https://arxiv.org/abs/2510.15273)
*Liner Xiang,Jiayi Wang,Hengrui Cai*

Main category: stat.ML

TL;DR: FRONT is a foresighted contextual bandit method that addresses interference in online decision-making by considering long-term impacts of current decisions on subsequent rewards, achieving sublinear regret through exploratory and exploitative strategies.


<details>
  <summary>Details</summary>
Motivation: Existing contextual bandit approaches assume no interference, but this assumption is often violated in practice, leading to short-sighted policies that maximize only immediate outcomes and result in suboptimal decisions over time.

Method: FRONT employs a sequence of exploratory and exploitative strategies to manage interference complexities, ensuring robust parameter inference and regret minimization while considering long-term impacts of current decisions.

Result: Theoretical analysis establishes tail bounds for online estimators, asymptotic parameter distributions under interference network conditions, and demonstrates sublinear regret under two definitions capturing immediate and consequential decision impacts.

Conclusion: FRONT effectively addresses interference in contextual bandits, providing theoretically sound performance guarantees and demonstrating practical effectiveness through simulations and real-world hotel profit applications.

Abstract: Contextual bandits, which leverage the baseline features of sequentially
arriving individuals to optimize cumulative rewards while balancing exploration
and exploitation, are critical for online decision-making. Existing approaches
typically assume no interference, where each individual's action affects only
their own reward. Yet, such an assumption can be violated in many practical
scenarios, and the oversight of interference can lead to short-sighted policies
that focus solely on maximizing the immediate outcomes for individuals, which
further results in suboptimal decisions and potentially increased regret over
time. To address this significant gap, we introduce the foresighted online
policy with interference (FRONT) that innovatively considers the long-term
impact of the current decision on subsequent decisions and rewards. The
proposed FRONT method employs a sequence of exploratory and exploitative
strategies to manage the intricacies of interference, ensuring robust parameter
inference and regret minimization. Theoretically, we establish a tail bound for
the online estimator and derive the asymptotic distribution of the parameters
of interest under suitable conditions on the interference network. We further
show that FRONT attains sublinear regret under two distinct definitions,
capturing both the immediate and consequential impacts of decisions, and we
establish these results with and without statistical inference. The
effectiveness of FRONT is further demonstrated through extensive simulations
and a real-world application to urban hotel profits.

</details>


### [100] [Beyond PCA: Manifold Dimension Estimation via Local Graph Structure](https://arxiv.org/abs/2510.15141)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: stat.ML

TL;DR: A general framework for manifold dimension estimation that combines PCA with regression techniques to capture local graph structure, introducing QE and TLS estimators that outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing Local PCA and CA-PCA methods by developing a more comprehensive framework that better captures the manifold's local graph structure rather than assuming local flatness.

Method: Proposed a general framework integrating PCA with regression-based techniques, specifically introducing quadratic embedding (QE) and total least squares (TLS) estimators to model the manifold's local graph structure.

Result: Experiments on synthetic and real-world datasets show that the proposed QE and TLS methods perform competitively with and often outperform state-of-the-art dimension estimation alternatives.

Conclusion: The proposed framework successfully advances manifold dimension estimation by effectively capturing local graph structure through PCA-regression integration, with QE and TLS proving to be superior estimators.

Abstract: Local principal component analysis (Local PCA) has proven to be an effective
tool for estimating the intrinsic dimension of a manifold. More recently,
curvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly
accounting for the curvature of the underlying manifold, rather than assuming
local flatness. Building on these insights, we propose a general framework for
manifold dimension estimation that captures the manifold's local graph
structure by integrating PCA with regression-based techniques. Within this
framework, we introduce two representative estimators: quadratic embedding (QE)
and total least squares (TLS). Experiments on both synthetic and real-world
datasets demonstrate that these methods perform competitively with, and often
outperform, state-of-the-art alternatives.

</details>


### [101] [Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression](https://arxiv.org/abs/2510.15337)
*Yeichan Kim,Ilmun Kim,Seyoung Park*

Main category: stat.ML

TL;DR: The paper proposes a Transfer MNI approach that combines transfer learning with minimum-ℓ₂-norm interpolators, analyzing its excess risk and identifying conditions where it outperforms target-only methods, with practical detection and ensemble methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between transfer learning and minimum-ℓ₂-norm interpolators (MNI), exploring their intersection for enhanced generalization in high-dimensional linear regression.

Method: A two-step Transfer MNI approach with theoretical analysis of excess risk, plus data-driven source detection and ensemble methods incorporating multiple informative Transfer MNIs.

Result: Identifies free-lunch covariate shift regimes where heterogeneous data provides knowledge transfer benefits with limited cost, and demonstrates robustness to model and data heterogeneity in finite-sample experiments.

Conclusion: The Transfer MNI approach effectively leverages diverse data sources to improve performance over target-only methods, with practical detection and ensemble techniques that maintain robustness across heterogeneous settings.

Abstract: Transfer learning is a key component of modern machine learning, enhancing
the performance of target tasks by leveraging diverse data sources.
Simultaneously, overparameterized models such as the minimum-$\ell_2$-norm
interpolator (MNI) in high-dimensional linear regression have garnered
significant attention for their remarkable generalization capabilities, a
property known as benign overfitting. Despite their individual importance, the
intersection of transfer learning and MNI remains largely unexplored. Our
research bridges this gap by proposing a novel two-step Transfer MNI approach
and analyzing its trade-offs. We characterize its non-asymptotic excess risk
and identify conditions under which it outperforms the target-only MNI. Our
analysis reveals free-lunch covariate shift regimes, where leveraging
heterogeneous data yields the benefit of knowledge transfer at limited cost. To
operationalize our findings, we develop a data-driven procedure to detect
informative sources and introduce an ensemble method incorporating multiple
informative Transfer MNIs. Finite-sample experiments demonstrate the robustness
of our methods to model and data heterogeneity, confirming their advantage.

</details>


### [102] [RankSEG-RMA: An Efficient Segmentation Algorithm via Reciprocal Moment Approximation](https://arxiv.org/abs/2510.15362)
*Zixun Wang,Ben Dai*

Main category: stat.ML

TL;DR: RankSEG-RMA improves upon RankSEG by reducing computational complexity from O(d log d) or O(d²) to O(d) while maintaining performance, and extends applicability to non-overlapping segmentation settings.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods using argmax or thresholding don't directly optimize segmentation metrics like IoU and Dice, leading to suboptimal results. RankSEG addresses this but has high computational cost and limited applicability to overlapping segmentation only.

Method: Proposed reciprocal moment approximation (RMA) of RankSEG, called RankSEG-RMA, which reduces complexity while maintaining performance. Also developed a pixel-wise score function for efficient implementation in non-overlapping segmentation settings.

Result: RankSEG-RMA reduces computational complexity significantly - from 16.33 seconds to comparable performance with O(d) complexity, while extending applicability to standard non-overlapping segmentation benchmarks.

Conclusion: The proposed RankSEG-RMA overcomes the computational and applicability limitations of RankSEG, making consistent segmentation metric optimization practical for real-world applications.

Abstract: Semantic segmentation labels each pixel in an image with its corresponding
class, and is typically evaluated using the Intersection over Union (IoU) and
Dice metrics to quantify the overlap between predicted and ground-truth
segmentation masks. In the literature, most existing methods estimate
pixel-wise class probabilities, then apply argmax or thresholding to obtain the
final prediction. These methods have been shown to generally lead to
inconsistent or suboptimal results, as they do not directly maximize
segmentation metrics. To address this issue, a novel consistent segmentation
framework, RankSEG, has been proposed, which includes RankDice and RankIoU
specifically designed to optimize the Dice and IoU metrics, respectively.
Although RankSEG almost guarantees improved performance, it suffers from two
major drawbacks. First, it is its computational expense-RankDice has a
complexity of O(d log d) with a substantial constant factor (where d represents
the number of pixels), while RankIoU exhibits even higher complexity O(d^2),
thus limiting its practical application. For instance, in LiTS, prediction with
RankSEG takes 16.33 seconds compared to just 0.01 seconds with the argmax rule.
Second, RankSEG is only applicable to overlapping segmentation settings, where
multiple classes can occupy the same pixel, which contrasts with standard
benchmarks that typically assume non-overlapping segmentation. In this paper,
we overcome these two drawbacks via a reciprocal moment approximation (RMA) of
RankSEG with the following contributions: (i) we improve RankSEG using RMA,
namely RankSEG-RMA, reduces the complexity of both algorithms to O(d) while
maintaining comparable performance; (ii) inspired by RMA, we develop a
pixel-wise score function that allows efficient implementation for
non-overlapping segmentation settings.

</details>


### [103] [Kernel Regression in Structured Non-IID Settings: Theory and Implications for Denoising Score Learning](https://arxiv.org/abs/2510.15363)
*Dechen Zhang,Zhenmei Shi,Yi Zhang,Yingyu Liang,Difan Zou*

Main category: stat.ML

TL;DR: First systematic study of kernel ridge regression generalization for non-i.i.d. data with signal-noise causal structure, developing novel blockwise decomposition method for dependent data analysis.


<details>
  <summary>Details</summary>
Motivation: Existing KRR theory primarily addresses i.i.d. settings, but real-world data often has structured dependencies, especially in applications like denoising score learning where multiple noisy observations come from shared underlying signals.

Method: Developed a novel blockwise decomposition method enabling precise concentration analysis for dependent data, deriving excess risk bounds that depend on kernel spectrum, causal structure parameters, and sampling mechanisms.

Result: Established generalization guarantees for KRR with non-i.i.d. data, with explicit dependence on kernel spectrum, causal structure parameters, and sampling mechanisms (including relative sample sizes for signals and noises).

Conclusion: This work advances KRR theory while providing practical tools for analyzing dependent data in modern machine learning applications, with specific applications to denoising score learning and principled guidance for sampling noisy data points.

Abstract: Kernel ridge regression (KRR) is a foundational tool in machine learning,
with recent work emphasizing its connections to neural networks. However,
existing theory primarily addresses the i.i.d. setting, while real-world data
often exhibits structured dependencies - particularly in applications like
denoising score learning where multiple noisy observations derive from shared
underlying signals. We present the first systematic study of KRR generalization
for non-i.i.d. data with signal-noise causal structure, where observations
represent different noisy views of common signals. By developing a novel
blockwise decomposition method that enables precise concentration analysis for
dependent data, we derive excess risk bounds for KRR that explicitly depend on:
(1) the kernel spectrum, (2) causal structure parameters, and (3) sampling
mechanisms (including relative sample sizes for signals and noises). We further
apply our results to denoising score learning, establishing generalization
guarantees and providing principled guidance for sampling noisy data points.
This work advances KRR theory while providing practical tools for analyzing
dependent data in modern machine learning applications.

</details>


### [104] [Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction](https://arxiv.org/abs/2510.15422)
*Lin Wang*

Main category: stat.ML

TL;DR: This paper provides a comprehensive review of information theoretic approaches in open world machine learning, synthesizing research into open set recognition, novelty discovery, and continual learning, while establishing theoretical connections to provable learning frameworks.


<details>
  <summary>Details</summary>
Motivation: Open world machine learning lacks a unified theoretical foundation to quantify uncertainty, characterize information transfer, and explain learning adaptability in dynamic, nonstationary environments.

Method: The paper reviews and synthesizes information theoretic approaches using core concepts like entropy, mutual information, and Kullback-Leibler divergence to describe knowledge acquisition, uncertainty suppression, and risk control in open world conditions.

Result: The review organizes recent studies into three major research axes: information theoretic open set recognition, information driven novelty discovery, and information retentive continual learning, while establishing theoretical connections to provable learning frameworks.

Conclusion: The paper identifies key open problems and future directions including quantification of information risk, development of dynamic mutual information bounds, multimodal information fusion, and integration of information theory with causal reasoning and world model learning.

Abstract: Open world Machine Learning (OWML) aims to develop intelligent systems
capable of recognizing known categories, rejecting unknown samples, and
continually learning from novel information. Despite significant progress in
open set recognition, novelty detection, and continual learning, the field
still lacks a unified theoretical foundation that can quantify uncertainty,
characterize information transfer, and explain learning adaptability in
dynamic, nonstationary environments. This paper presents a comprehensive review
of information theoretic approaches in open world machine learning, emphasizing
how core concepts such as entropy, mutual information, and Kullback Leibler
divergence provide a mathematical language for describing knowledge
acquisition, uncertainty suppression, and risk control under open world
conditions. We synthesize recent studies into three major research axes:
information theoretic open set recognition enabling safe rejection of unknowns,
information driven novelty discovery guiding new concept formation, and
information retentive continual learning ensuring stable long term adaptation.
Furthermore, we discuss theoretical connections between information theory and
provable learning frameworks, including PAC Bayes bounds, open-space risk
theory, and causal information flow, to establish a pathway toward provable and
trustworthy open world intelligence. Finally, the review identifies key open
problems and future research directions, such as the quantification of
information risk, development of dynamic mutual information bounds, multimodal
information fusion, and integration of information theory with causal reasoning
and world model learning.

</details>


### [105] [Robust Optimization in Causal Models and G-Causal Normalizing Flows](https://arxiv.org/abs/2510.15458)
*Gabriele Visentin,Patrick Cheridito*

Main category: stat.ML

TL;DR: The paper shows that interventionally robust optimization problems in causal models are continuous under the G-causal Wasserstein distance but discontinuous under standard Wasserstein distance, highlighting the need for causal-aware generative models. The authors propose a new normalizing flow architecture with universal approximation for causal structural models that minimizes G-causal Wasserstein distance, outperforming standard generative models in causal regression and portfolio optimization tasks.


<details>
  <summary>Details</summary>
Motivation: To address the discontinuity of interventionally robust optimization problems under standard Wasserstein distance and emphasize the importance of using generative models that respect causal structure for data augmentation in causal tasks.

Method: Proposed a new normalizing flow architecture that satisfies universal approximation property for causal structural models and can be efficiently trained to minimize the G-causal Wasserstein distance.

Result: Empirical demonstrations show the proposed model outperforms standard (non-causal) generative models in data augmentation for causal regression and mean-variance portfolio optimization in causal factor models.

Conclusion: Causal-aware generative models that respect the causal structure are crucial for interventionally robust optimization problems, and the proposed normalizing flow architecture effectively addresses this need by minimizing G-causal Wasserstein distance.

Abstract: In this paper, we show that interventionally robust optimization problems in
causal models are continuous under the $G$-causal Wasserstein distance, but may
be discontinuous under the standard Wasserstein distance. This highlights the
importance of using generative models that respect the causal structure when
augmenting data for such tasks. To this end, we propose a new normalizing flow
architecture that satisfies a universal approximation property for causal
structural models and can be efficiently trained to minimize the $G$-causal
Wasserstein distance. Empirically, we demonstrate that our model outperforms
standard (non-causal) generative models in data augmentation for causal
regression and mean-variance portfolio optimization in causal factor models.

</details>


### [106] [Online Policy Learning via a Self-Normalized Maximal Inequality](https://arxiv.org/abs/2510.15483)
*Samuel Girard,Aurélien Bibaut,Houssam Zenati*

Main category: stat.ML

TL;DR: Developed a self-normalized maximal inequality for martingale empirical processes to address dependent data in adaptive experiments, enabling improved convergence rates for off-policy learning.


<details>
  <summary>Details</summary>
Motivation: Adaptive experiments produce dependent data that violate i.i.d. assumptions, breaking classical concentration bounds and invalidating standard learning guarantees.

Method: Proposed adaptive sample-variance penalization for general dependent data, then derived a variance-regularized pessimistic off-policy learning objective with sequential updates.

Result: Achieved fast convergence rates in both parametric and nonparametric regimes, improving over the usual 1/√n baseline, with numerical simulations confirming practical gains.

Conclusion: The approach successfully handles dependent data in adaptive experiments through martingale-based analysis and variance regularization, providing improved learning guarantees and convergence rates.

Abstract: Adaptive experiments produce dependent data that break i.i.d. assumptions
that underlie classical concentration bounds and invalidate standard learning
guarantees. In this paper, we develop a self-normalized maximal inequality for
martingale empirical processes. Building on this, we first propose an adaptive
sample-variance penalization procedure which balances empirical loss and sample
variance, valid for general dependent data. Next, this allows us to derive a
new variance-regularized pessimistic off-policy learning objective, for which
we establish excess-risk guarantees. Subsequently, we show that, when combined
with sequential updates and under standard complexity and margin conditions,
the resulting estimator achieves fast convergence rates in both parametric and
nonparametric regimes, improving over the usual $1/\sqrt{n}$
  baseline. We complement our theoretical findings with numerical simulations
that illustrate the practical gains of our approach.

</details>


### [107] [Geometric Convergence Analysis of Variational Inference via Bregman Divergences](https://arxiv.org/abs/2510.15548)
*Sushil Bohara,Amedeo Roberto Esposito*

Main category: stat.ML

TL;DR: This paper establishes a geometric framework for analyzing VI convergence by expressing negative ELBO as a Bregman divergence, proving non-asymptotic convergence rates for gradient descent algorithms.


<details>
  <summary>Details</summary>
Motivation: Convergence analysis of Variational Inference (VI) is challenging due to the non-convexity and non-smoothness of the ELBO objective in Euclidean space, requiring new theoretical approaches.

Method: Exploit exponential family structure to express negative ELBO as a Bregman divergence with respect to the log-partition function, enabling geometric analysis of the optimization landscape and establishing weak monotonicity properties.

Result: Derived bounds on the objective function along parameter space rays governed by Fisher information matrix spectral characteristics, and proved non-asymptotic convergence rates for gradient descent with constant and diminishing step sizes.

Conclusion: The Bregman divergence representation provides sufficient structure for rigorous convergence analysis of VI despite weaker than convexity properties, enabling geometric understanding and convergence rate guarantees.

Abstract: Variational Inference (VI) provides a scalable framework for Bayesian
inference by optimizing the Evidence Lower Bound (ELBO), but convergence
analysis remains challenging due to the objective's non-convexity and
non-smoothness in Euclidean space. We establish a novel theoretical framework
for analyzing VI convergence by exploiting the exponential family structure of
distributions. We express negative ELBO as a Bregman divergence with respect to
the log-partition function, enabling a geometric analysis of the optimization
landscape. We show that this Bregman representation admits a weak monotonicity
property that, while weaker than convexity, provides sufficient structure for
rigorous convergence analysis. By deriving bounds on the objective function
along rays in parameter space, we establish properties governed by the spectral
characteristics of the Fisher information matrix. Under this geometric
framework, we prove non-asymptotic convergence rates for gradient descent
algorithms with both constant and diminishing step sizes.

</details>


### [108] [Kernel-Based Evaluation of Conditional Biological Sequence Models](https://arxiv.org/abs/2510.15601)
*Pierre Glaser,Steffanie Paul,Alissa M. Hummer,Charlotte M. Deane,Debora S. Marks,Alan N. Amin*

Main category: stat.ML

TL;DR: The paper introduces kernel-based tools for evaluating conditional sequence models, featuring a new discrepancy measure called ACMMD that enables unbiased estimation of model fit, hypothesis testing, and hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: To address the need for better evaluation and tuning methods for conditional sequence models in computational biology, particularly for assessing model fit and reliability.

Method: Proposes Augmented Conditional Maximum Mean Discrepancy (ACMMD) - a kernel-based discrepancy measure between true conditional distributions and model estimates. The method allows unbiased estimation from data and can be used for hypothesis testing and hyperparameter optimization.

Result: Applied to ProteinMPNN protein design model, the approach successfully rejected the hypothesis that ProteinMPNN fits its data for various protein families and enabled temperature hyperparameter tuning to achieve better model fit.

Conclusion: The ACMMD-based toolkit provides effective methods for evaluating conditional sequence models, quantifying model fit, conducting hypothesis tests, and optimizing hyperparameters in computational biology applications.

Abstract: We propose a set of kernel-based tools to evaluate the designs and tune the
hyperparameters of conditional sequence models, with a focus on problems in
computational biology. The backbone of our tools is a new measure of
discrepancy between the true conditional distribution and the model's estimate,
called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided
that the model can be sampled from, the ACMMD can be estimated unbiasedly from
data to quantify absolute model fit, integrated within hypothesis tests, and
used to evaluate model reliability. We demonstrate the utility of our approach
by analyzing a popular protein design model, ProteinMPNN. We are able to reject
the hypothesis that ProteinMPNN fits its data for various protein families, and
tune the model's temperature hyperparameter to achieve a better fit.

</details>


### [109] [Disentanglement of Sources in a Multi-Stream Variational Autoencoder](https://arxiv.org/abs/2510.15669)
*Veranika Boukun,Jörg Lücke*

Main category: stat.ML

TL;DR: The paper introduces a multi-stream VAE (MS-VAE) that uses discrete latents to combine VAE representations of individual sources via a linear combination model, demonstrating effective source separation on hand-written digits and acoustic data.


<details>
  <summary>Details</summary>
Motivation: To address the problem of learning disentangled representations by exploring an alternative approach using discrete latents to combine VAE representations of individual sources, particularly suited for applications like acoustic data.

Method: Proposes a multi-stream VAE (MS-VAE) approach with discrete latents that combines VAE representations based on an explicit linear combination model, deriving inference and learning equations for the framework.

Result: Clear separation of superimposed hand-written digits and low missed speaker rate in speaker diarization tasks, with numerical experiments showing flexibility across varying supervision levels and training data amounts.

Conclusion: The MS-VAE approach is domain-agnostic and effectively separates sources into different streams, demonstrating principled functionality and practical utility in source separation tasks.

Abstract: Variational autoencoders (VAEs) are a leading approach to address the problem
of learning disentangled representations. Typically a single VAE is used and
disentangled representations are sought in its continuous latent space. Here we
explore a different approach by using discrete latents to combine
VAE-representations of individual sources. The combination is done based on an
explicit model for source combination, and we here use a linear combination
model which is well suited, e.g., for acoustic data. We formally define such a
multi-stream VAE (MS-VAE) approach, derive its inference and learning
equations, and we numerically investigate its principled functionality. The
MS-VAE is domain-agnostic, and we here explore its ability to separate sources
into different streams using superimposed hand-written digits, and mixed
acoustic sources in a speaker diarization task. We observe a clear separation
of digits, and on speaker diarization we observe an especially low rate of
missed speakers. Numerical experiments further highlight the flexibility of the
approach across varying amounts of supervision and training data.

</details>


### [110] [On Universality of Deep Equivariant Networks](https://arxiv.org/abs/2510.15814)
*Marco Pacini,Mircea Petrache,Bruno Lepri,Shubhendu Trivedi,Robin Walters*

Main category: stat.ML

TL;DR: This paper establishes universality theorems for equivariant neural networks, showing that depth and readout layers enable approximation of separation-constrained continuous functions for invariant networks and entry-wise separable functions for equivariant networks.


<details>
  <summary>Details</summary>
Motivation: Existing universality results for equivariant neural networks are limited to restrictive settings - either requiring impractical high-dimensional representations or specialized architectures confined to invariant settings. The paper aims to develop a more general theoretical foundation.

Method: For invariant networks: establish universality under separation constraints with fully connected readout layers. For equivariant networks: introduce the concept of 'entry-wise separability' as a sharper criterion and show that sufficient depth or appropriate readout layers achieve universality within this regime.

Result: The paper demonstrates that with sufficient depth or readout layers, equivariant networks can achieve universality for entry-wise separable functions, while invariant networks can approximate separation-constrained continuous functions.

Conclusion: Depth and readout layers are identified as decisive mechanisms for universality in equivariant neural networks, providing a unified perspective that extends earlier specialized results and addresses the limitations of shallow models.

Abstract: Universality results for equivariant neural networks remain rare. Those that
do exist typically hold only in restrictive settings: either they rely on
regular or higher-order tensor representations, leading to impractically
high-dimensional hidden spaces, or they target specialized architectures, often
confined to the invariant setting. This work develops a more general account.
For invariant networks, we establish a universality theorem under separation
constraints, showing that the addition of a fully connected readout layer
secures approximation within the class of separation-constrained continuous
functions. For equivariant networks, where results are even scarcer, we
demonstrate that standard separability notions are inadequate and introduce the
sharper criterion of $\textit{entry-wise separability}$. We show that with
sufficient depth or with the addition of appropriate readout layers,
equivariant networks attain universality within the entry-wise separable
regime. Together with prior results showing the failure of universality for
shallow models, our findings identify depth and readout layers as a decisive
mechanism for universality, additionally offering a unified perspective that
subsumes and extends earlier specialized results.

</details>


### [111] [Error analysis of a compositional score-based algorithm for simulation-based inference](https://arxiv.org/abs/2510.15817)
*Camille Touron,Gabriel V. Cardoso,Julyan Arbel,Pedro L. C. Rodrigues*

Main category: stat.ML

TL;DR: This paper analyzes the compositional score method in simulation-based inference (SBI), specifically examining how error accumulates when combining multiple observations. It provides theoretical bounds on mean squared error for the GAUSS algorithm.


<details>
  <summary>Details</summary>
Motivation: To understand how effectively combining multiple observations improves parameter inference in SBI, and to address the unexplored theoretical issue of error accumulation in compositional score methods as the number of observations increases.

Method: Theoretical analysis of the compositional score produced by the GAUSS algorithm, establishing upper bounds on mean squared error in terms of individual score errors and number of observations. Validation using a Gaussian example with closed-form analytical expressions.

Result: Established an upper bound on the mean squared error of compositional scores, showing how error scales with both individual score errors and the number of observations. Demonstrated theoretical findings on a Gaussian example.

Conclusion: The study provides important theoretical insights into error accumulation in compositional score methods for SBI, offering bounds that help understand sampling quality degradation as more observations are combined.

Abstract: Simulation-based inference (SBI) has become a widely used framework in
applied sciences for estimating the parameters of stochastic models that best
explain experimental observations. A central question in this setting is how to
effectively combine multiple observations in order to improve parameter
inference and obtain sharper posterior distributions. Recent advances in
score-based diffusion methods address this problem by constructing a
compositional score, obtained by aggregating individual posterior scores within
the diffusion process. While it is natural to suspect that the accumulation of
individual errors may significantly degrade sampling quality as the number of
observations grows, this important theoretical issue has so far remained
unexplored. In this paper, we study the compositional score produced by the
GAUSS algorithm of Linhart et al. (2024) and establish an upper bound on its
mean squared error in terms of both the individual score errors and the number
of observations. We illustrate our theoretical findings on a Gaussian example,
where all analytical expressions can be derived in a closed form.

</details>


### [112] [Blackwell's Approachability for Sequential Conformal Inference](https://arxiv.org/abs/2510.15824)
*Guillaume Principato,Gilles Stoltz*

Main category: stat.ML

TL;DR: The paper analyzes conformal inference in non-exchangeable settings using Blackwell's approachability theory, recasting adaptive conformal inference as a game and developing calibration-based strategies with strong theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address conformal inference in non-exchangeable environments where traditional exchangeability assumptions don't hold, requiring new theoretical frameworks and practical methods.

Method: Recast adaptive conformal inference as a repeated two-player vector-valued finite game, characterize coverage-efficiency tradeoffs, construct objectives under adversary restrictions, and design calibration-based approachability strategies.

Result: Developed an algorithm with strong theoretical guarantees that provides practical insights into coverage-efficiency tradeoffs in non-exchangeable conformal inference settings.

Conclusion: The proposed approach successfully extends conformal inference to non-exchangeable environments using Blackwell's theory, though computational complexity may limit practical deployment.

Abstract: We study conformal inference in non-exchangeable environments through the
lens of Blackwell's theory of approachability. We first recast adaptive
conformal inference (ACI, Gibbs and Cand\`es, 2021) as a repeated two-player
vector-valued finite game and characterize attainable coverage--efficiency
tradeoffs. We then construct coverage and efficiency objectives under potential
restrictions on the adversary's play, and design a calibration-based
approachability strategy to achieve these goals. The resulting algorithm enjoys
strong theoretical guarantees and provides practical insights, though its
computational burden may limit deployment in practice.

</details>
