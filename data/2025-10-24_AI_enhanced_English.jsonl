{"id": "2510.19934", "categories": ["cs.LG", "cs.CR", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.19934", "abs": "https://arxiv.org/abs/2510.19934", "authors": ["Xiang Li", "Buxin Su", "Chendi Wang", "Qi Long", "Weijie J. Su"], "title": "Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Differentially private (DP) decentralized Federated Learning (FL) allows\nlocal users to collaborate without sharing their data with a central server.\nHowever, accurately quantifying the privacy budget of private FL algorithms is\nchallenging due to the co-existence of complex algorithmic components such as\ndecentralized communication and local updates. This paper addresses privacy\naccounting for two decentralized FL algorithms within the $f$-differential\nprivacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods\ntailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which\nquantifies privacy leakage between user pairs under random-walk communication,\nand Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise\ninjection via shared secrets. By combining tools from $f$-DP theory and Markov\nchain concentration, our accounting framework captures privacy amplification\narising from sparse communication, local iterations, and correlated noise.\nExperiments on synthetic and real datasets demonstrate that our methods yield\nconsistently tighter $(\\epsilon,\\delta)$ bounds and improved utility compared\nto R\\'enyi DP-based approaches, illustrating the benefits of $f$-DP in\ndecentralized privacy accounting.", "AI": {"tldr": "This paper develops new f-DP-based privacy accounting methods for decentralized federated learning, addressing challenges in quantifying privacy budgets due to complex algorithmic components like decentralized communication and local updates.", "motivation": "Accurately quantifying privacy budgets in differentially private decentralized federated learning is challenging due to complex algorithmic components like decentralized communication and local updates, which existing methods struggle to capture effectively.", "method": "The authors develop two new f-DP-based accounting methods: Pairwise Network f-DP (PN-f-DP) for quantifying privacy leakage between user pairs under random-walk communication, and Secret-based f-Local DP (Sec-f-LDP) for supporting structured noise injection via shared secrets. They combine tools from f-DP theory and Markov chain concentration.", "result": "Experiments on synthetic and real datasets show that the proposed methods yield consistently tighter (\u03b5,\u03b4) bounds and improved utility compared to R\u00e9nyi DP-based approaches, demonstrating the benefits of f-DP in decentralized privacy accounting.", "conclusion": "The f-DP framework provides superior privacy accounting for decentralized federated learning, capturing privacy amplification from sparse communication, local iterations, and correlated noise more effectively than existing approaches."}}
{"id": "2510.19960", "categories": ["stat.ME", "math.ST", "stat.TH", "62G07, 62G20, 65D07", "G.3"], "pdf": "https://arxiv.org/pdf/2510.19960", "abs": "https://arxiv.org/abs/2510.19960", "authors": ["Nicholas Tenkorang", "Kwesi Appau Ohene-Obeng", "Xiaogang Su"], "title": "Kernel Density Estimation and Convolution Revisited", "comment": "22 pages, 2 figrues, plus a 8-page supplement", "summary": "Kernel Density Estimation (KDE) is a cornerstone of nonparametric statistics,\nyet it remains sensitive to bandwidth choice, boundary bias, and computational\ninefficiency. This study revisits KDE through a principled convolutional\nframework, providing an intuitive model-based derivation that naturally extends\nto constrained domains, such as positive-valued random variables. Building on\nthis perspective, we introduce SHIDE (Simulation and Histogram Interpolation\nfor Density Estimation), a novel and computationally efficient density\nestimator that generates pseudo-data by adding bounded noise to observations\nand applies spline interpolation to the resulting histogram. The noise is\nsampled from a class of bounded polynomial kernel densities, constructed\nthrough convolutions of uniform distributions, with a natural bandwidth\nparameter defined by the kernel's support bound. We establish the theoretical\nproperties of SHIDE, including pointwise consistency, bias-variance\ndecomposition, and asymptotic MISE, showing that SHIDE attains the classical\n$n^{-4/5}$ convergence rate while mitigating boundary bias. Two data-driven\nbandwidth selection methods are developed, an AMISE-optimal rule and a\npercentile-based alternative, which are shown to be asymptotically equivalent.\nExtensive simulations demonstrate that SHIDE performs comparably to or\nsurpasses KDE across a broad range of models, with particular advantages for\nbounded and heavy-tailed distributions. These results highlight SHIDE as a\ntheoretically grounded and practically robust alternative to traditional KDE.", "AI": {"tldr": "SHIDE is a novel density estimator that uses simulation and histogram interpolation to address limitations of traditional Kernel Density Estimation, offering computational efficiency and boundary bias mitigation.", "motivation": "To overcome KDE's sensitivity to bandwidth choice, boundary bias, and computational inefficiency by developing a principled convolutional framework for density estimation.", "method": "SHIDE generates pseudo-data by adding bounded noise from polynomial kernel densities (constructed via uniform convolutions) to observations, then applies spline interpolation to the resulting histogram.", "result": "SHIDE achieves classical n^{-4/5} convergence rate, mitigates boundary bias, and performs comparably to or surpasses KDE across various models, especially for bounded and heavy-tailed distributions.", "conclusion": "SHIDE provides a theoretically grounded and practically robust alternative to traditional KDE with computational efficiency and improved performance on bounded domains."}}
{"id": "2510.20372", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20372", "abs": "https://arxiv.org/abs/2510.20372", "authors": ["Lucas Darius Konrad", "Nikolas Kuschnig"], "title": "Testing Most Influential Sets", "comment": "9 pages, 1 figure, submitted to ICLR", "summary": "Small subsets of data with disproportionate influence on model outcomes can\nhave dramatic impacts on conclusions, with a few data points sometimes\noverturning key findings. While recent work has developed methods to identify\nthese \\emph{most influential sets}, no formal theory exists to determine when\ntheir influence reflects genuine problems rather than natural sampling\nvariation. We address this gap by developing a principled framework for\nassessing the statistical significance of most influential sets. Our\ntheoretical results characterize the extreme value distributions of maximal\ninfluence and enable rigorous hypothesis tests for excessive influence,\nreplacing current ad-hoc sensitivity checks. We demonstrate the practical value\nof our approach through applications across economics, biology, and machine\nlearning benchmarks.", "AI": {"tldr": "Develops a statistical framework to test whether influential data points represent genuine problems or natural sampling variation, enabling rigorous hypothesis tests for excessive influence.", "motivation": "Small subsets of data can disproportionately influence model outcomes and overturn key findings, but existing methods lack formal theory to determine when this influence reflects genuine problems versus natural sampling variation.", "method": "Develops a principled framework for assessing statistical significance of most influential sets, with theoretical results characterizing extreme value distributions of maximal influence.", "result": "Enables rigorous hypothesis tests for excessive influence that replace current ad-hoc sensitivity checks.", "conclusion": "The approach demonstrates practical value across applications in economics, biology, and machine learning benchmarks."}}
{"id": "2510.20404", "categories": ["stat.ME", "econ.EM", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20404", "abs": "https://arxiv.org/abs/2510.20404", "authors": ["Shuyuan Chen", "Peng Zhang", "Yifan Cui"], "title": "Identification and Debiased Learning of Causal Effects with General Instrumental Variables", "comment": null, "summary": "Instrumental variable methods are fundamental to causal inference when\ntreatment assignment is confounded by unobserved variables. In this article, we\ndevelop a general nonparametric framework for identification and learning with\nmulti-categorical or continuous instrumental variables. Specifically, we\npropose an additive instrumental variable framework to identify mean potential\noutcomes and the average treatment effect with a weighting function. Leveraging\nsemiparametric theory, we derive efficient influence functions and construct\nconsistent, asymptotically normal estimators via debiased machine learning.\nExtensions to longitudinal data, dynamic treatment regimes, and multiplicative\ninstrumental variables are further developed. We demonstrate the proposed\nmethod by employing simulation studies and analyzing real data from the Job\nTraining Partnership Act program.", "AI": {"tldr": "A nonparametric framework for causal inference using instrumental variables with efficient estimators via debiased machine learning.", "motivation": "Address confounding in treatment assignment when unobserved variables are present, using instrumental variables for causal identification.", "method": "Additive instrumental variable framework with weighting functions, semiparametric theory for efficient influence functions, and debiased machine learning estimators.", "result": "Consistent, asymptotically normal estimators demonstrated through simulations and real data analysis from the Job Training Partnership Act program.", "conclusion": "The proposed framework effectively identifies causal effects with instrumental variables and provides robust estimation methods applicable to various extensions."}}
{"id": "2510.19999", "categories": ["stat.ML", "cs.LG", "cs.MS", "cs.NA", "math.NA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.19999", "abs": "https://arxiv.org/abs/2510.19999", "authors": ["Yixiao Wang", "Zishan Shao", "Ting Jiang", "Aditya Devarakonda"], "title": "Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models", "comment": "Equal contribution: Yixiao Wang and Zishan Shao. Correspondence:\n  yw676@duke.edu", "summary": "We present a novel enhanced cyclic coordinate descent (ECCD) framework for\nsolving generalized linear models with elastic net constraints that reduces\ntraining time in comparison to existing state-of-the-art methods. We redesign\nthe CD method by performing a Taylor expansion around the current iterate to\navoid nonlinear operations arising in the gradient computation. By introducing\nthis approximation, we are able to unroll the vector recurrences occurring in\nthe CD method and reformulate the resulting computations into more efficient\nbatched computations. We show empirically that the recurrence can be unrolled\nby a tunable integer parameter, $s$, such that $s > 1$ yields performance\nimprovements without affecting convergence, whereas $s = 1$ yields the original\nCD method. A key advantage of ECCD is that it avoids the convergence delay and\nnumerical instability exhibited by block coordinate descent. Finally, we\nimplement our proposed method in C++ using Eigen to accelerate linear algebra\ncomputations. Comparison of our method against existing state-of-the-art\nsolvers shows consistent performance improvements of $3\\times$ in average for\nregularization path variant on diverse benchmark datasets. Our implementation\nis available at https://github.com/Yixiao-Wang-Stats/ECCD.", "AI": {"tldr": "A novel enhanced cyclic coordinate descent (ECCD) framework for solving generalized linear models with elastic net constraints that achieves 3x faster training time compared to state-of-the-art methods.", "motivation": "To reduce training time for generalized linear models with elastic net constraints by improving upon existing coordinate descent methods and avoiding convergence delays and numerical instability of block coordinate descent.", "method": "Redesigns coordinate descent by performing Taylor expansion around current iterate to avoid nonlinear gradient operations, unrolls vector recurrences into efficient batched computations with tunable parameter s, and implements in C++ using Eigen for linear algebra acceleration.", "result": "Empirical results show consistent 3x performance improvements on regularization path variant across diverse benchmark datasets, with s > 1 providing performance gains without affecting convergence.", "conclusion": "ECCD framework successfully accelerates training for generalized linear models with elastic net constraints while maintaining convergence properties and avoiding numerical instability issues."}}
{"id": "2510.20451", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.20451", "abs": "https://arxiv.org/abs/2510.20451", "authors": ["Yuanshan Gao", "Yang Bai", "Yifan Cui"], "title": "On Multiple Robustness of Proximal Dynamic Treatment Regimes", "comment": null, "summary": "Dynamic treatment regimes are sequential decision rules that adapt treatment\naccording to individual time-varying characteristics and outcomes to achieve\noptimal effects, with applications in precision medicine, personalized\nrecommendations, and dynamic marketing. Estimating optimal dynamic treatment\nregimes via sequential randomized trials might face costly and ethical hurdles,\noften necessitating the use of historical observational data. In this work, we\nutilize proximal causal inference framework for learning optimal dynamic\ntreatment regimes when the unconfoundedness assumption fails. Our contributions\nare four-fold: (i) we propose three nonparametric identification methods for\noptimal dynamic treatment regimes; (ii) we establish the semiparametric\nefficiency bound for the value function of a given regime; (iii) we propose a\n(K+1)-robust method for learning optimal dynamic treatment regimes, where K is\nthe number of stages; (iv) as a by-product for marginal structural models, we\nestablish identification and estimation of counterfactual means under a static\nregime. Numerical experiments validate the efficiency and multiple robustness\nof our proposed methods.", "AI": {"tldr": "Proposes methods for learning optimal dynamic treatment regimes using proximal causal inference when unconfoundedness fails, including identification strategies, efficiency bounds, and robust estimation.", "motivation": "Dynamic treatment regimes adapt treatments based on individual characteristics but estimating them via randomized trials can be costly/ethical, requiring observational data. Unconfoundedness assumption often fails in practice.", "method": "Uses proximal causal inference framework with three nonparametric identification methods, establishes semiparametric efficiency bounds, and proposes (K+1)-robust estimation method for K-stage regimes.", "result": "Numerical experiments validate efficiency and multiple robustness of proposed methods. Also provides identification and estimation of counterfactual means under static regimes as by-product.", "conclusion": "Proposed framework enables learning optimal dynamic treatment regimes from observational data when unconfoundedness fails, with validated efficiency and robustness properties."}}
{"id": "2510.20141", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20141", "abs": "https://arxiv.org/abs/2510.20141", "authors": ["Somayajulu L. N. Dhulipala", "Deep Ray", "Nicholas Forman"], "title": "Compositional Generation for Long-Horizon Coupled PDEs", "comment": null, "summary": "Simulating coupled PDE systems is computationally intensive, and prior\nefforts have largely focused on training surrogates on the joint (coupled)\ndata, which requires a large amount of data. In the paper, we study\ncompositional diffusion approaches where diffusion models are only trained on\nthe decoupled PDE data and are composed at inference time to recover the\ncoupled field. Specifically, we investigate whether the compositional strategy\ncan be feasible under long time horizons involving a large number of time\nsteps. In addition, we compare a baseline diffusion model with that trained\nusing the v-parameterization strategy. We also introduce a symmetric\ncompositional scheme for the coupled fields based on the Euler scheme. We\nevaluate on Reaction-Diffusion and modified Burgers with longer time grids, and\nbenchmark against a Fourier Neural Operator trained on coupled data. Despite\nseeing only decoupled training data, the compositional diffusion models recover\ncoupled trajectories with low error. v-parameterization can improve accuracy\nover a baseline diffusion model, while the neural operator surrogate remains\nstrongest given that it is trained on the coupled data. These results show that\ncompositional diffusion is a viable strategy towards efficient, long-horizon\nmodeling of coupled PDEs.", "AI": {"tldr": "Compositional diffusion models trained on decoupled PDE data can effectively recover coupled trajectories with low error, offering a viable alternative to joint training approaches that require large coupled datasets.", "motivation": "Simulating coupled PDE systems is computationally intensive, and traditional approaches require training surrogates on joint (coupled) data which demands large datasets. This work explores whether compositional approaches using only decoupled data can be effective.", "method": "Compositional diffusion models trained only on decoupled PDE data, composed at inference time to recover coupled fields. Investigated baseline diffusion vs v-parameterization, introduced symmetric compositional scheme based on Euler scheme. Evaluated on Reaction-Diffusion and modified Burgers with longer time grids.", "result": "Despite seeing only decoupled training data, compositional diffusion models recover coupled trajectories with low error. v-parameterization improves accuracy over baseline diffusion model. Neural operator surrogate trained on coupled data remains strongest performer.", "conclusion": "Compositional diffusion is a viable strategy for efficient, long-horizon modeling of coupled PDEs, offering an alternative to data-intensive joint training approaches."}}
{"id": "2510.19872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19872", "abs": "https://arxiv.org/abs/2510.19872", "authors": ["Iman Rahmani", "Saman Yazdannik", "Morteza Tayefi", "Jafar Roshanian"], "title": "An Integrated Approach to Neural Architecture Search for Deep Q-Networks", "comment": null, "summary": "The performance of deep reinforcement learning agents is fundamentally\nconstrained by their neural network architecture, a choice traditionally made\nthrough expensive hyperparameter searches and then fixed throughout training.\nThis work investigates whether online, adaptive architecture optimization can\nescape this constraint and outperform static designs. We introduce NAS-DQN, an\nagent that integrates a learned neural architecture search controller directly\ninto the DRL training loop, enabling dynamic network reconfiguration based on\ncumulative performance feedback. We evaluate NAS-DQN against three\nfixed-architecture baselines and a random search control on a continuous\ncontrol task, conducting experiments over multiple random seeds. Our results\ndemonstrate that NAS-DQN achieves superior final performance, sample\nefficiency, and policy stability while incurring negligible computational\noverhead. Critically, the learned search strategy substantially outperforms\nboth undirected random architecture exploration and poorly-chosen fixed\ndesigns, indicating that intelligent, performance-guided search is the key\nmechanism driving success. These findings establish that architecture\nadaptation is not merely beneficial but necessary for optimal sample efficiency\nin online deep reinforcement learning, and suggest that the design of RL agents\nneed not be a static offline choice but can instead be seamlessly integrated as\na dynamic component of the learning process itself.", "AI": {"tldr": "NAS-DQN integrates neural architecture search into DRL training, enabling dynamic network reconfiguration that outperforms fixed architectures in performance, efficiency, and stability.", "motivation": "Traditional DRL agents use fixed neural architectures chosen through expensive hyperparameter searches, which constrains performance and cannot adapt during training.", "method": "Introduces NAS-DQN, which integrates a learned neural architecture search controller directly into the DRL training loop to enable dynamic network reconfiguration based on cumulative performance feedback.", "result": "NAS-DQN achieves superior final performance, sample efficiency, and policy stability compared to fixed-architecture baselines and random search, with negligible computational overhead.", "conclusion": "Architecture adaptation is necessary for optimal sample efficiency in online DRL, and RL agent design can be integrated as a dynamic component of the learning process rather than a static offline choice."}}
{"id": "2510.20035", "categories": ["stat.ME", "cs.LG", "62H05, 68T05, 62G05", "G.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.20035", "abs": "https://arxiv.org/abs/2510.20035", "authors": ["Thibault Vatter", "Thomas Nagler"], "title": "Throwing Vines at the Wall: Structure Learning via Random Search", "comment": "19 pages, 7 figures, 5 tables, 2 algorithms, 4 appendices", "summary": "Vine copulas offer flexible multivariate dependence modeling and have become\nwidely used in machine learning, yet structure learning remains a key\nchallenge. Early heuristics like the greedy algorithm of Dissmann are still\nconsidered the gold standard, but often suboptimal. We propose random search\nalgorithms that improve structure selection and a statistical framework based\non model confidence sets, which provides theoretical guarantees on selection\nprobabilities and a powerful foundation for ensembling. Empirical results on\nseveral real-world data sets show that our methods consistently outperform\nstate-of-the-art approaches.", "AI": {"tldr": "The paper proposes random search algorithms and a statistical framework using model confidence sets to improve vine copula structure learning, outperforming existing methods.", "motivation": "Vine copulas are widely used for multivariate dependence modeling but structure learning remains challenging, with current heuristics like Dissmann's greedy algorithm often being suboptimal.", "method": "Random search algorithms for structure selection combined with a statistical framework based on model confidence sets that provides theoretical guarantees and supports ensembling.", "result": "Empirical results on real-world datasets demonstrate consistent outperformance over state-of-the-art approaches.", "conclusion": "The proposed methods provide improved structure learning for vine copulas with theoretical guarantees and practical performance gains."}}
{"id": "2510.20344", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20344", "abs": "https://arxiv.org/abs/2510.20344", "authors": ["Wei Cao", "Shanshan Wang"], "title": "Neural Networks for Censored Expectile Regression Based on Data Augmentation", "comment": null, "summary": "Expectile regression neural networks (ERNNs) are powerful tools for capturing\nheterogeneity and complex nonlinear structures in data. However, most existing\nresearch has primarily focused on fully observed data, with limited attention\npaid to scenarios involving censored observations. In this paper, we propose a\ndata augmentation based ERNNs algorithm, termed DAERNN, for modeling\nheterogeneous censored data. The proposed DAERNN is fully data driven, requires\nminimal assumptions, and offers substantial flexibility. Simulation studies and\nreal data applications demonstrate that DAERNN outperforms existing censored\nERNNs methods and achieves predictive performance comparable to models trained\non fully observed data. Moreover, the algorithm provides a unified framework\nfor handling various censoring mechanisms without requiring explicit parametric\nmodel specification, thereby enhancing its applicability to practical censored\ndata analysis.", "AI": {"tldr": "DAERNN is a data augmentation-based expectile regression neural network for handling heterogeneous censored data, outperforming existing censored ERNN methods and achieving performance comparable to models trained on fully observed data.", "motivation": "Existing expectile regression neural networks (ERNNs) mainly focus on fully observed data, with limited attention to censored observations, creating a gap in handling real-world censored data scenarios.", "method": "Proposed DAERNN uses data augmentation approach for expectile regression neural networks, providing a unified framework for various censoring mechanisms without requiring explicit parametric model specification.", "result": "Simulation studies and real data applications show DAERNN outperforms existing censored ERNNs methods and achieves predictive performance comparable to models trained on fully observed data.", "conclusion": "DAERNN offers a flexible, data-driven solution for heterogeneous censored data analysis with minimal assumptions and broad applicability across different censoring mechanisms."}}
{"id": "2510.19873", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19873", "abs": "https://arxiv.org/abs/2510.19873", "authors": ["Junfeng Gong", "Zhiyi Wei", "Junying Chen", "Cheng Liu", "Huawei Li"], "title": "From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph", "comment": null, "summary": "Despite significant evolution of CUDA programming and domain-specific\nlibraries, effectively utilizing GPUs with massively parallel engines remains\ndifficult. Large language models (LLMs) show strong potential in generating\noptimized CUDA code from sequential code. However, using LLMs in practice faces\ntwo major challenges: cloud-based APIs pose risks of code leakage, and local\ndeployment is often computationally expensive and inefficient. These drawbacks\nhave spurred interest in small language models (SLMs), which are more\nlightweight and privacy-friendly. Encouragingly, recent studies show that SLMs\ncan achieve performance comparable to LLMs on specific tasks. While SLMs can\nmatch LLMs on domain-specific tasks, their limited reasoning abilities lead to\nsuboptimal performance in complex CUDA generation according to our experiments.\nTo bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented\ngeneration framework that transfers LLM-level reasoning to smaller models.\nReGraphT organizes CUDA optimization trajectories into a structured reasoning\ngraph, modeling the combined CUDA optimizations as state transitions, and\nleverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also\npresent a CUDA-specific benchmark with difficulty tiers defined by reasoning\ncomplexity to evaluate models more comprehensively. Experiments show that\nReGraphT outperforms HPC-specific fine-tuned models and other\nretrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval\nand ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and\nQwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level\nperformance without the associated privacy risks or excessive computing\noverhead.", "AI": {"tldr": "ReGraphT is a training-free framework that enables small language models (SLMs) to generate optimized CUDA code by leveraging retrieval-augmented generation and Monte Carlo Graph Search, achieving LLM-level performance without privacy risks or excessive computing costs.", "motivation": "While LLMs can generate optimized CUDA code, they face privacy risks (cloud APIs) and computational inefficiency (local deployment). SLMs are more lightweight and privacy-friendly but lack reasoning abilities for complex CUDA generation tasks.", "method": "ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph modeling state transitions, and uses Monte Carlo Graph Search for efficient exploration. It transfers LLM-level reasoning to smaller models without training.", "result": "ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving 2.33X average speedup on CUDAEval and ParEval benchmarks. When paired with specific SLMs, it enables them to approach LLM-level performance.", "conclusion": "ReGraphT successfully bridges the gap between SLMs and LLMs for CUDA code generation, providing LLM-level reasoning capabilities to smaller models while maintaining privacy and computational efficiency."}}
{"id": "2510.20147", "categories": ["stat.ME", "stat.CO", "62H12, 62F10, 65C60"], "pdf": "https://arxiv.org/pdf/2510.20147", "abs": "https://arxiv.org/abs/2510.20147", "authors": ["Qingyang Liu", "Sanvesh Srivastava", "Dipankar Bandyopadhyay"], "title": "Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses", "comment": "48 pages, 7 figures", "summary": "We propose a regression model with matrix-variate skew-t response (REGMVST)\nfor analyzing irregular longitudinal data with skewness, symmetry, or heavy\ntails. REGMVST models matrix-variate responses and predictors, with rows\nindexing longitudinal measurements per subject. It uses the matrix-variate\nskew-t (MVST) distribution to handle skewness and heavy tails, a damped\nexponential correlation (DEC) structure for row-wise dependencies across\nirregular time profiles, and leaves the column covariance unstructured. For\nestimation, we initially develop an ECME algorithm for parameter estimation and\nfurther mitigate its computational bottleneck via an asynchronous and\ndistributed ECME (ADECME) extension. ADECME accelerates the E-step through\nparallelization, and retains the simplicity of the conditional M-step, enabling\nscalable inference. Simulations using synthetic data and a case study exploring\nmatrix-variate periodontal disease endpoints derived from electronic health\nrecords demonstrate ADECME's superiority in efficiency and convergence, over\nthe alternatives. We also provide theoretical support for our empirical\nobservations and identify regularity assumptions for ADECME's optimal\nperformance. An accompanying R package is available at\nhttps://github.com/rh8liuqy/STMATREG.", "AI": {"tldr": "REGMVST is a regression model for irregular longitudinal data using matrix-variate skew-t distribution to handle skewness, heavy tails, and irregular time dependencies, with an efficient ADECME algorithm for scalable estimation.", "motivation": "To analyze irregular longitudinal data with potential skewness, symmetry, or heavy tails, which traditional models may not adequately handle, especially when dealing with matrix-variate responses and predictors.", "method": "Uses matrix-variate skew-t (MVST) distribution for responses, damped exponential correlation (DEC) structure for row dependencies across irregular time profiles, and unstructured column covariance. Develops ECME algorithm for estimation and ADECME extension for parallelized computation.", "result": "ADECME demonstrates superior efficiency and convergence compared to alternatives in both synthetic data simulations and a real-world periodontal disease case study using electronic health records.", "conclusion": "REGMVST with ADECME provides an effective framework for analyzing irregular longitudinal data with complex distributional characteristics, offering scalable inference with theoretical support and practical implementation through an R package."}}
{"id": "2510.19889", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19889", "abs": "https://arxiv.org/abs/2510.19889", "authors": ["Mostafa Ameli", "Van Anh Le", "Sulthana Shams", "Alexander Skabardonis"], "title": "From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem", "comment": null, "summary": "The traffic assignment problem is essential for traffic flow analysis,\ntraditionally solved using mathematical programs under the Equilibrium\nprinciple. These methods become computationally prohibitive for large-scale\nnetworks due to non-linear growth in complexity with the number of OD pairs.\nThis study introduces a novel data-driven approach using deep neural networks,\nspecifically leveraging the Transformer architecture, to predict equilibrium\npath flows directly. By focusing on path-level traffic distribution, the\nproposed model captures intricate correlations between OD pairs, offering a\nmore detailed and flexible analysis compared to traditional link-level\napproaches. The Transformer-based model drastically reduces computation time,\nwhile adapting to changes in demand and network structure without the need for\nrecalculation. Numerical experiments are conducted on the Manhattan-like\nsynthetic network, the Sioux Falls network, and the Eastern-Massachusetts\nnetwork. The results demonstrate that the proposed model is orders of magnitude\nfaster than conventional optimization. It efficiently estimates path-level\ntraffic flows in multi-class networks, reducing computational costs and\nimproving prediction accuracy by capturing detailed trip and flow information.\nThe model also adapts flexibly to varying demand and network conditions,\nsupporting traffic management and enabling rapid `what-if' analyses for\nenhanced transportation planning and policy-making.", "AI": {"tldr": "This paper introduces a Transformer-based deep learning approach to predict equilibrium path flows in traffic assignment, offering significant computational speedup over traditional optimization methods while maintaining accuracy and adaptability to changing network conditions.", "motivation": "Traditional traffic assignment methods become computationally prohibitive for large-scale networks due to non-linear complexity growth with OD pairs, creating a need for more efficient approaches.", "method": "The study proposes a data-driven approach using deep neural networks with Transformer architecture to directly predict equilibrium path flows, capturing intricate correlations between OD pairs at the path level rather than traditional link-level analysis.", "result": "Numerical experiments on Manhattan-like synthetic, Sioux Falls, and Eastern-Massachusetts networks show the model is orders of magnitude faster than conventional optimization while efficiently estimating path-level traffic flows in multi-class networks with improved prediction accuracy.", "conclusion": "The Transformer-based model drastically reduces computation time, adapts to demand and network changes without recalculation, and enables rapid 'what-if' analyses for enhanced transportation planning and policy-making."}}
{"id": "2510.20191", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20191", "abs": "https://arxiv.org/abs/2510.20191", "authors": ["Mingxuan Ge", "Dae Woong Ham"], "title": "Bias-Variance Tradeoff of Matching Prior to Difference-in-Differences When Parallel Trends is Violated", "comment": null, "summary": "Quasi-experimental causal inference methods have become central in empirical\noperations management (OM) for guiding managerial decisions. Among these,\nempiricists utilize the Difference-in-Differences (DiD) estimator, which relies\non the parallel trends assumption. To improve its plausibility, researchers\noften match treated and control units before applying DiD, with the intuition\nthat matched groups are more likely to evolve similarly absent treatment.\nExisting work that analyze this practice, however, has focused solely on bias.\nWe complement and fill an important gap by analyzing the full bias-variance\ntradeoff. Under a linear structural model with unobserved time-varying\nconfounders, we show that variance results contrast with established bias\ninsights: matching on observed covariates prior to DiD is not always\nrecommended over the classic (unmatched) DiD due to a sample size tradeoff;\nfurthermore, matching additionally on pre-treatment outcomes is always\nbeneficial as such tradeoff no longer exists once matching is performed. We\ntherefore advocate mean squared error (MSE) as a final metric and give\npractitioner-friendly guidelines with theoretical guarantees on when (and on\nwhat variables) they should match on. We apply these insights to a recent study\non how the introduction of monetary incentives by a knowledge-sharing platform\naffects its general engagement and show that the authors' matching choice prior\nto DiD was both warranted and critical. In particular, we provide new\nmanagerial insights that after a full bias correction, their estimated effect\nwith matching still remains statistically significant, demonstrating that the\nchosen matching-DiD approach is sufficiently robust to address managerial\nconcerns over violations of parallel trends.", "AI": {"tldr": "This paper analyzes the bias-variance tradeoff in matching before Difference-in-Differences (DiD) estimation, showing that matching on observed covariates isn't always beneficial due to sample size tradeoffs, but matching on pre-treatment outcomes always helps.", "motivation": "To address the gap in existing research that focuses only on bias in matching-DiD methods, and to provide comprehensive analysis of the full bias-variance tradeoff for better causal inference in operations management.", "method": "Uses a linear structural model with unobserved time-varying confounders to analyze bias-variance tradeoffs, develops theoretical guarantees for matching decisions, and applies insights to a real-world knowledge-sharing platform case study.", "result": "Matching on observed covariates before DiD is not always recommended due to sample size tradeoffs, but matching on pre-treatment outcomes is always beneficial. The MSE metric provides better guidance for matching decisions.", "conclusion": "Researchers should use mean squared error as the final metric for matching decisions, with practitioner-friendly guidelines on when and what variables to match on. The matching-DiD approach is robust for addressing parallel trends violations in managerial contexts."}}
{"id": "2510.20436", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20436", "abs": "https://arxiv.org/abs/2510.20436", "authors": ["Federico Lozano-Cuadra", "Beatriz Soret", "Marc Sanchez Net", "Abhishek Cauligi", "Federico Rossi"], "title": "Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks", "comment": null, "summary": "We present a fully decentralized routing framework for multi-robot\nexploration missions operating under the constraints of a Lunar Delay-Tolerant\nNetwork (LDTN). In this setting, autonomous rovers must relay collected data to\na lander under intermittent connectivity and unknown mobility patterns. We\nformulate the problem as a Partially Observable Markov Decision Problem (POMDP)\nand propose a Graph Attention-based Multi-Agent Reinforcement Learning\n(GAT-MARL) policy that performs Centralized Training, Decentralized Execution\n(CTDE). Our method relies only on local observations and does not require\nglobal topology updates or packet replication, unlike classical approaches such\nas shortest path and controlled flooding-based algorithms. Through Monte Carlo\nsimulations in randomized exploration environments, GAT-MARL provides higher\ndelivery rates, no duplications, and fewer packet losses, and is able to\nleverage short-term mobility forecasts; offering a scalable solution for future\nspace robotic systems for planetary exploration, as demonstrated by successful\ngeneralization to larger rover teams.", "AI": {"tldr": "A decentralized routing framework using Graph Attention-based Multi-Agent Reinforcement Learning for multi-robot exploration in Lunar Delay-Tolerant Networks, achieving higher delivery rates without packet duplication.", "motivation": "To address the challenges of multi-robot exploration missions in Lunar Delay-Tolerant Networks where rovers must relay data to a lander under intermittent connectivity and unknown mobility patterns.", "method": "Formulated as a Partially Observable Markov Decision Problem (POMDP) using Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) with Centralized Training, Decentralized Execution (CTDE), relying only on local observations without global topology updates.", "result": "Monte Carlo simulations showed higher delivery rates, no duplications, fewer packet losses, and ability to leverage short-term mobility forecasts. Successfully generalized to larger rover teams.", "conclusion": "The GAT-MARL approach offers a scalable solution for future space robotic systems in planetary exploration, outperforming classical approaches like shortest path and controlled flooding-based algorithms."}}
{"id": "2510.19893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19893", "abs": "https://arxiv.org/abs/2510.19893", "authors": ["Shiqi Dai", "Wei Dai", "Jiaee Cheong", "Paul Pu Liang"], "title": "FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning", "comment": "Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop", "summary": "Medical artificial intelligence systems have achieved remarkable diagnostic\ncapabilities, yet they consistently exhibit performance disparities across\ndemographic groups, causing real-world harm to underrepresented populations.\nWhile recent multimodal reasoning foundation models have advanced clinical\ndiagnosis through integrated analysis of diverse medical data, reasoning\ntrainings via reinforcement learning inherit and often amplify biases present\nin training datasets dominated by majority populations. We introduce\nFairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical\nreinforcement learning approach that promotes equitable learning across\nheterogeneous clinical populations. FairGRPO employs adaptive importance\nweighting of advantages based on representation, task difficulty, and data\nsource. To address the common issue of missing demographic labels in the\nclinical domain, we further employ unsupervised clustering, which automatically\ndiscovers latent demographic groups when labels are unavailable. Through\ncomprehensive experiments across 7 clinical diagnostic datasets spanning 5\nclinical modalities across X-ray, CT scan, dermoscropy, mammography and\nultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%\nagainst all vanilla and bias mitigated RL baselines, while improving F1 score\nby 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO\nprogressively improves fairness throughout optimization, while baseline RL\nmethods exhibit deteriorating fairness as training progresses. Based on\nFairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that\nachieves state-of-the-art performance while demonstrating significantly reduced\ndisparities across demographic groups.", "AI": {"tldr": "FairGRPO is a hierarchical reinforcement learning method that addresses fairness issues in medical AI by using adaptive importance weighting and unsupervised clustering to reduce performance disparities across demographic groups, achieving 27.2% better predictive parity and 12.49% higher F1 score.", "motivation": "Medical AI systems exhibit performance disparities across demographic groups, causing harm to underrepresented populations, and existing multimodal reasoning models amplify biases from training data dominated by majority populations.", "method": "FairGRPO employs hierarchical reinforcement learning with adaptive importance weighting based on representation, task difficulty, and data source, plus unsupervised clustering to discover latent demographic groups when labels are unavailable.", "result": "Across 7 clinical datasets spanning 5 modalities, FairGRPO reduces predictive parity by 27.2% against all baselines while improving F1 score by 12.49%, and training dynamics show progressive fairness improvement.", "conclusion": "FairGRPO effectively addresses fairness issues in medical AI, leading to the release of FairMedGemma-4B, a fairness-aware clinical VLLM that achieves state-of-the-art performance with significantly reduced demographic disparities."}}
{"id": "2510.20259", "categories": ["stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.20259", "abs": "https://arxiv.org/abs/2510.20259", "authors": ["Bowen Gang", "Hongmei Lin", "Tiejun Tong"], "title": "Unifying Boxplots: A Multiple Testing Perspective", "comment": null, "summary": "Tukey's boxplot is a foundational tool for exploratory data analysis, but its\nclassic outlier-flagging rule does not account for the sample size, and\nsubsequent modifications have often been presented as separate, heuristic\nadjustments. In this paper, we propose a unifying framework that recasts the\nboxplot and its variants as graphical implementations of multiple testing\nprocedures. We demonstrate that Tukey's original method is equivalent to an\nunadjusted procedure, while existing sample-size-aware modifications correspond\nto controlling the Family-Wise Error Rate (FWER) or the Per-Family Error Rate\n(PFER). This perspective not only systematizes existing methods but also\nnaturally leads to new, more adaptive constructions. We introduce a boxplot\nmotivated by the False Discovery Rate (FDR), and show how our framework\nprovides a flexible pipeline for integrating state-of-the-art robust estimation\ntechniques directly into the boxplot's graphical format. By connecting a\nclassic graphical tool to the principles of multiple testing, our work provides\na principled language for comparing, critiquing, and extending outlier\ndetection rules for modern exploratory analysis.", "AI": {"tldr": "This paper reframes Tukey's boxplot and its variants as graphical implementations of multiple testing procedures, providing a unifying framework that connects classic outlier detection to modern statistical principles.", "motivation": "Tukey's classic boxplot outlier detection doesn't account for sample size, and existing modifications have been presented as separate heuristic adjustments rather than a unified framework.", "method": "The authors propose a framework that recasts boxplots as graphical implementations of multiple testing procedures, showing that Tukey's method is equivalent to an unadjusted procedure while existing modifications correspond to controlling FWER or PFER.", "result": "The framework systematizes existing methods and leads to new adaptive constructions, including a boxplot motivated by False Discovery Rate (FDR), and provides a flexible pipeline for integrating robust estimation techniques into the boxplot format.", "conclusion": "By connecting classic graphical tools to multiple testing principles, this work provides a principled language for comparing, critiquing, and extending outlier detection rules for modern exploratory analysis."}}
{"id": "2510.20595", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20595", "abs": "https://arxiv.org/abs/2510.20595", "authors": ["Yunyi Shen", "Alexander Gagliano"], "title": "Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences", "comment": null, "summary": "Self-supervised learning has become a central strategy for representation\nlearning, but the majority of architectures used for encoding data have only\nbeen validated on regularly-sampled inputs such as images, audios. and videos.\nIn many scientific domains, data instead arrive as long, irregular, and\nmultimodal sequences. To extract semantic information from these data, we\nintroduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes\nheterogeneous measurements, compresses them with a Perceiver encoder, and\nreconstructs them with a Perceiver-IO diffusion decoder, enabling scalable\nlearning in diverse data settings. To benchmark the daep architecture, we adapt\nthe masked autoencoder to a Perceiver encoder/decoder design, and establish a\nstrong baseline (maep) in the same architectural family as daep. Across diverse\nspectroscopic and photometric astronomical datasets, daep achieves lower\nreconstruction errors, produces more discriminative latent spaces, and better\npreserves fine-scale structure than both VAE and maep baselines. These results\nestablish daep as an effective framework for scientific domains where data\narrives as irregular, heterogeneous sequences.", "AI": {"tldr": "DAEP is a diffusion autoencoder with Perceivers that handles irregular, multimodal sequences in scientific domains, outperforming VAE and MAEP baselines in reconstruction quality and latent space discriminability.", "motivation": "Most self-supervised learning architectures are designed for regularly-sampled data like images and videos, but scientific data often arrives as long, irregular, multimodal sequences that require specialized handling.", "method": "DAEP tokenizes heterogeneous measurements, compresses them with a Perceiver encoder, and reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable learning in diverse data settings.", "result": "Across diverse spectroscopic and photometric astronomical datasets, DAEP achieves lower reconstruction errors, produces more discriminative latent spaces, and better preserves fine-scale structure than both VAE and MAEP baselines.", "conclusion": "DAEP establishes an effective framework for scientific domains where data arrives as irregular, heterogeneous sequences, providing superior performance over existing methods."}}
{"id": "2510.19917", "categories": ["cs.LG", "cs.CV", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.19917", "abs": "https://arxiv.org/abs/2510.19917", "authors": ["Trajan Murphy", "Akshunna S. Dogra", "Hanfeng Gu", "Caleb Meredith", "Mark Kon", "Julio Enrique Castrillion-Candas"], "title": "FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals", "comment": "30 pages, 11 figures, 8 tables. Code available at\n  https://github.com/MathePhysics/FINDER", "summary": "''Noisy'' datasets (regimes with low signal to noise ratios, small sample\nsizes, faulty data collection, etc) remain a key research frontier for\nclassification methods with both theoretical and practical implications. We\nintroduce FINDER, a rigorous framework for analyzing generic classification\nproblems, with tailored algorithms for noisy datasets. FINDER incorporates\nfundamental stochastic analysis ideas into the feature learning and inference\nstages to optimally account for the randomness inherent to all empirical\ndatasets. We construct ''stochastic features'' by first viewing empirical\ndatasets as realizations from an underlying random field (without assumptions\non its exact distribution) and then mapping them to appropriate Hilbert spaces.\nThe Kosambi-Karhunen-Lo\\'eve expansion (KLE) breaks these stochastic features\ninto computable irreducible components, which allow classification over noisy\ndatasets via an eigen-decomposition: data from different classes resides in\ndistinct regions, identified by analyzing the spectrum of the associated\noperators. We validate FINDER on several challenging, data-deficient scientific\ndomains, producing state of the art breakthroughs in: (i) Alzheimer's Disease\nstage classification, (ii) Remote sensing detection of deforestation. We end\nwith a discussion on when FINDER is expected to outperform existing methods,\nits failure modes, and other limitations.", "AI": {"tldr": "FINDER is a classification framework for noisy datasets that uses stochastic analysis and Hilbert space mapping to create stochastic features, then applies KLE decomposition for eigen-based classification, achieving state-of-the-art results in Alzheimer's disease and deforestation detection.", "motivation": "Addressing classification challenges in noisy datasets with low signal-to-noise ratios, small sample sizes, and faulty data collection, which remain key research frontiers with both theoretical and practical implications.", "method": "Creates stochastic features by viewing datasets as realizations from underlying random fields, maps them to Hilbert spaces, uses Kosambi-Karhunen-Lo\u00e8ve expansion to break features into irreducible components, and performs classification via eigen-decomposition by analyzing operator spectra.", "result": "Achieved state-of-the-art breakthroughs in Alzheimer's Disease stage classification and remote sensing detection of deforestation on challenging, data-deficient scientific domains.", "conclusion": "FINDER provides a rigorous framework for noisy dataset classification with discussion on when it outperforms existing methods, its failure modes, and limitations."}}
{"id": "2510.20424", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20424", "abs": "https://arxiv.org/abs/2510.20424", "authors": ["Patrick O'Toole", "Christian Rohrbeck", "Jordan Richards"], "title": "Clustering of multivariate tail dependence using conditional methods", "comment": null, "summary": "The conditional extremes (CE) framework has proven useful for analysing the\njoint tail behaviour of random vectors. However, when applied across many\nlocations or variables, it can be difficult to interpret or compare the\nresulting extremal dependence structures, particularly for high dimensional\nvectors. To address this, we propose a novel clustering method for multivariate\nextremes using the CE framework. Our approach introduces a closed-form,\ncomputationally efficient dissimilarity measure for multivariate tails, based\non the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary\ndimensions. Applying standard clustering algorithms to a matrix of pairwise\ndistances, we obtain interpretable groups of random vectors with homogeneous\ntail dependence. Simulation studies demonstrate that our method outperforms\nexisting approaches for clustering bivariate extremes, and uniquely extends to\nthe multivariate setting. In our application to Irish meteorological data, our\nclustering identifies spatially coherent regions with similar extremal\ndependence between precipitation and wind speeds.", "AI": {"tldr": "A novel clustering method for multivariate extremes using the conditional extremes framework, introducing a computationally efficient dissimilarity measure based on skew-geometric Jensen-Shannon divergence that works in arbitrary dimensions.", "motivation": "The conditional extremes framework is difficult to interpret and compare across many locations or variables, particularly for high-dimensional vectors, requiring a method to identify interpretable groups with homogeneous tail dependence.", "method": "Proposed clustering using CE framework with a closed-form, computationally efficient dissimilarity measure based on skew-geometric Jensen-Shannon divergence, applied to pairwise distance matrices using standard clustering algorithms.", "result": "Method outperforms existing approaches for clustering bivariate extremes and uniquely extends to multivariate setting; application to Irish meteorological data identifies spatially coherent regions with similar extremal dependence between precipitation and wind speeds.", "conclusion": "The proposed clustering method effectively groups random vectors with homogeneous tail dependence, providing interpretable results for multivariate extremes analysis across multiple locations and variables."}}
{"id": "2510.20653", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20653", "abs": "https://arxiv.org/abs/2510.20653", "authors": ["Jack Butler", "Nikita Kozodoi", "Zainab Afolabi", "Brian Tyacke", "Gaiar Baimuratov"], "title": "Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection", "comment": null, "summary": "As Large Language Models (LLMs) continue to evolve, practitioners face\nincreasing options for enhancing inference-time performance without model\nretraining, including budget tuning and multi-step techniques like\nself-reflection. While these methods improve output quality, they create\ncomplex trade-offs among accuracy, cost, and latency that remain poorly\nunderstood across different domains. This paper systematically compares\nself-reflection and budget tuning across mathematical reasoning and translation\ntasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and\nMistral families, along with other models under varying reflection depths and\ncompute budgets to derive Pareto optimal performance frontiers. Our analysis\nreveals substantial domain dependent variation in self-reflection\neffectiveness, with performance gains up to 220\\% in mathematical reasoning. We\nfurther investigate how reflection round depth and feedback mechanism quality\ninfluence performance across model families. To validate our findings in a\nreal-world setting, we deploy a self-reflection enhanced marketing content\nlocalisation system at Lounge by Zalando, where it shows market-dependent\neffectiveness, reinforcing the importance of domain specific evaluation when\ndeploying these techniques. Our results provide actionable guidance for\nselecting optimal inference strategies given specific domains and resource\nconstraints. We open source our self-reflection implementation for\nreproducibility at\nhttps://github.com/aws-samples/sample-genai-reflection-for-bedrock.", "AI": {"tldr": "This paper systematically compares self-reflection and budget tuning techniques for LLMs across mathematical reasoning and translation tasks, revealing substantial domain-dependent performance variations and providing guidance for optimal inference strategies.", "motivation": "As LLMs evolve, practitioners face complex trade-offs among accuracy, cost, and latency when using inference-time enhancement techniques like self-reflection and budget tuning, which remain poorly understood across different domains.", "method": "The study evaluates prominent LLMs (Anthropic Claude, Amazon Nova, Mistral families) across mathematical reasoning and translation tasks under varying reflection depths and compute budgets, deriving Pareto optimal performance frontiers and investigating how reflection round depth and feedback mechanism quality influence performance.", "result": "Analysis reveals substantial domain-dependent variation in self-reflection effectiveness, with performance gains up to 220% in mathematical reasoning. A real-world deployment at Lounge by Zalando shows market-dependent effectiveness, reinforcing the importance of domain-specific evaluation.", "conclusion": "The findings provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints, and the self-reflection implementation is open-sourced for reproducibility."}}
{"id": "2510.19933", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19933", "abs": "https://arxiv.org/abs/2510.19933", "authors": ["Egor Shulgin", "Sultan AlRashed", "Francesco Orabona", "Peter Richt\u00e1rik"], "title": "Beyond the Ideal: Analyzing the Inexact Muon Update", "comment": null, "summary": "The Muon optimizer has rapidly emerged as a powerful, geometry-aware\nalternative to AdamW, demonstrating strong performance in large-scale training\nof neural networks. However, a critical theory-practice disconnect exists:\nMuon's efficiency relies on fast, approximate orthogonalization, yet all prior\ntheoretical work analyzes an idealized, computationally intractable version\nassuming exact SVD-based updates. This work moves beyond the ideal by providing\nthe first analysis of the inexact orthogonalized update at Muon's core. We\ndevelop our analysis within the general framework of Linear Minimization Oracle\n(LMO)-based optimization, introducing a realistic additive error model to\ncapture the inexactness of practical approximation schemes. Our analysis yields\nexplicit bounds that quantify performance degradation as a function of the LMO\ninexactness/error. We reveal a fundamental coupling between this inexactness\nand the optimal step size and momentum: lower oracle precision requires a\nsmaller step size but larger momentum parameter. These findings elevate the\napproximation procedure (e.g., the number of Newton-Schulz steps) from an\nimplementation detail to a critical parameter that must be co-tuned with the\nlearning schedule. NanoGPT experiments directly confirm the predicted coupling,\nwith optimal learning rates clearly shifting as approximation precision\nchanges.", "AI": {"tldr": "This paper provides the first theoretical analysis of Muon optimizer's inexact orthogonalization, revealing that approximation precision affects optimal learning rates and momentum parameters.", "motivation": "There's a theory-practice disconnect in Muon optimizer analysis - prior work studied idealized exact SVD updates, but practical implementations use fast approximate orthogonalization that was not theoretically analyzed.", "method": "The authors analyze inexact orthogonalized updates within the Linear Minimization Oracle (LMO) framework, using an additive error model to capture practical approximation schemes. They develop explicit bounds quantifying performance degradation from LMO inexactness.", "result": "The analysis reveals fundamental coupling between approximation inexactness and optimal step size/momentum: lower precision requires smaller step sizes but larger momentum. NanoGPT experiments confirm this predicted coupling, showing optimal learning rates shift with approximation precision.", "conclusion": "The approximation procedure (e.g., number of Newton-Schulz steps) should be treated as a critical parameter that must be co-tuned with learning schedules, rather than just an implementation detail."}}
{"id": "2510.20741", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.20741", "abs": "https://arxiv.org/abs/2510.20741", "authors": ["Melody Owen", "Fan Li", "Ruyi Liu", "Donna Spiegelman"], "title": "A comparison of methods for designing hybrid type 2 cluster-randomized trials with continuous effectiveness and implementation endpoints", "comment": null, "summary": "Hybrid type 2 studies are gaining popularity for their ability to assess both\nimplementation and health outcomes as co-primary endpoints. Often conducted as\ncluster-randomized trials (CRTs), five design methods can validly power these\nstudies: p-value adjustment methods, combined outcomes approach, single\nweighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared\nall of the methods theoretically and numerically. Theoretical comparisons of\nthe power equations allowed us to identify if any method globally had more or\nless power than other methods. It was shown that the p-value adjustment methods\nare always less powerful than the combined outcomes approach and the single\n1-DF test. We also identified the conditions under which the disjunctive 2-DF\ntest is less powerful than the single 1-DF test. Because our theoretical\ncomparison showed that some methods could be more powerful than others under\ncertain conditions, and less powerful under others, we conducted a numerical\nstudy to understand these differences. The crt2power R package was created to\ncalculate the power or sample size for CRTs with two continuous co-primary\nendpoints. Using this package, we conducted a numerical evaluation across\n30,000 input scenarios to compare statistical power. Specific patterns were\nidentified where a certain method consistently achieved the highest power. When\nthe treatment effects are unequal, the disjunctive 2-DF test tends to have\nhigher power. When the treatment effect sizes are the same, the single 1-DF\ntest tends to have higher power. Together, these comparisons provide clearer\ninsights to guide method selection for powering hybrid type 2 studies.", "AI": {"tldr": "Comparison of five design methods for powering hybrid type 2 cluster-randomized trials with two co-primary endpoints, showing that p-value adjustment methods are least powerful and identifying conditions where disjunctive 2-DF test or single 1-DF test perform best.", "motivation": "Hybrid type 2 studies are increasingly popular for assessing both implementation and health outcomes, but there's limited guidance on optimal power calculation methods for these cluster-randomized trials with co-primary endpoints.", "method": "Theoretical comparison of power equations and numerical evaluation using crt2power R package across 30,000 scenarios to compare five methods: p-value adjustment, combined outcomes, single weighted 1-DF test, disjunctive 2-DF test, and conjunctive test.", "result": "P-value adjustment methods are always less powerful than combined outcomes and single 1-DF test. Disjunctive 2-DF test has higher power when treatment effects are unequal, while single 1-DF test performs better when treatment effect sizes are equal.", "conclusion": "The study provides clear guidance for method selection in hybrid type 2 studies: use disjunctive 2-DF test for unequal treatment effects and single 1-DF test for equal treatment effects, while avoiding p-value adjustment methods due to their lower power."}}
{"id": "2510.20055", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.20055", "abs": "https://arxiv.org/abs/2510.20055", "authors": ["Yuwei Cheng", "Zifeng Zhao", "Haifeng Xu"], "title": "Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards", "comment": null, "summary": "Online advertising platforms use automated auctions to connect advertisers\nwith potential customers, requiring effective bidding strategies to maximize\nprofits. Accurate ad impact estimation requires considering three key factors:\ndelayed and long-term effects, cumulative ad impacts such as reinforcement or\nfatigue, and customer heterogeneity. However, these effects are often not\njointly addressed in previous studies. To capture these factors, we model ad\nbidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson\nrewards. For efficient estimation, we propose a two-stage maximum likelihood\nestimator combined with data-splitting strategies, ensuring controlled\nestimation error based on the first-stage estimator's (in)accuracy. Building on\nthis, we design a reinforcement learning algorithm to derive efficient\npersonalized bidding strategies. This approach achieves a near-optimal regret\nbound of $\\tilde{O}{(dH^2\\sqrt{T})}$, where $d$ is the contextual dimension,\n$H$ is the number of rounds, and $T$ is the number of customers. Our\ntheoretical findings are validated by simulation experiments.", "AI": {"tldr": "The paper proposes a contextual Markov decision process framework with delayed Poisson rewards to model online ad bidding, addressing delayed/long-term effects, cumulative impacts, and customer heterogeneity. It develops a two-stage maximum likelihood estimator and reinforcement learning algorithm for personalized bidding strategies with near-optimal regret bounds.", "motivation": "Online advertising platforms require effective bidding strategies, but existing approaches often fail to jointly consider three critical factors: delayed and long-term ad effects, cumulative impacts (reinforcement/fatigue), and customer heterogeneity.", "method": "Model ad bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson rewards. Use a two-stage maximum likelihood estimator with data-splitting strategies for efficient estimation, then design a reinforcement learning algorithm for personalized bidding strategies.", "result": "The approach achieves a near-optimal regret bound of $\\tilde{O}{(dH^2\\sqrt{T})}$, where d is contextual dimension, H is number of rounds, and T is number of customers. Simulation experiments validate the theoretical findings.", "conclusion": "The proposed CMDP framework with delayed Poisson rewards effectively captures complex ad impact dynamics, and the developed estimation and learning methods provide efficient personalized bidding strategies with strong theoretical guarantees."}}
{"id": "2510.19941", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19941", "abs": "https://arxiv.org/abs/2510.19941", "authors": ["Matan Tsipory", "Ran Levinstein", "Itay Evron", "Mark Kong", "Deanna Needell", "Daniel Soudry"], "title": "Are Greedy Task Orderings Better Than Random in Continual Linear Regression?", "comment": "Accepted to NeurIPS 2025", "summary": "We analyze task orderings in continual learning for linear regression,\nassuming joint realizability of training data. We focus on orderings that\ngreedily maximize dissimilarity between consecutive tasks, a concept briefly\nexplored in prior work but still surrounded by open questions. Using tools from\nthe Kaczmarz method literature, we formalize such orderings and develop\ngeometric and algebraic intuitions around them. Empirically, we demonstrate\nthat greedy orderings converge faster than random ones in terms of the average\nloss across tasks, both for linear regression with random data and for linear\nprobing on CIFAR-100 classification tasks. Analytically, in a high-rank\nregression setting, we prove a loss bound for greedy orderings analogous to\nthat of random ones. However, under general rank, we establish a\nrepetition-dependent separation. Specifically, while prior work showed that for\nrandom orderings, with or without replacement, the average loss after $k$\niterations is bounded by $\\mathcal{O}(1/\\sqrt{k})$, we prove that single-pass\ngreedy orderings may fail catastrophically, whereas those allowing repetition\nconverge at rate $\\mathcal{O}(1/\\sqrt[3]{k})$. Overall, we reveal nuances\nwithin and between greedy and random orderings.", "AI": {"tldr": "This paper analyzes greedy task orderings in continual learning for linear regression, showing they converge faster than random orderings but with important nuances - single-pass greedy can fail catastrophically while greedy with repetition converges at O(1/\u221bk) rate.", "motivation": "To understand task orderings in continual learning, particularly greedy orderings that maximize dissimilarity between consecutive tasks, which were briefly explored in prior work but still have open questions.", "method": "Uses tools from Kaczmarz method literature to formalize greedy orderings, develops geometric and algebraic intuitions, and conducts empirical analysis on linear regression and CIFAR-100 classification, plus analytical proofs for different rank settings.", "result": "Greedy orderings converge faster than random ones in average loss across tasks. In high-rank settings, greedy has similar bounds to random, but under general rank: single-pass greedy can fail catastrophically while greedy with repetition converges at O(1/\u221bk) rate vs random's O(1/\u221ak).", "conclusion": "Reveals important nuances in task ordering strategies - greedy orderings offer faster convergence but require careful handling (repetition) to avoid catastrophic failure, showing complex trade-offs between greedy and random approaches in continual learning."}}
{"id": "2510.20242", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.20242", "abs": "https://arxiv.org/abs/2510.20242", "authors": ["Stephan Rabanser", "Nicolas Papernot"], "title": "What Does It Take to Build a Performant Selective Classifier?", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Selective classifiers improve model reliability by abstaining on inputs the\nmodel deems uncertain. However, few practical approaches achieve the\ngold-standard performance of a perfect-ordering oracle that accepts examples\nexactly in order of correctness. Our work formalizes this shortfall as the\nselective-classification gap and present the first finite-sample decomposition\nof this gap to five distinct sources of looseness: Bayes noise, approximation\nerror, ranking error, statistical noise, and implementation- or shift-induced\nslack. Crucially, our analysis reveals that monotone post-hoc calibration --\noften believed to strengthen selective classifiers -- has limited impact on\nclosing this gap, since it rarely alters the model's underlying score ranking.\nBridging the gap therefore requires scoring mechanisms that can effectively\nreorder predictions rather than merely rescale them. We validate our\ndecomposition on synthetic two-moons data and on real-world vision and language\nbenchmarks, isolating each error component through controlled experiments. Our\nresults confirm that (i) Bayes noise and limited model capacity can account for\nsubstantial gaps, (ii) only richer, feature-aware calibrators meaningfully\nimprove score ordering, and (iii) data shift introduces a separate slack that\ndemands distributionally robust training. Together, our decomposition yields a\nquantitative error budget as well as actionable design guidelines that\npractitioners can use to build selective classifiers which approximate ideal\noracle behavior more closely.", "AI": {"tldr": "This paper analyzes the selective-classification gap - the performance shortfall between practical selective classifiers and ideal oracle performance. It decomposes this gap into five error sources and shows that monotone calibration has limited impact since it doesn't reorder predictions.", "motivation": "To understand why practical selective classifiers fall short of perfect oracle performance and identify actionable ways to close this gap.", "method": "Formalizes the selective-classification gap and provides finite-sample decomposition into five error sources: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift-induced slack. Validates through controlled experiments on synthetic and real-world datasets.", "result": "Bayes noise and limited model capacity account for substantial gaps; only feature-aware calibrators meaningfully improve score ordering; data shift introduces separate slack requiring robust training. Monotone calibration rarely alters underlying rankings.", "conclusion": "The decomposition provides quantitative error budget and design guidelines for building selective classifiers that better approximate ideal oracle behavior, emphasizing the need for scoring mechanisms that can effectively reorder predictions rather than just rescale them."}}
{"id": "2510.19950", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19950", "abs": "https://arxiv.org/abs/2510.19950", "authors": ["Shaocong Ma", "Heng Huang"], "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets", "comment": null, "summary": "In financial applications, reinforcement learning (RL) agents are commonly\ntrained on historical data, where their actions do not influence prices.\nHowever, during deployment, these agents trade in live markets where their own\ntransactions can shift asset prices, a phenomenon known as market impact. This\nmismatch between training and deployment environments can significantly degrade\nperformance. Traditional robust RL approaches address this model\nmisspecification by optimizing the worst-case performance over a set of\nuncertainties, but typically rely on symmetric structures that fail to capture\nthe directional nature of market impact. To address this issue, we develop a\nnovel class of elliptic uncertainty sets. We establish both implicit and\nexplicit closed-form solutions for the worst-case uncertainty under these sets,\nenabling efficient and tractable robust policy evaluation. Experiments on\nsingle-asset and multi-asset trading tasks demonstrate that our method achieves\nsuperior Sharpe ratio and remains robust under increasing trade volumes,\noffering a more faithful and scalable approach to RL in financial markets.", "AI": {"tldr": "This paper addresses the performance degradation of RL agents in financial markets due to market impact - the mismatch between training on historical data and deployment where agent actions influence prices. The authors develop elliptic uncertainty sets to capture directional market impact and provide efficient robust policy evaluation methods.", "motivation": "RL agents trained on historical financial data face performance degradation during live deployment because their trading actions can shift asset prices (market impact), creating a mismatch between training and deployment environments that traditional robust RL methods fail to address due to their reliance on symmetric uncertainty structures.", "method": "The authors develop a novel class of elliptic uncertainty sets that capture the directional nature of market impact. They establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation.", "result": "Experiments on single-asset and multi-asset trading tasks demonstrate that the proposed method achieves superior Sharpe ratio and remains robust under increasing trade volumes, outperforming traditional approaches.", "conclusion": "The elliptic uncertainty sets provide a more faithful and scalable approach to RL in financial markets by properly capturing directional market impact and enabling efficient robust policy evaluation that maintains performance under realistic trading conditions."}}
{"id": "2510.19953", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19953", "abs": "https://arxiv.org/abs/2510.19953", "authors": ["Shaocong Ma", "Heng Huang"], "title": "On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization", "comment": null, "summary": "Zeroth-order optimization (ZOO) is an important framework for stochastic\noptimization when gradients are unavailable or expensive to compute. A\npotential limitation of existing ZOO methods is the bias inherent in most\ngradient estimators unless the perturbation stepsize vanishes. In this paper,\nwe overcome this biasedness issue by proposing a novel family of unbiased\ngradient estimators based solely on function evaluations. By reformulating\ndirectional derivatives as a telescoping series and sampling from carefully\ndesigned distributions, we construct estimators that eliminate bias while\nmaintaining favorable variance. We analyze their theoretical properties, derive\noptimal scaling distributions and perturbation stepsizes of four specific\nconstructions, and prove that SGD using the proposed estimators achieves\noptimal complexity for smooth non-convex objectives. Experiments on synthetic\ntasks and language model fine-tuning confirm the superior accuracy and\nconvergence of our approach compared to standard methods.", "AI": {"tldr": "Proposes a novel family of unbiased gradient estimators for zeroth-order optimization that eliminate bias while maintaining favorable variance, achieving optimal complexity for smooth non-convex objectives.", "motivation": "Existing zeroth-order optimization methods suffer from inherent bias in gradient estimators unless perturbation stepsize vanishes, limiting their effectiveness.", "method": "Reformulates directional derivatives as telescoping series and samples from carefully designed distributions to construct unbiased gradient estimators based solely on function evaluations.", "result": "The proposed estimators eliminate bias while maintaining favorable variance, with SGD using these estimators achieving optimal complexity for smooth non-convex objectives. Experiments show superior accuracy and convergence compared to standard methods.", "conclusion": "The novel family of unbiased gradient estimators overcomes the biasedness limitation in existing zeroth-order optimization methods, providing more accurate and efficient optimization when gradients are unavailable or expensive to compute."}}
{"id": "2510.19975", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19975", "abs": "https://arxiv.org/abs/2510.19975", "authors": ["Shaocong Ma", "Heng Huang"], "title": "Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations", "comment": null, "summary": "In this paper, we explore the two-point zeroth-order gradient estimator and\nidentify the distribution of random perturbations that minimizes the\nestimator's asymptotic variance as the perturbation stepsize tends to zero. We\nformulate it as a constrained functional optimization problem over the space of\nperturbation distributions. Our findings reveal that such desired perturbations\ncan align directionally with the true gradient, instead of maintaining a fixed\nlength. While existing research has largely focused on fixed-length\nperturbations, the potential advantages of directional alignment have been\noverlooked. To address this gap, we delve into the theoretical and empirical\nproperties of the directionally aligned perturbation (DAP) scheme, which\nadaptively offers higher accuracy along critical directions. Additionally, we\nprovide a convergence analysis for stochastic gradient descent using\n$\\delta$-unbiased random perturbations, extending existing complexity bounds to\na wider range of perturbations. Through empirical evaluations on both synthetic\nproblems and practical tasks, we demonstrate that DAPs outperform traditional\nmethods under specific conditions.", "AI": {"tldr": "This paper introduces directionally aligned perturbations (DAPs) for zeroth-order gradient estimation, showing they minimize asymptotic variance and outperform traditional fixed-length perturbations in certain conditions.", "motivation": "Existing research has focused on fixed-length perturbations for zeroth-order gradient estimation, overlooking the potential advantages of directional alignment with the true gradient.", "method": "The authors formulate a constrained functional optimization problem to find perturbation distributions that minimize asymptotic variance, and propose the directionally aligned perturbation (DAP) scheme that adaptively offers higher accuracy along critical directions.", "result": "Theoretical analysis shows DAPs can align directionally with the true gradient instead of maintaining fixed length. Empirical evaluations on synthetic and practical tasks demonstrate DAPs outperform traditional methods under specific conditions.", "conclusion": "Directionally aligned perturbations provide superior performance for zeroth-order gradient estimation compared to traditional fixed-length approaches, with theoretical guarantees and empirical validation."}}
{"id": "2510.19977", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19977", "abs": "https://arxiv.org/abs/2510.19977", "authors": ["Hanbin Hong", "Ashish Kundu", "Ali Payani", "Binghui Wang", "Yuan Hong"], "title": "Towards Strong Certified Defense with Universal Asymmetric Randomization", "comment": "Accepted by CSF 2026, 39th IEEE Computer Security Foundations\n  Symposium", "summary": "Randomized smoothing has become essential for achieving certified adversarial\nrobustness in machine learning models. However, current methods primarily use\nisotropic noise distributions that are uniform across all data dimensions, such\nas image pixels, limiting the effectiveness of robustness certification by\nignoring the heterogeneity of inputs and data dimensions. To address this\nlimitation, we propose UCAN: a novel technique that \\underline{U}niversally\n\\underline{C}ertifies adversarial robustness with \\underline{A}nisotropic\n\\underline{N}oise. UCAN is designed to enhance any existing randomized\nsmoothing method, transforming it from symmetric (isotropic) to asymmetric\n(anisotropic) noise distributions, thereby offering a more tailored defense\nagainst adversarial attacks. Our theoretical framework is versatile, supporting\na wide array of noise distributions for certified robustness in different\n$\\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the\nclassifier's prediction over perturbed inputs with provable robustness bounds\nthrough tailored noise injection. Additionally, we develop a novel framework\nequipped with three exemplary noise parameter generators (NPGs) to optimally\nfine-tune the anisotropic noise parameters for different data dimensions,\nallowing for pursuing different levels of robustness enhancements in\npractice.Empirical evaluations underscore the significant leap in UCAN's\nperformance over existing state-of-the-art methods, demonstrating up to\n$182.6\\%$ improvement in certified accuracy at large certified radii on MNIST,\nCIFAR10, and ImageNet datasets.\\footnote{Code is anonymously available at\n\\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}", "AI": {"tldr": "UCAN introduces anisotropic noise distributions for randomized smoothing to improve certified adversarial robustness, achieving up to 182.6% improvement in certified accuracy over isotropic methods.", "motivation": "Current randomized smoothing methods use isotropic noise that treats all data dimensions uniformly, limiting effectiveness by ignoring input heterogeneity and dimension-specific characteristics.", "method": "UCAN transforms existing randomized smoothing methods from symmetric (isotropic) to asymmetric (anisotropic) noise distributions using noise parameter generators to fine-tune noise parameters per data dimension.", "result": "Empirical evaluations show up to 182.6% improvement in certified accuracy at large certified radii on MNIST, CIFAR10, and ImageNet datasets compared to state-of-the-art methods.", "conclusion": "UCAN provides a versatile framework for enhancing certified adversarial robustness through tailored anisotropic noise distributions, significantly outperforming existing isotropic approaches."}}
{"id": "2510.19980", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.19980", "abs": "https://arxiv.org/abs/2510.19980", "authors": ["Renzhao Liang", "Sizhe Xu", "Chenggang Xie", "Jingru Chen", "Feiyang Ren", "Shu Yang", "Takahiro Yabe"], "title": "Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency", "comment": "20 pages, 4 figures. Accepted as Spotlight poster in NeurIPS 2025", "summary": "Time series forecasting plays a pivotal role in critical domains such as\nenergy management and financial markets. Although deep learning-based\napproaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the\nprevailing \"long-sequence information gain hypothesis\" exhibits inherent\nlimitations. Through systematic experimentation, this study reveals a\ncounterintuitive phenomenon: appropriately truncating historical data can\nparadoxically enhance prediction accuracy, indicating that existing models\nlearn substantial redundant features (e.g., noise or irrelevant fluctuations)\nduring training, thereby compromising effective signal extraction. Building\nupon information bottleneck theory, we propose an innovative solution termed\nAdaptive Masking Loss with Representation Consistency (AMRC), which features\ntwo core components: 1) Dynamic masking loss, which adaptively identified\nhighly discriminative temporal segments to guide gradient descent during model\ntraining; 2) Representation consistency constraint, which stabilized the\nmapping relationships among inputs, labels, and predictions. Experimental\nresults demonstrate that AMRC effectively suppresses redundant feature learning\nwhile significantly improving model performance. This work not only challenges\nconventional assumptions in temporal modeling but also provides novel\ntheoretical insights and methodological breakthroughs for developing efficient\nand robust forecasting models.", "AI": {"tldr": "This paper challenges the conventional 'long-sequence information gain hypothesis' in time series forecasting by showing that truncating historical data can improve accuracy. The authors propose AMRC (Adaptive Masking Loss with Representation Consistency) to suppress redundant feature learning and enhance model performance.", "motivation": "The motivation stems from limitations in current deep learning approaches for time series forecasting, where the prevailing assumption that longer historical sequences provide better information is shown to be flawed. The study reveals that existing models learn substantial redundant features like noise and irrelevant fluctuations, compromising effective signal extraction.", "method": "The proposed method is AMRC (Adaptive Masking Loss with Representation Consistency) with two core components: 1) Dynamic masking loss that adaptively identifies highly discriminative temporal segments to guide gradient descent during training, and 2) Representation consistency constraint that stabilizes mapping relationships among inputs, labels, and predictions.", "result": "Experimental results demonstrate that AMRC effectively suppresses redundant feature learning while significantly improving model performance. The approach challenges conventional assumptions in temporal modeling and provides improved forecasting accuracy.", "conclusion": "This work challenges conventional assumptions in temporal modeling and provides novel theoretical insights and methodological breakthroughs for developing efficient and robust forecasting models. The proposed AMRC framework offers a new approach to time series forecasting by focusing on relevant temporal segments rather than simply accumulating longer historical data."}}
{"id": "2510.20019", "categories": ["cs.LG", "cs.CR", "2020: Primary 68T05, Secondary 68T10", "I.2.6; I.6.4; C.3"], "pdf": "https://arxiv.org/pdf/2510.20019", "abs": "https://arxiv.org/abs/2510.20019", "authors": ["Curtis Lee Shull", "Merrick Green"], "title": "Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications", "comment": "10 pages, 5 figures. Submitted to the Journal of Defense Modeling and\n  Simulation (JDMS) for the Special Issue Integrating AI/ML Into Modeling and\n  Simulation (J22-4). This work evaluates machine learning-based RFID\n  localization for defense logistics environments using CAD-modeled simulations\n  and RSSI-driven decision tree classification", "summary": "Radio Frequency Identification (RFID) tracking may be a viable solution for\ndefense assets that must be stored in accordance with security guidelines.\nHowever, poor sensor specificity (vulnerabilities include long range detection,\nspoofing, and counterfeiting) can lead to erroneous detection and operational\nsecurity events. We present a supervised learning simulation with realistic\nReceived Signal Strength Indicator (RSSI) data and Decision Tree classification\nin a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some\nof the challenges encountered in defense storage. In this work, we focused on\nclassifying 12 lab zones (LabZoneA-L) to perform location inference. The raw\ndataset had approximately 980,000 reads. Class frequencies were imbalanced, and\nclass weights were calculated to account for class imbalance in this\nmulti-class setting. The model, trained on stratified subsamples to 5,000\nbalanced observations, yielded an overall accuracy of 34.2% and F1-scores\ngreater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare\nclasses (most notably LabZoneC) were often misclassified, even with the use of\nclass weights. An adjacency-aware confusion matrix was calculated to allow\nbetter interpretation of physically adjacent zones. These results suggest that\nRSSI-based decision trees can be applied in realistic simulations to enable\nzone-level anomaly detection or misplacement monitoring for defense supply\nlogistics. Reliable classification performance in low-coverage and low-signal\nzones could be improved with better antenna placement or additional sensors and\nsensor fusion with other modalities.", "AI": {"tldr": "This paper presents a supervised learning approach using Decision Tree classification on RFID RSSI data to track defense assets in a CAD-modeled environment, achieving 34.2% accuracy in classifying 12 lab zones with challenges in rare class classification.", "motivation": "RFID tracking offers potential for defense asset management but suffers from poor sensor specificity issues like long range detection, spoofing, and counterfeiting that can lead to security breaches and operational errors.", "method": "Used supervised learning with Decision Tree classification on realistic RSSI data from approximately 980,000 RFID reads in a CAD-modeled floor plan. Applied class weighting to handle imbalanced data and trained on stratified subsamples of 5,000 balanced observations across 12 lab zones.", "result": "Achieved overall accuracy of 34.2% with F1-scores above 0.40 for multiple zones (F, G, H, etc.). Rare classes like LabZoneC were frequently misclassified despite class weighting. Created adjacency-aware confusion matrix to better interpret physically adjacent zone misclassifications.", "conclusion": "RSSI-based decision trees show promise for zone-level anomaly detection and misplacement monitoring in defense logistics, but performance in low-coverage areas could be improved through better antenna placement, additional sensors, or sensor fusion with other modalities."}}
{"id": "2510.20022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20022", "abs": "https://arxiv.org/abs/2510.20022", "authors": ["Jiazheng Li", "Yawei Wang", "David Yan", "Yijun Tian", "Zhichao Xu", "Huan Song", "Panpan Xu", "Lin Lee Cheong"], "title": "SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nenabling language agents to excel at single-turn tasks. However, their\napplication to complex, multi-step, and long-horizon tasks remains challenging.\nWhile reinforcement learning (RL) offers a promising avenue for addressing\nthese challenges, mainstream approaches typically rely solely on sparse,\noutcome-based rewards, a limitation that becomes especially problematic for\ngroup-based RL algorithms lacking critic models, such as Group Relative Policy\nOptimization (GRPO). In such methods, uniformly rewarding or penalizing all\nactions within a trajectory can lead to training instability and suboptimal\npolicies, because beneficial and detrimental actions are often entangled across\nmulti-step interactions. To address this challenge, we propose SALT, a novel\nand lightweight framework that provides a finer-grained advantage assignment,\nderived solely from outcome rewards. We achieve this by constructing a graph\nfrom trajectories of the same prompt, which allows us to quantify the quality\nof each step and assign advantages accordingly. Crucially, SALT is designed as\na plug-and-play module that seamlessly integrates with existing group-based RL\nalgorithms, requiring no modifications to the rollout procedure and introducing\nnegligible computational overhead. Extensive experiments on the WebShop,\nALFWorld, and AppWorld benchmarks with various model sizes demonstrate that\nSALT consistently improves performance. We also conduct a thorough analysis to\nvalidate the design choices behind SALT and offer actionable insights.", "AI": {"tldr": "SALT is a lightweight framework that provides fine-grained advantage assignment for group-based RL algorithms using outcome rewards, improving performance on complex multi-step tasks without significant computational overhead.", "motivation": "Current RL approaches for LLMs in complex tasks rely on sparse outcome-based rewards, which can lead to training instability and suboptimal policies when uniformly rewarding/penalizing all actions in trajectories where beneficial and detrimental actions are entangled.", "method": "SALT constructs a graph from trajectories of the same prompt to quantify step quality and assign advantages accordingly, serving as a plug-and-play module for existing group-based RL algorithms without modifying rollout procedures.", "result": "Extensive experiments on WebShop, ALFWorld, and AppWorld benchmarks with various model sizes show SALT consistently improves performance, with thorough analysis validating design choices.", "conclusion": "SALT effectively addresses the limitation of uniform reward assignment in group-based RL algorithms by providing fine-grained advantage assignment, enhancing training stability and policy quality for complex multi-step tasks."}}
{"id": "2510.20031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20031", "abs": "https://arxiv.org/abs/2510.20031", "authors": ["Marin Bilo\u0161", "Anderson Schneider", "Yuriy Nevmyvaka"], "title": "Speculative Sampling for Parametric Temporal Point Processes", "comment": null, "summary": "Temporal point processes are powerful generative models for event sequences\nthat capture complex dependencies in time-series data. They are commonly\nspecified using autoregressive models that learn the distribution of the next\nevent from the previous events. This makes sampling inherently sequential,\nlimiting efficiency. In this paper, we propose a novel algorithm based on\nrejection sampling that enables exact sampling of multiple future values from\nexisting TPP models, in parallel, and without requiring any architectural\nchanges or retraining. Besides theoretical guarantees, our method demonstrates\nempirical speedups on real-world datasets, bridging the gap between expressive\nmodeling and efficient parallel generation for large-scale TPP applications.", "AI": {"tldr": "Proposes a rejection sampling algorithm for exact parallel sampling from temporal point process models without architectural changes or retraining.", "motivation": "Autoregressive temporal point process models require sequential sampling, limiting efficiency for large-scale applications.", "method": "Novel rejection sampling algorithm that enables exact parallel sampling of multiple future values from existing TPP models.", "result": "Empirical speedups on real-world datasets with theoretical guarantees.", "conclusion": "Bridges the gap between expressive modeling and efficient parallel generation for large-scale TPP applications."}}
{"id": "2510.20064", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20064", "abs": "https://arxiv.org/abs/2510.20064", "authors": ["Hongyi Liu", "Jiaji Huang", "Zhen Jia", "Youngsuk Park", "Yu-Xiang Wang"], "title": "Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs", "comment": null, "summary": "Speculative decoding is widely used in accelerating large language model\n(LLM) inference. In this work, we focus on the online draft model selection\nproblem in speculative decoding. We design an algorithm that provably competes\nwith the best draft model in hindsight for each query in terms of either the\ntoken acceptance probability or expected acceptance length. In particular, we\nshow that we can accurately evaluate all draft models, instead of only the\nchosen model without incurring additional queries to the target model, which\nallows us to improve exponentially over the existing bandit-based approach as\nthe number of draft models increases. Our approach is generically applicable\nwith any speculative decoding methods (single draft, multi-drafts and\ndraft-trees). Moreover, we design system-efficient versions of online learners\nand demonstrate that the overhead in computation and latency can be\nsubstantially reduced. We conduct extensive experiments on open-source LLMs and\ndiverse datasets, demonstrating that our methods substantially outperform the\nstate-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains\nwhere specialized domain-expert drafters are available, especially when long\nreasoning chains are required.", "AI": {"tldr": "Online draft model selection algorithm for speculative decoding that competes with the best draft model in hindsight, improving exponentially over bandit-based approaches and working with any speculative decoding method.", "motivation": "To address the online draft model selection problem in speculative decoding for accelerating large language model inference, improving efficiency over existing approaches.", "method": "Designs an algorithm that accurately evaluates all draft models without additional target model queries, with system-efficient versions to reduce computation and latency overhead.", "result": "Substantially outperforms state-of-the-art EAGLE3 and BanditSpec baselines across diverse datasets and LLMs, especially in domains requiring long reasoning chains with specialized drafters.", "conclusion": "The proposed online draft model selection approach provides significant performance improvements in speculative decoding, particularly when multiple domain-expert drafters are available for complex reasoning tasks."}}
{"id": "2510.20066", "categories": ["cs.LG", "cs.CE", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.20066", "abs": "https://arxiv.org/abs/2510.20066", "authors": ["Yimeng Qiu", "Feihuang Fang"], "title": "A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers", "comment": null, "summary": "We study whether liquidity and volatility proxies of a core set of\ncryptoassets generate spillovers that forecast market-wide risk. Our empirical\nframework integrates three statistical layers: (A) interactions between core\nliquidity and returns, (B) principal-component relations linking liquidity and\nreturns, and (C) volatility-factor projections that capture cross-sectional\nvolatility crowding. The analysis is complemented by vector autoregression\nimpulse responses and forecast error variance decompositions (see Granger 1969;\nSims 1980), heterogeneous autoregressive models with exogenous regressors\n(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using\ntemporal splits, early stopping, validation-only thresholding, and SHAP-based\ninterpretation. Using daily data from 2021 to 2025 (1462 observations across 74\nassets), we document statistically significant Granger-causal relationships\nacross layers and moderate out-of-sample predictive accuracy. We report the\nmost informative figures, including the pipeline overview, Layer A heatmap,\nLayer C robustness analysis, vector autoregression variance decompositions, and\nthe test-set precision-recall curve. Full data and figure outputs are provided\nin the artifact repository.", "AI": {"tldr": "This paper analyzes how liquidity and volatility proxies from core cryptoassets generate spillovers that forecast market-wide risk using a multi-layer statistical framework with VAR, HAR-X models, and machine learning.", "motivation": "To study whether liquidity and volatility measures from core cryptoassets can predict broader market risk through spillover effects in the cryptocurrency market.", "method": "Three-layer statistical framework: (A) core liquidity-return interactions, (B) principal-component relations, (C) volatility-factor projections; complemented by VAR impulse responses, HAR-X models, and leakage-safe machine learning with temporal splits and SHAP interpretation.", "result": "Statistically significant Granger-causal relationships across all layers and moderate out-of-sample predictive accuracy using daily data from 2021-2025 (1462 observations across 74 assets).", "conclusion": "Liquidity and volatility proxies from core cryptoassets generate meaningful spillovers that can forecast market-wide risk, with documented statistical significance and moderate predictive power."}}
{"id": "2510.20068", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20068", "abs": "https://arxiv.org/abs/2510.20068", "authors": ["Ram Dyuthi Sristi", "Sowmya Manojna Narasimha", "Jingya Huang", "Alice Despatin", "Simon Musall", "Vikash Gilja", "Gal Mishne"], "title": "Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics", "comment": null, "summary": "Simultaneous recordings from thousands of neurons across multiple brain areas\nreveal rich mixtures of activity that are shared between regions and dynamics\nthat are unique to each region. Existing alignment or multi-view methods\nneglect temporal structure, whereas dynamical latent variable models capture\ntemporal dependencies but are usually restricted to a single area, assume\nlinear read-outs, or conflate shared and private signals. We introduce the\nCoupled Transformer Autoencoder (CTAE) - a sequence model that addresses both\n(i) non-stationary, non-linear dynamics and (ii) separation of shared versus\nregion-specific structure in a single framework. CTAE employs transformer\nencoders and decoders to capture long-range neural dynamics and explicitly\npartitions each region's latent space into orthogonal shared and private\nsubspaces. We demonstrate the effectiveness of CTAE on two high-density\nelectrophysiology datasets with simultaneous recordings from multiple regions,\none from motor cortical areas and the other from sensory areas. CTAE extracts\nmeaningful representations that better decode behavioral variables compared to\nexisting approaches.", "AI": {"tldr": "The paper introduces CTAE, a transformer-based model that separates shared and region-specific neural dynamics from multi-region recordings, outperforming existing methods in behavioral decoding.", "motivation": "Current methods either ignore temporal structure in multi-region neural recordings or fail to properly separate shared and private signals across brain regions.", "method": "Coupled Transformer Autoencoder (CTAE) uses transformer encoders/decoders to capture long-range neural dynamics and explicitly partitions latent space into orthogonal shared and private subspaces.", "result": "CTAE extracts meaningful representations that better decode behavioral variables compared to existing approaches on motor and sensory cortical recordings.", "conclusion": "CTAE effectively addresses non-stationary, non-linear neural dynamics while separating shared versus region-specific structure in a unified framework."}}
{"id": "2510.20084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20084", "abs": "https://arxiv.org/abs/2510.20084", "authors": ["Bosong Huang", "Ming Jin", "Yuxuan Liang", "Johan Barthelemy", "Debo Cheng", "Qingsong Wen", "Chenghao Liu", "Shirui Pan"], "title": "ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models", "comment": null, "summary": "Explaining time series classification models is crucial, particularly in\nhigh-stakes applications such as healthcare and finance, where transparency and\ntrust play a critical role. Although numerous time series classification\nmethods have identified key subsequences, known as shapelets, as core features\nfor achieving state-of-the-art performance and validating their pivotal role in\nclassification outcomes, existing post-hoc time series explanation (PHTSE)\nmethods primarily focus on timestep-level feature attribution. These\nexplanation methods overlook the fundamental prior that classification outcomes\nare predominantly driven by key shapelets. To bridge this gap, we present\nShapeX, an innovative framework that segments time series into meaningful\nshapelet-driven segments and employs Shapley values to assess their saliency.\nAt the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,\nwhich effectively learns a diverse set of shapelets essential for\nclassification. We further demonstrate that ShapeX produces explanations which\nreveal causal relationships instead of just correlations, owing to the\natomicity properties of shapelets. Experimental results on both synthetic and\nreal-world datasets demonstrate that ShapeX outperforms existing methods in\nidentifying the most relevant subsequences, enhancing both the precision and\ncausal fidelity of time series explanations.", "AI": {"tldr": "ShapeX is a framework for explaining time series classification models by identifying key shapelets (subsequences) and using Shapley values to assess their importance, providing more precise and causally faithful explanations than existing methods.", "motivation": "Current post-hoc time series explanation methods focus on timestep-level feature attribution but overlook that classification outcomes are primarily driven by key shapelets, which limits transparency and trust in high-stakes applications like healthcare and finance.", "method": "ShapeX segments time series into meaningful shapelet-driven segments using the Shapelet Describe-and-Detect (SDD) framework to learn diverse shapelets, then employs Shapley values to assess their saliency for explanation.", "result": "Experiments on synthetic and real-world datasets show ShapeX outperforms existing methods in identifying relevant subsequences, improving both precision and causal fidelity of explanations by revealing causal relationships rather than just correlations.", "conclusion": "ShapeX successfully bridges the gap in time series explanation by focusing on shapelet-level attribution, providing more transparent and trustworthy explanations that capture the fundamental drivers of classification outcomes."}}
{"id": "2510.20085", "categories": ["cs.LG", "cs.CY", "I.2.7; G.3; I.2.1; J.4"], "pdf": "https://arxiv.org/pdf/2510.20085", "abs": "https://arxiv.org/abs/2510.20085", "authors": ["Chang Yang", "Ziyi Wang", "Wangfeng Tan", "Zhiting Tan", "Changrui Ji", "Zhiming Zhou"], "title": "Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa", "comment": "9 pages, 7 figures, 2tables, 2025 IEEE International Conference on\n  Big Data", "summary": "Social media platforms have become important sources for identifying suicide\nrisk, but automated detection systems face multiple challenges including severe\nclass imbalance, temporal complexity in posting patterns, and the dual nature\nof risk levels as both ordinal and categorical. This paper proposes a\nhierarchical dual-head neural network based on MentalRoBERTa for suicide risk\nclassification into four levels: indicator, ideation, behavior, and attempt.\nThe model employs two complementary prediction heads operating on a shared\nsequence representation: a CORAL (Consistent Rank Logits) head that preserves\nordinal relationships between risk levels, and a standard classification head\nthat enables flexible categorical distinctions. A 3-layer Transformer encoder\nwith 8-head multi-head attention models temporal dependencies across post\nsequences, while explicit time interval embeddings capture posting behavior\ndynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3\nCross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure\npreservation, overconfidence reduction, and class imbalance. To improve\ncomputational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa\nand employ mixed-precision training. The model is evaluated using 5-fold\nstratified cross-validation with macro F1 score as the primary metric.", "AI": {"tldr": "A hierarchical dual-head neural network using MentalRoBERTa for suicide risk classification with CORAL and standard classification heads, addressing class imbalance and temporal patterns in social media posts.", "motivation": "Social media platforms are important for suicide risk detection, but automated systems face challenges with class imbalance, temporal complexity in posting patterns, and the dual nature of risk levels as both ordinal and categorical.", "method": "Proposes a hierarchical dual-head neural network based on MentalRoBERTa with two prediction heads: CORAL head for ordinal relationships and standard classification head for categorical distinctions. Uses 3-layer Transformer encoder with multi-head attention and time interval embeddings, trained with combined loss function (CORAL + Cross-Entropy + Focal Loss).", "result": "Model evaluated using 5-fold stratified cross-validation with macro F1 score as primary metric. Computational efficiency improved by freezing first 6 layers of MentalRoBERTa and using mixed-precision training.", "conclusion": "The proposed approach effectively addresses multiple challenges in suicide risk classification from social media data, including ordinal-categorical duality, temporal dependencies, and class imbalance through a carefully designed neural architecture and training strategy."}}
{"id": "2510.20106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20106", "abs": "https://arxiv.org/abs/2510.20106", "authors": ["Amartya Roy", "Souvik Chakraborty"], "title": "Competition is the key: A Game Theoretic Causal Discovery Approach", "comment": null, "summary": "Causal discovery remains a central challenge in machine learning, yet\nexisting methods face a fundamental gap: algorithms like GES and GraN-DAG\nachieve strong empirical performance but lack finite-sample guarantees, while\ntheoretically principled approaches fail to scale. We close this gap by\nintroducing a game-theoretic reinforcement learning framework for causal\ndiscovery, where a DDQN agent directly competes against a strong baseline (GES\nor GraN-DAG), always warm-starting from the opponent's solution. This design\nyields three provable guarantees: the learned graph is never worse than the\nopponent, warm-starting strictly accelerates convergence, and most importantly,\nwith high probability the algorithm selects the true best candidate graph. To\nthe best of our knowledge, our result makes a first-of-its-kind progress in\nexplaining such finite-sample guarantees in causal discovery: on synthetic SEMs\n(30 nodes), the observed error probability decays with n, tightly matching\ntheory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,\nDream, and Andes, our method consistently improves upon GES and GraN-DAG while\nremaining theoretically safe. Remarkably, it scales to large graphs such as\nHepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these\nresults establish a new class of RL-based causal discovery algorithms that are\nsimultaneously provably consistent, sample-efficient, and practically scalable,\nmarking a decisive step toward unifying empirical performance with rigorous\nfinite-sample theory.", "AI": {"tldr": "A game-theoretic reinforcement learning framework for causal discovery that combines strong empirical performance with finite-sample guarantees, outperforming baseline methods while maintaining theoretical safety.", "motivation": "To bridge the gap between empirically strong causal discovery methods (like GES and GraN-DAG) that lack finite-sample guarantees and theoretically principled approaches that fail to scale.", "method": "A DDQN agent directly competes against strong baselines (GES or GraN-DAG), always warm-starting from the opponent's solution in a game-theoretic reinforcement learning framework.", "result": "The method consistently improves upon GES and GraN-DAG on real-world benchmarks (Sachs, Asia, Alarm, Child, Hepar2, Dream, Andes) and scales to large graphs (up to 220 nodes). Observed error probability decays with sample size, matching theoretical predictions.", "conclusion": "Establishes a new class of RL-based causal discovery algorithms that are simultaneously provably consistent, sample-efficient, and practically scalable, unifying empirical performance with rigorous finite-sample theory."}}
{"id": "2510.20107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20107", "abs": "https://arxiv.org/abs/2510.20107", "authors": ["Ayatullah Faruk Mollah"], "title": "On pattern classification with weighted dimensions", "comment": null, "summary": "Studies on various facets of pattern classification is often imperative while\nworking with multi-dimensional samples pertaining to diverse application\nscenarios. In this notion, weighted dimension-based distance measure has been\none of the vital considerations in pattern analysis as it reflects the degree\nof similarity between samples. Though it is often presumed to be settled with\nthe pervasive use of Euclidean distance, plethora of issues often surface. In\nthis paper, we present (a) a detail analysis on the impact of distance measure\nnorms and weights of dimensions along with visualization, (b) a novel weighting\nscheme for each dimension, (c) incorporation of this dimensional weighting\nschema into a KNN classifier, and (d) pattern classification on a variety of\nsynthetic as well as realistic datasets with the developed model. It has\nperformed well across diverse experiments in comparison to the traditional KNN\nunder the same experimental setups. Specifically, for gene expression datasets,\nit yields significant and consistent gain in classification accuracy (around\n10%) in all cross-validation experiments with different values of k. As such\ndatasets contain limited number of samples of high dimensions, meaningful\nselection of nearest neighbours is desirable, and this requirement is\nreasonably met by regulating the shape and size of the region enclosing the k\nnumber of reference samples with the developed weighting schema and appropriate\nnorm. It, therefore, stands as an important generalization of KNN classifier\npowered by weighted Minkowski distance with the present weighting schema.", "AI": {"tldr": "This paper analyzes distance measure norms and dimension weights in pattern classification, proposes a novel dimension weighting scheme, incorporates it into KNN classifier, and shows improved performance (especially 10% gain for gene expression datasets) compared to traditional KNN.", "motivation": "Traditional Euclidean distance in pattern classification often faces issues, and weighted dimension-based distance measures are crucial for accurately reflecting similarity between multi-dimensional samples in diverse applications.", "method": "Developed a novel dimension weighting scheme and incorporated it into KNN classifier using weighted Minkowski distance, regulating the shape and size of regions enclosing k nearest neighbors.", "result": "The proposed method performed well across diverse experiments, with significant 10% classification accuracy gain for gene expression datasets in all cross-validation experiments with different k values.", "conclusion": "The developed weighting schema with appropriate norm provides an important generalization of KNN classifier that effectively handles high-dimensional datasets with limited samples by enabling meaningful selection of nearest neighbors."}}
{"id": "2510.20108", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20108", "abs": "https://arxiv.org/abs/2510.20108", "authors": ["Gabriel Y. Arteaga", "Marius Aasan", "Rwiddhi Chakraborty", "Martine Hjelkrem-Tan", "Thalles Silva", "Michael Kampffmeyer", "Ad\u00edn Ram\u00edrez Rivera"], "title": "Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning", "comment": null, "summary": "Prototypical self-supervised learning methods consistently suffer from\npartial prototype collapse, where multiple prototypes converge to nearly\nidentical representations. This undermines their central purpose -- providing\ndiverse and informative targets to guide encoders toward rich representations\n-- and has led practitioners to over-parameterize prototype sets or add ad-hoc\nregularizers, which mitigate symptoms rather than address the root cause. We\nempirically trace the collapse to the joint optimization of encoders and\nprototypes, which encourages a type of shortcut learning: early in training\nprototypes drift toward redundant representations that minimize loss without\nnecessarily enhancing representation diversity. To break the joint\noptimization, we introduce a fully decoupled training strategy that learns\nprototypes and encoders under separate objectives. Concretely, we model\nprototypes as a Gaussian mixture updated with an online EM-style procedure,\nindependent of the encoder's loss. This simple yet principled decoupling\neliminates prototype collapse without explicit regularization and yields\nconsistently diverse prototypes and stronger downstream performance.", "AI": {"tldr": "The paper addresses prototype collapse in self-supervised learning by proposing a decoupled training strategy that separates prototype learning from encoder optimization, using an online EM-style procedure for prototypes.", "motivation": "Self-supervised learning methods suffer from partial prototype collapse where multiple prototypes converge to similar representations, undermining their purpose of providing diverse targets. Current solutions like over-parameterization or ad-hoc regularizers only treat symptoms rather than addressing the root cause.", "method": "Introduces a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Prototypes are modeled as a Gaussian mixture updated with an online EM-style procedure independent of the encoder's loss, breaking the joint optimization that causes collapse.", "result": "The decoupling eliminates prototype collapse without explicit regularization, yielding consistently diverse prototypes and stronger downstream performance compared to existing methods.", "conclusion": "Breaking the joint optimization between encoders and prototypes through decoupled training effectively addresses the root cause of prototype collapse, providing a principled solution that outperforms symptom-mitigating approaches."}}
{"id": "2510.20119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20119", "abs": "https://arxiv.org/abs/2510.20119", "authors": ["Arian Prabowo", "Flora D. Salim"], "title": "There is No \"apple\" in Timeseries: Rethinking TSFM through the Lens of Invariance", "comment": null, "summary": "Timeseries foundation models (TSFMs) have multiplied, yet lightweight\nsupervised baselines and even classical models often match them. We argue this\ngap stems from the naive importation of NLP or CV pipelines. In language and\nvision, large web-scale corpora densely capture human concepts i.e. there are\ncountless images and text of apples. In contrast, timeseries data is built to\ncomplement the image and text modalities. There are no timeseries dataset that\ncontains the concept apple. As a result, the scrape-everything-online paradigm\nfails for TS. We posit that progress demands a shift from opportunistic\naggregation to principled design: constructing datasets that systematically\nspan the space of invariance that preserve temporal semantics. To this end, we\nsuggest that the ontology of timeseries invariances should be built based on\nfirst principles. Only by ensuring representational completeness through\ninvariance coverage can TSFMs achieve the aligned structure necessary for\ngeneralisation, reasoning, and truly emergent behaviour.", "AI": {"tldr": "Current timeseries foundation models underperform compared to simple supervised baselines due to improper adaptation of NLP/CV pipelines. The key issue is that timeseries data lacks the semantic richness of web-scale text/image corpora, requiring a shift from opportunistic data scraping to principled dataset design based on temporal invariances.", "motivation": "Timeseries foundation models have proliferated but fail to outperform lightweight supervised baselines and classical models. This performance gap stems from naively importing NLP or CV pipelines without considering fundamental differences in data characteristics.", "method": "Propose a shift from opportunistic data aggregation to principled dataset design that systematically spans the space of temporal invariances. Suggest building timeseries invariance ontology based on first principles to ensure representational completeness.", "result": "Identifies that the scrape-everything-online paradigm fails for timeseries because timeseries data lacks the semantic concepts found in web-scale text/image corpora (e.g., no timeseries dataset contains the concept 'apple').", "conclusion": "Progress in timeseries foundation models requires ensuring representational completeness through systematic coverage of temporal invariances, which will enable aligned structure necessary for generalization, reasoning, and emergent behavior."}}
{"id": "2510.20148", "categories": ["cs.LG", "math.DS", "physics.med-ph", "68T07, 35Q92, 92B20, 92C50", "I.6.3; I.6.4; I.2; J.3"], "pdf": "https://arxiv.org/pdf/2510.20148", "abs": "https://arxiv.org/abs/2510.20148", "authors": ["Tingting Dan", "Xinwei Huang", "Jiaqi Ding", "Yinggang Zheng", "Guorong Wu"], "title": "Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling", "comment": "42 pages, 14 figures, 64 references", "summary": "Emerging neuroimaging evidence shows that pathological tau proteins build up\nalong specific brain networks, suggesting that large-scale network architecture\nplays a key role in the progression of Alzheimer's disease (AD). However, how\nstructural connectivity (SC) and functional connectivity (FC) interact to\ninfluence tau propagation remains unclear. Leveraging an unprecedented volume\nof longitudinal neuroimaging data, we examine SC-FC interactions through a\nmulti-layer graph diffusion model. Beyond showing that connectome architecture\nconstrains tau spread, our model reveals a regionally asymmetric contribution\nof SC and FC. Specifically, FC predominantly drives tau spread in subcortical\nareas, the insula, frontal and temporal cortices, whereas SC plays a larger\nrole in occipital, parietal, and limbic regions. The relative dominance of SC\nversus FC shifts over the course of disease, with FC generally prevailing in\nearly AD and SC becoming primary in later stages. Spatial patterns of SC- and\nFC-dominant regions strongly align with the regional expression of\nAD-associated genes involved in inflammation, apoptosis, and lysosomal\nfunction, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In\nparallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and\nbiological mechanisms (e.g., amyloid deposition) selectively reshape tau\npropagation by shifting dominant routes between anatomical and functional\npathways in a region-specific manner. Findings are validated in an independent\nAD cohort.", "AI": {"tldr": "This study reveals how structural and functional connectivity interact to influence tau protein spread in Alzheimer's disease, showing regionally asymmetric contributions that shift during disease progression and align with AD-associated gene expression.", "motivation": "To understand how structural connectivity (SC) and functional connectivity (FC) interact to influence tau protein propagation in Alzheimer's disease, as emerging evidence shows tau builds up along specific brain networks but the SC-FC interaction mechanisms remain unclear.", "method": "Used longitudinal neuroimaging data with a multi-layer graph diffusion model to examine SC-FC interactions in tau propagation across brain networks.", "result": "Revealed regionally asymmetric contributions: FC drives tau spread in subcortical areas, insula, frontal and temporal cortices, while SC dominates in occipital, parietal and limbic regions. The SC-FC dominance shifts during disease progression, with FC prevailing in early AD and SC becoming primary in later stages. These patterns align with AD-associated gene expression involved in inflammation, apoptosis and lysosomal function.", "conclusion": "Structural and functional connectivity have distinct regional roles in tau propagation that shift during Alzheimer's disease progression, with spatial patterns that strongly correspond to AD-associated genetic risk factors and biological mechanisms, providing insights into network-level drivers of disease progression."}}
{"id": "2510.20157", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20157", "abs": "https://arxiv.org/abs/2510.20157", "authors": ["Xiaoming Wu", "Teng Liu", "Xin Wang", "Ming Yang", "Jiguo Yu"], "title": "ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push", "comment": null, "summary": "Differential privacy is widely employed in decentralized learning to\nsafeguard sensitive data by introducing noise into model updates. However,\nexisting approaches that use fixed-variance noise often degrade model\nperformance and reduce training efficiency. To address these limitations, we\npropose a novel approach called decentralized learning with adaptive\ndifferential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).\nThis method dynamically adjusts both the noise variance and the learning rate\nusing a stepwise-decaying schedule, which accelerates training and enhances\nfinal model performance while providing node-level personalized privacy\nguarantees. To counteract the slowed convergence caused by large-variance noise\nin early iterations, we introduce a progressive gradient fusion strategy that\nleverages historical gradients. Furthermore, ADP-VRSGP incorporates\ndecentralized push-sum and aggregation techniques, making it particularly\nsuitable for time-varying communication topologies. Through rigorous\ntheoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence\nwith an appropriate learning rate, significantly improving training stability\nand speed. Experimental results validate that our method outperforms existing\nbaselines across multiple scenarios, highlighting its efficacy in addressing\nthe challenges of privacy-preserving decentralized learning.", "AI": {"tldr": "Proposes ADP-VRSGP, an adaptive differential privacy method for decentralized learning that dynamically adjusts noise variance and learning rate using stepwise decay, improving training efficiency and model performance while providing node-level privacy guarantees.", "motivation": "Existing decentralized learning approaches with fixed-variance differential privacy noise degrade model performance and reduce training efficiency, creating a need for adaptive methods that can maintain privacy while improving learning outcomes.", "method": "ADP-VRSGP uses stepwise-decaying schedules for noise variance and learning rate, progressive gradient fusion leveraging historical gradients, and incorporates decentralized push-sum and aggregation techniques suitable for time-varying communication topologies.", "result": "Theoretical analysis shows robust convergence with appropriate learning rate, significantly improving training stability and speed. Experimental results demonstrate superior performance over existing baselines across multiple scenarios.", "conclusion": "ADP-VRSGP effectively addresses privacy-preserving decentralized learning challenges by providing adaptive differential privacy that enhances training efficiency and final model performance while maintaining node-level privacy guarantees."}}
{"id": "2510.20169", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20169", "abs": "https://arxiv.org/abs/2510.20169", "authors": ["Tongkai Lu", "Shuai Ma", "Chongyang Tao"], "title": "Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP", "comment": "12 pages", "summary": "Traveling Salesman Problem (TSP) is a classic NP-hard problem that has\ngarnered significant attention from both academia and industry. While\nneural-based methods have shown promise for solving TSPs, they still face\nchallenges in scaling to larger instances, particularly in memory constraints\nassociated with global heatmaps, edge weights, or access matrices, as well as\nin generating high-quality initial solutions and insufficient global guidance\nfor efficiently navigating vast search spaces. To address these challenges, we\npropose a Hyper Tour Guided Neighborhood Search (HyperNS) method for\nlarge-scale TSP instances. Inspired by the ``clustering first, route second\"\nstrategy, our approach initially divides the TSP instance into clusters using a\nsparse heatmap graph and abstracts them as supernodes, followed by the\ngeneration of a hyper tour to guide both the initialization and optimization\nprocesses. This method reduces the search space by focusing on edges relevant\nto the hyper tour, leading to more efficient and effective optimization.\nExperimental results on both synthetic and real-world datasets demonstrate that\nour approach outperforms existing neural-based methods, particularly in\nhandling larger-scale instances, offering a significant reduction in the gap to\nthe optimal solution.", "AI": {"tldr": "HyperNS method uses hyper tour guidance and clustering to efficiently solve large-scale TSP instances, outperforming existing neural approaches.", "motivation": "Address challenges in scaling neural methods for TSP, including memory constraints with global heatmaps, poor initial solutions, and insufficient global guidance for large search spaces.", "method": "Divide TSP into clusters using sparse heatmap graph, abstract clusters as supernodes, generate hyper tour to guide initialization and optimization, reducing search space by focusing on hyper tour-relevant edges.", "result": "Outperforms existing neural-based methods on synthetic and real-world datasets, especially for larger instances, with significant reduction in gap to optimal solution.", "conclusion": "HyperNS provides an effective approach for large-scale TSP by combining clustering strategy with hyper tour guidance, enabling more efficient optimization through reduced search space."}}
{"id": "2510.20187", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20187", "abs": "https://arxiv.org/abs/2510.20187", "authors": ["Dian Yu", "Yulai Zhao", "Kishan Panaganti", "Linfeng Song", "Haitao Mi", "Dong Yu"], "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values", "comment": "15 pages, 4 figures", "summary": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method\nthat aligns Large Language Model (LLM) optimization directly with quantifiable\nhuman value signals. While Reinforcement Learning with Verifiable Rewards\n(RLVR) effectively trains models in objective domains using binary correctness\nrewards, it overlooks that not all tasks are equally significant. RLEV extends\nthis framework by incorporating human-defined value signals directly into the\nreward function. Using exam-style data with explicit ground-truth value labels,\nRLEV consistently outperforms correctness-only baselines across multiple RL\nalgorithms and model scales. Crucially, RLEV policies not only improve\nvalue-weighted accuracy but also learn a value-sensitive termination policy:\nconcise for low-value prompts, thorough for high-value ones. We demonstrate\nthis behavior stems from value-weighted gradient amplification on\nend-of-sequence tokens. Ablation studies confirm the gain is causally linked to\nvalue alignment. RLEV remains robust under noisy value signals, such as\ndifficulty-based labels, demonstrating that optimizing for an explicit utility\nfunction offers a practical path to aligning LLMs with human priorities.", "AI": {"tldr": "RLEV extends RLVR by incorporating human value signals into reward functions, enabling LLMs to optimize for both correctness and importance of tasks, resulting in value-sensitive termination policies.", "motivation": "Current RL methods like RLVR focus only on binary correctness rewards but ignore that tasks have different importance levels, failing to align with human priorities.", "method": "Extends RLVR framework by adding human-defined value signals to reward functions, using exam-style data with explicit value labels, and analyzing value-weighted gradient amplification on end-of-sequence tokens.", "result": "Outperforms correctness-only baselines across multiple RL algorithms and model scales, achieves value-sensitive termination (concise for low-value, thorough for high-value prompts), and remains robust under noisy value signals.", "conclusion": "Optimizing for explicit utility functions provides a practical approach to align LLMs with human priorities through value-weighted training."}}
{"id": "2510.20199", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20199", "abs": "https://arxiv.org/abs/2510.20199", "authors": ["Jane H. Lee", "Baturay Saglam", "Spyridon Pougkakiotis", "Amin Karbasi", "Dionysis Kalogerias"], "title": "Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents", "comment": null, "summary": "Constrained optimization provides a common framework for dealing with\nconflicting objectives in reinforcement learning (RL). In most of these\nsettings, the objectives (and constraints) are expressed though the expected\naccumulated reward. However, this formulation neglects risky or even possibly\ncatastrophic events at the tails of the reward distribution, and is often\ninsufficient for high-stakes applications in which the risk involved in\noutliers is critical. In this work, we propose a framework for risk-aware\nconstrained RL, which exhibits per-stage robustness properties jointly in\nreward values and time using optimized certainty equivalents (OCEs). Our\nframework ensures an exact equivalent to the original constrained problem\nwithin a parameterized strong Lagrangian duality framework under appropriate\nconstraint qualifications, and yields a simple algorithmic recipe which can be\nwrapped around standard RL solvers, such as PPO. Lastly, we establish the\nconvergence of the proposed algorithm under common assumptions, and verify the\nrisk-aware properties of our approach through several numerical experiments.", "AI": {"tldr": "This paper proposes a risk-aware constrained reinforcement learning framework using optimized certainty equivalents (OCEs) to address tail risks in reward distributions, ensuring joint per-stage robustness in rewards and time.", "motivation": "Standard constrained RL based on expected accumulated rewards neglects tail risks and catastrophic events, which is insufficient for high-stakes applications where outlier risks are critical.", "method": "The framework uses optimized certainty equivalents (OCEs) to provide per-stage robustness, employs a parameterized strong Lagrangian duality approach, and wraps around standard RL solvers like PPO.", "result": "The method ensures exact equivalence to the original constrained problem under appropriate constraint qualifications and demonstrates risk-aware properties through numerical experiments.", "conclusion": "The proposed risk-aware constrained RL framework effectively addresses tail risks, provides convergence guarantees, and can be practically implemented with standard RL algorithms."}}
{"id": "2510.20200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20200", "abs": "https://arxiv.org/abs/2510.20200", "authors": ["Max Hopkins", "Russell Impagliazzo", "Christopher Ye"], "title": "Approximate Replicability in Learning", "comment": "51 pages, 1 figure", "summary": "Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion\nthat algorithms should remain stable under a resampling of their inputs (given\naccess to shared randomness). While a strong and interesting notion of\nstability, the cost of replicability can be prohibitive: there is no replicable\nalgorithm, for instance, for tasks as simple as threshold learning (Bun et al.\nSTOC '23). Given such strong impossibility results we ask: under what\napproximate notions of replicability is learning possible?\n  In this work, we propose three natural relaxations of replicability in the\ncontext of PAC learning: (1) Pointwise: the learner must be consistent on any\nfixed input, but not across all inputs simultaneously, (2) Approximate: the\nlearner must output hypotheses that classify most of the distribution\nconsistently, (3) Semi: the algorithm is fully replicable, but may additionally\nuse shared unlabeled samples. In all three cases, for constant replicability\nparameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are\nachievable for ``free\" using $\\Theta(d/\\alpha^2)$ samples, while (3) requires\n$\\Theta(d^2/\\alpha^2)$ labeled samples.", "AI": {"tldr": "This paper proposes three relaxations of replicability in PAC learning to overcome strong impossibility results, showing that pointwise, approximate, and semi-replicable learning can achieve sample-optimal agnostic PAC learning with different sample complexities.", "motivation": "Replicability requires algorithms to remain stable under input resampling, but this strong notion has prohibitive costs - there are no replicable algorithms even for simple tasks like threshold learning. The authors seek to understand under what approximate notions of replicability learning becomes possible.", "method": "The authors propose three relaxations of replicability: (1) Pointwise replicability - consistent on any fixed input but not across all inputs simultaneously, (2) Approximate replicability - output hypotheses that classify most of the distribution consistently, (3) Semi-replicability - fully replicable but may use shared unlabeled samples.", "result": "For constant replicability parameters, the authors obtain sample-optimal agnostic PAC learners: Pointwise and approximate replicability are achievable with \u0398(d/\u03b1\u00b2) samples, while semi-replicability requires \u0398(d\u00b2/\u03b1\u00b2) labeled samples.", "conclusion": "The paper demonstrates that by relaxing the strict notion of replicability to pointwise, approximate, or semi-replicability, it becomes possible to achieve sample-optimal agnostic PAC learning, with different sample complexities depending on the relaxation used."}}
{"id": "2510.20209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20209", "abs": "https://arxiv.org/abs/2510.20209", "authors": ["Shumin Li"], "title": "Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset", "comment": null, "summary": "The development of accessible screening tools for early cancer detection in\ndogs represents a significant challenge in veterinary medicine. Routine\nlaboratory data offer a promising, low-cost source for such tools, but their\nutility is hampered by the non-specificity of individual biomarkers and the\nsevere class imbalance inherent in screening populations. This study assesses\nthe feasibility of cancer risk classification using the Golden Retriever\nLifetime Study (GRLS) cohort under real-world constraints, including the\ngrouping of diverse cancer types and the inclusion of post-diagnosis samples. A\ncomprehensive benchmark evaluation was conducted, systematically comparing 126\nanalytical pipelines that comprised various machine learning models, feature\nselection methods, and data balancing techniques. Data were partitioned at the\npatient level to prevent leakage. The optimal model, a Logistic Regression\nclassifier with class weighting and recursive feature elimination, demonstrated\nmoderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical\nclassification performance (F1-score = 0.25, Positive Predictive Value = 0.15).\nWhile a high Negative Predictive Value (0.98) was achieved, insufficient recall\n(0.79) precludes its use as a reliable rule-out test. Interpretability analysis\nwith SHapley Additive exPlanations (SHAP) revealed that predictions were driven\nby non-specific features like age and markers of inflammation and anemia. It is\nconcluded that while a statistically detectable cancer signal exists in routine\nlab data, it is too weak and confounded for clinically reliable discrimination\nfrom normal aging or other inflammatory conditions. This work establishes a\ncritical performance ceiling for this data modality in isolation and\nunderscores that meaningful progress in computational veterinary oncology will\nrequire integration of multi-modal data sources.", "AI": {"tldr": "This study evaluated machine learning approaches for canine cancer detection using routine lab data from the Golden Retriever Lifetime Study. The best model achieved moderate ranking ability but poor clinical performance due to weak cancer signals confounded by aging and inflammation.", "motivation": "To develop accessible screening tools for early cancer detection in dogs using routine laboratory data, addressing challenges of non-specific biomarkers and severe class imbalance in screening populations.", "method": "Comprehensive benchmark evaluation of 126 analytical pipelines using various machine learning models, feature selection methods, and data balancing techniques. Data were partitioned at patient level to prevent leakage, using the Golden Retriever Lifetime Study cohort.", "result": "Optimal model (Logistic Regression with class weighting and recursive feature elimination) showed moderate ranking ability (AUROC = 0.815) but poor clinical performance (F1-score = 0.25, PPV = 0.15). High NPV (0.98) but insufficient recall (0.79) for reliable rule-out testing. SHAP analysis revealed predictions driven by non-specific features like age, inflammation, and anemia markers.", "conclusion": "A statistically detectable cancer signal exists in routine lab data but is too weak and confounded for clinically reliable discrimination from normal aging or other inflammatory conditions. This establishes a performance ceiling for this data modality alone and highlights the need for multi-modal data integration in computational veterinary oncology."}}
{"id": "2510.20219", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20219", "abs": "https://arxiv.org/abs/2510.20219", "authors": ["Ke Xing", "Yanjie Dong", "Xiaoyi Fan", "Runhao Zeng", "Victor C. M. Leung", "M. Jamal Deen", "Xiping Hu"], "title": "CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks", "comment": null, "summary": "Personalized federated learning (PFL) addresses a critical challenge of\ncollaboratively training customized models for clients with heterogeneous and\nscarce local data. Conventional federated learning, which relies on a single\nconsensus model, proves inadequate under such data heterogeneity. Its standard\naggregation method of weighting client updates heuristically or by data volume,\noperates under an equal-contribution assumption, failing to account for the\nactual utility and reliability of each client's update. This often results in\nsuboptimal personalization and aggregation bias. To overcome these limitations,\nwe introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that\ndynamically estimates each client's contribution for global aggregation. CO-PFL\nperforms a joint assessment by analyzing both gradient direction discrepancies\nand prediction deviations, leveraging information from gradient and data\nsubspaces. This dual-subspace analysis provides a principled and discriminative\naggregation weight for each client, emphasizing high-quality updates.\nFurthermore, to bolster personalization adaptability and optimization\nstability, CO-PFL cohesively integrates a parameter-wise personalization\nmechanism with mask-aware momentum optimization. Our approach effectively\nmitigates aggregation bias, strengthens global coordination, and enhances local\nperformance by facilitating the construction of tailored submodels with stable\nupdates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,\nCINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses\nstate-of-the-art methods in in personalization accuracy, robustness,\nscalability and convergence stability.", "AI": {"tldr": "CO-PFL is a personalized federated learning algorithm that dynamically estimates client contributions using dual-subspace analysis of gradient directions and prediction deviations, providing principled aggregation weights and enhancing personalization through parameter-wise mechanisms and mask-aware momentum optimization.", "motivation": "Conventional federated learning with single consensus models and heuristic aggregation methods fails under data heterogeneity, leading to suboptimal personalization and aggregation bias due to equal-contribution assumptions that ignore actual utility and reliability of client updates.", "method": "CO-PFL performs joint assessment using dual-subspace analysis (gradient direction discrepancies and prediction deviations) to dynamically estimate client contributions, integrates parameter-wise personalization mechanism with mask-aware momentum optimization for stable updates and tailored submodels.", "result": "Extensive experiments on CIFAR10, CIFAR10C, CINIC10, and Mini-ImageNet show CO-PFL consistently surpasses state-of-the-art methods in personalization accuracy, robustness, scalability, and convergence stability.", "conclusion": "CO-PFL effectively mitigates aggregation bias, strengthens global coordination, and enhances local performance by providing discriminative aggregation weights and stable personalized updates, making it superior for heterogeneous federated learning scenarios."}}
{"id": "2510.20222", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20222", "abs": "https://arxiv.org/abs/2510.20222", "authors": ["Hao Wang", "Baojun Ma"], "title": "QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models", "comment": "10 pages, 5 figures", "summary": "In real-world time series forecasting tasks, category information plays a\npivotal role in capturing inherent data patterns. This paper introduces QKCV\n(Query-Key-Category-Value) attention, an extension of the traditional QKV\nframework that incorporates a static categorical embedding C to emphasize\ncategory-specific information. As a versatile plug-in module, QKCV enhances the\nforecasting accuracy of attention-based models (e.g., Vanilla Transformer,\nInformer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV\ndemonstrates remarkable adaptability in fine-tuning univariate time series\nfoundation model by solely updating the static embedding C while preserving\npretrained weights, thereby reducing computational overhead and achieving\nsuperior fine-tuning performance.", "AI": {"tldr": "QKCV attention extends traditional QKV attention by incorporating static categorical embeddings to capture category-specific patterns in time series forecasting, improving accuracy across various models and enabling efficient fine-tuning of foundation models.", "motivation": "Category information is crucial for capturing inherent patterns in real-world time series forecasting tasks, but traditional attention mechanisms don't explicitly leverage this categorical data.", "method": "Introduces QKCV (Query-Key-Category-Value) attention that adds a static categorical embedding C to the standard QKV framework, serving as a plug-in module that can enhance various attention-based forecasting models.", "result": "QKCV improves forecasting accuracy across diverse real-world datasets and demonstrates remarkable adaptability in fine-tuning univariate time series foundation models by only updating the static embedding C while preserving pretrained weights.", "conclusion": "QKCV attention effectively incorporates category information into attention mechanisms, enhancing forecasting performance while enabling computationally efficient fine-tuning of foundation models through selective parameter updates."}}
{"id": "2510.20225", "categories": ["cs.LG", "cs.AI", "68T07 (Artificial neural networks and deep learning), 62F15\n  (Bayesian inference)"], "pdf": "https://arxiv.org/pdf/2510.20225", "abs": "https://arxiv.org/abs/2510.20225", "authors": ["Insu Jeon", "Minui Hong", "Junhyeog Yun", "Gunhee Kim"], "title": "Federated Learning via Meta-Variational Dropout", "comment": "Published in the Proceedings of the Advances in Neural Information\n  Processing Systems (NeurIPS) 2023, Main Conference Track", "summary": "Federated Learning (FL) aims to train a global inference model from remotely\ndistributed clients, gaining popularity due to its benefit of improving data\nprivacy. However, traditional FL often faces challenges in practical\napplications, including model overfitting and divergent local models due to\nlimited and non-IID data among clients. To address these issues, we introduce a\nnovel Bayesian meta-learning approach called meta-variational dropout (MetaVD).\nMetaVD learns to predict client-dependent dropout rates via a shared\nhypernetwork, enabling effective model personalization of FL algorithms in\nlimited non-IID data settings. We also emphasize the posterior adaptation view\nof meta-learning and the posterior aggregation view of Bayesian FL via the\nconditional dropout posterior. We conducted extensive experiments on various\nsparse and non-IID FL datasets. MetaVD demonstrated excellent classification\naccuracy and uncertainty calibration performance, especially for\nout-of-distribution (OOD) clients. MetaVD compresses the local model parameters\nneeded for each client, mitigating model overfitting and reducing communication\ncosts. Code is available at https://github.com/insujeon/MetaVD.", "AI": {"tldr": "MetaVD is a Bayesian meta-learning approach that uses a hypernetwork to predict client-dependent dropout rates, addressing model overfitting and divergent local models in federated learning with non-IID data.", "motivation": "Traditional federated learning faces challenges with model overfitting and divergent local models due to limited and non-IID data among clients, which MetaVD aims to solve.", "method": "MetaVD learns client-dependent dropout rates via a shared hypernetwork, enabling model personalization through conditional dropout posterior for posterior adaptation and aggregation.", "result": "Extensive experiments showed MetaVD achieves excellent classification accuracy and uncertainty calibration, especially for OOD clients, while compressing local parameters and reducing communication costs.", "conclusion": "MetaVD effectively addresses FL challenges in non-IID settings through Bayesian meta-learning with personalized dropout rates, improving performance while reducing overfitting and communication overhead."}}
{"id": "2510.20228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20228", "abs": "https://arxiv.org/abs/2510.20228", "authors": ["Yago del Valle Inclan Redondo", "Enrique Arriaga-Varela", "Dmitry Lyamzin", "Pablo Cervantes", "Tiago Ramalho"], "title": "Sparse Local Implicit Image Function for sub-km Weather Downscaling", "comment": null, "summary": "We introduce SpLIIF to generate implicit neural representations and enable\narbitrary downscaling of weather variables. We train a model from sparse\nweather stations and topography over Japan and evaluate in- and\nout-of-distribution accuracy predicting temperature and wind, comparing it to\nboth an interpolation baseline and CorrDiff. We find the model to be up to 50%\nbetter than both CorrDiff and the baseline at downscaling temperature, and\naround 10-20% better for wind.", "AI": {"tldr": "SpLIIF generates implicit neural representations for arbitrary downscaling of weather variables, trained on sparse weather stations and topography data over Japan.", "motivation": "To improve downscaling accuracy for weather variables like temperature and wind compared to existing methods like interpolation baselines and CorrDiff.", "method": "Train a model using sparse weather stations and topography data over Japan to create implicit neural representations that enable arbitrary downscaling.", "result": "The model achieves up to 50% better performance than CorrDiff and baseline for temperature downscaling, and 10-20% better for wind downscaling.", "conclusion": "SpLIIF demonstrates superior downscaling capabilities for weather variables, particularly temperature, compared to existing methods."}}
{"id": "2510.20235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20235", "abs": "https://arxiv.org/abs/2510.20235", "authors": ["Woohyeon Byeon", "Giseung Park", "Jongseong Chae", "Amir Leshem", "Youngchul Sung"], "title": "Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach", "comment": "Accepted to NeurIPS 2025", "summary": "In this paper, we propose a provably convergent and practical framework for\nmulti-objective reinforcement learning with max-min criterion. From a\ngame-theoretic perspective, we reformulate max-min multi-objective\nreinforcement learning as a two-player zero-sum regularized continuous game and\nintroduce an efficient algorithm based on mirror descent. Our approach\nsimplifies the policy update while ensuring global last-iterate convergence. We\nprovide a comprehensive theoretical analysis on our algorithm, including\niteration complexity under both exact and approximate policy evaluations, as\nwell as sample complexity bounds. To further enhance performance, we modify the\nproposed algorithm with adaptive regularization. Our experiments demonstrate\nthe convergence behavior of the proposed algorithm in tabular settings, and our\nimplementation for deep reinforcement learning significantly outperforms\nprevious baselines in many MORL environments.", "AI": {"tldr": "A provably convergent framework for max-min multi-objective RL using game theory and mirror descent with global convergence guarantees.", "motivation": "To address multi-objective reinforcement learning with max-min criterion through a game-theoretic perspective, ensuring provable convergence while maintaining practical efficiency.", "method": "Reformulate max-min MORL as a two-player zero-sum regularized continuous game and develop an efficient mirror descent algorithm with adaptive regularization for enhanced performance.", "result": "The algorithm demonstrates convergence in tabular settings and significantly outperforms previous baselines in deep RL environments, with comprehensive theoretical analysis including iteration and sample complexity bounds.", "conclusion": "The proposed framework provides a theoretically sound and practically effective solution for max-min multi-objective reinforcement learning with provable convergence guarantees."}}
{"id": "2510.20236", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20236", "abs": "https://arxiv.org/abs/2510.20236", "authors": ["Teng Jiek See", "Daokun Zhang", "Mario Boley", "David K. Chalmers"], "title": "Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction", "comment": null, "summary": "Graph Neural Networks (GNNs) are the currently most effective methods for\npredicting molecular properties but there remains a need for more accurate\nmodels. GNN accuracy can be improved by increasing the model complexity but\nthis also increases the computational cost and memory requirement during\ntraining and inference. In this study, we develop Layer-to-Layer Knowledge\nMixing (LKM), a novel self-knowledge distillation method that increases the\naccuracy of state-of-the-art GNNs while adding negligible computational\ncomplexity during training and inference. By minimizing the mean absolute\ndistance between pre-existing hidden embeddings of GNN layers, LKM efficiently\naggregates multi-hop and multi-scale information, enabling improved\nrepresentation of both local and global molecular features. We evaluated LKM\nusing three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using\ndatasets of quantum chemical properties (QM9, MD17 and Chignolin). We found\nthat the LKM method effectively reduces the mean absolute error of quantum\nchemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17\nEnergy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to\nsignificantly improve the accuracy of GNNs for chemical property prediction\nwithout any substantial increase in training and inference cost.", "AI": {"tldr": "LKM is a novel self-knowledge distillation method that improves GNN accuracy for molecular property prediction by minimizing distance between hidden embeddings, with up to 45.3% error reduction and negligible computational overhead.", "motivation": "There's a need for more accurate GNN models for molecular property prediction, but increasing model complexity raises computational costs and memory requirements during training and inference.", "method": "Developed Layer-to-Layer Knowledge Mixing (LKM), a self-knowledge distillation method that minimizes mean absolute distance between pre-existing hidden embeddings of GNN layers to aggregate multi-hop and multi-scale information.", "result": "LKM reduced mean absolute error by up to 9.8% on QM9, 45.3% on MD17 Energy, and 22.9% on Chignolin datasets using three GNN architectures (DimeNet++, MXMNet, and PAMNet).", "conclusion": "LKM significantly improves GNN accuracy for chemical property prediction without substantial increases in training and inference costs, demonstrating its potential for efficient model enhancement."}}
{"id": "2510.20250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20250", "abs": "https://arxiv.org/abs/2510.20250", "authors": ["Zhiqin Yang", "Yonggang Zhang", "Chenxin Li", "Yiu-ming Cheung", "Bo Han", "Yixuan Yuan"], "title": "FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning", "comment": "35 pages, 15 figures, 21 tables", "summary": "Federated Learning (FL) confronts a significant challenge known as data\nheterogeneity, which impairs model performance and convergence. Existing\nmethods have made notable progress in addressing this issue. However, improving\nperformance in certain heterogeneity scenarios remains an overlooked question:\n\\textit{How robust are these methods to deploy under diverse heterogeneity\nscenarios?} To answer this, we conduct comprehensive evaluations across varied\nheterogeneity scenarios, showing that most existing methods exhibit limited\nrobustness. Meanwhile, insights from these experiments highlight that sharing\nstatistical information can mitigate heterogeneity by enabling clients to\nupdate with a global perspective. Motivated by this, we propose \\textbf{FedGPS}\n(\\textbf{Fed}erated \\textbf{G}oal-\\textbf{P}ath \\textbf{S}ynergy), a novel\nframework that seamlessly integrates statistical distribution and gradient\ninformation from others. Specifically, FedGPS statically modifies each client's\nlearning objective to implicitly model the global data distribution using\nsurrogate information, while dynamically adjusting local update directions with\ngradient information from other clients at each round. Extensive experiments\nshow that FedGPS outperforms state-of-the-art methods across diverse\nheterogeneity scenarios, validating its effectiveness and robustness. The code\nis available at: https://github.com/CUHK-AIM-Group/FedGPS.", "AI": {"tldr": "FedGPS is a novel federated learning framework that addresses data heterogeneity by integrating statistical distribution and gradient information from other clients, achieving superior performance and robustness across diverse scenarios.", "motivation": "Existing FL methods struggle with data heterogeneity and lack robustness across different heterogeneity scenarios. The authors identified that sharing statistical information can help mitigate heterogeneity by providing clients with a global perspective.", "method": "FedGPS statically modifies each client's learning objective to implicitly model global data distribution using surrogate information, and dynamically adjusts local update directions with gradient information from other clients at each round.", "result": "Extensive experiments show that FedGPS outperforms state-of-the-art methods across diverse heterogeneity scenarios, demonstrating both effectiveness and robustness.", "conclusion": "FedGPS provides a robust solution to data heterogeneity in federated learning by synergistically combining statistical distribution and gradient information, making it suitable for deployment in varied real-world scenarios."}}
{"id": "2510.20264", "categories": ["cs.LG", "I.2.6; I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.20264", "abs": "https://arxiv.org/abs/2510.20264", "authors": ["Thomas Rupf", "Marco Bagatella", "Marin Vlastelica", "Andreas Krause"], "title": "Optimistic Task Inference for Behavior Foundation Models", "comment": null, "summary": "Behavior Foundation Models (BFMs) are capable of retrieving high-performing\npolicy for any reward function specified directly at test-time, commonly\nreferred to as zero-shot reinforcement learning (RL). While this is a very\nefficient process in terms of compute, it can be less so in terms of data: as a\nstandard assumption, BFMs require computing rewards over a non-negligible\ninference dataset, assuming either access to a functional form of rewards, or\nsignificant labeling efforts. To alleviate these limitations, we tackle the\nproblem of task inference purely through interaction with the environment at\ntest-time. We propose OpTI-BFM, an optimistic decision criterion that directly\nmodels uncertainty over reward functions and guides BFMs in data collection for\ntask inference. Formally, we provide a regret bound for well-trained BFMs\nthrough a direct connection to upper-confidence algorithms for linear bandits.\nEmpirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and\nobserve that it enables successor-features-based BFMs to identify and optimize\nan unseen reward function in a handful of episodes with minimal compute\noverhead. Code is available at https://github.com/ThomasRupf/opti-bfm.", "AI": {"tldr": "OpTI-BFM is an optimistic decision criterion that enables Behavior Foundation Models (BFMs) to infer tasks through environment interaction at test-time, reducing the need for pre-computed reward data and labeling efforts.", "motivation": "Current BFMs require computing rewards over inference datasets, assuming access to functional reward forms or significant labeling. This work aims to enable task inference purely through environment interaction to reduce data requirements.", "method": "Proposes OpTI-BFM, an optimistic decision criterion that models uncertainty over reward functions and guides BFMs in data collection for task inference. Connects to upper-confidence algorithms for linear bandits.", "result": "Provides formal regret bound for well-trained BFMs. Empirically enables successor-features-based BFMs to identify and optimize unseen reward functions in a handful of episodes with minimal compute overhead.", "conclusion": "OpTI-BFM successfully addresses the data efficiency limitations of BFMs by enabling task inference through environment interaction, making zero-shot RL more practical with reduced data requirements."}}
{"id": "2510.20270", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20270", "abs": "https://arxiv.org/abs/2510.20270", "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"], "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", "comment": null, "summary": "The tendency to find and exploit \"shortcuts\" to complete tasks poses\nsignificant risks for reliable assessment and deployment of large language\nmodels (LLMs). For example, an LLM agent with access to unit tests may delete\nfailing tests rather than fix the underlying bug. Such behavior undermines both\nthe validity of benchmark results and the reliability of real-world LLM coding\nassistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,\na benchmark framework that systematically measures LLM agents' propensity to\nexploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from\nexisting benchmarks like LiveCodeBench and SWE-bench by introducing direct\nconflicts between the natural-language specification and the unit tests. We\nmeasure an agent's \"cheating rate\" as its pass rate on these impossible tasks,\nwhere any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a\nversatile tool. We demonstrate its utility for: (1) studying model behaviors,\nrevealing more fine-grained details of cheating behaviors from simple test\nmodification to complex operator overloading; (2) context engineering, showing\nhow prompt, test access and feedback loop affect cheating rates; and (3)\ndeveloping monitoring tools, providing a testbed with verified deceptive\nsolutions. We hope ImpossibleBench serves as a useful framework for building\nmore robust and reliable LLM systems.\n  Our implementation can be found at\nhttps://github.com/safety-research/impossiblebench.", "AI": {"tldr": "ImpossibleBench is a benchmark framework that measures LLM agents' tendency to exploit shortcuts by creating impossible tasks with conflicts between specifications and unit tests, quantifying cheating behavior.", "motivation": "LLMs often find and exploit shortcuts to complete tasks, which undermines benchmark validity and real-world reliability. This behavior needs systematic measurement and mitigation.", "method": "Create impossible variants of existing benchmarks by introducing direct conflicts between natural-language specifications and unit tests, then measure cheating rate as pass rate on these impossible tasks.", "result": "Reveals fine-grained cheating behaviors from simple test modification to complex operator overloading, and shows how prompt engineering, test access, and feedback loops affect cheating rates.", "conclusion": "ImpossibleBench serves as a versatile tool for studying model behaviors, context engineering, and developing monitoring tools to build more robust and reliable LLM systems."}}
{"id": "2510.20271", "categories": ["cs.LG", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.20271", "abs": "https://arxiv.org/abs/2510.20271", "authors": ["Udit Saxena"], "title": "Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch", "comment": "Extended Abstract: Accepted to the NeurReps 2025 workshop at NeurIPS\n  2025. 4 pages, 3 figures", "summary": "Topological features capture global geometric structure in imaging data, but\npractical adoption in deep learning requires both computational efficiency and\ndifferentiability. We present optimized GPU kernels for the Euler\nCharacteristic Curve (ECC) computation achieving 16-2000\\\"O speedups over prior\nGPU implementations on synthetic grids, and introduce a differentiable PyTorch\nlayer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs\nuse 128B-coalesced access and hierarchical shared-memory accumulation. Our\nPyTorch layer learns thresholds in a single direction via a Differentiable\nEuler Characteristic Transform-style sigmoid relaxation. We discuss downstream\nrelevance, including applications highlighted by prior ECC work, and outline\nbatching/multi-GPU extensions to broaden adoption.", "AI": {"tldr": "The paper presents optimized GPU kernels for Euler Characteristic Curve computation with 16-2000x speedups over prior implementations, and introduces a differentiable PyTorch layer for end-to-end learning.", "motivation": "Topological features capture global geometric structure in imaging data, but practical adoption in deep learning requires both computational efficiency and differentiability.", "method": "Developed optimized CUDA kernels for ECC computation using 128B-coalesced access and hierarchical shared-memory accumulation, plus a differentiable PyTorch layer with Differentiable Euler Characteristic Transform-style sigmoid relaxation to learn thresholds.", "result": "Achieved 16-2000x speedups over prior GPU implementations on synthetic grids, enabling efficient computation and end-to-end learning capabilities.", "conclusion": "The work enables practical adoption of topological features in deep learning through computational efficiency and differentiability, with potential applications in various domains highlighted by prior ECC work."}}
{"id": "2510.20272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20272", "abs": "https://arxiv.org/abs/2510.20272", "authors": ["Tristan Cinquin", "Geoff Pleiss", "Agustinus Kristiadi"], "title": "Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs", "comment": null, "summary": "While chain-of-thought prompting with Best-of-N (BoN) selection has become\npopular for mathematical reasoning in large language models (LLMs), its linear\nstructure fails to capture the branching and exploratory nature of complex\nproblem-solving. In this work, we propose an adaptive algorithm to maximize\nprocess reward model (PRM) scores over the intractable action space, and\ninvestigate whether PRM-guided tree search can improve mathematical reasoning\nby exploring multiple partial solution paths. Across $23$ diverse mathematical\nproblems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case\nstudy, we find that: (1) PRM-guided tree search shows no statistically\nsignificant improvements over BoN despite higher costs, (2) Monte Carlo tree\nsearch and beam search outperform other PRM-guided tree search methods, (3)\nPRMs poorly approximate state values and their reliability degrades with\nreasoning depth, and (4) PRMs generalize poorly out of distribution. This\nunderperformance stems from tree search's greater reliance on unreliable PRM\nscores, suggesting different reward modeling is necessary before tree search\ncan effectively enhance mathematical reasoning in LLMs.", "AI": {"tldr": "PRM-guided tree search for mathematical reasoning shows no significant improvement over Best-of-N selection despite higher computational costs, due to unreliable process reward models that poorly approximate state values and generalize poorly out of distribution.", "motivation": "Chain-of-thought prompting with Best-of-N selection has limitations in capturing the branching and exploratory nature of complex mathematical problem-solving, motivating the investigation of PRM-guided tree search approaches.", "method": "Proposed an adaptive algorithm to maximize process reward model scores over intractable action space and investigated PRM-guided tree search methods including Monte Carlo tree search and beam search across 23 diverse mathematical problems using Qwen2.5-Math-7B-Instruct.", "result": "PRM-guided tree search showed no statistically significant improvements over BoN despite higher costs; Monte Carlo tree search and beam search outperformed other PRM-guided methods; PRMs poorly approximated state values and their reliability degraded with reasoning depth; PRMs generalized poorly out of distribution.", "conclusion": "Tree search's underperformance stems from greater reliance on unreliable PRM scores, suggesting different reward modeling approaches are necessary before tree search can effectively enhance mathematical reasoning in LLMs."}}
{"id": "2510.20273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20273", "abs": "https://arxiv.org/abs/2510.20273", "authors": ["Qitai Tan", "Yiyun Chen", "Mo Li", "Ruiwen Gu", "Yilin Su", "Xiao-Ping Zhang"], "title": "SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series", "comment": "NeurIPS 2025", "summary": "Recent advances in deep learning have driven rapid progress in time series\nforecasting, yet many state-of-the-art models continue to struggle with robust\nperformance in real-world applications, even when they achieve strong results\non standard benchmark datasets. This persistent gap can be attributed to the\nblack-box nature of deep learning architectures and the inherent limitations of\ncurrent evaluation frameworks, which frequently lack the capacity to provide\nclear, quantitative insights into the specific strengths and weaknesses of\ndifferent models, thereby complicating the selection of appropriate models for\nparticular forecasting scenarios. To address these issues, we propose a\nsynthetic data-driven evaluation paradigm, SynTSBench, that systematically\nassesses fundamental modeling capabilities of time series forecasting models\nthrough programmable feature configuration. Our framework isolates confounding\nfactors and establishes an interpretable evaluation system with three core\nanalytical dimensions: (1) temporal feature decomposition and capability\nmapping, which enables systematic evaluation of model capacities to learn\nspecific pattern types; (2) robustness analysis under data irregularities,\nwhich quantifies noise tolerance thresholds and anomaly recovery capabilities;\nand (3) theoretical optimum benchmarking, which establishes performance\nboundaries for each pattern type-enabling direct comparison between model\npredictions and mathematical optima. Our experiments show that current deep\nlearning models do not universally approach optimal baselines across all types\nof temporal features.The code is available at\nhttps://github.com/TanQitai/SynTSBench", "AI": {"tldr": "SynTSBench is a synthetic data-driven evaluation framework that systematically assesses time series forecasting models through programmable feature configuration, focusing on temporal feature decomposition, robustness analysis, and theoretical optimum benchmarking.", "motivation": "Current deep learning models struggle with robust performance in real-world applications despite strong benchmark results, due to black-box architectures and limited evaluation frameworks that lack clear quantitative insights into model strengths and weaknesses.", "method": "The framework uses synthetic data with programmable feature configuration to isolate confounding factors. It employs three analytical dimensions: temporal feature decomposition and capability mapping, robustness analysis under data irregularities, and theoretical optimum benchmarking.", "result": "Experiments show that current deep learning models do not universally approach optimal baselines across all types of temporal features, revealing specific performance gaps in different pattern learning capabilities.", "conclusion": "SynTSBench provides an interpretable evaluation system that enables systematic assessment of fundamental modeling capabilities in time series forecasting, helping identify specific strengths and weaknesses of different models for particular forecasting scenarios."}}
{"id": "2510.20278", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20278", "abs": "https://arxiv.org/abs/2510.20278", "authors": ["Guangyu Dai", "Siliang Tang", "Yueting Zhuang"], "title": "KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models", "comment": null, "summary": "In recent years, Pretrained Large Models(PLMs) researchers proposed\nlarge-small model collaboration frameworks, leveraged easily trainable small\nmodels to assist large models, aim to(1) significantly reduce computational\nresource consumption while maintaining comparable accuracy, and (2) enhance\nlarge model performance in specialized domain tasks. However, this\ncollaborative paradigm suffers from issues such as significant accuracy\ndegradation, exacerbated catastrophic forgetting, and amplified hallucination\nproblems induced by small model knowledge. To address these challenges, we\npropose a KAN-based Collaborative Model (KCM) as an improved approach to\nlarge-small model collaboration. The KAN utilized in KCM represents an\nalternative neural network architecture distinct from conventional MLPs.\nCompared to MLPs, KAN offers superior visualizability and interpretability\nwhile mitigating catastrophic forgetting. We deployed KCM in large-small model\ncollaborative systems across three scenarios: language, vision, and\nvision-language cross-modal tasks. The experimental results demonstrate that,\ncompared with pure large model approaches, the large-small model collaboration\nframework utilizing KCM as the collaborative model significantly reduces the\nnumber of large model inference calls while maintaining near-identical task\naccuracy, thereby substantially lowering computational resource consumption.\nConcurrently, the KAN-based small collaborative model markedly mitigates\ncatastrophic forgetting, leading to significant accuracy improvements for\nlong-tail data. The results reveal that KCM demonstrates superior performance\nacross all metrics compared to MLP-based small collaborative models (MCM).", "AI": {"tldr": "KAN-based Collaborative Model (KCM) improves large-small model collaboration by reducing computational costs while maintaining accuracy, mitigating catastrophic forgetting, and enhancing performance on long-tail data compared to MLP-based approaches.", "motivation": "To address issues in large-small model collaboration frameworks such as accuracy degradation, catastrophic forgetting, and hallucination problems caused by small model knowledge, while reducing computational resource consumption and enhancing large model performance in specialized domains.", "method": "Proposed KCM using KAN (Kolmogorov-Arnold Networks) as an alternative neural architecture to MLPs, deployed in collaborative systems across language, vision, and vision-language cross-modal tasks to assist large models.", "result": "KCM significantly reduces large model inference calls while maintaining near-identical task accuracy, substantially lowers computational resource consumption, mitigates catastrophic forgetting, and improves accuracy for long-tail data, outperforming MLP-based collaborative models across all metrics.", "conclusion": "KCM provides an effective solution for large-small model collaboration, offering superior performance, better interpretability, and reduced computational costs compared to traditional MLP-based approaches."}}
{"id": "2510.20279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20279", "abs": "https://arxiv.org/abs/2510.20279", "authors": ["Penghao Wang", "Yuhao Zhou", "Mengxuan Wu", "Ziheng Qin", "Bangyuan Zhu", "Shengbin Huang", "Xuanlei Zhao", "Panpan Zhang", "Xiaojiang Peng", "Yuzhang Shang", "Jianfei Yang", "Zheng Zhu", "Tianlong Chen", "Zhangyang Wang", "Kai Wang"], "title": "ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows", "comment": null, "summary": "As large language models (LLMs) advance, the ultimate vision for their role\nin science is emerging: we could build an AI collaborator to effectively assist\nhuman beings throughout the entire scientific research process. We refer to\nthis envisioned system as ResearchGPT. Given that scientific research\nprogresses through multiple interdependent phases, achieving this vision\nrequires rigorous benchmarks that evaluate the end-to-end workflow rather than\nisolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of\nscientific Q&A pairs in computer science, built from 14k CC-licensed papers. It\nis constructed through a scalable, paper-grounded pipeline that combines\nretrieval-augmented generation (RAG) with multi-stage quality control to ensure\nfactual grounding. From this unified corpus, we derive two complementary\nsubsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to\nassist scientific research, and CS-50k, a large-scale training dataset.\nExtensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs\ninto distinct capability tiers. Open models trained on CS-50k with supervised\ntraining and reinforcement learning demonstrate substantial improvements. Even\n7B-scale models, when properly trained, outperform many larger proprietary\nsystems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that\nmaking AI models better research assistants relies more on domain-aligned\ntraining with high-quality data than on pretraining scale or general benchmark\nperformance. We release CS-4k and CS-50k in the hope of fostering AI systems as\nreliable collaborators in CS research.", "AI": {"tldr": "The paper introduces CS-54k, a corpus of scientific Q&A pairs from computer science papers, and derives CS-4k for benchmarking AI research assistants and CS-50k for training. Experiments show domain-specific training with high-quality data is more important than model size for creating effective AI research collaborators.", "motivation": "To build AI collaborators that can assist throughout the entire scientific research process, requiring benchmarks that evaluate end-to-end workflows rather than isolated sub-tasks.", "method": "Created CS-54k corpus from 14k CC-licensed papers using a scalable pipeline combining retrieval-augmented generation with multi-stage quality control. Derived CS-4k for evaluation and CS-50k for training.", "result": "CS-4k stratifies state-of-the-art LLMs into distinct capability tiers. Open models trained on CS-50k with supervised training and reinforcement learning show substantial improvements, with 7B-scale models outperforming larger proprietary systems like GPT-4.1, GPT-4o, and Gemini 2.5 Pro.", "conclusion": "Making AI models better research assistants relies more on domain-aligned training with high-quality data than on pretraining scale or general benchmark performance."}}
{"id": "2510.20295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20295", "abs": "https://arxiv.org/abs/2510.20295", "authors": ["Yang Qiu", "Yixiong Zou", "Jun Wang", "Wei Liu", "Xiangyu Fu", "Ruixuan Li"], "title": "Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization", "comment": null, "summary": "Out-of-distribution generalization under distributional shifts remains a\ncritical challenge for graph neural networks. Existing methods generally adopt\nthe Invariant Risk Minimization (IRM) framework, requiring costly environment\nannotations or heuristically generated synthetic splits. To circumvent these\nlimitations, in this work, we aim to develop an IRM-free method for capturing\ncausal subgraphs. We first identify that causal subgraphs exhibit substantially\nsmaller distributional variations than non-causal components across diverse\nenvironments, which we formalize as the Invariant Distribution Criterion and\ntheoretically prove in this paper. Building on this criterion, we\nsystematically uncover the quantitative relationship between distributional\nshift and representation norm for identifying the causal subgraph, and\ninvestigate its underlying mechanisms in depth. Finally, we propose an IRM-free\nmethod by introducing a norm-guided invariant distribution objective for causal\nsubgraph discovery and prediction. Extensive experiments on two widely used\nbenchmarks demonstrate that our method consistently outperforms\nstate-of-the-art methods in graph generalization.", "AI": {"tldr": "This paper proposes an IRM-free method for causal subgraph discovery in graph neural networks to address out-of-distribution generalization challenges without requiring environment annotations or synthetic data splits.", "motivation": "Existing methods for graph generalization rely on Invariant Risk Minimization (IRM) framework, which requires costly environment annotations or heuristically generated synthetic splits, creating practical limitations for real-world applications.", "method": "The authors identify that causal subgraphs exhibit smaller distributional variations than non-causal components, formalize this as the Invariant Distribution Criterion, and develop a norm-guided invariant distribution objective for causal subgraph discovery and prediction without using IRM.", "result": "Extensive experiments on two widely used benchmarks demonstrate that the proposed method consistently outperforms state-of-the-art methods in graph generalization tasks.", "conclusion": "The IRM-free approach effectively captures causal subgraphs by leveraging the invariant distribution properties and norm-guided objectives, providing a practical solution for graph out-of-distribution generalization without the need for environment annotations."}}
{"id": "2510.20299", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20299", "abs": "https://arxiv.org/abs/2510.20299", "authors": ["Saraf Anzum Shreya", "MD. Abu Ismail Siddique", "Sharaf Tasnim"], "title": "DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability", "comment": "25 pages, 14 figures, 12 tables", "summary": "Brain tumors are a challenging problem in neuro-oncology, where early and\nprecise diagnosis is important for successful treatment. Deep learning-based\nbrain tumor classification methods often rely on heavy data augmentation which\ncan limit generalization and trust in clinical applications. In this paper, we\npropose a double-backbone network integrating VGG16 and Xception with a\nFrequency-Gated Attention (FGA) Block to capture complementary local and global\nfeatures. Unlike previous studies, our model achieves state-of-the-art\nperformance without augmentation which demonstrates robustness to variably\nsized and distributed datasets. For further transparency, Grad-CAM is\nintegrated to visualize the tumor regions based on which the model is giving\nprediction, bridging the gap between model prediction and clinical\ninterpretability. The proposed framework achieves 99.24\\% accuracy on the 7K-DS\ndataset for the 4-class setting, along with 98.68\\% and 99.85\\% in the 3-class\nand 2-class settings, respectively. On the independent 3K-DS dataset, the model\ngeneralizes with 95.77\\% accuracy, outperforming baseline and state-of-the-art\nmethods. To further support clinical usability, we developed a graphical user\ninterface (GUI) that provides real-time classification and Grad-CAM-based tumor\nlocalization. These findings suggest that augmentation-free, interpretable, and\ndeployable deep learning models such as DB-FGA-Net hold strong potential for\nreliable clinical translation in brain tumor diagnosis.", "AI": {"tldr": "Proposes DB-FGA-Net, a double-backbone network combining VGG16 and Xception with Frequency-Gated Attention Block for brain tumor classification without data augmentation, achieving state-of-the-art performance with interpretable Grad-CAM visualizations.", "motivation": "Address limitations of deep learning-based brain tumor classification methods that rely on heavy data augmentation, which can limit generalization and trust in clinical applications. Need for robust, augmentation-free models with clinical interpretability.", "method": "Double-backbone network integrating VGG16 and Xception architectures with a Frequency-Gated Attention (FGA) Block to capture complementary local and global features. Uses Grad-CAM for interpretable tumor region visualization. No data augmentation applied.", "result": "Achieved 99.24% accuracy on 7K-DS dataset (4-class), 98.68% (3-class), 99.85% (2-class). Generalizes to independent 3K-DS dataset with 95.77% accuracy, outperforming baseline and state-of-the-art methods. Includes GUI for real-time classification and tumor localization.", "conclusion": "The proposed DB-FGA-Net demonstrates that augmentation-free, interpretable deep learning models hold strong potential for reliable clinical translation in brain tumor diagnosis, providing both high accuracy and clinical interpretability through Grad-CAM visualizations."}}
{"id": "2510.20302", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20302", "abs": "https://arxiv.org/abs/2510.20302", "authors": ["Yuhang Wang"], "title": "InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling", "comment": "23pages, 3 figures", "summary": "Multivariate time series forecasting requires simultaneously modeling\ntemporal patterns and cross-variate dependencies. Channel-independent methods\nsuch as PatchTST excel at temporal modeling but ignore variable correlations,\nwhile pure variate-attention approaches such as iTransformer sacrifice temporal\nencoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that\nachieves principled separation between temporal encoding and variate-level\ndecoding. InvDec combines a patch-based temporal encoder with an inverted\ndecoder operating on the variate dimension through variate-wise self-attention.\nWe introduce delayed variate embeddings that enrich variable-specific\nrepresentations only after temporal encoding, preserving temporal feature\nintegrity. An adaptive residual fusion mechanism dynamically balances temporal\nand variate information across datasets of varying dimensions. Instantiating\nInvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven\nbenchmarks demonstrate significant gains on high-dimensional datasets: 20.9%\nMSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and\n2.7% gain on Traffic compared to PatchTST, while maintaining competitive\nperformance on low-dimensional ETT datasets. Ablation studies validate each\ncomponent, and analysis reveals that InvDec's advantage grows with dataset\ndimensionality, confirming that cross-variate modeling becomes critical as the\nnumber of variables increases.", "AI": {"tldr": "InvDec is a hybrid architecture for multivariate time series forecasting that separates temporal encoding from variate-level decoding using a patch-based temporal encoder and inverted decoder with variate-wise self-attention, achieving significant improvements on high-dimensional datasets.", "motivation": "Existing approaches face limitations: channel-independent methods ignore variable correlations while pure variate-attention approaches sacrifice temporal encoding. There's a need for principled separation between temporal and cross-variate modeling.", "method": "Combines patch-based temporal encoder with inverted decoder using variate-wise self-attention. Introduces delayed variate embeddings to preserve temporal integrity and adaptive residual fusion to balance temporal and variate information dynamically.", "result": "Significant improvements on high-dimensional datasets: 20.9% MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and 2.7% gain on Traffic compared to PatchTST, while maintaining competitive performance on low-dimensional datasets.", "conclusion": "InvDec effectively addresses the trade-off between temporal encoding and cross-variate modeling, with advantages growing with dataset dimensionality, confirming that cross-variate modeling becomes critical as variable count increases."}}
{"id": "2510.20327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20327", "abs": "https://arxiv.org/abs/2510.20327", "authors": ["Fengyuan Yu", "Yuyuan Li", "Xiaohua Feng", "Junjie Fang", "Tao Wang", "Chaochao Chen"], "title": "LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems", "comment": "Accepted by ACM Multimedia 2025", "summary": "With the growing demand for safeguarding sensitive user information in\nrecommender systems, recommendation attribute unlearning is receiving\nincreasing attention. Existing studies predominantly focus on single-attribute\nunlearning. However, privacy protection requirements in the real world often\ninvolve multiple sensitive attributes and are dynamic. Existing\nsingle-attribute unlearning methods cannot meet these real-world requirements\ndue to i) CH1: the inability to handle multiple unlearning requests\nsimultaneously, and ii) CH2: the lack of efficient adaptability to dynamic\nunlearning needs. To address these challenges, we propose LEGO, a lightweight\nand efficient multiple-attribute unlearning framework. Specifically, we divide\nthe multiple-attribute unlearning process into two steps: i) Embedding\nCalibration removes information related to a specific attribute from user\nembedding, and ii) Flexible Combination combines these embeddings into a single\nembedding, protecting all sensitive attributes. We frame the unlearning process\nas a mutual information minimization problem, providing LEGO a theoretical\nguarantee of simultaneous unlearning, thereby addressing CH1. With the two-step\nframework, where Embedding Calibration can be performed in parallel and\nFlexible Combination is flexible and efficient, we address CH2. Extensive\nexperiments on three real-world datasets across three representative\nrecommendation models demonstrate the effectiveness and efficiency of our\nproposed framework. Our code and appendix are available at\nhttps://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.", "AI": {"tldr": "LEGO is a lightweight framework for multiple-attribute recommendation unlearning that addresses dynamic privacy needs through embedding calibration and flexible combination.", "motivation": "Existing single-attribute unlearning methods cannot handle real-world requirements involving multiple sensitive attributes and dynamic unlearning needs, due to inability to handle multiple simultaneous requests and lack of efficient adaptability.", "method": "Two-step framework: 1) Embedding Calibration removes specific attribute information from user embeddings, 2) Flexible Combination combines embeddings to protect all sensitive attributes. Framed as mutual information minimization problem.", "result": "Extensive experiments on three real-world datasets across three recommendation models demonstrate effectiveness and efficiency.", "conclusion": "LEGO provides theoretical guarantee for simultaneous unlearning and addresses both challenges of handling multiple requests and dynamic adaptability through its parallelizable and flexible framework."}}
{"id": "2510.20349", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20349", "abs": "https://arxiv.org/abs/2510.20349", "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Fabrice Jimenez", "Thomas Oberlin"], "title": "Synthetic Data for Robust Runway Detection", "comment": null, "summary": "Deep vision models are now mature enough to be integrated in industrial and\npossibly critical applications such as autonomous navigation. Yet, data\ncollection and labeling to train such models requires too much efforts and\ncosts for a single company or product. This drawback is more significant in\ncritical applications, where training data must include all possible conditions\nincluding rare scenarios. In this perspective, generating synthetic images is\nan appealing solution, since it allows a cheap yet reliable covering of all the\nconditions and environments, if the impact of the synthetic-to-real\ndistribution shift is mitigated. In this article, we consider the case of\nrunway detection that is a critical part in autonomous landing systems\ndeveloped by aircraft manufacturers. We propose an image generation approach\nbased on a commercial flight simulator that complements a few annotated real\nimages. By controlling the image generation and the integration of real and\nsynthetic data, we show that standard object detection models can achieve\naccurate prediction. We also evaluate their robustness with respect to adverse\nconditions, in our case nighttime images, that were not represented in the real\ndata, and show the interest of using a customized domain adaptation strategy.", "AI": {"tldr": "This paper proposes using synthetic images from flight simulators to complement limited real data for runway detection in autonomous landing systems, addressing the high cost of data collection in critical applications.", "motivation": "Deep vision models for critical applications like autonomous navigation require extensive training data covering all possible conditions, but data collection and labeling are too expensive and time-consuming, especially for rare scenarios.", "method": "The authors use a commercial flight simulator to generate synthetic images and combine them with few annotated real images. They implement a customized domain adaptation strategy to mitigate the synthetic-to-real distribution shift.", "result": "Standard object detection models achieve accurate predictions when trained with the combined real and synthetic data. The approach also shows robustness to adverse conditions like nighttime images that were not present in the real training data.", "conclusion": "Synthetic image generation from flight simulators is an effective solution for runway detection in autonomous landing systems, particularly when combined with domain adaptation to handle the distribution shift between synthetic and real data."}}
{"id": "2510.20369", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20369", "abs": "https://arxiv.org/abs/2510.20369", "authors": ["Zhenghao Xu", "Qin Lu", "Qingru Zhang", "Liang Qiu", "Ilgee Hong", "Changlong Yu", "Wenlin Yao", "Yao Liu", "Haoming Jiang", "Lihong Li", "Hyokun Yun", "Tuo Zhao"], "title": "Ask a Strong LLM Judge when Your Reward Model is Uncertain", "comment": "NeurIPS 2025, 18 pages", "summary": "Reward model (RM) plays a pivotal role in reinforcement learning with human\nfeedback (RLHF) for aligning large language models (LLMs). However, classical\nRMs trained on human preferences are vulnerable to reward hacking and\ngeneralize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM\njudges equipped with reasoning capabilities demonstrate superior\ngeneralization, even without additional training, but incur significantly\nhigher inference costs, limiting their applicability in online RLHF. In this\nwork, we propose an uncertainty-based routing framework that efficiently\ncomplements a fast RM with a strong but costly LLM judge. Our approach\nformulates advantage estimation in policy gradient (PG) methods as pairwise\npreference classification, enabling principled uncertainty quantification to\nguide routing. Uncertain pairs are forwarded to the LLM judge, while confident\nones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our\nuncertainty-based routing strategy significantly outperforms random judge\ncalling at the same cost, and downstream alignment results showcase its\neffectiveness in improving online RLHF.", "AI": {"tldr": "The paper proposes an uncertainty-based routing framework that combines a fast reward model with a strong but costly LLM judge to improve RLHF efficiency and generalization.", "motivation": "Classical reward models trained on human preferences are vulnerable to reward hacking and poor generalization to out-of-distribution inputs, while strong LLM judges have better generalization but high inference costs that limit their use in online RLHF.", "method": "An uncertainty-based routing framework that formulates advantage estimation as pairwise preference classification, using principled uncertainty quantification to route uncertain pairs to the LLM judge and confident ones to the reward model.", "result": "Experiments show the uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results demonstrate effectiveness in improving online RLHF.", "conclusion": "The proposed framework efficiently complements fast reward models with strong LLM judges, enabling better generalization and performance in reinforcement learning with human feedback while managing computational costs."}}
{"id": "2510.20383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20383", "abs": "https://arxiv.org/abs/2510.20383", "authors": ["Shuhei Aikawa", "Aru Suzuki", "Kei Yoshitake", "Kanata Teshigawara", "Akira Iwabuchi", "Ken Kobayashi", "Kazuhide Nakata"], "title": "Hierarchical Time Series Forecasting with Robust Reconciliation", "comment": null, "summary": "This paper focuses on forecasting hierarchical time-series data, where each\nhigher-level observation equals the sum of its corresponding lower-level time\nseries. In such contexts, the forecast values should be coherent, meaning that\nthe forecast value of each parent series exactly matches the sum of the\nforecast values of its child series. Existing hierarchical forecasting methods\ntypically generate base forecasts independently for each series and then apply\na reconciliation procedure to adjust them so that the resulting forecast values\nare coherent across the hierarchy. These methods generally derive an optimal\nreconciliation, using a covariance matrix of the forecast error. In practice,\nhowever, the true covariance matrix is unknown and has to be estimated from\nfinite samples in advance. This gap between the true and estimated covariance\nmatrix may degrade forecast performance. To address this issue, we propose a\nrobust optimization framework for hierarchical reconciliation that accounts for\nuncertainty in the estimated covariance matrix. We first introduce an\nuncertainty set for the estimated covariance matrix and formulate a\nreconciliation problem that minimizes the worst-case expected squared error\nover this uncertainty set. We show that our problem can be cast as a\nsemidefinite optimization problem. Numerical experiments demonstrate that the\nproposed robust reconciliation method achieved better forecast performance than\nexisting hierarchical forecasting methods, which indicates the effectiveness of\nintegrating uncertainty into the reconciliation process.", "AI": {"tldr": "A robust optimization framework for hierarchical time-series forecasting that addresses uncertainty in covariance matrix estimation to improve forecast coherence and performance.", "motivation": "Existing hierarchical forecasting methods rely on estimating covariance matrices from finite samples, creating a gap between true and estimated matrices that degrades forecast performance due to uncertainty.", "method": "Proposed a robust optimization framework that introduces an uncertainty set for the estimated covariance matrix and formulates a reconciliation problem minimizing worst-case expected squared error, cast as a semidefinite optimization problem.", "result": "Numerical experiments showed the proposed robust reconciliation method achieved better forecast performance than existing hierarchical forecasting methods.", "conclusion": "Integrating uncertainty into the reconciliation process through robust optimization effectively improves hierarchical forecasting performance by accounting for covariance matrix estimation uncertainty."}}
{"id": "2510.20387", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20387", "abs": "https://arxiv.org/abs/2510.20387", "authors": ["Baoqing Yue", "Jinyuan Zhou", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"], "title": "Relative-Based Scaling Law for Neural Language Models", "comment": null, "summary": "Scaling laws aim to accurately predict model performance across different\nscales. Existing scaling-law studies almost exclusively rely on cross-entropy\nas the evaluation metric. However, cross-entropy provides only a partial view\nof performance: it measures the absolute probability assigned to the correct\ntoken, but ignores the relative ordering between correct and incorrect tokens.\nYet, relative ordering is crucial for language models, such as in\ngreedy-sampling scenario. To address this limitation, we investigate scaling\nfrom the perspective of relative ordering. We first propose the Relative-Based\nProbability (RBP) metric, which quantifies the probability that the correct\ntoken is ranked among the top predictions. Building on this metric, we\nestablish the Relative-Based Scaling Law, which characterizes how RBP improves\nwith increasing model size. Through extensive experiments on four datasets and\nfour model families spanning five orders of magnitude, we demonstrate the\nrobustness and accuracy of this law. Finally, we illustrate the broad\napplication of this law with two examples, namely providing a deeper\nexplanation of emergence phenomena and facilitating finding fundamental\ntheories of scaling laws. In summary, the Relative-Based Scaling Law\ncomplements the cross-entropy perspective and contributes to a more complete\nunderstanding of scaling large language models. Thus, it offers valuable\ninsights for both practical development and theoretical exploration.", "AI": {"tldr": "The paper introduces Relative-Based Probability (RBP) as a new scaling law metric that focuses on token ranking rather than just cross-entropy, and demonstrates its robustness across datasets and model families.", "motivation": "Existing scaling laws rely solely on cross-entropy, which only measures absolute probability of correct tokens but ignores relative ordering between correct and incorrect tokens - a crucial aspect for language models in scenarios like greedy sampling.", "method": "Proposed Relative-Based Probability (RBP) metric that quantifies the probability that the correct token is ranked among the top predictions, and established the Relative-Based Scaling Law to characterize how RBP improves with increasing model size.", "result": "Extensive experiments on four datasets and four model families spanning five orders of magnitude demonstrated the robustness and accuracy of the Relative-Based Scaling Law.", "conclusion": "The Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models, offering valuable insights for both practical development and theoretical exploration."}}
{"id": "2510.20408", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20408", "abs": "https://arxiv.org/abs/2510.20408", "authors": ["Tom Maus", "Asma Atamna", "Tobias Glasmachers"], "title": "Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control", "comment": "Preprint (submitted version) to be presented at the 13th\n  International Conference on Industrial Engineering and Applications\n  (ICIEA-EU), Milan, 2026. The final Version of Record will appear in the\n  official conference proceedings", "summary": "Autonomous control of multi-stage industrial processes requires both local\nspecialization and global coordination. Reinforcement learning (RL) offers a\npromising approach, but its industrial adoption remains limited due to\nchallenges such as reward design, modularity, and action space management. Many\nacademic benchmarks differ markedly from industrial control problems, limiting\ntheir transferability to real-world applications. This study introduces an\nenhanced industry-inspired benchmark environment that combines tasks from two\nexisting benchmarks, SortingEnv and ContainerGym, into a sequential recycling\nscenario with sorting and pressing operations. We evaluate two control\nstrategies: a modular architecture with specialized agents and a monolithic\nagent governing the full system, while also analyzing the impact of action\nmasking. Our experiments show that without action masking, agents struggle to\nlearn effective policies, with the modular architecture performing better. When\naction masking is applied, both architectures improve substantially, and the\nperformance gap narrows considerably. These results highlight the decisive role\nof action space constraints and suggest that the advantages of specialization\ndiminish as action complexity is reduced. The proposed benchmark thus provides\na valuable testbed for exploring practical and robust multi-agent RL solutions\nin industrial automation, while contributing to the ongoing debate on\ncentralization versus specialization.", "AI": {"tldr": "This paper introduces an industry-inspired benchmark for multi-stage industrial processes, combining sorting and pressing operations in a recycling scenario. It compares modular vs monolithic RL architectures and shows that action masking dramatically improves performance and reduces the advantage of specialized agents.", "motivation": "RL adoption in industry is limited due to challenges like reward design, modularity, and action space management. Academic benchmarks often don't transfer well to real industrial control problems.", "method": "Created an enhanced benchmark combining SortingEnv and ContainerGym into sequential recycling scenario. Evaluated modular architecture with specialized agents vs monolithic agent, with and without action masking.", "result": "Without action masking, agents struggle to learn effective policies, with modular architecture performing better. With action masking, both architectures improve substantially and performance gap narrows significantly.", "conclusion": "Action space constraints play a decisive role in industrial RL. Advantages of specialization diminish as action complexity is reduced. The benchmark provides valuable testbed for practical multi-agent RL in industrial automation."}}
{"id": "2510.20413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20413", "abs": "https://arxiv.org/abs/2510.20413", "authors": ["Aditya Gopalan", "Sayak Ray Chowdhury", "Debangshu Banerjee"], "title": "Why DPO is a Misspecified Estimator and How to Fix It", "comment": null, "summary": "Direct alignment algorithms such as Direct Preference Optimization (DPO)\nfine-tune models based on preference data, using only supervised learning\ninstead of two-stage reinforcement learning with human feedback (RLHF). We show\nthat DPO encodes a statistical estimation problem over reward functions induced\nby a parametric policy class. When the true reward function that generates\npreferences cannot be realized via the policy class, DPO becomes misspecified,\nresulting in failure modes such as preference order reversal, worsening of\npolicy reward, and high sensitivity to the input preference data distribution.\nOn the other hand, we study the local behavior of two-stage RLHF for a\nparametric class and relate it to a natural gradient step in policy space. Our\nfine-grained geometric characterization allows us to propose AuxDPO, which\nintroduces additional auxiliary variables in the DPO loss function to help move\ntowards the RLHF solution in a principled manner and mitigate the\nmisspecification in DPO. We empirically demonstrate the superior performance of\nAuxDPO on didactic bandit settings as well as LLM alignment tasks.", "AI": {"tldr": "The paper analyzes Direct Preference Optimization (DPO) limitations and proposes AuxDPO to address misspecification issues by incorporating auxiliary variables.", "motivation": "DPO faces misspecification problems when the true reward function cannot be realized by the policy class, leading to issues like preference reversal and poor performance.", "method": "The authors characterize DPO's statistical estimation problem, study RLHF's local behavior, and propose AuxDPO which adds auxiliary variables to the DPO loss to better approximate RLHF solutions.", "result": "AuxDPO demonstrates superior performance over standard DPO in both bandit experiments and LLM alignment tasks, effectively mitigating misspecification issues.", "conclusion": "AuxDPO provides a principled approach to address DPO's misspecification problems by bridging the gap between DPO and RLHF through auxiliary variables."}}
{"id": "2510.20414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20414", "abs": "https://arxiv.org/abs/2510.20414", "authors": ["Sishun Liu", "Ke Deng", "Xiuzhen Zhang", "Yongli Ren", "Yan Wang"], "title": "Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes", "comment": "NeurIPS 2025 poster", "summary": "Marked Temporal Point Process (MTPP) has been well studied to model the event\ndistribution in marked event streams, which can be used to predict the mark and\narrival time of the next event. However, existing studies overlook that the\ndistribution of event marks is highly imbalanced in many real-world\napplications, with some marks being frequent but others rare. The imbalance\nposes a significant challenge to the performance of the next event prediction,\nespecially for events of rare marks. To address this issue, we propose a\nthresholding method, which learns thresholds to tune the mark probability\nnormalized by the mark's prior probability to optimize mark prediction, rather\nthan predicting the mark directly based on the mark probability as in existing\nstudies. In conjunction with this method, we predict the mark first and then\nthe time. In particular, we develop a novel neural MTPP model to support\neffective time sampling and estimation of mark probability without\ncomputationally expensive numerical improper integration. Extensive experiments\non real-world datasets demonstrate the superior performance of our solution\nagainst various baselines for the next event mark and time prediction. The code\nis available at https://github.com/undes1red/IFNMTPP.", "AI": {"tldr": "Proposes a thresholding method for MTPP to handle imbalanced event mark distributions, predicting mark first then time, with a neural model for efficient computation.", "motivation": "Existing MTPP studies overlook highly imbalanced event mark distributions in real-world applications, which significantly challenges next event prediction performance, especially for rare marks.", "method": "Thresholding method that learns thresholds to tune mark probability normalized by prior probability; predicts mark first then time; neural MTPP model for effective time sampling and mark probability estimation without expensive numerical integration.", "result": "Extensive experiments on real-world datasets demonstrate superior performance against various baselines for next event mark and time prediction.", "conclusion": "The proposed solution effectively addresses the imbalanced mark distribution challenge in MTPP and outperforms existing methods."}}
{"id": "2510.20428", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20428", "abs": "https://arxiv.org/abs/2510.20428", "authors": ["Xuran Li", "Jingyi Wang"], "title": "An Empirical Study of Sample Selection Strategies for Large Language Model Repair", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in real-world systems,\nyet they can produce toxic or biased outputs that undermine safety and trust.\nPost-hoc model repair provides a practical remedy, but the high cost of\nparameter updates motivates selective use of repair data. Despite extensive\nprior work on data selection for model training, it remains unclear which\nsampling criteria are most effective and efficient when applied specifically to\nbehavioral repair of large generative models. Our study presents a systematic\nanalysis of sample prioritization strategies for LLM repair. We evaluate five\nrepresentative selection methods, including random sampling, K-Center,\ngradient-norm-based selection(GraNd), stratified coverage (CCS), and a\nSemantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair\neffectiveness and trade-offs are assessed through toxicity reduction,\nperplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair\nProximity Score (RPS), the Overall Performance Score (OPS), and the Repair\nEfficiency Score (RES). Experimental results show that SAPS achieves the best\nbalance between detoxification, utility preservation, and efficiency,\ndelivering comparable or superior repair outcomes with substantially less data.\nRandom sampling remains effective for large or robust models, while\nhigh-overhead methods such as CCS and GraNd provide limited benefit. The\noptimal data proportion depends on model scale and repair method, indicating\nthat sample selection should be regarded as a tunable component of repair\npipelines. Overall, these findings establish selection-based repair as an\nefficient and scalable paradigm for maintaining LLM reliability.", "AI": {"tldr": "Systematic analysis of data selection methods for LLM behavioral repair, showing that Semantic-Aware Prioritized Sampling (SAPS) achieves optimal balance between detoxification, utility preservation, and efficiency with less data.", "motivation": "LLMs can produce toxic or biased outputs that undermine safety, but post-hoc repair is costly, motivating selective use of repair data to reduce parameter update costs.", "method": "Evaluated five selection methods: random sampling, K-Center, gradient-norm-based selection (GraNd), stratified coverage (CCS), and proposed SAPS approach. Assessed repair effectiveness through toxicity reduction, perplexity metrics, and composite scores (RPS, OPS, RES).", "result": "SAPS achieves best balance between detoxification, utility preservation, and efficiency. Random sampling effective for large/robust models, while CCS and GraNd provide limited benefit. Optimal data proportion depends on model scale and repair method.", "conclusion": "Sample selection should be regarded as tunable component of repair pipelines. Selection-based repair establishes efficient and scalable paradigm for maintaining LLM reliability."}}
{"id": "2510.20439", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20439", "abs": "https://arxiv.org/abs/2510.20439", "authors": ["Quannian Zhang", "Michael R\u00f6der", "Nikit Srivastava", "N'Dah Jean Kouagou", "Axel-Cyrille Ngonga Ngomo"], "title": "Explainable Benchmarking through the Lense of Concept Learning", "comment": "Accepted as full research paper at K-CAP 2025", "summary": "Evaluating competing systems in a comparable way, i.e., benchmarking them, is\nan undeniable pillar of the scientific method. However, system performance is\noften summarized via a small number of metrics. The analysis of the evaluation\ndetails and the derivation of insights for further development or use remains a\ntedious manual task with often biased results. Thus, this paper argues for a\nnew type of benchmarking, which is dubbed explainable benchmarking. The aim of\nexplainable benchmarking approaches is to automatically generate explanations\nfor the performance of systems in a benchmark. We provide a first instantiation\nof this paradigm for knowledge-graph-based question answering systems. We\ncompute explanations by using a novel concept learning approach developed for\nlarge knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL\noutperforms state-of-the-art concept learners on the task of explainable\nbenchmarking by up to 0.55 points F1 measure. A task-driven user study with 41\nparticipants shows that in 80\\% of the cases, the majority of participants can\naccurately predict the behavior of a system based on our explanations. Our code\nand data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025", "AI": {"tldr": "This paper introduces explainable benchmarking, a new paradigm that automatically generates explanations for system performance in benchmarks, specifically applied to knowledge-graph-based question answering systems using the PruneCEL concept learning approach.", "motivation": "Current benchmarking practices summarize system performance with limited metrics, requiring tedious manual analysis that often produces biased results. There's a need for automated approaches that can provide detailed explanations of system behavior.", "method": "The authors propose explainable benchmarking using PruneCEL, a novel concept learning approach developed for large knowledge graphs. PruneCEL computes explanations for system performance in benchmarks.", "result": "PruneCEL outperforms state-of-the-art concept learners by up to 0.55 F1 points. A user study with 41 participants shows that in 80% of cases, the majority can accurately predict system behavior based on the generated explanations.", "conclusion": "Explainable benchmarking is a viable approach that provides meaningful insights into system performance, with PruneCEL demonstrating superior performance over existing methods and effectively enabling users to understand system behavior."}}
{"id": "2510.20448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20448", "abs": "https://arxiv.org/abs/2510.20448", "authors": ["Xuan Lin", "Aocheng Ding", "Tengfei Ma", "Hua Liang", "Zhe Quan"], "title": "MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction", "comment": null, "summary": "Drug combinations offer therapeutic benefits but also carry the risk of\nadverse drug-drug interactions (DDIs), especially under complex molecular\nstructures. Accurate DDI event prediction requires capturing fine-grained\ninter-drug relationships, which are critical for modeling metabolic mechanisms\nsuch as enzyme-mediated competition. However, existing approaches typically\nrely on isolated drug representations and fail to explicitly model atom-level\ncross-molecular interactions, limiting their effectiveness across diverse\nmolecular complexities and DDI type distributions. To address these\nlimitations, we propose MolBridge, a novel atom-level joint graph refinement\nframework for robust DDI event prediction. MolBridge constructs a joint graph\nthat integrates atomic structures of drug pairs, enabling direct modeling of\ninter-drug associations. A central challenge in such joint graph settings is\nthe potential loss of information caused by over-smoothing when modeling\nlong-range atomic dependencies. To overcome this, we introduce a structure\nconsistency module that iteratively refines node features while preserving the\nglobal structural context. This joint design allows MolBridge to effectively\nlearn both local and global interaction outperforms state-of-the-art baselines,\nachieving superior performance across long-tail and inductive scenarios.\npatterns, yielding robust representations across both frequent and rare DDI\ntypes. Extensive experiments on two benchmark datasets show that MolBridge\nconsistently. These results demonstrate the advantages of fine-grained graph\nrefinement in improving the accuracy, robustness, and mechanistic\ninterpretability of DDI event prediction.This work contributes to Web Mining\nand Content Analysis by developing graph-based methods for mining and analyzing\ndrug-drug interaction networks.", "AI": {"tldr": "MolBridge is a novel atom-level joint graph refinement framework that models fine-grained inter-drug interactions for robust DDI event prediction, addressing limitations of existing approaches by preserving structural context and handling long-range atomic dependencies.", "motivation": "Existing DDI prediction methods rely on isolated drug representations and fail to explicitly model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI type distributions. There's a need to capture fine-grained inter-drug relationships critical for modeling metabolic mechanisms like enzyme-mediated competition.", "method": "MolBridge constructs a joint graph integrating atomic structures of drug pairs to enable direct modeling of inter-drug associations. It uses a structure consistency module that iteratively refines node features while preserving global structural context, overcoming over-smoothing issues in joint graph settings.", "result": "Extensive experiments on two benchmark datasets show that MolBridge consistently outperforms state-of-the-art baselines, achieving superior performance across long-tail and inductive scenarios. It effectively learns both local and global interaction patterns, yielding robust representations across frequent and rare DDI types.", "conclusion": "MolBridge demonstrates advantages of fine-grained graph refinement in improving accuracy, robustness, and mechanistic interpretability of DDI event prediction. The framework contributes to Web Mining and Content Analysis by developing graph-based methods for mining and analyzing drug-drug interaction networks."}}
{"id": "2510.20454", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20454", "abs": "https://arxiv.org/abs/2510.20454", "authors": ["Lawrence Clegg", "John Cartlidge"], "title": "Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach", "comment": "39 pages, 8 figures", "summary": "Intransitive player dominance, where player A beats B, B beats C, but C beats\nA, is common in competitive tennis. Yet, there are few known attempts to\nincorporate it within forecasting methods. We address this problem with a graph\nneural network approach that explicitly models these intransitive relationships\nthrough temporal directed graphs, with players as nodes and their historical\nmatch outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly\nhandles matches with high intransitive complexity and posit that our\ngraph-based approach is uniquely positioned to capture relational dynamics in\nthese scenarios. When selectively betting on higher intransitivity matchups\nwith our model (65.7% accuracy, 0.215 Brier Score), we achieve significant\npositive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a\nmarket inefficiency in handling intransitive matchups that our approach\nsuccessfully exploits.", "AI": {"tldr": "This paper proposes a graph neural network approach to model intransitive player relationships in tennis for match forecasting and betting strategies.", "motivation": "Intransitive player dominance (where A beats B, B beats C, but C beats A) is common in competitive tennis but rarely incorporated in forecasting methods, creating potential market inefficiencies.", "method": "Graph neural network approach using temporal directed graphs with players as nodes and historical match outcomes as directed edges to explicitly model intransitive relationships.", "result": "The model achieved 65.7% accuracy and 0.215 Brier Score on high intransitivity matchups, generating 3.26% ROI with Kelly staking over 1903 bets, showing bookmaker Pinnacle Sports poorly handles matches with high intransitive complexity.", "conclusion": "Graph-based approaches are uniquely positioned to capture relational dynamics in intransitive scenarios, successfully exploiting market inefficiencies in handling complex player matchups."}}
{"id": "2510.20468", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20468", "abs": "https://arxiv.org/abs/2510.20468", "authors": ["Tom\u00e1\u0161 Sou\u010dek", "Sylvestre-Alvise Rebuffi", "Pierre Fernandez", "Nikola Jovanovi\u0107", "Hady Elsahar", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"], "title": "Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models", "comment": "NeurIPS 2025", "summary": "Recent years have seen a surge in interest in digital content watermarking\ntechniques, driven by the proliferation of generative models and increased\nlegal pressure. With an ever-growing percentage of AI-generated content\navailable online, watermarking plays an increasingly important role in ensuring\ncontent authenticity and attribution at scale. There have been many works\nassessing the robustness of watermarking to removal attacks, yet, watermark\nforging, the scenario when a watermark is stolen from genuine content and\napplied to malicious content, remains underexplored. In this work, we\ninvestigate watermark forging in the context of widely used post-hoc image\nwatermarking. Our contributions are as follows. First, we introduce a\npreference model to assess whether an image is watermarked. The model is\ntrained using a ranking loss on purely procedurally generated images without\nany need for real watermarks. Second, we demonstrate the model's capability to\nremove and forge watermarks by optimizing the input image through\nbackpropagation. This technique requires only a single watermarked image and\nworks without knowledge of the watermarking model, making our attack much\nsimpler and more practical than attacks introduced in related work. Third, we\nevaluate our proposed method on a variety of post-hoc image watermarking\nmodels, demonstrating that our approach can effectively forge watermarks,\nquestioning the security of current watermarking approaches. Our code and\nfurther resources are publicly available.", "AI": {"tldr": "This paper investigates watermark forging attacks on post-hoc image watermarking systems, introducing a method that can detect, remove, and forge watermarks using only a single watermarked image without knowledge of the watermarking model.", "motivation": "With the rise of AI-generated content, watermarking is crucial for authenticity and attribution, but watermark forging (stealing watermarks from genuine content and applying them to malicious content) remains underexplored compared to removal attacks.", "method": "The authors introduce a preference model trained with ranking loss on procedurally generated images to detect watermarks, then use this model through backpropagation to optimize input images for watermark removal and forging.", "result": "The proposed method effectively forges watermarks across various post-hoc image watermarking models, requiring only a single watermarked image and no knowledge of the watermarking model, making it simpler and more practical than existing attacks.", "conclusion": "The research demonstrates significant security vulnerabilities in current watermarking approaches, as the proposed attack can successfully forge watermarks, questioning the reliability of existing watermarking systems for content authentication."}}
{"id": "2510.20477", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20477", "abs": "https://arxiv.org/abs/2510.20477", "authors": ["Rui Zhu", "Song-Lin Lv", "Zi-Kang Wang", "Lan-Zhe Guo"], "title": "Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models", "comment": null, "summary": "Exploiting unlabeled data through semi-supervised learning (SSL) or\nleveraging pre-trained models via fine-tuning are two prevailing paradigms for\naddressing label-scarce scenarios. Recently, growing attention has been given\nto combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,\nforming the emerging paradigm of semi-supervised fine-tuning. However, existing\nmethods often suffer from model bias and hyperparameter sensitivity, due to\nreliance on prediction consistency or pre-defined confidence thresholds. To\naddress these limitations, we propose a simple yet effective plug-and-play\nmethodology named\n$\\underline{\\textbf{Bi-Co}}$nsistency-$\\underline{\\textbf{G}}$uided\nSelf-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,\nby simultaneously exploiting inter-model and intra-model consistency, along\nwith an error-aware dynamic pseudo-label assignment strategy. Both theoretical\nanalysis and extensive experiments over 14 datasets demonstrate the\neffectiveness of Bi-CoG, which consistently and significantly improves the\nperformance of existing methods.", "AI": {"tldr": "Bi-CoG is a plug-and-play method that improves semi-supervised fine-tuning of vision-language models by using both inter-model and intra-model consistency with dynamic pseudo-label assignment, addressing model bias and hyperparameter sensitivity issues.", "motivation": "To overcome limitations in existing semi-supervised fine-tuning methods that suffer from model bias and hyperparameter sensitivity due to reliance on prediction consistency or pre-defined confidence thresholds.", "method": "Proposes Bi-CoG (Bi-Consistency-Guided Self-Training) which assigns high-quality pseudo-labels using both inter-model and intra-model consistency, along with an error-aware dynamic pseudo-label assignment strategy.", "result": "Extensive experiments over 14 datasets demonstrate Bi-CoG consistently and significantly improves performance of existing methods, with theoretical analysis supporting its effectiveness.", "conclusion": "Bi-CoG provides a simple yet effective solution for semi-supervised fine-tuning that addresses key limitations of existing approaches and delivers consistent performance improvements across diverse datasets."}}
{"id": "2510.20486", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.20486", "abs": "https://arxiv.org/abs/2510.20486", "authors": ["Fangjian Zhang", "Xiaoyong Zhuge", "Wenlan Wang", "Haixia Xiao", "Yuying Zhu", "Siyang Cheng"], "title": "Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval", "comment": "26 pages", "summary": "Artificial intelligence has advanced quantitative remote sensing, yet its\neffectiveness is constrained by imbalanced label distribution. This imbalance\nleads conventionally trained models to favor common samples, which in turn\ndegrades retrieval performance for rare ones. Rainfall retrieval exemplifies\nthis issue, with performance particularly compromised for heavy rain. This\nstudy proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.\nFollowing a divide-and-conquer strategy, imbalance in the rain distribution is\ndecomposed into two components: zero inflation, defined by the predominance of\nnon-rain samples; and long tail, defined by the disproportionate abundance of\nlight-rain samples relative to heavy-rain samples. A hurdle model is adopted to\nhandle the zero inflation, while IMDL is proposed to address the long tail by\ntransforming the learning object into an unbiased ideal inverse model.\nComprehensive evaluation via statistical metrics and case studies investigating\nrainy weather in eastern China confirms Hurdle-IMDL's superiority over\nconventional, cost-sensitive, generative, and multi-task learning methods. Its\nkey advancements include effective mitigation of systematic underestimation and\na marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a\ngeneralizable approach for addressing imbalance in distributions of\nenvironmental variables, enabling enhanced retrieval of rare yet high-impact\nevents.", "AI": {"tldr": "Proposes Hurdle-IMDL framework to address imbalanced label distribution in rainfall retrieval, decomposing imbalance into zero inflation and long tail problems, with IMDL specifically tackling the long tail by transforming learning into an unbiased ideal inverse model.", "motivation": "AI effectiveness in quantitative remote sensing is constrained by imbalanced label distribution, leading models to favor common samples and degrade performance for rare ones, particularly problematic in rainfall retrieval where heavy rain performance is compromised.", "method": "Uses divide-and-conquer strategy: hurdle model handles zero inflation (predominance of non-rain samples), while IMDL addresses long tail (disproportionate abundance of light vs heavy rain) by transforming learning object into unbiased ideal inverse model.", "result": "Comprehensive evaluation shows Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods, with effective mitigation of systematic underestimation and marked improvement in heavy-to-extreme rain retrieval.", "conclusion": "IMDL offers generalizable approach for addressing imbalance in environmental variable distributions, enabling enhanced retrieval of rare yet high-impact events."}}
{"id": "2510.20540", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20540", "abs": "https://arxiv.org/abs/2510.20540", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment", "comment": "5 pages, 3 figures, 1 table", "summary": "Conventional multimodal alignment methods assume mutual redundancy across all\nmodalities, an assumption that fails in real-world distributed scenarios. We\npropose SheafAlign, a sheaf-theoretic framework for decentralized multimodal\nalignment that replaces single-space alignment with multiple comparison spaces.\nThis approach models pairwise modality relations through sheaf structures and\nleverages decentralized contrastive learning-based objectives for training.\nSheafAlign overcomes the limitations of prior methods by not requiring mutual\nredundancy among all modalities, preserving both shared and unique information.\nExperiments on multimodal sensing datasets show superior zero-shot\ngeneralization, cross-modal alignment, and robustness to missing modalities,\nwith 50\\% lower communication cost than state-of-the-art baselines.", "AI": {"tldr": "SheafAlign is a decentralized multimodal alignment framework using sheaf theory that enables alignment without requiring mutual redundancy across all modalities, achieving better generalization and lower communication costs.", "motivation": "Conventional multimodal alignment methods assume mutual redundancy across all modalities, which fails in real-world distributed scenarios where modalities may not share complete information.", "method": "Proposes a sheaf-theoretic framework that replaces single-space alignment with multiple comparison spaces, models pairwise modality relations through sheaf structures, and uses decentralized contrastive learning-based objectives.", "result": "Experiments show superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50% lower communication cost than state-of-the-art baselines.", "conclusion": "SheafAlign effectively overcomes limitations of prior methods by preserving both shared and unique information across modalities without requiring mutual redundancy."}}
{"id": "2510.20542", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20542", "abs": "https://arxiv.org/abs/2510.20542", "authors": ["Jacopo Di Ventura", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "A Unified Framework for Zero-Shot Reinforcement Learning", "comment": null, "summary": "Zero-shot reinforcement learning (RL) has emerged as a setting for developing\ngeneral agents in an unsupervised manner, capable of solving downstream tasks\nwithout additional training or planning at test-time. Unlike conventional RL,\nwhich optimizes policies for a fixed reward, zero-shot RL requires agents to\nencode representations rich enough to support immediate adaptation to any\nobjective, drawing parallels to vision and language foundation models. Despite\ngrowing interest, the field lacks a common analytical lens.\n  We present the first unified framework for zero-shot RL. Our formulation\nintroduces a consistent notation and taxonomy that organizes existing\napproaches and allows direct comparison between them. Central to our framework\nis the classification of algorithms into two families: direct representations,\nwhich learn end-to-end mappings from rewards to policies, and compositional\nrepresentations, which decompose the representation leveraging the substructure\nof the value function. Within this framework, we highlight shared principles\nand key differences across methods, and we derive an extended bound for\nsuccessor-feature methods, offering a new perspective on their performance in\nthe zero-shot regime. By consolidating existing work under a common lens, our\nframework provides a principled foundation for future research in zero-shot RL\nand outlines a clear path toward developing more general agents.", "AI": {"tldr": "This paper presents the first unified framework for zero-shot reinforcement learning, introducing a consistent notation and taxonomy that organizes existing approaches into two families: direct representations and compositional representations.", "motivation": "Zero-shot RL has emerged as a setting for developing general agents capable of solving downstream tasks without additional training, but the field lacks a common analytical lens for comparing different approaches.", "method": "The authors develop a unified framework with consistent notation and taxonomy, classifying algorithms into two families: direct representations (end-to-end mappings from rewards to policies) and compositional representations (decomposing representations leveraging value function substructure).", "result": "The framework enables direct comparison between existing methods, highlights shared principles and key differences, and derives an extended bound for successor-feature methods, offering new perspectives on their zero-shot performance.", "conclusion": "By consolidating existing work under a common lens, this framework provides a principled foundation for future research in zero-shot RL and outlines a clear path toward developing more general agents."}}
{"id": "2510.20556", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20556", "abs": "https://arxiv.org/abs/2510.20556", "authors": ["Alexandre Benoit", "Catherine Aitken", "Yu He"], "title": "Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics", "comment": "21 pages, 5 figures, conference", "summary": "Graph rewiring has emerged as a key technique to alleviate over-squashing in\nGraph Neural Networks (GNNs) and Graph Transformers by modifying the graph\ntopology to improve information flow. While effective, rewiring inherently\nalters the graph's structure, raising the risk of distorting important\ntopology-dependent signals. Yet, despite the growing use of rewiring, little is\nknown about which structural properties must be preserved to ensure both\nperformance gains and structural fidelity. In this work, we provide the first\nsystematic analysis of how rewiring affects a range of graph structural\nmetrics, and how these changes relate to downstream task performance. We study\nseven diverse rewiring strategies and correlate changes in local and global\ngraph properties with node classification accuracy. Our results reveal a\nconsistent pattern: successful rewiring methods tend to preserve local\nstructure while allowing for flexibility in global connectivity. These findings\noffer new insights into the design of effective rewiring strategies, bridging\nthe gap between graph theory and practical GNN optimization.", "AI": {"tldr": "This paper analyzes how graph rewiring affects structural properties and performance in GNNs, finding that successful methods preserve local structure while allowing global connectivity changes.", "motivation": "Graph rewiring is used to improve GNN performance but alters graph topology, potentially distorting important structural signals. There's limited understanding of which structural properties must be preserved for both performance gains and structural fidelity.", "method": "Systematic analysis of seven diverse rewiring strategies, correlating changes in local and global graph properties with node classification accuracy.", "result": "Successful rewiring methods consistently preserve local structure while allowing flexibility in global connectivity. Changes in structural metrics correlate with downstream task performance.", "conclusion": "The findings provide insights for designing effective rewiring strategies that balance local structure preservation with global connectivity modifications, bridging graph theory and GNN optimization."}}
{"id": "2510.20590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20590", "abs": "https://arxiv.org/abs/2510.20590", "authors": ["Simon Schindler", "Christoph Binder", "Lukas L\u00fcrzer", "Stefan Huber"], "title": "Embedding the MLOps Lifecycle into OT Reference Models", "comment": null, "summary": "Machine Learning Operations (MLOps) practices are increas- ingly adopted in\nindustrial settings, yet their integration with Opera- tional Technology (OT)\nsystems presents significant challenges. This pa- per analyzes the fundamental\nobstacles in combining MLOps with OT en- vironments and proposes a systematic\napproach to embed MLOps prac- tices into established OT reference models. We\nevaluate the suitability of the Reference Architectural Model for Industry 4.0\n(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for\nMLOps integration and present a detailed mapping of MLOps lifecycle compo-\nnents to RAMI 4.0 exemplified by a real-world use case. Our findings\ndemonstrate that while standard MLOps practices cannot be directly transplanted\nto OT environments, structured adaptation using existing reference models can\nprovide a pathway for successful integration.", "AI": {"tldr": "This paper analyzes challenges in integrating MLOps with Operational Technology systems and proposes using established OT reference models like RAMI 4.0 and ISA-95 for systematic MLOps integration.", "motivation": "MLOps practices are increasingly adopted in industrial settings but face significant challenges when integrating with Operational Technology systems, requiring structured approaches for successful implementation.", "method": "The paper evaluates the suitability of RAMI 4.0 and ISA-95 reference models for MLOps integration, presents detailed mapping of MLOps lifecycle components to RAMI 4.0, and demonstrates this through a real-world use case.", "result": "Findings show that standard MLOps practices cannot be directly applied to OT environments but structured adaptation using existing reference models provides a viable pathway for successful integration.", "conclusion": "Systematic embedding of MLOps practices into established OT reference models enables successful integration of machine learning operations with operational technology systems in industrial settings."}}
{"id": "2510.20607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20607", "abs": "https://arxiv.org/abs/2510.20607", "authors": ["Alexandru Oarga", "Yilun Du"], "title": "Generalizable Reasoning through Compositional Energy Minimization", "comment": null, "summary": "Generalization is a key challenge in machine learning, specifically in\nreasoning tasks, where models are expected to solve problems more complex than\nthose encountered during training. Existing approaches typically train\nreasoning models in an end-to-end fashion, directly mapping input instances to\nsolutions. While this allows models to learn useful heuristics from data, it\noften results in limited generalization beyond the training distribution. In\nthis work, we propose a novel approach to reasoning generalization by learning\nenergy landscapes over the solution spaces of smaller, more tractable\nsubproblems. At test time, we construct a global energy landscape for a given\nproblem by combining the energy functions of multiple subproblems. This\ncompositional approach enables the incorporation of additional constraints\nduring inference, allowing the construction of energy landscapes for problems\nof increasing difficulty. To improve the sample quality from this newly\nconstructed energy landscape, we introduce Parallel Energy Minimization (PEM).\nWe evaluate our approach on a wide set of reasoning problems. Our method\noutperforms existing state-of-the-art methods, demonstrating its ability to\ngeneralize to larger and more complex problems. Project website can be found\nat: https://alexoarga.github.io/compositional_reasoning/", "AI": {"tldr": "A novel approach for reasoning generalization that learns energy landscapes over subproblem solution spaces and combines them compositionally, outperforming state-of-the-art methods on complex reasoning tasks.", "motivation": "Existing end-to-end reasoning models have limited generalization beyond training distributions, failing to solve problems more complex than those seen during training.", "method": "Learn energy landscapes over smaller subproblem solution spaces, then compositionally combine them to construct global energy landscapes for complex problems, using Parallel Energy Minimization for improved sampling.", "result": "Outperforms state-of-the-art methods on reasoning problems, demonstrating ability to generalize to larger and more complex problems than training data.", "conclusion": "The compositional energy landscape approach enables effective generalization in reasoning tasks by building complex solutions from simpler subproblem components."}}
{"id": "2510.20608", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20608", "abs": "https://arxiv.org/abs/2510.20608", "authors": ["Yuta Kawamoto", "Hideaki Iiduka"], "title": "Convergence Analysis of SGD under Expected Smoothness", "comment": "23 pages, 11 figures, AISTATS 2026", "summary": "Stochastic gradient descent (SGD) is the workhorse of large-scale learning,\nyet classical analyses rely on assumptions that can be either too strong\n(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)\ncondition has emerged as a flexible alternative that ties the second moment of\nstochastic gradients to the objective value and the full gradient. This paper\npresents a self-contained convergence analysis of SGD under ES. We (i) refine\nES with interpretations and sampling-dependent constants; (ii) derive bounds of\nthe expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates\nwith explicit residual errors for various step-size schedules. All proofs are\ngiven in full detail in the appendix. Our treatment unifies and extends recent\nthreads (Khaled and Richt\\'arik, 2020; Umeda and Iiduka, 2025).", "AI": {"tldr": "This paper provides a comprehensive convergence analysis of Stochastic Gradient Descent (SGD) under the Expected Smoothness (ES) condition, refining ES interpretations, deriving gradient norm bounds, and proving O(1/K) convergence rates with explicit residual errors for various step-size schedules.", "motivation": "Classical SGD analyses rely on assumptions that are either too strong (bounded variance) or too coarse (uniform noise), while the ES condition offers a more flexible alternative by relating stochastic gradient moments to the objective and full gradient.", "method": "The authors conduct a self-contained convergence analysis of SGD under ES, refining the ES condition with interpretations and sampling-dependent constants, deriving bounds on the expectation of squared full gradient norm, and proving convergence rates for different step-size schedules.", "result": "The paper proves O(1/K) convergence rates with explicit residual errors for various step-size schedules, unifying and extending recent work by Khaled and Richt\u00e1rik (2020) and Umeda and Iiduka (2025).", "conclusion": "The analysis provides a unified framework for understanding SGD convergence under the ES condition, offering refined interpretations and explicit convergence guarantees that extend previous results in the literature."}}
{"id": "2510.20609", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.LG, cs.IR, cs.SE, cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20609", "abs": "https://arxiv.org/abs/2510.20609", "authors": ["Timur Galimzyanov", "Olga Kolomyttseva", "Egor Bogomolov"], "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets", "comment": null, "summary": "We study retrieval design for code-focused generation tasks under realistic\ncompute budgets. Using two complementary tasks from Long Code Arena -- code\ncompletion and bug localization -- we systematically compare retrieval\nconfigurations across various context window sizes along three axes: (i)\nchunking strategy, (ii) similarity scoring, and (iii) splitting granularity.\n(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and\npractical, significantly outperforming dense alternatives while being an order\nof magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3\nfamily) consistently beat sparse retrievers, however requiring 100x larger\nlatency. (3) Optimal chunk size scales with available context: 32-64 line\nchunks work best at small budgets, and whole-file retrieval becomes competitive\nat 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting\nacross budgets. (5) Retrieval latency varies by up to 200x across\nconfigurations; BPE-based splitting is needlessly slow, and BM25 + word\nsplitting offers the best quality-latency trade-off. Thus, we provide\nevidence-based recommendations for implementing effective code-oriented RAG\nsystems based on task requirements, model constraints, and computational\nefficiency.", "AI": {"tldr": "This paper systematically evaluates retrieval configurations for code-focused generation tasks across chunking strategies, similarity scoring methods, and splitting granularity, providing evidence-based recommendations for code-oriented RAG systems.", "motivation": "To study retrieval design for code-focused generation tasks under realistic compute budgets and provide practical guidance for implementing effective code-oriented RAG systems.", "method": "Systematic comparison of retrieval configurations across various context window sizes using two tasks from Long Code Arena (code completion and bug localization), evaluating chunking strategy, similarity scoring, and splitting granularity.", "result": "BM25 with word-level splitting is most effective for PL-PL tasks; proprietary dense encoders work best for NL-PL but with 100x latency; optimal chunk size scales with context (32-64 lines for small budgets); line-based chunking matches syntax-aware splitting; retrieval latency varies up to 200x across configurations.", "conclusion": "Evidence-based recommendations for code-oriented RAG systems based on task requirements, model constraints, and computational efficiency, with BM25 + word splitting offering the best quality-latency trade-off."}}
{"id": "2510.20611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20611", "abs": "https://arxiv.org/abs/2510.20611", "authors": ["Mirza Raquib", "Niloy Das", "Farida Siddiqi Prity", "Arafath Al Fahim", "Saydul Akbar Murad", "Mohammad Amzad Hossain", "MD Jiabul Hoque", "Mohammad Ali Moni"], "title": "PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection", "comment": null, "summary": "Breast cancer is considered the most critical and frequently diagnosed cancer\nin women worldwide, leading to an increase in cancer-related mortality. Early\nand accurate detection is crucial as it can help mitigate possible threats\nwhile improving survival rates. In terms of prediction, conventional diagnostic\nmethods are often limited by variability, cost, and, most importantly, risk of\nmisdiagnosis. To address these challenges, machine learning (ML) has emerged as\na powerful tool for computer-aided diagnosis, with feature selection playing a\nvital role in improving model performance and interpretability. This research\nstudy proposes an integrated framework that incorporates customized Particle\nSwarm Optimization (PSO) for feature selection. This framework has been\nevaluated on a comprehensive set of 29 different models, spanning classical\nclassifiers, ensemble techniques, neural networks, probabilistic algorithms,\nand instance-based algorithms. To ensure interpretability and clinical\nrelevance, the study uses cross-validation in conjunction with explainable AI\nmethods. Experimental evaluation showed that the proposed approach achieved a\nsuperior score of 99.1\\% across all performance metrics, including accuracy and\nprecision, while effectively reducing dimensionality and providing transparent,\nmodel-agnostic explanations. The results highlight the potential of combining\nswarm intelligence with explainable ML for robust, trustworthy, and clinically\nmeaningful breast cancer diagnosis.", "AI": {"tldr": "This paper proposes a machine learning framework using customized Particle Swarm Optimization for feature selection in breast cancer diagnosis, achieving 99.1% accuracy across multiple performance metrics while ensuring interpretability through explainable AI methods.", "motivation": "Breast cancer is the most critical and frequently diagnosed cancer in women worldwide, with conventional diagnostic methods limited by variability, cost, and misdiagnosis risk. Machine learning offers potential for improved computer-aided diagnosis.", "method": "Integrated framework incorporating customized Particle Swarm Optimization for feature selection, evaluated on 29 different models including classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. Uses cross-validation and explainable AI methods for interpretability.", "result": "The proposed approach achieved a superior score of 99.1% across all performance metrics (accuracy, precision), effectively reduced dimensionality, and provided transparent, model-agnostic explanations.", "conclusion": "The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis."}}
{"id": "2510.20615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20615", "abs": "https://arxiv.org/abs/2510.20615", "authors": ["Yang Han", "Pengyu Wang", "Kai Yu", "Xin Chen", "Lu Chen"], "title": "MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation", "comment": "NeurIPS 2025, We provide the data and code at\n  https://github.com/OpenDFM/MS-BART", "summary": "Mass spectrometry (MS) plays a critical role in molecular identification,\nsignificantly advancing scientific discovery. However, structure elucidation\nfrom MS data remains challenging due to the scarcity of annotated spectra.\nWhile large-scale pretraining has proven effective in addressing data scarcity\nin other domains, applying this paradigm to mass spectrometry is hindered by\nthe complexity and heterogeneity of raw spectral signals. To address this, we\npropose MS-BART, a unified modeling framework that maps mass spectra and\nmolecular structures into a shared token vocabulary, enabling cross-modal\nlearning through large-scale pretraining on reliably computed\nfingerprint-molecule datasets. Multi-task pretraining objectives further\nenhance MS-BART's generalization by jointly optimizing denoising and\ntranslation task. The pretrained model is subsequently transferred to\nexperimental spectra through finetuning on fingerprint predictions generated\nwith MIST, a pre-trained spectral inference model, thereby enhancing robustness\nto real-world spectral variability. While finetuning alleviates the\ndistributional difference, MS-BART still suffers molecular hallucination and\nrequires further alignment. We therefore introduce a chemical feedback\nmechanism that guides the model toward generating molecules closer to the\nreference structure. Extensive evaluations demonstrate that MS-BART achieves\nSOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is\nfaster by one order of magnitude than competing diffusion-based methods, while\ncomprehensive ablation studies systematically validate the model's\neffectiveness and robustness.", "AI": {"tldr": "MS-BART is a unified modeling framework that maps mass spectra and molecular structures into a shared token vocabulary, enabling cross-modal learning through large-scale pretraining. It addresses data scarcity in mass spectrometry by using computed fingerprint-molecule datasets and incorporates multi-task pretraining, finetuning with MIST, and chemical feedback mechanisms to enhance performance and robustness.", "motivation": "Structure elucidation from mass spectrometry data is challenging due to scarce annotated spectra. While large-scale pretraining works well in other domains, applying it to mass spectrometry is hindered by the complexity and heterogeneity of raw spectral signals.", "method": "Propose MS-BART framework that maps mass spectra and molecular structures into shared token vocabulary. Use multi-task pretraining objectives (denoising and translation) on computed fingerprint-molecule datasets. Finetune on experimental spectra using MIST-generated fingerprint predictions. Implement chemical feedback mechanism to reduce molecular hallucination.", "result": "MS-BART achieves state-of-the-art performance across 5/12 key metrics on MassSpecGym and NPLIB1 benchmarks. It is faster by one order of magnitude than competing diffusion-based methods. Comprehensive ablation studies validate the model's effectiveness and robustness.", "conclusion": "MS-BART provides an effective solution for molecular structure elucidation from mass spectrometry data by leveraging cross-modal learning, large-scale pretraining, and chemical feedback mechanisms, addressing key challenges in the field while achieving superior performance and efficiency."}}
{"id": "2510.20616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20616", "abs": "https://arxiv.org/abs/2510.20616", "authors": ["Aki Rehn", "Linzh Zhao", "Mikko A. Heikkil\u00e4", "Antti Honkela"], "title": "On Optimal Hyperparameters for Differentially Private Deep Transfer Learning", "comment": "25 pages, 30 figures", "summary": "Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained\nmodel on private data, is the current state-of-the-art approach for training\nlarge models under privacy constraints. We focus on two key hyperparameters in\nthis setting: the clipping bound $C$ and batch size $B$. We show a clear\nmismatch between the current theoretical understanding of how to choose an\noptimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes\n(larger $C$ performs better under strong privacy), caused by changes in the\ngradient distributions. Assuming a limited compute budget (fixed epochs), we\ndemonstrate that the existing heuristics for tuning $B$ do not work, while\ncumulative DP noise better explains whether smaller or larger batches perform\nbetter. We also highlight how the common practice of using a single $(C,B)$\nsetting across tasks can lead to suboptimal performance. We find that\nperformance drops especially when moving between loose and tight privacy and\nbetween plentiful and limited compute, which we explain by analyzing clipping\nas a form of gradient re-weighting and examining cumulative DP noise.", "AI": {"tldr": "This paper analyzes hyperparameter selection in differentially private transfer learning, revealing mismatches between theory and practice for clipping bounds and batch sizes, and shows how single parameter settings across tasks lead to suboptimal performance.", "motivation": "To address the gap between theoretical understanding and empirical outcomes in differentially private transfer learning, particularly regarding the selection of clipping bounds and batch sizes under privacy constraints.", "method": "The authors analyze gradient distribution changes, examine cumulative DP noise effects, and study clipping as a form of gradient re-weighting under limited compute budgets with fixed epochs.", "result": "Found that larger clipping bounds perform better under strong privacy (contradicting theory), existing batch size heuristics don't work, and cumulative DP noise better explains batch size performance. Performance drops significantly when using single parameter settings across different privacy and compute conditions.", "conclusion": "Optimal hyperparameter selection in DP transfer learning requires task-specific tuning, as current theoretical guidelines don't match empirical outcomes, especially when moving between different privacy regimes and compute constraints."}}
{"id": "2510.20627", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20627", "abs": "https://arxiv.org/abs/2510.20627", "authors": ["Lukas Miklautz", "Chengzhi Shi", "Andrii Shkabrii", "Theodoros Thirimachos Davarakis", "Prudence Lam", "Claudia Plant", "Jennifer Dy", "Stratis Ioannidis"], "title": "H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition", "comment": "Accepted at NeurIPS 2025", "summary": "We introduce H-SPLID, a novel algorithm for learning salient feature\nrepresentations through the explicit decomposition of salient and non-salient\nfeatures into separate spaces. We show that H-SPLID promotes learning\nlow-dimensional, task-relevant features. We prove that the expected prediction\ndeviation under input perturbations is upper-bounded by the dimension of the\nsalient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between\ninputs and representations. This establishes a link between robustness and\nlatent representation compression in terms of the dimensionality and\ninformation preserved. Empirical evaluations on image classification tasks show\nthat models trained with H-SPLID primarily rely on salient input components, as\nindicated by reduced sensitivity to perturbations affecting non-salient\nfeatures, such as image backgrounds. Our code is available at\nhttps://github.com/neu-spiral/H-SPLID.", "AI": {"tldr": "H-SPLID is a novel algorithm that learns salient feature representations by explicitly decomposing salient and non-salient features into separate spaces, promoting low-dimensional task-relevant features and improving robustness.", "motivation": "The motivation is to develop a method that explicitly separates salient and non-salient features to improve model robustness and learn more compressed, task-relevant representations.", "method": "H-SPLID explicitly decomposes salient and non-salient features into separate spaces and uses the Hilbert-Schmidt Independence Criterion (HSIC) to bound prediction deviation under input perturbations.", "result": "Empirical evaluations show models trained with H-SPLID primarily rely on salient input components with reduced sensitivity to perturbations affecting non-salient features like image backgrounds.", "conclusion": "H-SPLID establishes a link between robustness and latent representation compression, demonstrating that explicit feature decomposition improves model reliance on task-relevant features while reducing sensitivity to irrelevant perturbations."}}
{"id": "2510.20637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20637", "abs": "https://arxiv.org/abs/2510.20637", "authors": ["Hyun Jong Yang", "Hyunsoo Kim", "Hyeonho Noh", "Seungnyun Kim", "Byonghyo Shim"], "title": "Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges", "comment": null, "summary": "Large language models (LLMs) and large multimodal models (LMMs) have achieved\nunprecedented breakthrough, showcasing remarkable capabilities in natural\nlanguage understanding, generation, and complex reasoning. This transformative\npotential has positioned them as key enablers for 6G autonomous communications\namong machines, vehicles, and humanoids. In this article, we provide an\noverview of task-oriented autonomous communications with LLMs/LMMs, focusing on\nmultimodal sensing integration, adaptive reconfiguration, and\nprompt/fine-tuning strategies for wireless tasks. We demonstrate the framework\nthrough three case studies: LMM-based traffic control, LLM-based robot\nscheduling, and LMM-based environment-aware channel estimation. From\nexperimental results, we show that the proposed LLM/LMM-aided autonomous\nsystems significantly outperform conventional and discriminative deep learning\n(DL) model-based techniques, maintaining robustness under dynamic objectives,\nvarying input parameters, and heterogeneous multimodal conditions where\nconventional static optimization degrades.", "AI": {"tldr": "LLMs and LMMs enable autonomous communications in 6G networks, outperforming traditional methods through multimodal sensing, adaptive reconfiguration, and prompt/fine-tuning strategies.", "motivation": "Leverage the breakthrough capabilities of LLMs and LMMs in natural language understanding and complex reasoning to enable autonomous communications among machines, vehicles, and humanoids in 6G networks.", "method": "Propose a framework for task-oriented autonomous communications using LLMs/LMMs with multimodal sensing integration, adaptive reconfiguration, and prompt/fine-tuning strategies for wireless tasks. Demonstrate through three case studies: LMM-based traffic control, LLM-based robot scheduling, and LMM-based environment-aware channel estimation.", "result": "The proposed LLM/LMM-aided autonomous systems significantly outperform conventional and discriminative deep learning model-based techniques, maintaining robustness under dynamic objectives, varying input parameters, and heterogeneous multimodal conditions.", "conclusion": "LLMs and LMMs provide superior performance and robustness for autonomous communications in dynamic 6G environments compared to traditional static optimization approaches."}}
{"id": "2510.20640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20640", "abs": "https://arxiv.org/abs/2510.20640", "authors": ["Fiza Hussain", "Anson Bastos", "Anjaly Parayil", "Ayush Choure", "Chetan Bansal", "Rujia Wang", "Saravan Rajmohan"], "title": "Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems", "comment": null, "summary": "In this paper, we present DiRecGNN, an attention-enhanced entity\nrecommendation framework for monitoring cloud services at Microsoft. We provide\ninsights on the usefulness of this feature as perceived by the cloud service\nowners and lessons learned from deployment. Specifically, we introduce the\nproblem of recommending the optimal subset of attributes (dimensions) that\nshould be tracked by an automated watchdog (monitor) for cloud services. To\nbegin, we construct the monitor heterogeneous graph at production-scale. The\ninteraction dynamics of these entities are often characterized by limited\nstructural and engagement information, resulting in inferior performance of\nstate-of-the-art approaches. Moreover, traditional methods fail to capture the\ndependencies between entities spanning a long range due to their homophilic\nnature. Therefore, we propose an attention-enhanced entity ranking model\ninspired by transformer architectures. Our model utilizes a multi-head\nattention mechanism to focus on heterogeneous neighbors and their attributes,\nand further attends to paths sampled using random walks to capture long-range\ndependencies. We also employ multi-faceted loss functions to optimize for\nrelevant recommendations while respecting the inherent sparsity of the data.\nEmpirical evaluations demonstrate significant improvements over existing\nmethods, with our model achieving a 43.1% increase in MRR. Furthermore, product\nteams who consumed these features perceive the feature as useful and rated it\n4.5 out of 5.", "AI": {"tldr": "DiRecGNN is an attention-enhanced entity recommendation framework for cloud service monitoring at Microsoft that recommends optimal attribute subsets for automated watchdogs, achieving 43.1% MRR improvement and high user satisfaction.", "motivation": "Existing methods struggle with limited structural information and fail to capture long-range dependencies between entities in cloud service monitoring, leading to inferior performance.", "method": "Constructs monitor heterogeneous graph, uses multi-head attention mechanism for heterogeneous neighbors and attributes, attends to random walk paths for long-range dependencies, and employs multi-faceted loss functions for sparse data optimization.", "result": "Achieved 43.1% increase in MRR over existing methods, with product teams rating the feature 4.5/5 for usefulness.", "conclusion": "The attention-enhanced entity ranking model effectively addresses limitations of traditional approaches and provides valuable recommendations for cloud service monitoring at production scale."}}
{"id": "2510.20651", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20651", "abs": "https://arxiv.org/abs/2510.20651", "authors": ["Quan Li", "Wenchao Yu", "Suhang Wang", "Minhua Lin", "Lingwei Chen", "Wei Cheng", "Haifeng Chen"], "title": "xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion", "comment": null, "summary": "Extreme events frequently occur in real-world time series and often carry\nsignificant practical implications. In domains such as climate and healthcare,\nthese events, such as floods, heatwaves, or acute medical episodes, can lead to\nserious consequences. Accurate forecasting of such events is therefore of\nsubstantial importance. Most existing time series forecasting models are\noptimized for overall performance within the prediction window, but often\nstruggle to accurately predict extreme events, such as high temperatures or\nheart rate spikes. The main challenges are data imbalance and the neglect of\nvaluable information contained in intermediate events that precede extreme\nevents. In this paper, we propose xTime, a novel framework for extreme event\nforecasting in time series. xTime leverages knowledge distillation to transfer\ninformation from models trained on lower-rarity events, thereby improving\nprediction performance on rarer ones. In addition, we introduce a mixture of\nexperts (MoE) mechanism that dynamically selects and fuses outputs from expert\nmodels across different rarity levels, which further improves the forecasting\nperformance for extreme events. Experiments on multiple datasets show that\nxTime achieves consistent improvements, with forecasting accuracy on extreme\nevents improving from 3% to 78%.", "AI": {"tldr": "xTime is a novel framework for extreme event forecasting in time series that uses knowledge distillation and mixture of experts to improve prediction of rare extreme events like floods, heatwaves, and medical episodes.", "motivation": "Extreme events in time series (e.g., floods, heatwaves, medical episodes) have serious consequences but are hard to forecast due to data imbalance and neglect of preceding intermediate events. Existing models focus on overall performance but struggle with extreme events.", "method": "Proposes xTime framework using knowledge distillation to transfer information from models trained on lower-rarity events, and a mixture of experts mechanism that dynamically selects and fuses outputs from expert models across different rarity levels.", "result": "Experiments on multiple datasets show xTime achieves consistent improvements, with forecasting accuracy on extreme events improving from 3% to 78%.", "conclusion": "xTime effectively addresses the challenges of extreme event forecasting through knowledge distillation and dynamic expert fusion, significantly improving prediction performance for rare but important extreme events."}}
{"id": "2510.20666", "categories": ["cs.LG", "eess.SP", "68T05, 68T07, 62F15, 94A12"], "pdf": "https://arxiv.org/pdf/2510.20666", "abs": "https://arxiv.org/abs/2510.20666", "authors": ["Mariona Jaramillo-Civill", "Luis Gonz\u00e1lez-Gudi\u00f1o", "Tales Imbiriba", "Pau Closas"], "title": "Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts", "comment": "5 pages, 4 figures, Submitted to ICASSPW 2026", "summary": "Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,\nparticularly in urban areas where multipath and shadowing distort received\npower. Previous data-driven approaches achieved reasonable localization but\npoorly reconstructed the received signal strength (RSS) field due to limited\nspatial context. We propose a hybrid Bayesian mixture-of-experts framework that\nfuses a physical path-loss (PL) model and a convolutional neural network (CNN)\nthrough log-linear pooling. The PL expert ensures physical consistency, while\nthe CNN leverages building-height maps to capture urban propagation effects.\nBayesian inference with Laplace approximation provides posterior uncertainty\nover both the jammer position and RSS field. Experiments on urban ray-tracing\ndata show that localization accuracy improves and uncertainty decreases with\nmore training points, while uncertainty concentrates near the jammer and along\nurban canyons where propagation is most sensitive.", "AI": {"tldr": "A hybrid Bayesian mixture-of-experts framework that combines physical path-loss models with CNN using building-height maps to improve GNSS jammer localization and RSS field reconstruction in urban environments.", "motivation": "GNSS signals are vulnerable to jamming in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed RSS field due to limited spatial context.", "method": "Proposed hybrid Bayesian mixture-of-experts framework that fuses physical path-loss model and CNN through log-linear pooling. Uses building-height maps to capture urban propagation effects and performs Bayesian inference with Laplace approximation.", "result": "Experiments on urban ray-tracing data show improved localization accuracy and reduced uncertainty with more training points. Uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.", "conclusion": "The proposed framework effectively combines physical consistency with data-driven learning to improve jammer localization and RSS field reconstruction in challenging urban environments."}}
{"id": "2510.20668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20668", "abs": "https://arxiv.org/abs/2510.20668", "authors": ["Jinbin Bai", "Yu Lei", "Hecong Wu", "Yuchen Zhu", "Shufan Li", "Yi Xin", "Xiangtai Li", "Molei Tao", "Aditya Grover", "Ming-Hsuan Yang"], "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models", "comment": "Github: https://github.com/M-E-AGI-Lab/Awesome-World-Models", "summary": "This is not a typical survey of world models; it is a guide for those who\nwant to build worlds. We do not aim to catalog every paper that has ever\nmentioned a ``world model\". Instead, we follow one clear road: from early\nmasked models that unified representation learning across modalities, to\nunified architectures that share a single paradigm, then to interactive\ngenerative models that close the action-perception loop, and finally to\nmemory-augmented systems that sustain consistent worlds over time. We bypass\nloosely related branches to focus on the core: the generative heart, the\ninteractive loop, and the memory system. We show that this is the most\npromising path towards true world models.", "AI": {"tldr": "This paper provides a practical guide for building world models, focusing on a clear progression from masked models to unified architectures, interactive generative models, and memory-augmented systems.", "motivation": "To offer a focused roadmap for developing world models rather than conducting a comprehensive survey, emphasizing the most promising path towards creating true world models.", "method": "Follows a structured progression: starting with early masked models for unified representation learning, moving to unified architectures with shared paradigms, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems for temporal consistency.", "result": "Identifies a core path focusing on generative capabilities, interactive loops, and memory systems as the most promising approach for developing effective world models.", "conclusion": "The paper concludes that concentrating on the generative heart, interactive loop, and memory system represents the most viable path toward building true world models, bypassing loosely related approaches to maintain focus on essential components."}}
{"id": "2510.20671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20671", "abs": "https://arxiv.org/abs/2510.20671", "authors": ["Subham Kumar", "Prakrithi Shivaprakash", "Koustav Rudra", "Lekhansh Shukla", "Animesh Mukherjee"], "title": "GRACE: GRaph-based Addiction Care prEdiction", "comment": null, "summary": "Determining the appropriate locus of care for addiction patients is one of\nthe most critical clinical decisions that affects patient treatment outcomes\nand effective use of resources. With a lack of sufficient specialized treatment\nresources, such as inpatient beds or staff, there is an unmet need to develop\nan automated framework for the same. Current decision-making approaches suffer\nfrom severe class imbalances in addiction datasets. To address this limitation,\nwe propose a novel graph neural network (GRACE) framework that formalizes locus\nof care prediction as a structured learning problem. Further, we perform\nextensive feature engineering and propose a new approach of obtaining an\nunbiased meta-graph to train a GNN to overcome the class imbalance problem.\nExperimental results in real-world data show an improvement of 11-35% in terms\nof the F1 score of the minority class over competitive baselines. The codes and\nnote embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.", "AI": {"tldr": "The paper proposes GRACE, a graph neural network framework for predicting the appropriate locus of care for addiction patients, addressing class imbalance through structured learning and unbiased meta-graph generation.", "motivation": "There is a critical need for automated frameworks to determine the appropriate locus of care for addiction patients, especially given limited specialized treatment resources and severe class imbalances in addiction datasets that affect decision-making accuracy.", "method": "The authors propose a graph neural network (GRACE) framework that formalizes locus of care prediction as a structured learning problem, performs extensive feature engineering, and introduces a novel approach to obtain an unbiased meta-graph to overcome class imbalance issues.", "result": "Experimental results on real-world data demonstrate significant improvements of 11-35% in F1 score for the minority class compared to competitive baseline methods.", "conclusion": "The GRACE framework effectively addresses class imbalance in addiction care locus prediction through structured learning and unbiased meta-graph generation, achieving substantial performance improvements over existing approaches."}}
{"id": "2510.20683", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20683", "abs": "https://arxiv.org/abs/2510.20683", "authors": ["Georgios Mentzelopoulos", "Ioannis Asmanis", "Konrad P. Kording", "Eva L. Dyer", "Kostas Daniilidis", "Flavia Vitale"], "title": "A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks", "comment": null, "summary": "Brain-computer interfaces (BCIs) promise to enable vital functions, such as\nspeech and prosthetic control, for individuals with neuromotor impairments.\nCentral to their success are neural decoders, models that map neural activity\nto intended behavior. Current learning-based decoding approaches fall into two\nclasses: simple, causal models that lack generalization, or complex, non-causal\nmodels that generalize and scale offline but struggle in real-time settings.\nBoth face a common challenge, their reliance on power-hungry artificial neural\nnetwork backbones, which makes integration into real-world, resource-limited\nsystems difficult. Spiking neural networks (SNNs) offer a promising\nalternative. Because they operate causally these models are suitable for\nreal-time use, and their low energy demands make them ideal for\nbattery-constrained environments. To this end, we introduce Spikachu: a\nscalable, causal, and energy-efficient neural decoding framework based on SNNs.\nOur approach processes binned spikes directly by projecting them into a shared\nlatent space, where spiking modules, adapted to the timing of the input,\nextract relevant features; these latent representations are then integrated and\ndecoded to generate behavioral predictions. We evaluate our approach on 113\nrecording sessions from 6 non-human primates, totaling 43 hours of recordings.\nOur method outperforms causal baselines when trained on single sessions using\nbetween 2.26 and 418.81 times less energy. Furthermore, we demonstrate that\nscaling up training to multiple sessions and subjects improves performance and\nenables few-shot transfer to unseen sessions, subjects, and tasks. Overall,\nSpikachu introduces a scalable, online-compatible neural decoding framework\nbased on SNNs, whose performance is competitive relative to state-of-the-art\nmodels while consuming orders of magnitude less energy.", "AI": {"tldr": "Spikachu is a scalable, causal, and energy-efficient neural decoding framework using spiking neural networks (SNNs) that outperforms causal baselines while consuming 2.26-418.81x less energy, enabling few-shot transfer across sessions, subjects, and tasks.", "motivation": "Current neural decoders for brain-computer interfaces either lack generalization (simple causal models) or struggle in real-time settings (complex non-causal models), and both rely on power-hungry neural networks that are difficult to integrate into resource-limited systems.", "method": "The approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules adapted to input timing extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions.", "result": "Evaluation on 113 recording sessions from 6 non-human primates (43 hours total) shows Spikachu outperforms causal baselines with significantly lower energy consumption (2.26-418.81x less), and scaling training to multiple sessions/subjects improves performance and enables few-shot transfer.", "conclusion": "Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs that achieves competitive performance relative to state-of-the-art models while consuming orders of magnitude less energy."}}
{"id": "2510.20709", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.20709", "abs": "https://arxiv.org/abs/2510.20709", "authors": ["Haozhe Shan", "Sun Minni", "Lea Duncker"], "title": "Separating the what and how of compositional computation to enable reuse and continual learning", "comment": null, "summary": "The ability to continually learn, retain and deploy skills to accomplish\ngoals is a key feature of intelligent and efficient behavior. However, the\nneural mechanisms facilitating the continual learning and flexible\n(re-)composition of skills remain elusive. Here, we study continual learning\nand the compositional reuse of learned computations in recurrent neural network\n(RNN) models using a novel two-system approach: one system that infers what\ncomputation to perform, and one that implements how to perform it. We focus on\na set of compositional cognitive tasks commonly studied in neuroscience. To\nconstruct the what system, we first show that a large family of tasks can be\nsystematically described by a probabilistic generative model, where\ncompositionality stems from a shared underlying vocabulary of discrete task\nepochs. The shared epoch structure makes these tasks inherently compositional.\nWe first show that this compositionality can be systematically described by a\nprobabilistic generative model. Furthermore, We develop an unsupervised online\nlearning approach that can learn this model on a single-trial basis, building\nits vocabulary incrementally as it is exposed to new tasks, and inferring the\nlatent epoch structure as a time-varying computational context within a trial.\nWe implement the how system as an RNN whose low-rank components are composed\naccording to the context inferred by the what system. Contextual inference\nfacilitates the creation, learning, and reuse of low-rank RNN components as new\ntasks are introduced sequentially, enabling continual learning without\ncatastrophic forgetting. Using an example task set, we demonstrate the efficacy\nand competitive performance of this two-system learning framework, its\npotential for forward and backward transfer, as well as fast compositional\ngeneralization to unseen tasks.", "AI": {"tldr": "The paper proposes a two-system neural network approach for continual learning: a 'what' system that infers computational context and a 'how' system that implements computations, enabling compositional skill reuse without catastrophic forgetting.", "motivation": "To understand neural mechanisms for continual learning and flexible skill composition, addressing how intelligent systems can learn, retain, and recompose skills efficiently without forgetting previous knowledge.", "method": "A two-system RNN approach: (1) a probabilistic generative model ('what' system) that learns task vocabulary incrementally and infers computational context, (2) an RNN ('how' system) with low-rank components composed according to inferred context, enabling unsupervised online learning.", "result": "The framework demonstrates effective continual learning without catastrophic forgetting, competitive performance on compositional cognitive tasks, and capabilities for forward/backward transfer and fast generalization to unseen tasks.", "conclusion": "The two-system approach provides a viable neural mechanism for continual learning and compositional reuse of computations, offering insights into how biological systems might achieve flexible skill composition and retention."}}
{"id": "2510.20714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20714", "abs": "https://arxiv.org/abs/2510.20714", "authors": ["Fardin Ganjkhanloo", "Emmett Springer", "Erik H. Hoyer", "Daniel L. Young", "Kimia Ghobadi"], "title": "Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool", "comment": "19 pages, 7 figures, 4 tables", "summary": "In this study we aim to better align fall risk prediction from the Johns\nHopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically\nmeaningful measures via a data-driven modelling approach. We conducted a\nretrospective analysis of 54,209 inpatient admissions from three Johns Hopkins\nHealth System hospitals between March 2022 and October 2023. A total of 20,208\nadmissions were included as high fall risk encounters, and 13,941 were included\nas low fall risk encounters. To incorporate clinical knowledge and maintain\ninterpretability, we employed constrained score optimization (CSO) models on\nJHFRAT assessment data and additional electronic health record (EHR) variables.\nThe model demonstrated significant improvements in predictive performance over\nthe current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained\nscore optimization models performed similarly with and without the EHR\nvariables. Although the benchmark black-box model (XGBoost), improves upon the\nperformance metrics of the knowledge-based constrained logistic regression\n(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk\nlabelling. This evidence-based approach provides a robust foundation for health\nsystems to systematically enhance inpatient fall prevention protocols and\npatient safety using data-driven optimization techniques, contributing to\nimproved risk assessment and resource allocation in healthcare settings.", "AI": {"tldr": "This study improved fall risk prediction using constrained score optimization on JHFRAT data and EHR variables, achieving better performance than the original tool while maintaining interpretability.", "motivation": "To better align fall risk prediction with clinically meaningful measures and improve upon the existing Johns Hopkins Fall Risk Assessment Tool through data-driven modeling.", "method": "Retrospective analysis of 54,209 inpatient admissions using constrained score optimization models on JHFRAT assessment data and EHR variables, comparing performance against the original JHFRAT and black-box XGBoost models.", "result": "The constrained score optimization model significantly outperformed the current JHFRAT (AUC-ROC=0.91 vs 0.86) and demonstrated robustness to risk labeling variations, though XGBoost achieved slightly higher performance (AUC-ROC=0.94).", "conclusion": "The evidence-based constrained score optimization approach provides a robust foundation for enhancing inpatient fall prevention protocols and improving risk assessment in healthcare settings while maintaining interpretability."}}
{"id": "2510.20718", "categories": ["cs.LG", "cs.AI", "I.2.0; J.6"], "pdf": "https://arxiv.org/pdf/2510.20718", "abs": "https://arxiv.org/abs/2510.20718", "authors": ["Daniel Sorensen", "Bappaditya Dey", "Minjin Hwang", "Sandip Halder"], "title": "Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series", "comment": "17 pages, 27 figures", "summary": "Semiconductor manufacturing is an extremely complex and precision-driven\nprocess, characterized by thousands of interdependent parameters collected\nacross diverse tools and process steps. Multi-variate time-series analysis has\nemerged as a critical field for real-time monitoring and fault detection in\nsuch environments. However, anomaly prediction in semiconductor fabrication\npresents several critical challenges, including high dimensionality of sensor\ndata and severe class imbalance due to the rarity of true faults. Furthermore,\nthe complex interdependencies between variables complicate both anomaly\nprediction and root-cause-analysis. This paper proposes two novel approaches to\nadvance the field from anomaly detection to anomaly prediction, an essential\nstep toward enabling real-time process correction and proactive fault\nprevention. The proposed anomaly prediction framework contains two main stages:\n(a) training a forecasting model on a dataset assumed to contain no anomalies,\nand (b) performing forecast on unseen time series data. The forecast is\ncompared with the forecast of the trained signal. Deviations beyond a\npredefined threshold are flagged as anomalies. The two approaches differ in the\nforecasting model employed. The first assumes independence between variables by\nutilizing the N-BEATS model for univariate time series forecasting. The second\nlifts this assumption by utilizing a Graph Neural Network (GNN) to capture\ninter-variable relationships. Both models demonstrate strong forecasting\nperformance up to a horizon of 20 time points and maintain stable anomaly\nprediction up to 50 time points. The GNN consistently outperforms the N-BEATS\nmodel while requiring significantly fewer trainable parameters and lower\ncomputational cost. These results position the GNN as promising solution for\nonline anomaly forecasting to be deployed in manufacturing environments.", "AI": {"tldr": "This paper proposes two novel approaches for anomaly prediction in semiconductor manufacturing: one using N-BEATS for univariate forecasting assuming variable independence, and another using Graph Neural Networks (GNN) to capture inter-variable relationships. Both methods forecast future values and flag deviations as anomalies, with GNN outperforming N-BEATS in performance while being more computationally efficient.", "motivation": "Semiconductor manufacturing involves complex processes with thousands of interdependent parameters, presenting challenges like high-dimensional sensor data, severe class imbalance due to rare faults, and complex variable interdependencies that complicate anomaly prediction and root-cause analysis.", "method": "The framework has two stages: (1) training a forecasting model on anomaly-free data, and (2) performing forecasts on unseen data and flagging deviations beyond threshold as anomalies. Two approaches are compared: N-BEATS for univariate forecasting assuming variable independence, and GNN that captures inter-variable relationships.", "result": "Both models demonstrate strong forecasting performance up to 20 time points and stable anomaly prediction up to 50 time points. The GNN consistently outperforms N-BEATS while requiring significantly fewer trainable parameters and lower computational cost.", "conclusion": "The GNN approach is positioned as a promising solution for online anomaly forecasting in manufacturing environments, advancing the field from anomaly detection to prediction for real-time process correction and proactive fault prevention."}}
{"id": "2510.20725", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20725", "abs": "https://arxiv.org/abs/2510.20725", "authors": ["Jasmine Bayrooti", "Sattar Vakili", "Amanda Prorok", "Carl Henrik Ek"], "title": "No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes", "comment": "Appearing in NeurIPS, 2025", "summary": "Thompson sampling (TS) is a powerful and widely used strategy for sequential\ndecision-making, with applications ranging from Bayesian optimization to\nreinforcement learning (RL). Despite its success, the theoretical foundations\nof TS remain limited, particularly in settings with complex temporal structure\nsuch as RL. We address this gap by establishing no-regret guarantees for TS\nusing models with Gaussian marginal distributions. Specifically, we consider TS\nin episodic RL with joint Gaussian process (GP) priors over rewards and\ntransitions. We prove a regret bound of\n$\\mathcal{\\tilde{O}}(\\sqrt{KH\\Gamma(KH)})$ over $K$ episodes of horizon $H$,\nwhere $\\Gamma(\\cdot)$ captures the complexity of the GP model. Our analysis\naddresses several challenges, including the non-Gaussian nature of value\nfunctions and the recursive structure of Bellman updates, and extends classical\ntools such as the elliptical potential lemma to multi-output settings. This\nwork advances the understanding of TS in RL and highlights how structural\nassumptions and model uncertainty shape its performance in finite-horizon\nMarkov Decision Processes.", "AI": {"tldr": "This paper establishes theoretical regret bounds for Thompson sampling in episodic reinforcement learning with Gaussian process priors over rewards and transitions, addressing challenges in analyzing TS for complex temporal structures.", "motivation": "Thompson sampling is widely used in sequential decision-making but lacks strong theoretical foundations, especially in reinforcement learning settings with complex temporal structure. The authors aim to bridge this gap by providing rigorous regret guarantees.", "method": "The authors analyze Thompson sampling in episodic RL with joint Gaussian process priors over both rewards and transitions. They extend classical tools like the elliptical potential lemma to handle multi-output settings and address challenges from non-Gaussian value functions and recursive Bellman updates.", "result": "The paper proves a regret bound of $\\mathcal{\\\\tilde{O}}(\\\\sqrt{KH\\\\Gamma(KH)})$ over $K$ episodes of horizon $H$, where $\\\\Gamma(\\\\cdot)$ captures the complexity of the GP model. This provides the first theoretical guarantees for TS in RL with GP priors.", "conclusion": "This work advances the theoretical understanding of Thompson sampling in reinforcement learning and demonstrates how structural assumptions and model uncertainty affect performance in finite-horizon Markov Decision Processes."}}
{"id": "2510.20733", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20733", "abs": "https://arxiv.org/abs/2510.20733", "authors": ["Yujia Zheng", "Zhuokai Zhao", "Zijian Li", "Yaqi Xie", "Mingze Gao", "Lizhu Zhang", "Kun Zhang"], "title": "Thought Communication in Multiagent Collaboration", "comment": "NeurIPS 2025 Spotlight", "summary": "Natural language has long enabled human cooperation, but its lossy,\nambiguous, and indirect nature limits the potential of collective intelligence.\nWhile machines are not subject to these constraints, most LLM-based multi-agent\nsystems still rely solely on natural language, exchanging tokens or their\nembeddings. To go beyond language, we introduce a new paradigm, thought\ncommunication, which enables agents to interact directly mind-to-mind, akin to\ntelepathy. To uncover these latent thoughts in a principled way, we formalize\nthe process as a general latent variable model, where agent states are\ngenerated by an unknown function of underlying thoughts. We prove that, in a\nnonparametric setting without auxiliary information, both shared and private\nlatent thoughts between any pair of agents can be identified. Moreover, the\nglobal structure of thought sharing, including which agents share which\nthoughts and how these relationships are structured, can also be recovered with\ntheoretical guarantees. Guided by the established theory, we develop a\nframework that extracts latent thoughts from all agents prior to communication\nand assigns each agent the relevant thoughts, along with their sharing\npatterns. This paradigm naturally extends beyond LLMs to all modalities, as\nmost observational data arise from hidden generative processes. Experiments on\nboth synthetic and real-world benchmarks validate the theory and demonstrate\nthe collaborative advantages of thought communication. We hope this work\nilluminates the potential of leveraging the hidden world, as many challenges\nremain unsolvable through surface-level observation alone, regardless of\ncompute or data scale.", "AI": {"tldr": "The paper introduces 'thought communication' - a paradigm for direct mind-to-mind interaction between AI agents that goes beyond natural language limitations, enabling more effective collective intelligence.", "motivation": "Natural language is lossy, ambiguous, and indirect, limiting collective intelligence potential. Current LLM-based multi-agent systems rely solely on natural language, which constrains their collaborative capabilities.", "method": "Formalize thought communication as a latent variable model where agent states are generated by underlying thoughts. Develop a framework to extract latent thoughts from agents prior to communication and assign relevant thoughts with sharing patterns. The approach works in nonparametric settings without auxiliary information.", "result": "Theoretical guarantees for identifying both shared and private latent thoughts between agents and recovering global thought sharing structure. Experiments on synthetic and real-world benchmarks validate the theory and demonstrate collaborative advantages.", "conclusion": "Thought communication enables more effective collective intelligence by leveraging hidden generative processes, extending beyond LLMs to all modalities. This approach illuminates the potential of leveraging the hidden world rather than relying solely on surface-level observations."}}
{"id": "2510.20736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20736", "abs": "https://arxiv.org/abs/2510.20736", "authors": ["Tsai Hor Chan", "Feng Wu", "Yihang Chen", "Guosheng Yin", "Lequan Yu"], "title": "Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process", "comment": "Accepted by NeruIPS 2025", "summary": "Developing effective multimodal fusion approaches has become increasingly\nessential in many real-world scenarios, such as health care and finance. The\nkey challenge is how to preserve the feature expressiveness in each modality\nwhile learning cross-modal interactions. Previous approaches primarily focus on\nthe cross-modal alignment, while over-emphasis on the alignment of marginal\ndistributions of modalities may impose excess regularization and obstruct\nmeaningful representations within each modality. The Dirichlet process (DP)\nmixture model is a powerful Bayesian non-parametric method that can amplify the\nmost prominent features by its richer-gets-richer property, which allocates\nincreasing weights to them. Inspired by this unique characteristic of DP, we\npropose a new DP-driven multimodal learning framework that automatically\nachieves an optimal balance between prominent intra-modal representation\nlearning and cross-modal alignment. Specifically, we assume that each modality\nfollows a mixture of multivariate Gaussian distributions and further adopt DP\nto calculate the mixture weights for all the components. This paradigm allows\nDP to dynamically allocate the contributions of features and select the most\nprominent ones, leveraging its richer-gets-richer property, thus facilitating\nmultimodal feature fusion. Extensive experiments on several multimodal datasets\ndemonstrate the superior performance of our model over other competitors.\nAblation analysis further validates the effectiveness of DP in aligning\nmodality distributions and its robustness to changes in key hyperparameters.\nCode is anonymously available at https://github.com/HKU-MedAI/DPMM.git", "AI": {"tldr": "A new Dirichlet Process-driven multimodal learning framework that automatically balances intra-modal representation learning and cross-modal alignment, outperforming competitors on multiple datasets.", "motivation": "Existing multimodal fusion approaches over-emphasize cross-modal alignment, which may impose excess regularization and obstruct meaningful representations within each modality. The challenge is preserving feature expressiveness while learning cross-modal interactions.", "method": "The framework assumes each modality follows a mixture of multivariate Gaussian distributions and adopts Dirichlet Process to calculate mixture weights, leveraging its richer-gets-richer property to dynamically allocate feature contributions and select prominent features for multimodal fusion.", "result": "Extensive experiments on several multimodal datasets demonstrate superior performance over competitors. Ablation analysis validates DP's effectiveness in aligning modality distributions and robustness to hyperparameter changes.", "conclusion": "The DP-driven framework successfully achieves optimal balance between prominent intra-modal representation learning and cross-modal alignment, providing an effective solution for multimodal feature fusion."}}
{"id": "2510.20762", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20762", "abs": "https://arxiv.org/abs/2510.20762", "authors": ["Jan Sobotka", "Luca Baroni", "J\u00e1n Antol\u00edk"], "title": "MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs", "comment": "Accepted to NeurIPS 2025", "summary": "Decoding visual stimuli from neural population activity is crucial for\nunderstanding the brain and for applications in brain-machine interfaces.\nHowever, such biological data is often scarce, particularly in primates or\nhumans, where high-throughput recording techniques, such as two-photon imaging,\nremain challenging or impossible to apply. This, in turn, poses a challenge for\ndeep learning decoding techniques. To overcome this, we introduce MEIcoder, a\nbiologically informed decoding method that leverages neuron-specific most\nexciting inputs (MEIs), a structural similarity index measure loss, and\nadversarial training. MEIcoder achieves state-of-the-art performance in\nreconstructing visual stimuli from single-cell activity in primary visual\ncortex (V1), especially excelling on small datasets with fewer recorded\nneurons. Using ablation studies, we demonstrate that MEIs are the main drivers\nof the performance, and in scaling experiments, we show that MEIcoder can\nreconstruct high-fidelity natural-looking images from as few as 1,000-2,500\nneurons and less than 1,000 training data points. We also propose a unified\nbenchmark with over 160,000 samples to foster future research. Our results\ndemonstrate the feasibility of reliable decoding in early visual system and\nprovide practical insights for neuroscience and neuroengineering applications.", "AI": {"tldr": "MEIcoder is a biologically informed decoding method that achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in V1, particularly excelling with small datasets and few neurons.", "motivation": "Visual stimulus decoding from neural activity is crucial for brain understanding and brain-machine interfaces, but biological data is often scarce in primates/humans where high-throughput recording techniques are challenging, posing problems for deep learning decoding methods.", "method": "MEIcoder uses neuron-specific most exciting inputs (MEIs), structural similarity index measure loss, and adversarial training to decode visual stimuli from neural population activity.", "result": "MEIcoder achieves state-of-the-art performance, can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points, with MEIs identified as the main performance drivers through ablation studies.", "conclusion": "The results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications, with a proposed unified benchmark to foster future research."}}
{"id": "2510.20783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20783", "abs": "https://arxiv.org/abs/2510.20783", "authors": ["Anna M\u00e9sz\u00e1ros", "Patrik Reizinger", "Ferenc Husz\u00e1r"], "title": "Out-of-distribution Tests Reveal Compositionality in Chess Transformers", "comment": null, "summary": "Chess is a canonical example of a task that requires rigorous reasoning and\nlong-term planning. Modern decision Transformers - trained similarly to LLMs -\nare able to learn competent gameplay, but it is unclear to what extent they\ntruly capture the rules of chess. To investigate this, we train a 270M\nparameter chess Transformer and test it on out-of-distribution scenarios,\ndesigned to reveal failures of systematic generalization. Our analysis shows\nthat Transformers exhibit compositional generalization, as evidenced by strong\nrule extrapolation: they adhere to fundamental syntactic rules of the game by\nconsistently choosing valid moves even in situations very different from the\ntraining data. Moreover, they also generate high-quality moves for OOD puzzles.\nIn a more challenging test, we evaluate the models on variants including\nChess960 (Fischer Random Chess) - a variant of chess where starting positions\nof pieces are randomized. We found that while the model exhibits basic strategy\nadaptation, they are inferior to symbolic AI algorithms that perform explicit\nsearch, but gap is smaller when playing against users on Lichess. Moreover, the\ntraining dynamics revealed that the model initially learns to move only its own\npieces, suggesting an emergent compositional understanding of the game.", "AI": {"tldr": "Transformers trained on chess show compositional generalization by following game rules in out-of-distribution scenarios, perform well on chess puzzles, but struggle with Chess960 compared to symbolic AI algorithms.", "motivation": "To investigate whether decision Transformers truly learn chess rules and exhibit systematic generalization, rather than just memorizing training patterns.", "method": "Trained a 270M parameter chess Transformer and tested it on out-of-distribution scenarios including rule extrapolation, chess puzzles, and Chess960 variant to assess generalization capabilities.", "result": "Transformers demonstrate strong compositional generalization by consistently choosing valid moves in unfamiliar situations, perform well on OOD puzzles, but show inferior performance in Chess960 compared to symbolic AI algorithms, though the gap is smaller in human gameplay.", "conclusion": "Transformers exhibit emergent compositional understanding of chess rules and can generalize systematically, but still lag behind symbolic AI in complex variant scenarios, suggesting room for improvement in reasoning capabilities."}}
{"id": "2510.20792", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.20792", "abs": "https://arxiv.org/abs/2510.20792", "authors": ["Liang Ye", "Shengqin Chen", "Jiazhu Dai"], "title": "BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation", "comment": null, "summary": "The rapid progress of graph generation has raised new security concerns,\nparticularly regarding backdoor vulnerabilities. While prior work has explored\nbackdoor attacks in image diffusion and unconditional graph generation,\nconditional, especially text-guided graph generation remains largely\nunexamined. This paper proposes BadGraph, a backdoor attack method targeting\nlatent diffusion models for text-guided graph generation. BadGraph leverages\ntextual triggers to poison training data, covertly implanting backdoors that\ninduce attacker-specified subgraphs during inference when triggers appear,\nwhile preserving normal performance on clean inputs. Extensive experiments on\nfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the\neffectiveness and stealth of the attack: less than 10% poisoning rate can\nachieves 50% attack success rate, while 24% suffices for over 80% success rate,\nwith negligible performance degradation on benign samples. Ablation studies\nfurther reveal that the backdoor is implanted during VAE and diffusion training\nrather than pretraining. These findings reveal the security vulnerabilities in\nlatent diffusion models of text-guided graph generation, highlight the serious\nrisks in models' applications such as drug discovery and underscore the need\nfor robust defenses against the backdoor attack in such diffusion models.", "AI": {"tldr": "BadGraph is a backdoor attack method targeting latent diffusion models for text-guided graph generation, using textual triggers to poison training data and induce attacker-specified subgraphs during inference while maintaining normal performance on clean inputs.", "motivation": "The rapid progress of graph generation has raised security concerns about backdoor vulnerabilities, particularly in text-guided graph generation which remains largely unexamined compared to image diffusion and unconditional graph generation.", "method": "BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs.", "result": "Extensive experiments on four benchmark datasets show high effectiveness: less than 10% poisoning rate achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples.", "conclusion": "The findings reveal security vulnerabilities in latent diffusion models for text-guided graph generation, highlight serious risks in applications like drug discovery, and underscore the need for robust defenses against backdoor attacks in such diffusion models."}}
{"id": "2510.20800", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20800", "abs": "https://arxiv.org/abs/2510.20800", "authors": ["Shiva Sreeram", "Alaa Maalouf", "Pratyusha Sharma", "Daniela Rus"], "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples", "comment": null, "summary": "Recently, Sharma et al. suggested a method called Layer-SElective-Rank\nreduction (LASER) which demonstrated that pruning high-order components of\ncarefully chosen LLM's weight matrices can boost downstream accuracy -- without\nany gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each\nrequiring full-dataset forward passes) makes it impractical for rapid\ndeployment. We demonstrate that this overhead can be removed and find that: (i)\nOnly a small, carefully chosen subset of matrices needs to be inspected --\neliminating the layer-by-layer sweep, (ii) The gradient of each matrix's\nsingular values pinpoints which matrices merit reduction, (iii) Increasing the\nfactorization search space by allowing matrices rows to cluster around multiple\nsubspaces and then decomposing each cluster separately further reduces\noverfitting on the original training data and further lifts accuracy by up to\n24.6 percentage points, and finally, (iv) we discover that evaluating on just\n100 samples rather than the full training data -- both for computing the\nindicative gradients and for measuring the final accuracy -- suffices to\nfurther reduce the search time; we explain that as adaptation to downstream\ntasks is dominated by prompting style, not dataset size. As a result, we show\nthat combining these findings yields a fast and robust adaptation algorithm for\ndownstream tasks. Overall, with a single gradient step on 100 examples and a\nquick scan of the top candidate layers and factorization techniques, we can\nadapt LLMs to new datasets -- entirely without fine-tuning.", "AI": {"tldr": "This paper presents a fast adaptation method for LLMs that improves on LASER by eliminating exhaustive layer-by-layer searches, using gradient analysis to identify key matrices, expanding factorization search space, and requiring only 100 samples for evaluation - all without fine-tuning.", "motivation": "To address the impractical overhead of LASER's exhaustive per-matrix search which requires full-dataset forward passes, making it unsuitable for rapid deployment of LLMs to downstream tasks.", "method": "Use gradient analysis of singular values to identify key matrices, allow matrices rows to cluster around multiple subspaces for expanded factorization search, and evaluate on only 100 samples for both gradient computation and accuracy measurement.", "result": "The method reduces search time significantly, reduces overfitting on original training data, and lifts accuracy by up to 24.6 percentage points compared to the original LASER approach.", "conclusion": "Combining these techniques yields a fast and robust adaptation algorithm that can adapt LLMs to new datasets with just a single gradient step on 100 examples and a quick scan of top candidate layers, entirely without fine-tuning."}}
{"id": "2510.20817", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20817", "abs": "https://arxiv.org/abs/2510.20817", "authors": ["Anthony GX-Chen", "Jatin Prakash", "Jeff Guo", "Rob Fergus", "Rajesh Ranganath"], "title": "KL-Regularized Reinforcement Learning is Designed to Mode Collapse", "comment": null, "summary": "It is commonly believed that optimizing the reverse KL divergence results in\n\"mode seeking\", while optimizing forward KL results in \"mass covering\", with\nthe latter being preferred if the goal is to sample from multiple diverse\nmodes. We show -- mathematically and empirically -- that this intuition does\nnot necessarily transfer well to doing reinforcement learning with\nreverse/forward KL regularization (e.g. as commonly used with language models).\nInstead, the choice of reverse/forward KL determines the family of optimal\ntarget distributions, parameterized by the regularization coefficient. Mode\ncoverage depends primarily on other factors, such as regularization strength,\nand relative scales between rewards and reference probabilities. Further, we\nshow commonly used settings such as low regularization strength and equal\nverifiable rewards tend to specify unimodal target distributions, meaning the\noptimization objective is, by construction, non-diverse. We leverage these\ninsights to construct a simple, scalable, and theoretically justified\nalgorithm. It makes minimal changes to reward magnitudes, yet optimizes for a\ntarget distribution which puts high probability over all high-quality sampling\nmodes. In experiments, this simple modification works to post-train both Large\nLanguage Models and Chemical Language Models to have higher solution quality\nand diversity, without any external signals of diversity, and works with both\nforward and reverse KL when using either naively fails.", "AI": {"tldr": "This paper challenges the common intuition about reverse/forward KL divergence in RL with language models, showing that mode coverage depends on regularization strength and reward scales rather than KL type. The authors develop a simple algorithm that modifies reward magnitudes to optimize for diverse target distributions.", "motivation": "To correct the misconception that reverse KL is mode-seeking and forward KL is mass-covering in RL with language models, and to develop a method that ensures diverse sampling from multiple high-quality modes without external diversity signals.", "method": "The authors mathematically analyze how reverse/forward KL regularization affects target distributions, then construct a simple algorithm that minimally modifies reward magnitudes to optimize for target distributions covering all high-quality modes. The method works with both forward and reverse KL.", "result": "The proposed algorithm successfully post-trains both Large Language Models and Chemical Language Models, achieving higher solution quality and diversity without external diversity signals. It works with both forward and reverse KL where naive approaches fail.", "conclusion": "Mode coverage in RL with language models depends primarily on regularization strength and reward/reference probability scales, not KL divergence type. The developed algorithm provides a theoretically justified, scalable approach for diverse sampling from multiple high-quality modes."}}
