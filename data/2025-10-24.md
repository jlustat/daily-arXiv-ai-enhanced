<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 7]
- [stat.ME](#stat.ME) [Total: 9]
- [cs.LG](#cs.LG) [Total: 95]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Testing Most Influential Sets](https://arxiv.org/abs/2510.20372)
*Lucas Darius Konrad,Nikolas Kuschnig*

Main category: stat.ML

TL;DR: Develops a statistical framework to test whether influential data points represent genuine problems or natural sampling variation, enabling rigorous hypothesis tests for excessive influence.


<details>
  <summary>Details</summary>
Motivation: Small subsets of data can disproportionately influence model outcomes and overturn key findings, but existing methods lack formal theory to determine when this influence reflects genuine problems versus natural sampling variation.

Method: Develops a principled framework for assessing statistical significance of most influential sets, with theoretical results characterizing extreme value distributions of maximal influence.

Result: Enables rigorous hypothesis tests for excessive influence that replace current ad-hoc sensitivity checks.

Conclusion: The approach demonstrates practical value across applications in economics, biology, and machine learning benchmarks.

Abstract: Small subsets of data with disproportionate influence on model outcomes can
have dramatic impacts on conclusions, with a few data points sometimes
overturning key findings. While recent work has developed methods to identify
these \emph{most influential sets}, no formal theory exists to determine when
their influence reflects genuine problems rather than natural sampling
variation. We address this gap by developing a principled framework for
assessing the statistical significance of most influential sets. Our
theoretical results characterize the extreme value distributions of maximal
influence and enable rigorous hypothesis tests for excessive influence,
replacing current ad-hoc sensitivity checks. We demonstrate the practical value
of our approach through applications across economics, biology, and machine
learning benchmarks.

</details>


### [2] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: A novel enhanced cyclic coordinate descent (ECCD) framework for solving generalized linear models with elastic net constraints that achieves 3x faster training time compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To reduce training time for generalized linear models with elastic net constraints by improving upon existing coordinate descent methods and avoiding convergence delays and numerical instability of block coordinate descent.

Method: Redesigns coordinate descent by performing Taylor expansion around current iterate to avoid nonlinear gradient operations, unrolls vector recurrences into efficient batched computations with tunable parameter s, and implements in C++ using Eigen for linear algebra acceleration.

Result: Empirical results show consistent 3x performance improvements on regularization path variant across diverse benchmark datasets, with s > 1 providing performance gains without affecting convergence.

Conclusion: ECCD framework successfully accelerates training for generalized linear models with elastic net constraints while maintaining convergence properties and avoiding numerical instability issues.

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


### [3] [Compositional Generation for Long-Horizon Coupled PDEs](https://arxiv.org/abs/2510.20141)
*Somayajulu L. N. Dhulipala,Deep Ray,Nicholas Forman*

Main category: stat.ML

TL;DR: Compositional diffusion models trained on decoupled PDE data can effectively recover coupled trajectories with low error, offering a viable alternative to joint training approaches that require large coupled datasets.


<details>
  <summary>Details</summary>
Motivation: Simulating coupled PDE systems is computationally intensive, and traditional approaches require training surrogates on joint (coupled) data which demands large datasets. This work explores whether compositional approaches using only decoupled data can be effective.

Method: Compositional diffusion models trained only on decoupled PDE data, composed at inference time to recover coupled fields. Investigated baseline diffusion vs v-parameterization, introduced symmetric compositional scheme based on Euler scheme. Evaluated on Reaction-Diffusion and modified Burgers with longer time grids.

Result: Despite seeing only decoupled training data, compositional diffusion models recover coupled trajectories with low error. v-parameterization improves accuracy over baseline diffusion model. Neural operator surrogate trained on coupled data remains strongest performer.

Conclusion: Compositional diffusion is a viable strategy for efficient, long-horizon modeling of coupled PDEs, offering an alternative to data-intensive joint training approaches.

Abstract: Simulating coupled PDE systems is computationally intensive, and prior
efforts have largely focused on training surrogates on the joint (coupled)
data, which requires a large amount of data. In the paper, we study
compositional diffusion approaches where diffusion models are only trained on
the decoupled PDE data and are composed at inference time to recover the
coupled field. Specifically, we investigate whether the compositional strategy
can be feasible under long time horizons involving a large number of time
steps. In addition, we compare a baseline diffusion model with that trained
using the v-parameterization strategy. We also introduce a symmetric
compositional scheme for the coupled fields based on the Euler scheme. We
evaluate on Reaction-Diffusion and modified Burgers with longer time grids, and
benchmark against a Fourier Neural Operator trained on coupled data. Despite
seeing only decoupled training data, the compositional diffusion models recover
coupled trajectories with low error. v-parameterization can improve accuracy
over a baseline diffusion model, while the neural operator surrogate remains
strongest given that it is trained on the coupled data. These results show that
compositional diffusion is a viable strategy towards efficient, long-horizon
modeling of coupled PDEs.

</details>


### [4] [Neural Networks for Censored Expectile Regression Based on Data Augmentation](https://arxiv.org/abs/2510.20344)
*Wei Cao,Shanshan Wang*

Main category: stat.ML

TL;DR: DAERNN is a data augmentation-based expectile regression neural network for handling heterogeneous censored data, outperforming existing censored ERNN methods and achieving performance comparable to models trained on fully observed data.


<details>
  <summary>Details</summary>
Motivation: Existing expectile regression neural networks (ERNNs) mainly focus on fully observed data, with limited attention to censored observations, creating a gap in handling real-world censored data scenarios.

Method: Proposed DAERNN uses data augmentation approach for expectile regression neural networks, providing a unified framework for various censoring mechanisms without requiring explicit parametric model specification.

Result: Simulation studies and real data applications show DAERNN outperforms existing censored ERNNs methods and achieves predictive performance comparable to models trained on fully observed data.

Conclusion: DAERNN offers a flexible, data-driven solution for heterogeneous censored data analysis with minimal assumptions and broad applicability across different censoring mechanisms.

Abstract: Expectile regression neural networks (ERNNs) are powerful tools for capturing
heterogeneity and complex nonlinear structures in data. However, most existing
research has primarily focused on fully observed data, with limited attention
paid to scenarios involving censored observations. In this paper, we propose a
data augmentation based ERNNs algorithm, termed DAERNN, for modeling
heterogeneous censored data. The proposed DAERNN is fully data driven, requires
minimal assumptions, and offers substantial flexibility. Simulation studies and
real data applications demonstrate that DAERNN outperforms existing censored
ERNNs methods and achieves predictive performance comparable to models trained
on fully observed data. Moreover, the algorithm provides a unified framework
for handling various censoring mechanisms without requiring explicit parametric
model specification, thereby enhancing its applicability to practical censored
data analysis.

</details>


### [5] [Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks](https://arxiv.org/abs/2510.20436)
*Federico Lozano-Cuadra,Beatriz Soret,Marc Sanchez Net,Abhishek Cauligi,Federico Rossi*

Main category: stat.ML

TL;DR: A decentralized routing framework using Graph Attention-based Multi-Agent Reinforcement Learning for multi-robot exploration in Lunar Delay-Tolerant Networks, achieving higher delivery rates without packet duplication.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of multi-robot exploration missions in Lunar Delay-Tolerant Networks where rovers must relay data to a lander under intermittent connectivity and unknown mobility patterns.

Method: Formulated as a Partially Observable Markov Decision Problem (POMDP) using Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) with Centralized Training, Decentralized Execution (CTDE), relying only on local observations without global topology updates.

Result: Monte Carlo simulations showed higher delivery rates, no duplications, fewer packet losses, and ability to leverage short-term mobility forecasts. Successfully generalized to larger rover teams.

Conclusion: The GAT-MARL approach offers a scalable solution for future space robotic systems in planetary exploration, outperforming classical approaches like shortest path and controlled flooding-based algorithms.

Abstract: We present a fully decentralized routing framework for multi-robot
exploration missions operating under the constraints of a Lunar Delay-Tolerant
Network (LDTN). In this setting, autonomous rovers must relay collected data to
a lander under intermittent connectivity and unknown mobility patterns. We
formulate the problem as a Partially Observable Markov Decision Problem (POMDP)
and propose a Graph Attention-based Multi-Agent Reinforcement Learning
(GAT-MARL) policy that performs Centralized Training, Decentralized Execution
(CTDE). Our method relies only on local observations and does not require
global topology updates or packet replication, unlike classical approaches such
as shortest path and controlled flooding-based algorithms. Through Monte Carlo
simulations in randomized exploration environments, GAT-MARL provides higher
delivery rates, no duplications, and fewer packet losses, and is able to
leverage short-term mobility forecasts; offering a scalable solution for future
space robotic systems for planetary exploration, as demonstrated by successful
generalization to larger rover teams.

</details>


### [6] [Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences](https://arxiv.org/abs/2510.20595)
*Yunyi Shen,Alexander Gagliano*

Main category: stat.ML

TL;DR: DAEP is a diffusion autoencoder with Perceivers that handles irregular, multimodal sequences in scientific domains, outperforming VAE and MAEP baselines in reconstruction quality and latent space discriminability.


<details>
  <summary>Details</summary>
Motivation: Most self-supervised learning architectures are designed for regularly-sampled data like images and videos, but scientific data often arrives as long, irregular, multimodal sequences that require specialized handling.

Method: DAEP tokenizes heterogeneous measurements, compresses them with a Perceiver encoder, and reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable learning in diverse data settings.

Result: Across diverse spectroscopic and photometric astronomical datasets, DAEP achieves lower reconstruction errors, produces more discriminative latent spaces, and better preserves fine-scale structure than both VAE and MAEP baselines.

Conclusion: DAEP establishes an effective framework for scientific domains where data arrives as irregular, heterogeneous sequences, providing superior performance over existing methods.

Abstract: Self-supervised learning has become a central strategy for representation
learning, but the majority of architectures used for encoding data have only
been validated on regularly-sampled inputs such as images, audios. and videos.
In many scientific domains, data instead arrive as long, irregular, and
multimodal sequences. To extract semantic information from these data, we
introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes
heterogeneous measurements, compresses them with a Perceiver encoder, and
reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable
learning in diverse data settings. To benchmark the daep architecture, we adapt
the masked autoencoder to a Perceiver encoder/decoder design, and establish a
strong baseline (maep) in the same architectural family as daep. Across diverse
spectroscopic and photometric astronomical datasets, daep achieves lower
reconstruction errors, produces more discriminative latent spaces, and better
preserves fine-scale structure than both VAE and maep baselines. These results
establish daep as an effective framework for scientific domains where data
arrives as irregular, heterogeneous sequences.

</details>


### [7] [Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection](https://arxiv.org/abs/2510.20653)
*Jack Butler,Nikita Kozodoi,Zainab Afolabi,Brian Tyacke,Gaiar Baimuratov*

Main category: stat.ML

TL;DR: This paper systematically compares self-reflection and budget tuning techniques for LLMs across mathematical reasoning and translation tasks, revealing substantial domain-dependent performance variations and providing guidance for optimal inference strategies.


<details>
  <summary>Details</summary>
Motivation: As LLMs evolve, practitioners face complex trade-offs among accuracy, cost, and latency when using inference-time enhancement techniques like self-reflection and budget tuning, which remain poorly understood across different domains.

Method: The study evaluates prominent LLMs (Anthropic Claude, Amazon Nova, Mistral families) across mathematical reasoning and translation tasks under varying reflection depths and compute budgets, deriving Pareto optimal performance frontiers and investigating how reflection round depth and feedback mechanism quality influence performance.

Result: Analysis reveals substantial domain-dependent variation in self-reflection effectiveness, with performance gains up to 220% in mathematical reasoning. A real-world deployment at Lounge by Zalando shows market-dependent effectiveness, reinforcing the importance of domain-specific evaluation.

Conclusion: The findings provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints, and the self-reflection implementation is open-sourced for reproducibility.

Abstract: As Large Language Models (LLMs) continue to evolve, practitioners face
increasing options for enhancing inference-time performance without model
retraining, including budget tuning and multi-step techniques like
self-reflection. While these methods improve output quality, they create
complex trade-offs among accuracy, cost, and latency that remain poorly
understood across different domains. This paper systematically compares
self-reflection and budget tuning across mathematical reasoning and translation
tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and
Mistral families, along with other models under varying reflection depths and
compute budgets to derive Pareto optimal performance frontiers. Our analysis
reveals substantial domain dependent variation in self-reflection
effectiveness, with performance gains up to 220\% in mathematical reasoning. We
further investigate how reflection round depth and feedback mechanism quality
influence performance across model families. To validate our findings in a
real-world setting, we deploy a self-reflection enhanced marketing content
localisation system at Lounge by Zalando, where it shows market-dependent
effectiveness, reinforcing the importance of domain specific evaluation when
deploying these techniques. Our results provide actionable guidance for
selecting optimal inference strategies given specific domains and resource
constraints. We open source our self-reflection implementation for
reproducibility at
https://github.com/aws-samples/sample-genai-reflection-for-bedrock.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [8] [Kernel Density Estimation and Convolution Revisited](https://arxiv.org/abs/2510.19960)
*Nicholas Tenkorang,Kwesi Appau Ohene-Obeng,Xiaogang Su*

Main category: stat.ME

TL;DR: SHIDE is a novel density estimator that uses simulation and histogram interpolation to address limitations of traditional Kernel Density Estimation, offering computational efficiency and boundary bias mitigation.


<details>
  <summary>Details</summary>
Motivation: To overcome KDE's sensitivity to bandwidth choice, boundary bias, and computational inefficiency by developing a principled convolutional framework for density estimation.

Method: SHIDE generates pseudo-data by adding bounded noise from polynomial kernel densities (constructed via uniform convolutions) to observations, then applies spline interpolation to the resulting histogram.

Result: SHIDE achieves classical n^{-4/5} convergence rate, mitigates boundary bias, and performs comparably to or surpasses KDE across various models, especially for bounded and heavy-tailed distributions.

Conclusion: SHIDE provides a theoretically grounded and practically robust alternative to traditional KDE with computational efficiency and improved performance on bounded domains.

Abstract: Kernel Density Estimation (KDE) is a cornerstone of nonparametric statistics,
yet it remains sensitive to bandwidth choice, boundary bias, and computational
inefficiency. This study revisits KDE through a principled convolutional
framework, providing an intuitive model-based derivation that naturally extends
to constrained domains, such as positive-valued random variables. Building on
this perspective, we introduce SHIDE (Simulation and Histogram Interpolation
for Density Estimation), a novel and computationally efficient density
estimator that generates pseudo-data by adding bounded noise to observations
and applies spline interpolation to the resulting histogram. The noise is
sampled from a class of bounded polynomial kernel densities, constructed
through convolutions of uniform distributions, with a natural bandwidth
parameter defined by the kernel's support bound. We establish the theoretical
properties of SHIDE, including pointwise consistency, bias-variance
decomposition, and asymptotic MISE, showing that SHIDE attains the classical
$n^{-4/5}$ convergence rate while mitigating boundary bias. Two data-driven
bandwidth selection methods are developed, an AMISE-optimal rule and a
percentile-based alternative, which are shown to be asymptotically equivalent.
Extensive simulations demonstrate that SHIDE performs comparably to or
surpasses KDE across a broad range of models, with particular advantages for
bounded and heavy-tailed distributions. These results highlight SHIDE as a
theoretically grounded and practically robust alternative to traditional KDE.

</details>


### [9] [Identification and Debiased Learning of Causal Effects with General Instrumental Variables](https://arxiv.org/abs/2510.20404)
*Shuyuan Chen,Peng Zhang,Yifan Cui*

Main category: stat.ME

TL;DR: A nonparametric framework for causal inference using instrumental variables with efficient estimators via debiased machine learning.


<details>
  <summary>Details</summary>
Motivation: Address confounding in treatment assignment when unobserved variables are present, using instrumental variables for causal identification.

Method: Additive instrumental variable framework with weighting functions, semiparametric theory for efficient influence functions, and debiased machine learning estimators.

Result: Consistent, asymptotically normal estimators demonstrated through simulations and real data analysis from the Job Training Partnership Act program.

Conclusion: The proposed framework effectively identifies causal effects with instrumental variables and provides robust estimation methods applicable to various extensions.

Abstract: Instrumental variable methods are fundamental to causal inference when
treatment assignment is confounded by unobserved variables. In this article, we
develop a general nonparametric framework for identification and learning with
multi-categorical or continuous instrumental variables. Specifically, we
propose an additive instrumental variable framework to identify mean potential
outcomes and the average treatment effect with a weighting function. Leveraging
semiparametric theory, we derive efficient influence functions and construct
consistent, asymptotically normal estimators via debiased machine learning.
Extensions to longitudinal data, dynamic treatment regimes, and multiplicative
instrumental variables are further developed. We demonstrate the proposed
method by employing simulation studies and analyzing real data from the Job
Training Partnership Act program.

</details>


### [10] [On Multiple Robustness of Proximal Dynamic Treatment Regimes](https://arxiv.org/abs/2510.20451)
*Yuanshan Gao,Yang Bai,Yifan Cui*

Main category: stat.ME

TL;DR: Proposes methods for learning optimal dynamic treatment regimes using proximal causal inference when unconfoundedness fails, including identification strategies, efficiency bounds, and robust estimation.


<details>
  <summary>Details</summary>
Motivation: Dynamic treatment regimes adapt treatments based on individual characteristics but estimating them via randomized trials can be costly/ethical, requiring observational data. Unconfoundedness assumption often fails in practice.

Method: Uses proximal causal inference framework with three nonparametric identification methods, establishes semiparametric efficiency bounds, and proposes (K+1)-robust estimation method for K-stage regimes.

Result: Numerical experiments validate efficiency and multiple robustness of proposed methods. Also provides identification and estimation of counterfactual means under static regimes as by-product.

Conclusion: Proposed framework enables learning optimal dynamic treatment regimes from observational data when unconfoundedness fails, with validated efficiency and robustness properties.

Abstract: Dynamic treatment regimes are sequential decision rules that adapt treatment
according to individual time-varying characteristics and outcomes to achieve
optimal effects, with applications in precision medicine, personalized
recommendations, and dynamic marketing. Estimating optimal dynamic treatment
regimes via sequential randomized trials might face costly and ethical hurdles,
often necessitating the use of historical observational data. In this work, we
utilize proximal causal inference framework for learning optimal dynamic
treatment regimes when the unconfoundedness assumption fails. Our contributions
are four-fold: (i) we propose three nonparametric identification methods for
optimal dynamic treatment regimes; (ii) we establish the semiparametric
efficiency bound for the value function of a given regime; (iii) we propose a
(K+1)-robust method for learning optimal dynamic treatment regimes, where K is
the number of stages; (iv) as a by-product for marginal structural models, we
establish identification and estimation of counterfactual means under a static
regime. Numerical experiments validate the efficiency and multiple robustness
of our proposed methods.

</details>


### [11] [Throwing Vines at the Wall: Structure Learning via Random Search](https://arxiv.org/abs/2510.20035)
*Thibault Vatter,Thomas Nagler*

Main category: stat.ME

TL;DR: The paper proposes random search algorithms and a statistical framework using model confidence sets to improve vine copula structure learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Vine copulas are widely used for multivariate dependence modeling but structure learning remains challenging, with current heuristics like Dissmann's greedy algorithm often being suboptimal.

Method: Random search algorithms for structure selection combined with a statistical framework based on model confidence sets that provides theoretical guarantees and supports ensembling.

Result: Empirical results on real-world datasets demonstrate consistent outperformance over state-of-the-art approaches.

Conclusion: The proposed methods provide improved structure learning for vine copulas with theoretical guarantees and practical performance gains.

Abstract: Vine copulas offer flexible multivariate dependence modeling and have become
widely used in machine learning, yet structure learning remains a key
challenge. Early heuristics like the greedy algorithm of Dissmann are still
considered the gold standard, but often suboptimal. We propose random search
algorithms that improve structure selection and a statistical framework based
on model confidence sets, which provides theoretical guarantees on selection
probabilities and a powerful foundation for ensembling. Empirical results on
several real-world data sets show that our methods consistently outperform
state-of-the-art approaches.

</details>


### [12] [Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses](https://arxiv.org/abs/2510.20147)
*Qingyang Liu,Sanvesh Srivastava,Dipankar Bandyopadhyay*

Main category: stat.ME

TL;DR: REGMVST is a regression model for irregular longitudinal data using matrix-variate skew-t distribution to handle skewness, heavy tails, and irregular time dependencies, with an efficient ADECME algorithm for scalable estimation.


<details>
  <summary>Details</summary>
Motivation: To analyze irregular longitudinal data with potential skewness, symmetry, or heavy tails, which traditional models may not adequately handle, especially when dealing with matrix-variate responses and predictors.

Method: Uses matrix-variate skew-t (MVST) distribution for responses, damped exponential correlation (DEC) structure for row dependencies across irregular time profiles, and unstructured column covariance. Develops ECME algorithm for estimation and ADECME extension for parallelized computation.

Result: ADECME demonstrates superior efficiency and convergence compared to alternatives in both synthetic data simulations and a real-world periodontal disease case study using electronic health records.

Conclusion: REGMVST with ADECME provides an effective framework for analyzing irregular longitudinal data with complex distributional characteristics, offering scalable inference with theoretical support and practical implementation through an R package.

Abstract: We propose a regression model with matrix-variate skew-t response (REGMVST)
for analyzing irregular longitudinal data with skewness, symmetry, or heavy
tails. REGMVST models matrix-variate responses and predictors, with rows
indexing longitudinal measurements per subject. It uses the matrix-variate
skew-t (MVST) distribution to handle skewness and heavy tails, a damped
exponential correlation (DEC) structure for row-wise dependencies across
irregular time profiles, and leaves the column covariance unstructured. For
estimation, we initially develop an ECME algorithm for parameter estimation and
further mitigate its computational bottleneck via an asynchronous and
distributed ECME (ADECME) extension. ADECME accelerates the E-step through
parallelization, and retains the simplicity of the conditional M-step, enabling
scalable inference. Simulations using synthetic data and a case study exploring
matrix-variate periodontal disease endpoints derived from electronic health
records demonstrate ADECME's superiority in efficiency and convergence, over
the alternatives. We also provide theoretical support for our empirical
observations and identify regularity assumptions for ADECME's optimal
performance. An accompanying R package is available at
https://github.com/rh8liuqy/STMATREG.

</details>


### [13] [Bias-Variance Tradeoff of Matching Prior to Difference-in-Differences When Parallel Trends is Violated](https://arxiv.org/abs/2510.20191)
*Mingxuan Ge,Dae Woong Ham*

Main category: stat.ME

TL;DR: This paper analyzes the bias-variance tradeoff in matching before Difference-in-Differences (DiD) estimation, showing that matching on observed covariates isn't always beneficial due to sample size tradeoffs, but matching on pre-treatment outcomes always helps.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing research that focuses only on bias in matching-DiD methods, and to provide comprehensive analysis of the full bias-variance tradeoff for better causal inference in operations management.

Method: Uses a linear structural model with unobserved time-varying confounders to analyze bias-variance tradeoffs, develops theoretical guarantees for matching decisions, and applies insights to a real-world knowledge-sharing platform case study.

Result: Matching on observed covariates before DiD is not always recommended due to sample size tradeoffs, but matching on pre-treatment outcomes is always beneficial. The MSE metric provides better guidance for matching decisions.

Conclusion: Researchers should use mean squared error as the final metric for matching decisions, with practitioner-friendly guidelines on when and what variables to match on. The matching-DiD approach is robust for addressing parallel trends violations in managerial contexts.

Abstract: Quasi-experimental causal inference methods have become central in empirical
operations management (OM) for guiding managerial decisions. Among these,
empiricists utilize the Difference-in-Differences (DiD) estimator, which relies
on the parallel trends assumption. To improve its plausibility, researchers
often match treated and control units before applying DiD, with the intuition
that matched groups are more likely to evolve similarly absent treatment.
Existing work that analyze this practice, however, has focused solely on bias.
We complement and fill an important gap by analyzing the full bias-variance
tradeoff. Under a linear structural model with unobserved time-varying
confounders, we show that variance results contrast with established bias
insights: matching on observed covariates prior to DiD is not always
recommended over the classic (unmatched) DiD due to a sample size tradeoff;
furthermore, matching additionally on pre-treatment outcomes is always
beneficial as such tradeoff no longer exists once matching is performed. We
therefore advocate mean squared error (MSE) as a final metric and give
practitioner-friendly guidelines with theoretical guarantees on when (and on
what variables) they should match on. We apply these insights to a recent study
on how the introduction of monetary incentives by a knowledge-sharing platform
affects its general engagement and show that the authors' matching choice prior
to DiD was both warranted and critical. In particular, we provide new
managerial insights that after a full bias correction, their estimated effect
with matching still remains statistically significant, demonstrating that the
chosen matching-DiD approach is sufficiently robust to address managerial
concerns over violations of parallel trends.

</details>


### [14] [Unifying Boxplots: A Multiple Testing Perspective](https://arxiv.org/abs/2510.20259)
*Bowen Gang,Hongmei Lin,Tiejun Tong*

Main category: stat.ME

TL;DR: This paper reframes Tukey's boxplot and its variants as graphical implementations of multiple testing procedures, providing a unifying framework that connects classic outlier detection to modern statistical principles.


<details>
  <summary>Details</summary>
Motivation: Tukey's classic boxplot outlier detection doesn't account for sample size, and existing modifications have been presented as separate heuristic adjustments rather than a unified framework.

Method: The authors propose a framework that recasts boxplots as graphical implementations of multiple testing procedures, showing that Tukey's method is equivalent to an unadjusted procedure while existing modifications correspond to controlling FWER or PFER.

Result: The framework systematizes existing methods and leads to new adaptive constructions, including a boxplot motivated by False Discovery Rate (FDR), and provides a flexible pipeline for integrating robust estimation techniques into the boxplot format.

Conclusion: By connecting classic graphical tools to multiple testing principles, this work provides a principled language for comparing, critiquing, and extending outlier detection rules for modern exploratory analysis.

Abstract: Tukey's boxplot is a foundational tool for exploratory data analysis, but its
classic outlier-flagging rule does not account for the sample size, and
subsequent modifications have often been presented as separate, heuristic
adjustments. In this paper, we propose a unifying framework that recasts the
boxplot and its variants as graphical implementations of multiple testing
procedures. We demonstrate that Tukey's original method is equivalent to an
unadjusted procedure, while existing sample-size-aware modifications correspond
to controlling the Family-Wise Error Rate (FWER) or the Per-Family Error Rate
(PFER). This perspective not only systematizes existing methods but also
naturally leads to new, more adaptive constructions. We introduce a boxplot
motivated by the False Discovery Rate (FDR), and show how our framework
provides a flexible pipeline for integrating state-of-the-art robust estimation
techniques directly into the boxplot's graphical format. By connecting a
classic graphical tool to the principles of multiple testing, our work provides
a principled language for comparing, critiquing, and extending outlier
detection rules for modern exploratory analysis.

</details>


### [15] [Clustering of multivariate tail dependence using conditional methods](https://arxiv.org/abs/2510.20424)
*Patrick O'Toole,Christian Rohrbeck,Jordan Richards*

Main category: stat.ME

TL;DR: A novel clustering method for multivariate extremes using the conditional extremes framework, introducing a computationally efficient dissimilarity measure based on skew-geometric Jensen-Shannon divergence that works in arbitrary dimensions.


<details>
  <summary>Details</summary>
Motivation: The conditional extremes framework is difficult to interpret and compare across many locations or variables, particularly for high-dimensional vectors, requiring a method to identify interpretable groups with homogeneous tail dependence.

Method: Proposed clustering using CE framework with a closed-form, computationally efficient dissimilarity measure based on skew-geometric Jensen-Shannon divergence, applied to pairwise distance matrices using standard clustering algorithms.

Result: Method outperforms existing approaches for clustering bivariate extremes and uniquely extends to multivariate setting; application to Irish meteorological data identifies spatially coherent regions with similar extremal dependence between precipitation and wind speeds.

Conclusion: The proposed clustering method effectively groups random vectors with homogeneous tail dependence, providing interpretable results for multivariate extremes analysis across multiple locations and variables.

Abstract: The conditional extremes (CE) framework has proven useful for analysing the
joint tail behaviour of random vectors. However, when applied across many
locations or variables, it can be difficult to interpret or compare the
resulting extremal dependence structures, particularly for high dimensional
vectors. To address this, we propose a novel clustering method for multivariate
extremes using the CE framework. Our approach introduces a closed-form,
computationally efficient dissimilarity measure for multivariate tails, based
on the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary
dimensions. Applying standard clustering algorithms to a matrix of pairwise
distances, we obtain interpretable groups of random vectors with homogeneous
tail dependence. Simulation studies demonstrate that our method outperforms
existing approaches for clustering bivariate extremes, and uniquely extends to
the multivariate setting. In our application to Irish meteorological data, our
clustering identifies spatially coherent regions with similar extremal
dependence between precipitation and wind speeds.

</details>


### [16] [A comparison of methods for designing hybrid type 2 cluster-randomized trials with continuous effectiveness and implementation endpoints](https://arxiv.org/abs/2510.20741)
*Melody Owen,Fan Li,Ruyi Liu,Donna Spiegelman*

Main category: stat.ME

TL;DR: Comparison of five design methods for powering hybrid type 2 cluster-randomized trials with two co-primary endpoints, showing that p-value adjustment methods are least powerful and identifying conditions where disjunctive 2-DF test or single 1-DF test perform best.


<details>
  <summary>Details</summary>
Motivation: Hybrid type 2 studies are increasingly popular for assessing both implementation and health outcomes, but there's limited guidance on optimal power calculation methods for these cluster-randomized trials with co-primary endpoints.

Method: Theoretical comparison of power equations and numerical evaluation using crt2power R package across 30,000 scenarios to compare five methods: p-value adjustment, combined outcomes, single weighted 1-DF test, disjunctive 2-DF test, and conjunctive test.

Result: P-value adjustment methods are always less powerful than combined outcomes and single 1-DF test. Disjunctive 2-DF test has higher power when treatment effects are unequal, while single 1-DF test performs better when treatment effect sizes are equal.

Conclusion: The study provides clear guidance for method selection in hybrid type 2 studies: use disjunctive 2-DF test for unequal treatment effects and single 1-DF test for equal treatment effects, while avoiding p-value adjustment methods due to their lower power.

Abstract: Hybrid type 2 studies are gaining popularity for their ability to assess both
implementation and health outcomes as co-primary endpoints. Often conducted as
cluster-randomized trials (CRTs), five design methods can validly power these
studies: p-value adjustment methods, combined outcomes approach, single
weighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared
all of the methods theoretically and numerically. Theoretical comparisons of
the power equations allowed us to identify if any method globally had more or
less power than other methods. It was shown that the p-value adjustment methods
are always less powerful than the combined outcomes approach and the single
1-DF test. We also identified the conditions under which the disjunctive 2-DF
test is less powerful than the single 1-DF test. Because our theoretical
comparison showed that some methods could be more powerful than others under
certain conditions, and less powerful under others, we conducted a numerical
study to understand these differences. The crt2power R package was created to
calculate the power or sample size for CRTs with two continuous co-primary
endpoints. Using this package, we conducted a numerical evaluation across
30,000 input scenarios to compare statistical power. Specific patterns were
identified where a certain method consistently achieved the highest power. When
the treatment effects are unequal, the disjunctive 2-DF test tends to have
higher power. When the treatment effect sizes are the same, the single 1-DF
test tends to have higher power. Together, these comparisons provide clearer
insights to guide method selection for powering hybrid type 2 studies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: This paper develops new f-DP-based privacy accounting methods for decentralized federated learning, addressing challenges in quantifying privacy budgets due to complex algorithmic components like decentralized communication and local updates.


<details>
  <summary>Details</summary>
Motivation: Accurately quantifying privacy budgets in differentially private decentralized federated learning is challenging due to complex algorithmic components like decentralized communication and local updates, which existing methods struggle to capture effectively.

Method: The authors develop two new f-DP-based accounting methods: Pairwise Network f-DP (PN-f-DP) for quantifying privacy leakage between user pairs under random-walk communication, and Secret-based f-Local DP (Sec-f-LDP) for supporting structured noise injection via shared secrets. They combine tools from f-DP theory and Markov chain concentration.

Result: Experiments on synthetic and real datasets show that the proposed methods yield consistently tighter (ε,δ) bounds and improved utility compared to Rényi DP-based approaches, demonstrating the benefits of f-DP in decentralized privacy accounting.

Conclusion: The f-DP framework provides superior privacy accounting for decentralized federated learning, capturing privacy amplification from sparse communication, local iterations, and correlated noise more effectively than existing approaches.

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [18] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: NAS-DQN integrates neural architecture search into DRL training, enabling dynamic network reconfiguration that outperforms fixed architectures in performance, efficiency, and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional DRL agents use fixed neural architectures chosen through expensive hyperparameter searches, which constrains performance and cannot adapt during training.

Method: Introduces NAS-DQN, which integrates a learned neural architecture search controller directly into the DRL training loop to enable dynamic network reconfiguration based on cumulative performance feedback.

Result: NAS-DQN achieves superior final performance, sample efficiency, and policy stability compared to fixed-architecture baselines and random search, with negligible computational overhead.

Conclusion: Architecture adaptation is necessary for optimal sample efficiency in online DRL, and RL agent design can be integrated as a dynamic component of the learning process rather than a static offline choice.

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>


### [19] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: ReGraphT is a training-free framework that enables small language models (SLMs) to generate optimized CUDA code by leveraging retrieval-augmented generation and Monte Carlo Graph Search, achieving LLM-level performance without privacy risks or excessive computing costs.


<details>
  <summary>Details</summary>
Motivation: While LLMs can generate optimized CUDA code, they face privacy risks (cloud APIs) and computational inefficiency (local deployment). SLMs are more lightweight and privacy-friendly but lack reasoning abilities for complex CUDA generation tasks.

Method: ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph modeling state transitions, and uses Monte Carlo Graph Search for efficient exploration. It transfers LLM-level reasoning to smaller models without training.

Result: ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving 2.33X average speedup on CUDAEval and ParEval benchmarks. When paired with specific SLMs, it enables them to approach LLM-level performance.

Conclusion: ReGraphT successfully bridges the gap between SLMs and LLMs for CUDA code generation, providing LLM-level reasoning capabilities to smaller models while maintaining privacy and computational efficiency.

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [20] [From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem](https://arxiv.org/abs/2510.19889)
*Mostafa Ameli,Van Anh Le,Sulthana Shams,Alexander Skabardonis*

Main category: cs.LG

TL;DR: This paper introduces a Transformer-based deep learning approach to predict equilibrium path flows in traffic assignment, offering significant computational speedup over traditional optimization methods while maintaining accuracy and adaptability to changing network conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic assignment methods become computationally prohibitive for large-scale networks due to non-linear complexity growth with OD pairs, creating a need for more efficient approaches.

Method: The study proposes a data-driven approach using deep neural networks with Transformer architecture to directly predict equilibrium path flows, capturing intricate correlations between OD pairs at the path level rather than traditional link-level analysis.

Result: Numerical experiments on Manhattan-like synthetic, Sioux Falls, and Eastern-Massachusetts networks show the model is orders of magnitude faster than conventional optimization while efficiently estimating path-level traffic flows in multi-class networks with improved prediction accuracy.

Conclusion: The Transformer-based model drastically reduces computation time, adapts to demand and network changes without recalculation, and enables rapid 'what-if' analyses for enhanced transportation planning and policy-making.

Abstract: The traffic assignment problem is essential for traffic flow analysis,
traditionally solved using mathematical programs under the Equilibrium
principle. These methods become computationally prohibitive for large-scale
networks due to non-linear growth in complexity with the number of OD pairs.
This study introduces a novel data-driven approach using deep neural networks,
specifically leveraging the Transformer architecture, to predict equilibrium
path flows directly. By focusing on path-level traffic distribution, the
proposed model captures intricate correlations between OD pairs, offering a
more detailed and flexible analysis compared to traditional link-level
approaches. The Transformer-based model drastically reduces computation time,
while adapting to changes in demand and network structure without the need for
recalculation. Numerical experiments are conducted on the Manhattan-like
synthetic network, the Sioux Falls network, and the Eastern-Massachusetts
network. The results demonstrate that the proposed model is orders of magnitude
faster than conventional optimization. It efficiently estimates path-level
traffic flows in multi-class networks, reducing computational costs and
improving prediction accuracy by capturing detailed trip and flow information.
The model also adapts flexibly to varying demand and network conditions,
supporting traffic management and enabling rapid `what-if' analyses for
enhanced transportation planning and policy-making.

</details>


### [21] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: FairGRPO is a hierarchical reinforcement learning method that addresses fairness issues in medical AI by using adaptive importance weighting and unsupervised clustering to reduce performance disparities across demographic groups, achieving 27.2% better predictive parity and 12.49% higher F1 score.


<details>
  <summary>Details</summary>
Motivation: Medical AI systems exhibit performance disparities across demographic groups, causing harm to underrepresented populations, and existing multimodal reasoning models amplify biases from training data dominated by majority populations.

Method: FairGRPO employs hierarchical reinforcement learning with adaptive importance weighting based on representation, task difficulty, and data source, plus unsupervised clustering to discover latent demographic groups when labels are unavailable.

Result: Across 7 clinical datasets spanning 5 modalities, FairGRPO reduces predictive parity by 27.2% against all baselines while improving F1 score by 12.49%, and training dynamics show progressive fairness improvement.

Conclusion: FairGRPO effectively addresses fairness issues in medical AI, leading to the release of FairMedGemma-4B, a fairness-aware clinical VLLM that achieves state-of-the-art performance with significantly reduced demographic disparities.

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [22] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER is a classification framework for noisy datasets that uses stochastic analysis and Hilbert space mapping to create stochastic features, then applies KLE decomposition for eigen-based classification, achieving state-of-the-art results in Alzheimer's disease and deforestation detection.


<details>
  <summary>Details</summary>
Motivation: Addressing classification challenges in noisy datasets with low signal-to-noise ratios, small sample sizes, and faulty data collection, which remain key research frontiers with both theoretical and practical implications.

Method: Creates stochastic features by viewing datasets as realizations from underlying random fields, maps them to Hilbert spaces, uses Kosambi-Karhunen-Loève expansion to break features into irreducible components, and performs classification via eigen-decomposition by analyzing operator spectra.

Result: Achieved state-of-the-art breakthroughs in Alzheimer's Disease stage classification and remote sensing detection of deforestation on challenging, data-deficient scientific domains.

Conclusion: FINDER provides a rigorous framework for noisy dataset classification with discussion on when it outperforms existing methods, its failure modes, and limitations.

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [23] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: This paper provides the first theoretical analysis of Muon optimizer's inexact orthogonalization, revealing that approximation precision affects optimal learning rates and momentum parameters.


<details>
  <summary>Details</summary>
Motivation: There's a theory-practice disconnect in Muon optimizer analysis - prior work studied idealized exact SVD updates, but practical implementations use fast approximate orthogonalization that was not theoretically analyzed.

Method: The authors analyze inexact orthogonalized updates within the Linear Minimization Oracle (LMO) framework, using an additive error model to capture practical approximation schemes. They develop explicit bounds quantifying performance degradation from LMO inexactness.

Result: The analysis reveals fundamental coupling between approximation inexactness and optimal step size/momentum: lower precision requires smaller step sizes but larger momentum. NanoGPT experiments confirm this predicted coupling, showing optimal learning rates shift with approximation precision.

Conclusion: The approximation procedure (e.g., number of Newton-Schulz steps) should be treated as a critical parameter that must be co-tuned with learning schedules, rather than just an implementation detail.

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [24] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: The paper proposes a contextual Markov decision process framework with delayed Poisson rewards to model online ad bidding, addressing delayed/long-term effects, cumulative impacts, and customer heterogeneity. It develops a two-stage maximum likelihood estimator and reinforcement learning algorithm for personalized bidding strategies with near-optimal regret bounds.


<details>
  <summary>Details</summary>
Motivation: Online advertising platforms require effective bidding strategies, but existing approaches often fail to jointly consider three critical factors: delayed and long-term ad effects, cumulative impacts (reinforcement/fatigue), and customer heterogeneity.

Method: Model ad bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson rewards. Use a two-stage maximum likelihood estimator with data-splitting strategies for efficient estimation, then design a reinforcement learning algorithm for personalized bidding strategies.

Result: The approach achieves a near-optimal regret bound of $\tilde{O}{(dH^2\sqrt{T})}$, where d is contextual dimension, H is number of rounds, and T is number of customers. Simulation experiments validate the theoretical findings.

Conclusion: The proposed CMDP framework with delayed Poisson rewards effectively captures complex ad impact dynamics, and the developed estimation and learning methods provide efficient personalized bidding strategies with strong theoretical guarantees.

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [25] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: This paper analyzes greedy task orderings in continual learning for linear regression, showing they converge faster than random orderings but with important nuances - single-pass greedy can fail catastrophically while greedy with repetition converges at O(1/∛k) rate.


<details>
  <summary>Details</summary>
Motivation: To understand task orderings in continual learning, particularly greedy orderings that maximize dissimilarity between consecutive tasks, which were briefly explored in prior work but still have open questions.

Method: Uses tools from Kaczmarz method literature to formalize greedy orderings, develops geometric and algebraic intuitions, and conducts empirical analysis on linear regression and CIFAR-100 classification, plus analytical proofs for different rank settings.

Result: Greedy orderings converge faster than random ones in average loss across tasks. In high-rank settings, greedy has similar bounds to random, but under general rank: single-pass greedy can fail catastrophically while greedy with repetition converges at O(1/∛k) rate vs random's O(1/√k).

Conclusion: Reveals important nuances in task ordering strategies - greedy orderings offer faster convergence but require careful handling (repetition) to avoid catastrophic failure, showing complex trade-offs between greedy and random approaches in continual learning.

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [26] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: This paper analyzes the selective-classification gap - the performance shortfall between practical selective classifiers and ideal oracle performance. It decomposes this gap into five error sources and shows that monotone calibration has limited impact since it doesn't reorder predictions.


<details>
  <summary>Details</summary>
Motivation: To understand why practical selective classifiers fall short of perfect oracle performance and identify actionable ways to close this gap.

Method: Formalizes the selective-classification gap and provides finite-sample decomposition into five error sources: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift-induced slack. Validates through controlled experiments on synthetic and real-world datasets.

Result: Bayes noise and limited model capacity account for substantial gaps; only feature-aware calibrators meaningfully improve score ordering; data shift introduces separate slack requiring robust training. Monotone calibration rarely alters underlying rankings.

Conclusion: The decomposition provides quantitative error budget and design guidelines for building selective classifiers that better approximate ideal oracle behavior, emphasizing the need for scoring mechanisms that can effectively reorder predictions rather than just rescale them.

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [27] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: This paper addresses the performance degradation of RL agents in financial markets due to market impact - the mismatch between training on historical data and deployment where agent actions influence prices. The authors develop elliptic uncertainty sets to capture directional market impact and provide efficient robust policy evaluation methods.


<details>
  <summary>Details</summary>
Motivation: RL agents trained on historical financial data face performance degradation during live deployment because their trading actions can shift asset prices (market impact), creating a mismatch between training and deployment environments that traditional robust RL methods fail to address due to their reliance on symmetric uncertainty structures.

Method: The authors develop a novel class of elliptic uncertainty sets that capture the directional nature of market impact. They establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation.

Result: Experiments on single-asset and multi-asset trading tasks demonstrate that the proposed method achieves superior Sharpe ratio and remains robust under increasing trade volumes, outperforming traditional approaches.

Conclusion: The elliptic uncertainty sets provide a more faithful and scalable approach to RL in financial markets by properly capturing directional market impact and enabling efficient robust policy evaluation that maintains performance under realistic trading conditions.

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [28] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: Proposes a novel family of unbiased gradient estimators for zeroth-order optimization that eliminate bias while maintaining favorable variance, achieving optimal complexity for smooth non-convex objectives.


<details>
  <summary>Details</summary>
Motivation: Existing zeroth-order optimization methods suffer from inherent bias in gradient estimators unless perturbation stepsize vanishes, limiting their effectiveness.

Method: Reformulates directional derivatives as telescoping series and samples from carefully designed distributions to construct unbiased gradient estimators based solely on function evaluations.

Result: The proposed estimators eliminate bias while maintaining favorable variance, with SGD using these estimators achieving optimal complexity for smooth non-convex objectives. Experiments show superior accuracy and convergence compared to standard methods.

Conclusion: The novel family of unbiased gradient estimators overcomes the biasedness limitation in existing zeroth-order optimization methods, providing more accurate and efficient optimization when gradients are unavailable or expensive to compute.

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [29] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: This paper introduces directionally aligned perturbations (DAPs) for zeroth-order gradient estimation, showing they minimize asymptotic variance and outperform traditional fixed-length perturbations in certain conditions.


<details>
  <summary>Details</summary>
Motivation: Existing research has focused on fixed-length perturbations for zeroth-order gradient estimation, overlooking the potential advantages of directional alignment with the true gradient.

Method: The authors formulate a constrained functional optimization problem to find perturbation distributions that minimize asymptotic variance, and propose the directionally aligned perturbation (DAP) scheme that adaptively offers higher accuracy along critical directions.

Result: Theoretical analysis shows DAPs can align directionally with the true gradient instead of maintaining fixed length. Empirical evaluations on synthetic and practical tasks demonstrate DAPs outperform traditional methods under specific conditions.

Conclusion: Directionally aligned perturbations provide superior performance for zeroth-order gradient estimation compared to traditional fixed-length approaches, with theoretical guarantees and empirical validation.

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [30] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: UCAN introduces anisotropic noise distributions for randomized smoothing to improve certified adversarial robustness, achieving up to 182.6% improvement in certified accuracy over isotropic methods.


<details>
  <summary>Details</summary>
Motivation: Current randomized smoothing methods use isotropic noise that treats all data dimensions uniformly, limiting effectiveness by ignoring input heterogeneity and dimension-specific characteristics.

Method: UCAN transforms existing randomized smoothing methods from symmetric (isotropic) to asymmetric (anisotropic) noise distributions using noise parameter generators to fine-tune noise parameters per data dimension.

Result: Empirical evaluations show up to 182.6% improvement in certified accuracy at large certified radii on MNIST, CIFAR10, and ImageNet datasets compared to state-of-the-art methods.

Conclusion: UCAN provides a versatile framework for enhancing certified adversarial robustness through tailored anisotropic noise distributions, significantly outperforming existing isotropic approaches.

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [31] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: This paper challenges the conventional 'long-sequence information gain hypothesis' in time series forecasting by showing that truncating historical data can improve accuracy. The authors propose AMRC (Adaptive Masking Loss with Representation Consistency) to suppress redundant feature learning and enhance model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations in current deep learning approaches for time series forecasting, where the prevailing assumption that longer historical sequences provide better information is shown to be flawed. The study reveals that existing models learn substantial redundant features like noise and irrelevant fluctuations, compromising effective signal extraction.

Method: The proposed method is AMRC (Adaptive Masking Loss with Representation Consistency) with two core components: 1) Dynamic masking loss that adaptively identifies highly discriminative temporal segments to guide gradient descent during training, and 2) Representation consistency constraint that stabilizes mapping relationships among inputs, labels, and predictions.

Result: Experimental results demonstrate that AMRC effectively suppresses redundant feature learning while significantly improving model performance. The approach challenges conventional assumptions in temporal modeling and provides improved forecasting accuracy.

Conclusion: This work challenges conventional assumptions in temporal modeling and provides novel theoretical insights and methodological breakthroughs for developing efficient and robust forecasting models. The proposed AMRC framework offers a new approach to time series forecasting by focusing on relevant temporal segments rather than simply accumulating longer historical data.

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [32] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: This paper presents a supervised learning approach using Decision Tree classification on RFID RSSI data to track defense assets in a CAD-modeled environment, achieving 34.2% accuracy in classifying 12 lab zones with challenges in rare class classification.


<details>
  <summary>Details</summary>
Motivation: RFID tracking offers potential for defense asset management but suffers from poor sensor specificity issues like long range detection, spoofing, and counterfeiting that can lead to security breaches and operational errors.

Method: Used supervised learning with Decision Tree classification on realistic RSSI data from approximately 980,000 RFID reads in a CAD-modeled floor plan. Applied class weighting to handle imbalanced data and trained on stratified subsamples of 5,000 balanced observations across 12 lab zones.

Result: Achieved overall accuracy of 34.2% with F1-scores above 0.40 for multiple zones (F, G, H, etc.). Rare classes like LabZoneC were frequently misclassified despite class weighting. Created adjacency-aware confusion matrix to better interpret physically adjacent zone misclassifications.

Conclusion: RSSI-based decision trees show promise for zone-level anomaly detection and misplacement monitoring in defense logistics, but performance in low-coverage areas could be improved through better antenna placement, additional sensors, or sensor fusion with other modalities.

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [33] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: SALT is a lightweight framework that provides fine-grained advantage assignment for group-based RL algorithms using outcome rewards, improving performance on complex multi-step tasks without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for LLMs in complex tasks rely on sparse outcome-based rewards, which can lead to training instability and suboptimal policies when uniformly rewarding/penalizing all actions in trajectories where beneficial and detrimental actions are entangled.

Method: SALT constructs a graph from trajectories of the same prompt to quantify step quality and assign advantages accordingly, serving as a plug-and-play module for existing group-based RL algorithms without modifying rollout procedures.

Result: Extensive experiments on WebShop, ALFWorld, and AppWorld benchmarks with various model sizes show SALT consistently improves performance, with thorough analysis validating design choices.

Conclusion: SALT effectively addresses the limitation of uniform reward assignment in group-based RL algorithms by providing fine-grained advantage assignment, enhancing training stability and policy quality for complex multi-step tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [34] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: Proposes a rejection sampling algorithm for exact parallel sampling from temporal point process models without architectural changes or retraining.


<details>
  <summary>Details</summary>
Motivation: Autoregressive temporal point process models require sequential sampling, limiting efficiency for large-scale applications.

Method: Novel rejection sampling algorithm that enables exact parallel sampling of multiple future values from existing TPP models.

Result: Empirical speedups on real-world datasets with theoretical guarantees.

Conclusion: Bridges the gap between expressive modeling and efficient parallel generation for large-scale TPP applications.

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [35] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: Online draft model selection algorithm for speculative decoding that competes with the best draft model in hindsight, improving exponentially over bandit-based approaches and working with any speculative decoding method.


<details>
  <summary>Details</summary>
Motivation: To address the online draft model selection problem in speculative decoding for accelerating large language model inference, improving efficiency over existing approaches.

Method: Designs an algorithm that accurately evaluates all draft models without additional target model queries, with system-efficient versions to reduce computation and latency overhead.

Result: Substantially outperforms state-of-the-art EAGLE3 and BanditSpec baselines across diverse datasets and LLMs, especially in domains requiring long reasoning chains with specialized drafters.

Conclusion: The proposed online draft model selection approach provides significant performance improvements in speculative decoding, particularly when multiple domain-expert drafters are available for complex reasoning tasks.

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [36] [A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers](https://arxiv.org/abs/2510.20066)
*Yimeng Qiu,Feihuang Fang*

Main category: cs.LG

TL;DR: This paper analyzes how liquidity and volatility proxies from core cryptoassets generate spillovers that forecast market-wide risk using a multi-layer statistical framework with VAR, HAR-X models, and machine learning.


<details>
  <summary>Details</summary>
Motivation: To study whether liquidity and volatility measures from core cryptoassets can predict broader market risk through spillover effects in the cryptocurrency market.

Method: Three-layer statistical framework: (A) core liquidity-return interactions, (B) principal-component relations, (C) volatility-factor projections; complemented by VAR impulse responses, HAR-X models, and leakage-safe machine learning with temporal splits and SHAP interpretation.

Result: Statistically significant Granger-causal relationships across all layers and moderate out-of-sample predictive accuracy using daily data from 2021-2025 (1462 observations across 74 assets).

Conclusion: Liquidity and volatility proxies from core cryptoassets generate meaningful spillovers that can forecast market-wide risk, with documented statistical significance and moderate predictive power.

Abstract: We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.

</details>


### [37] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: The paper introduces CTAE, a transformer-based model that separates shared and region-specific neural dynamics from multi-region recordings, outperforming existing methods in behavioral decoding.


<details>
  <summary>Details</summary>
Motivation: Current methods either ignore temporal structure in multi-region neural recordings or fail to properly separate shared and private signals across brain regions.

Method: Coupled Transformer Autoencoder (CTAE) uses transformer encoders/decoders to capture long-range neural dynamics and explicitly partitions latent space into orthogonal shared and private subspaces.

Result: CTAE extracts meaningful representations that better decode behavioral variables compared to existing approaches on motor and sensory cortical recordings.

Conclusion: CTAE effectively addresses non-stationary, non-linear neural dynamics while separating shared versus region-specific structure in a unified framework.

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [38] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX is a framework for explaining time series classification models by identifying key shapelets (subsequences) and using Shapley values to assess their importance, providing more precise and causally faithful explanations than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current post-hoc time series explanation methods focus on timestep-level feature attribution but overlook that classification outcomes are primarily driven by key shapelets, which limits transparency and trust in high-stakes applications like healthcare and finance.

Method: ShapeX segments time series into meaningful shapelet-driven segments using the Shapelet Describe-and-Detect (SDD) framework to learn diverse shapelets, then employs Shapley values to assess their saliency for explanation.

Result: Experiments on synthetic and real-world datasets show ShapeX outperforms existing methods in identifying relevant subsequences, improving both precision and causal fidelity of explanations by revealing causal relationships rather than just correlations.

Conclusion: ShapeX successfully bridges the gap in time series explanation by focusing on shapelet-level attribution, providing more transparent and trustworthy explanations that capture the fundamental drivers of classification outcomes.

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [39] [Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa](https://arxiv.org/abs/2510.20085)
*Chang Yang,Ziyi Wang,Wangfeng Tan,Zhiting Tan,Changrui Ji,Zhiming Zhou*

Main category: cs.LG

TL;DR: A hierarchical dual-head neural network using MentalRoBERTa for suicide risk classification with CORAL and standard classification heads, addressing class imbalance and temporal patterns in social media posts.


<details>
  <summary>Details</summary>
Motivation: Social media platforms are important for suicide risk detection, but automated systems face challenges with class imbalance, temporal complexity in posting patterns, and the dual nature of risk levels as both ordinal and categorical.

Method: Proposes a hierarchical dual-head neural network based on MentalRoBERTa with two prediction heads: CORAL head for ordinal relationships and standard classification head for categorical distinctions. Uses 3-layer Transformer encoder with multi-head attention and time interval embeddings, trained with combined loss function (CORAL + Cross-Entropy + Focal Loss).

Result: Model evaluated using 5-fold stratified cross-validation with macro F1 score as primary metric. Computational efficiency improved by freezing first 6 layers of MentalRoBERTa and using mixed-precision training.

Conclusion: The proposed approach effectively addresses multiple challenges in suicide risk classification from social media data, including ordinal-categorical duality, temporal dependencies, and class imbalance through a carefully designed neural architecture and training strategy.

Abstract: Social media platforms have become important sources for identifying suicide
risk, but automated detection systems face multiple challenges including severe
class imbalance, temporal complexity in posting patterns, and the dual nature
of risk levels as both ordinal and categorical. This paper proposes a
hierarchical dual-head neural network based on MentalRoBERTa for suicide risk
classification into four levels: indicator, ideation, behavior, and attempt.
The model employs two complementary prediction heads operating on a shared
sequence representation: a CORAL (Consistent Rank Logits) head that preserves
ordinal relationships between risk levels, and a standard classification head
that enables flexible categorical distinctions. A 3-layer Transformer encoder
with 8-head multi-head attention models temporal dependencies across post
sequences, while explicit time interval embeddings capture posting behavior
dynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3
Cross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure
preservation, overconfidence reduction, and class imbalance. To improve
computational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa
and employ mixed-precision training. The model is evaluated using 5-fold
stratified cross-validation with macro F1 score as the primary metric.

</details>


### [40] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: A game-theoretic reinforcement learning framework for causal discovery that combines strong empirical performance with finite-sample guarantees, outperforming baseline methods while maintaining theoretical safety.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between empirically strong causal discovery methods (like GES and GraN-DAG) that lack finite-sample guarantees and theoretically principled approaches that fail to scale.

Method: A DDQN agent directly competes against strong baselines (GES or GraN-DAG), always warm-starting from the opponent's solution in a game-theoretic reinforcement learning framework.

Result: The method consistently improves upon GES and GraN-DAG on real-world benchmarks (Sachs, Asia, Alarm, Child, Hepar2, Dream, Andes) and scales to large graphs (up to 220 nodes). Observed error probability decays with sample size, matching theoretical predictions.

Conclusion: Establishes a new class of RL-based causal discovery algorithms that are simultaneously provably consistent, sample-efficient, and practically scalable, unifying empirical performance with rigorous finite-sample theory.

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [41] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: This paper analyzes distance measure norms and dimension weights in pattern classification, proposes a novel dimension weighting scheme, incorporates it into KNN classifier, and shows improved performance (especially 10% gain for gene expression datasets) compared to traditional KNN.


<details>
  <summary>Details</summary>
Motivation: Traditional Euclidean distance in pattern classification often faces issues, and weighted dimension-based distance measures are crucial for accurately reflecting similarity between multi-dimensional samples in diverse applications.

Method: Developed a novel dimension weighting scheme and incorporated it into KNN classifier using weighted Minkowski distance, regulating the shape and size of regions enclosing k nearest neighbors.

Result: The proposed method performed well across diverse experiments, with significant 10% classification accuracy gain for gene expression datasets in all cross-validation experiments with different k values.

Conclusion: The developed weighting schema with appropriate norm provides an important generalization of KNN classifier that effectively handles high-dimensional datasets with limited samples by enabling meaningful selection of nearest neighbors.

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [42] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: The paper addresses prototype collapse in self-supervised learning by proposing a decoupled training strategy that separates prototype learning from encoder optimization, using an online EM-style procedure for prototypes.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning methods suffer from partial prototype collapse where multiple prototypes converge to similar representations, undermining their purpose of providing diverse targets. Current solutions like over-parameterization or ad-hoc regularizers only treat symptoms rather than addressing the root cause.

Method: Introduces a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Prototypes are modeled as a Gaussian mixture updated with an online EM-style procedure independent of the encoder's loss, breaking the joint optimization that causes collapse.

Result: The decoupling eliminates prototype collapse without explicit regularization, yielding consistently diverse prototypes and stronger downstream performance compared to existing methods.

Conclusion: Breaking the joint optimization between encoders and prototypes through decoupled training effectively addresses the root cause of prototype collapse, providing a principled solution that outperforms symptom-mitigating approaches.

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [43] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: Current timeseries foundation models underperform compared to simple supervised baselines due to improper adaptation of NLP/CV pipelines. The key issue is that timeseries data lacks the semantic richness of web-scale text/image corpora, requiring a shift from opportunistic data scraping to principled dataset design based on temporal invariances.


<details>
  <summary>Details</summary>
Motivation: Timeseries foundation models have proliferated but fail to outperform lightweight supervised baselines and classical models. This performance gap stems from naively importing NLP or CV pipelines without considering fundamental differences in data characteristics.

Method: Propose a shift from opportunistic data aggregation to principled dataset design that systematically spans the space of temporal invariances. Suggest building timeseries invariance ontology based on first principles to ensure representational completeness.

Result: Identifies that the scrape-everything-online paradigm fails for timeseries because timeseries data lacks the semantic concepts found in web-scale text/image corpora (e.g., no timeseries dataset contains the concept 'apple').

Conclusion: Progress in timeseries foundation models requires ensuring representational completeness through systematic coverage of temporal invariances, which will enable aligned structure necessary for generalization, reasoning, and emergent behavior.

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [44] [Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling](https://arxiv.org/abs/2510.20148)
*Tingting Dan,Xinwei Huang,Jiaqi Ding,Yinggang Zheng,Guorong Wu*

Main category: cs.LG

TL;DR: This study reveals how structural and functional connectivity interact to influence tau protein spread in Alzheimer's disease, showing regionally asymmetric contributions that shift during disease progression and align with AD-associated gene expression.


<details>
  <summary>Details</summary>
Motivation: To understand how structural connectivity (SC) and functional connectivity (FC) interact to influence tau protein propagation in Alzheimer's disease, as emerging evidence shows tau builds up along specific brain networks but the SC-FC interaction mechanisms remain unclear.

Method: Used longitudinal neuroimaging data with a multi-layer graph diffusion model to examine SC-FC interactions in tau propagation across brain networks.

Result: Revealed regionally asymmetric contributions: FC drives tau spread in subcortical areas, insula, frontal and temporal cortices, while SC dominates in occipital, parietal and limbic regions. The SC-FC dominance shifts during disease progression, with FC prevailing in early AD and SC becoming primary in later stages. These patterns align with AD-associated gene expression involved in inflammation, apoptosis and lysosomal function.

Conclusion: Structural and functional connectivity have distinct regional roles in tau propagation that shift during Alzheimer's disease progression, with spatial patterns that strongly correspond to AD-associated genetic risk factors and biological mechanisms, providing insights into network-level drivers of disease progression.

Abstract: Emerging neuroimaging evidence shows that pathological tau proteins build up
along specific brain networks, suggesting that large-scale network architecture
plays a key role in the progression of Alzheimer's disease (AD). However, how
structural connectivity (SC) and functional connectivity (FC) interact to
influence tau propagation remains unclear. Leveraging an unprecedented volume
of longitudinal neuroimaging data, we examine SC-FC interactions through a
multi-layer graph diffusion model. Beyond showing that connectome architecture
constrains tau spread, our model reveals a regionally asymmetric contribution
of SC and FC. Specifically, FC predominantly drives tau spread in subcortical
areas, the insula, frontal and temporal cortices, whereas SC plays a larger
role in occipital, parietal, and limbic regions. The relative dominance of SC
versus FC shifts over the course of disease, with FC generally prevailing in
early AD and SC becoming primary in later stages. Spatial patterns of SC- and
FC-dominant regions strongly align with the regional expression of
AD-associated genes involved in inflammation, apoptosis, and lysosomal
function, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In
parallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and
biological mechanisms (e.g., amyloid deposition) selectively reshape tau
propagation by shifting dominant routes between anatomical and functional
pathways in a region-specific manner. Findings are validated in an independent
AD cohort.

</details>


### [45] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: Proposes ADP-VRSGP, an adaptive differential privacy method for decentralized learning that dynamically adjusts noise variance and learning rate using stepwise decay, improving training efficiency and model performance while providing node-level privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized learning approaches with fixed-variance differential privacy noise degrade model performance and reduce training efficiency, creating a need for adaptive methods that can maintain privacy while improving learning outcomes.

Method: ADP-VRSGP uses stepwise-decaying schedules for noise variance and learning rate, progressive gradient fusion leveraging historical gradients, and incorporates decentralized push-sum and aggregation techniques suitable for time-varying communication topologies.

Result: Theoretical analysis shows robust convergence with appropriate learning rate, significantly improving training stability and speed. Experimental results demonstrate superior performance over existing baselines across multiple scenarios.

Conclusion: ADP-VRSGP effectively addresses privacy-preserving decentralized learning challenges by providing adaptive differential privacy that enhances training efficiency and final model performance while maintaining node-level privacy guarantees.

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [46] [Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP](https://arxiv.org/abs/2510.20169)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: HyperNS method uses hyper tour guidance and clustering to efficiently solve large-scale TSP instances, outperforming existing neural approaches.


<details>
  <summary>Details</summary>
Motivation: Address challenges in scaling neural methods for TSP, including memory constraints with global heatmaps, poor initial solutions, and insufficient global guidance for large search spaces.

Method: Divide TSP into clusters using sparse heatmap graph, abstract clusters as supernodes, generate hyper tour to guide initialization and optimization, reducing search space by focusing on hyper tour-relevant edges.

Result: Outperforms existing neural-based methods on synthetic and real-world datasets, especially for larger instances, with significant reduction in gap to optimal solution.

Conclusion: HyperNS provides an effective approach for large-scale TSP by combining clustering strategy with hyper tour guidance, enabling more efficient optimization through reduced search space.

Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has
garnered significant attention from both academia and industry. While
neural-based methods have shown promise for solving TSPs, they still face
challenges in scaling to larger instances, particularly in memory constraints
associated with global heatmaps, edge weights, or access matrices, as well as
in generating high-quality initial solutions and insufficient global guidance
for efficiently navigating vast search spaces. To address these challenges, we
propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for
large-scale TSP instances. Inspired by the ``clustering first, route second"
strategy, our approach initially divides the TSP instance into clusters using a
sparse heatmap graph and abstracts them as supernodes, followed by the
generation of a hyper tour to guide both the initialization and optimization
processes. This method reduces the search space by focusing on edges relevant
to the hyper tour, leading to more efficient and effective optimization.
Experimental results on both synthetic and real-world datasets demonstrate that
our approach outperforms existing neural-based methods, particularly in
handling larger-scale instances, offering a significant reduction in the gap to
the optimal solution.

</details>


### [47] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: RLEV extends RLVR by incorporating human value signals into reward functions, enabling LLMs to optimize for both correctness and importance of tasks, resulting in value-sensitive termination policies.


<details>
  <summary>Details</summary>
Motivation: Current RL methods like RLVR focus only on binary correctness rewards but ignore that tasks have different importance levels, failing to align with human priorities.

Method: Extends RLVR framework by adding human-defined value signals to reward functions, using exam-style data with explicit value labels, and analyzing value-weighted gradient amplification on end-of-sequence tokens.

Result: Outperforms correctness-only baselines across multiple RL algorithms and model scales, achieves value-sensitive termination (concise for low-value, thorough for high-value prompts), and remains robust under noisy value signals.

Conclusion: Optimizing for explicit utility functions provides a practical approach to align LLMs with human priorities through value-weighted training.

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [48] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: This paper proposes a risk-aware constrained reinforcement learning framework using optimized certainty equivalents (OCEs) to address tail risks in reward distributions, ensuring joint per-stage robustness in rewards and time.


<details>
  <summary>Details</summary>
Motivation: Standard constrained RL based on expected accumulated rewards neglects tail risks and catastrophic events, which is insufficient for high-stakes applications where outlier risks are critical.

Method: The framework uses optimized certainty equivalents (OCEs) to provide per-stage robustness, employs a parameterized strong Lagrangian duality approach, and wraps around standard RL solvers like PPO.

Result: The method ensures exact equivalence to the original constrained problem under appropriate constraint qualifications and demonstrates risk-aware properties through numerical experiments.

Conclusion: The proposed risk-aware constrained RL framework effectively addresses tail risks, provides convergence guarantees, and can be practically implemented with standard RL algorithms.

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [49] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: This paper proposes three relaxations of replicability in PAC learning to overcome strong impossibility results, showing that pointwise, approximate, and semi-replicable learning can achieve sample-optimal agnostic PAC learning with different sample complexities.


<details>
  <summary>Details</summary>
Motivation: Replicability requires algorithms to remain stable under input resampling, but this strong notion has prohibitive costs - there are no replicable algorithms even for simple tasks like threshold learning. The authors seek to understand under what approximate notions of replicability learning becomes possible.

Method: The authors propose three relaxations of replicability: (1) Pointwise replicability - consistent on any fixed input but not across all inputs simultaneously, (2) Approximate replicability - output hypotheses that classify most of the distribution consistently, (3) Semi-replicability - fully replicable but may use shared unlabeled samples.

Result: For constant replicability parameters, the authors obtain sample-optimal agnostic PAC learners: Pointwise and approximate replicability are achievable with Θ(d/α²) samples, while semi-replicability requires Θ(d²/α²) labeled samples.

Conclusion: The paper demonstrates that by relaxing the strict notion of replicability to pointwise, approximate, or semi-replicability, it becomes possible to achieve sample-optimal agnostic PAC learning, with different sample complexities depending on the relaxation used.

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [50] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: This study evaluated machine learning approaches for canine cancer detection using routine lab data from the Golden Retriever Lifetime Study. The best model achieved moderate ranking ability but poor clinical performance due to weak cancer signals confounded by aging and inflammation.


<details>
  <summary>Details</summary>
Motivation: To develop accessible screening tools for early cancer detection in dogs using routine laboratory data, addressing challenges of non-specific biomarkers and severe class imbalance in screening populations.

Method: Comprehensive benchmark evaluation of 126 analytical pipelines using various machine learning models, feature selection methods, and data balancing techniques. Data were partitioned at patient level to prevent leakage, using the Golden Retriever Lifetime Study cohort.

Result: Optimal model (Logistic Regression with class weighting and recursive feature elimination) showed moderate ranking ability (AUROC = 0.815) but poor clinical performance (F1-score = 0.25, PPV = 0.15). High NPV (0.98) but insufficient recall (0.79) for reliable rule-out testing. SHAP analysis revealed predictions driven by non-specific features like age, inflammation, and anemia markers.

Conclusion: A statistically detectable cancer signal exists in routine lab data but is too weak and confounded for clinically reliable discrimination from normal aging or other inflammatory conditions. This establishes a performance ceiling for this data modality alone and highlights the need for multi-modal data integration in computational veterinary oncology.

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [51] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: CO-PFL is a personalized federated learning algorithm that dynamically estimates client contributions using dual-subspace analysis of gradient directions and prediction deviations, providing principled aggregation weights and enhancing personalization through parameter-wise mechanisms and mask-aware momentum optimization.


<details>
  <summary>Details</summary>
Motivation: Conventional federated learning with single consensus models and heuristic aggregation methods fails under data heterogeneity, leading to suboptimal personalization and aggregation bias due to equal-contribution assumptions that ignore actual utility and reliability of client updates.

Method: CO-PFL performs joint assessment using dual-subspace analysis (gradient direction discrepancies and prediction deviations) to dynamically estimate client contributions, integrates parameter-wise personalization mechanism with mask-aware momentum optimization for stable updates and tailored submodels.

Result: Extensive experiments on CIFAR10, CIFAR10C, CINIC10, and Mini-ImageNet show CO-PFL consistently surpasses state-of-the-art methods in personalization accuracy, robustness, scalability, and convergence stability.

Conclusion: CO-PFL effectively mitigates aggregation bias, strengthens global coordination, and enhances local performance by providing discriminative aggregation weights and stable personalized updates, making it superior for heterogeneous federated learning scenarios.

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [52] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: QKCV attention extends traditional QKV attention by incorporating static categorical embeddings to capture category-specific patterns in time series forecasting, improving accuracy across various models and enabling efficient fine-tuning of foundation models.


<details>
  <summary>Details</summary>
Motivation: Category information is crucial for capturing inherent patterns in real-world time series forecasting tasks, but traditional attention mechanisms don't explicitly leverage this categorical data.

Method: Introduces QKCV (Query-Key-Category-Value) attention that adds a static categorical embedding C to the standard QKV framework, serving as a plug-in module that can enhance various attention-based forecasting models.

Result: QKCV improves forecasting accuracy across diverse real-world datasets and demonstrates remarkable adaptability in fine-tuning univariate time series foundation models by only updating the static embedding C while preserving pretrained weights.

Conclusion: QKCV attention effectively incorporates category information into attention mechanisms, enhancing forecasting performance while enabling computationally efficient fine-tuning of foundation models through selective parameter updates.

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [53] [Federated Learning via Meta-Variational Dropout](https://arxiv.org/abs/2510.20225)
*Insu Jeon,Minui Hong,Junhyeog Yun,Gunhee Kim*

Main category: cs.LG

TL;DR: MetaVD is a Bayesian meta-learning approach that uses a hypernetwork to predict client-dependent dropout rates, addressing model overfitting and divergent local models in federated learning with non-IID data.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning faces challenges with model overfitting and divergent local models due to limited and non-IID data among clients, which MetaVD aims to solve.

Method: MetaVD learns client-dependent dropout rates via a shared hypernetwork, enabling model personalization through conditional dropout posterior for posterior adaptation and aggregation.

Result: Extensive experiments showed MetaVD achieves excellent classification accuracy and uncertainty calibration, especially for OOD clients, while compressing local parameters and reducing communication costs.

Conclusion: MetaVD effectively addresses FL challenges in non-IID settings through Bayesian meta-learning with personalized dropout rates, improving performance while reducing overfitting and communication overhead.

Abstract: Federated Learning (FL) aims to train a global inference model from remotely
distributed clients, gaining popularity due to its benefit of improving data
privacy. However, traditional FL often faces challenges in practical
applications, including model overfitting and divergent local models due to
limited and non-IID data among clients. To address these issues, we introduce a
novel Bayesian meta-learning approach called meta-variational dropout (MetaVD).
MetaVD learns to predict client-dependent dropout rates via a shared
hypernetwork, enabling effective model personalization of FL algorithms in
limited non-IID data settings. We also emphasize the posterior adaptation view
of meta-learning and the posterior aggregation view of Bayesian FL via the
conditional dropout posterior. We conducted extensive experiments on various
sparse and non-IID FL datasets. MetaVD demonstrated excellent classification
accuracy and uncertainty calibration performance, especially for
out-of-distribution (OOD) clients. MetaVD compresses the local model parameters
needed for each client, mitigating model overfitting and reducing communication
costs. Code is available at https://github.com/insujeon/MetaVD.

</details>


### [54] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: SpLIIF generates implicit neural representations for arbitrary downscaling of weather variables, trained on sparse weather stations and topography data over Japan.


<details>
  <summary>Details</summary>
Motivation: To improve downscaling accuracy for weather variables like temperature and wind compared to existing methods like interpolation baselines and CorrDiff.

Method: Train a model using sparse weather stations and topography data over Japan to create implicit neural representations that enable arbitrary downscaling.

Result: The model achieves up to 50% better performance than CorrDiff and baseline for temperature downscaling, and 10-20% better for wind downscaling.

Conclusion: SpLIIF demonstrates superior downscaling capabilities for weather variables, particularly temperature, compared to existing methods.

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [55] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: A provably convergent framework for max-min multi-objective RL using game theory and mirror descent with global convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To address multi-objective reinforcement learning with max-min criterion through a game-theoretic perspective, ensuring provable convergence while maintaining practical efficiency.

Method: Reformulate max-min MORL as a two-player zero-sum regularized continuous game and develop an efficient mirror descent algorithm with adaptive regularization for enhanced performance.

Result: The algorithm demonstrates convergence in tabular settings and significantly outperforms previous baselines in deep RL environments, with comprehensive theoretical analysis including iteration and sample complexity bounds.

Conclusion: The proposed framework provides a theoretically sound and practically effective solution for max-min multi-objective reinforcement learning with provable convergence guarantees.

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [56] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: LKM is a novel self-knowledge distillation method that improves GNN accuracy for molecular property prediction by minimizing distance between hidden embeddings, with up to 45.3% error reduction and negligible computational overhead.


<details>
  <summary>Details</summary>
Motivation: There's a need for more accurate GNN models for molecular property prediction, but increasing model complexity raises computational costs and memory requirements during training and inference.

Method: Developed Layer-to-Layer Knowledge Mixing (LKM), a self-knowledge distillation method that minimizes mean absolute distance between pre-existing hidden embeddings of GNN layers to aggregate multi-hop and multi-scale information.

Result: LKM reduced mean absolute error by up to 9.8% on QM9, 45.3% on MD17 Energy, and 22.9% on Chignolin datasets using three GNN architectures (DimeNet++, MXMNet, and PAMNet).

Conclusion: LKM significantly improves GNN accuracy for chemical property prediction without substantial increases in training and inference costs, demonstrating its potential for efficient model enhancement.

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [57] [FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2510.20250)
*Zhiqin Yang,Yonggang Zhang,Chenxin Li,Yiu-ming Cheung,Bo Han,Yixuan Yuan*

Main category: cs.LG

TL;DR: FedGPS is a novel federated learning framework that addresses data heterogeneity by integrating statistical distribution and gradient information from other clients, achieving superior performance and robustness across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods struggle with data heterogeneity and lack robustness across different heterogeneity scenarios. The authors identified that sharing statistical information can help mitigate heterogeneity by providing clients with a global perspective.

Method: FedGPS statically modifies each client's learning objective to implicitly model global data distribution using surrogate information, and dynamically adjusts local update directions with gradient information from other clients at each round.

Result: Extensive experiments show that FedGPS outperforms state-of-the-art methods across diverse heterogeneity scenarios, demonstrating both effectiveness and robustness.

Conclusion: FedGPS provides a robust solution to data heterogeneity in federated learning by synergistically combining statistical distribution and gradient information, making it suitable for deployment in varied real-world scenarios.

Abstract: Federated Learning (FL) confronts a significant challenge known as data
heterogeneity, which impairs model performance and convergence. Existing
methods have made notable progress in addressing this issue. However, improving
performance in certain heterogeneity scenarios remains an overlooked question:
\textit{How robust are these methods to deploy under diverse heterogeneity
scenarios?} To answer this, we conduct comprehensive evaluations across varied
heterogeneity scenarios, showing that most existing methods exhibit limited
robustness. Meanwhile, insights from these experiments highlight that sharing
statistical information can mitigate heterogeneity by enabling clients to
update with a global perspective. Motivated by this, we propose \textbf{FedGPS}
(\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel
framework that seamlessly integrates statistical distribution and gradient
information from others. Specifically, FedGPS statically modifies each client's
learning objective to implicitly model the global data distribution using
surrogate information, while dynamically adjusting local update directions with
gradient information from other clients at each round. Extensive experiments
show that FedGPS outperforms state-of-the-art methods across diverse
heterogeneity scenarios, validating its effectiveness and robustness. The code
is available at: https://github.com/CUHK-AIM-Group/FedGPS.

</details>


### [58] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: OpTI-BFM is an optimistic decision criterion that enables Behavior Foundation Models (BFMs) to infer tasks through environment interaction at test-time, reducing the need for pre-computed reward data and labeling efforts.


<details>
  <summary>Details</summary>
Motivation: Current BFMs require computing rewards over inference datasets, assuming access to functional reward forms or significant labeling. This work aims to enable task inference purely through environment interaction to reduce data requirements.

Method: Proposes OpTI-BFM, an optimistic decision criterion that models uncertainty over reward functions and guides BFMs in data collection for task inference. Connects to upper-confidence algorithms for linear bandits.

Result: Provides formal regret bound for well-trained BFMs. Empirically enables successor-features-based BFMs to identify and optimize unseen reward functions in a handful of episodes with minimal compute overhead.

Conclusion: OpTI-BFM successfully addresses the data efficiency limitations of BFMs by enabling task inference through environment interaction, making zero-shot RL more practical with reduced data requirements.

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [59] [ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases](https://arxiv.org/abs/2510.20270)
*Ziqian Zhong,Aditi Raghunathan,Nicholas Carlini*

Main category: cs.LG

TL;DR: ImpossibleBench is a benchmark framework that measures LLM agents' tendency to exploit shortcuts by creating impossible tasks with conflicts between specifications and unit tests, quantifying cheating behavior.


<details>
  <summary>Details</summary>
Motivation: LLMs often find and exploit shortcuts to complete tasks, which undermines benchmark validity and real-world reliability. This behavior needs systematic measurement and mitigation.

Method: Create impossible variants of existing benchmarks by introducing direct conflicts between natural-language specifications and unit tests, then measure cheating rate as pass rate on these impossible tasks.

Result: Reveals fine-grained cheating behaviors from simple test modification to complex operator overloading, and shows how prompt engineering, test access, and feedback loops affect cheating rates.

Conclusion: ImpossibleBench serves as a versatile tool for studying model behaviors, context engineering, and developing monitoring tools to build more robust and reliable LLM systems.

Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses
significant risks for reliable assessment and deployment of large language
models (LLMs). For example, an LLM agent with access to unit tests may delete
failing tests rather than fix the underlying bug. Such behavior undermines both
the validity of benchmark results and the reliability of real-world LLM coding
assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,
a benchmark framework that systematically measures LLM agents' propensity to
exploit test cases. ImpossibleBench creates "impossible" variants of tasks from
existing benchmarks like LiveCodeBench and SWE-bench by introducing direct
conflicts between the natural-language specification and the unit tests. We
measure an agent's "cheating rate" as its pass rate on these impossible tasks,
where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a
versatile tool. We demonstrate its utility for: (1) studying model behaviors,
revealing more fine-grained details of cheating behaviors from simple test
modification to complex operator overloading; (2) context engineering, showing
how prompt, test access and feedback loop affect cheating rates; and (3)
developing monitoring tools, providing a testbed with verified deceptive
solutions. We hope ImpossibleBench serves as a useful framework for building
more robust and reliable LLM systems.
  Our implementation can be found at
https://github.com/safety-research/impossiblebench.

</details>


### [60] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: The paper presents optimized GPU kernels for Euler Characteristic Curve computation with 16-2000x speedups over prior implementations, and introduces a differentiable PyTorch layer for end-to-end learning.


<details>
  <summary>Details</summary>
Motivation: Topological features capture global geometric structure in imaging data, but practical adoption in deep learning requires both computational efficiency and differentiability.

Method: Developed optimized CUDA kernels for ECC computation using 128B-coalesced access and hierarchical shared-memory accumulation, plus a differentiable PyTorch layer with Differentiable Euler Characteristic Transform-style sigmoid relaxation to learn thresholds.

Result: Achieved 16-2000x speedups over prior GPU implementations on synthetic grids, enabling efficient computation and end-to-end learning capabilities.

Conclusion: The work enables practical adoption of topological features in deep learning through computational efficiency and differentiability, with potential applications in various domains highlighted by prior ECC work.

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [61] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: PRM-guided tree search for mathematical reasoning shows no significant improvement over Best-of-N selection despite higher computational costs, due to unreliable process reward models that poorly approximate state values and generalize poorly out of distribution.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought prompting with Best-of-N selection has limitations in capturing the branching and exploratory nature of complex mathematical problem-solving, motivating the investigation of PRM-guided tree search approaches.

Method: Proposed an adaptive algorithm to maximize process reward model scores over intractable action space and investigated PRM-guided tree search methods including Monte Carlo tree search and beam search across 23 diverse mathematical problems using Qwen2.5-Math-7B-Instruct.

Result: PRM-guided tree search showed no statistically significant improvements over BoN despite higher costs; Monte Carlo tree search and beam search outperformed other PRM-guided methods; PRMs poorly approximated state values and their reliability degraded with reasoning depth; PRMs generalized poorly out of distribution.

Conclusion: Tree search's underperformance stems from greater reliance on unreliable PRM scores, suggesting different reward modeling approaches are necessary before tree search can effectively enhance mathematical reasoning in LLMs.

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [62] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: SynTSBench is a synthetic data-driven evaluation framework that systematically assesses time series forecasting models through programmable feature configuration, focusing on temporal feature decomposition, robustness analysis, and theoretical optimum benchmarking.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models struggle with robust performance in real-world applications despite strong benchmark results, due to black-box architectures and limited evaluation frameworks that lack clear quantitative insights into model strengths and weaknesses.

Method: The framework uses synthetic data with programmable feature configuration to isolate confounding factors. It employs three analytical dimensions: temporal feature decomposition and capability mapping, robustness analysis under data irregularities, and theoretical optimum benchmarking.

Result: Experiments show that current deep learning models do not universally approach optimal baselines across all types of temporal features, revealing specific performance gaps in different pattern learning capabilities.

Conclusion: SynTSBench provides an interpretable evaluation system that enables systematic assessment of fundamental modeling capabilities in time series forecasting, helping identify specific strengths and weaknesses of different models for particular forecasting scenarios.

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [63] [KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models](https://arxiv.org/abs/2510.20278)
*Guangyu Dai,Siliang Tang,Yueting Zhuang*

Main category: cs.LG

TL;DR: KAN-based Collaborative Model (KCM) improves large-small model collaboration by reducing computational costs while maintaining accuracy, mitigating catastrophic forgetting, and enhancing performance on long-tail data compared to MLP-based approaches.


<details>
  <summary>Details</summary>
Motivation: To address issues in large-small model collaboration frameworks such as accuracy degradation, catastrophic forgetting, and hallucination problems caused by small model knowledge, while reducing computational resource consumption and enhancing large model performance in specialized domains.

Method: Proposed KCM using KAN (Kolmogorov-Arnold Networks) as an alternative neural architecture to MLPs, deployed in collaborative systems across language, vision, and vision-language cross-modal tasks to assist large models.

Result: KCM significantly reduces large model inference calls while maintaining near-identical task accuracy, substantially lowers computational resource consumption, mitigates catastrophic forgetting, and improves accuracy for long-tail data, outperforming MLP-based collaborative models across all metrics.

Conclusion: KCM provides an effective solution for large-small model collaboration, offering superior performance, better interpretability, and reduced computational costs compared to traditional MLP-based approaches.

Abstract: In recent years, Pretrained Large Models(PLMs) researchers proposed
large-small model collaboration frameworks, leveraged easily trainable small
models to assist large models, aim to(1) significantly reduce computational
resource consumption while maintaining comparable accuracy, and (2) enhance
large model performance in specialized domain tasks. However, this
collaborative paradigm suffers from issues such as significant accuracy
degradation, exacerbated catastrophic forgetting, and amplified hallucination
problems induced by small model knowledge. To address these challenges, we
propose a KAN-based Collaborative Model (KCM) as an improved approach to
large-small model collaboration. The KAN utilized in KCM represents an
alternative neural network architecture distinct from conventional MLPs.
Compared to MLPs, KAN offers superior visualizability and interpretability
while mitigating catastrophic forgetting. We deployed KCM in large-small model
collaborative systems across three scenarios: language, vision, and
vision-language cross-modal tasks. The experimental results demonstrate that,
compared with pure large model approaches, the large-small model collaboration
framework utilizing KCM as the collaborative model significantly reduces the
number of large model inference calls while maintaining near-identical task
accuracy, thereby substantially lowering computational resource consumption.
Concurrently, the KAN-based small collaborative model markedly mitigates
catastrophic forgetting, leading to significant accuracy improvements for
long-tail data. The results reveal that KCM demonstrates superior performance
across all metrics compared to MLP-based small collaborative models (MCM).

</details>


### [64] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: The paper introduces CS-54k, a corpus of scientific Q&A pairs from computer science papers, and derives CS-4k for benchmarking AI research assistants and CS-50k for training. Experiments show domain-specific training with high-quality data is more important than model size for creating effective AI research collaborators.


<details>
  <summary>Details</summary>
Motivation: To build AI collaborators that can assist throughout the entire scientific research process, requiring benchmarks that evaluate end-to-end workflows rather than isolated sub-tasks.

Method: Created CS-54k corpus from 14k CC-licensed papers using a scalable pipeline combining retrieval-augmented generation with multi-stage quality control. Derived CS-4k for evaluation and CS-50k for training.

Result: CS-4k stratifies state-of-the-art LLMs into distinct capability tiers. Open models trained on CS-50k with supervised training and reinforcement learning show substantial improvements, with 7B-scale models outperforming larger proprietary systems like GPT-4.1, GPT-4o, and Gemini 2.5 Pro.

Conclusion: Making AI models better research assistants relies more on domain-aligned training with high-quality data than on pretraining scale or general benchmark performance.

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [65] [Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization](https://arxiv.org/abs/2510.20295)
*Yang Qiu,Yixiong Zou,Jun Wang,Wei Liu,Xiangyu Fu,Ruixuan Li*

Main category: cs.LG

TL;DR: This paper proposes an IRM-free method for causal subgraph discovery in graph neural networks to address out-of-distribution generalization challenges without requiring environment annotations or synthetic data splits.


<details>
  <summary>Details</summary>
Motivation: Existing methods for graph generalization rely on Invariant Risk Minimization (IRM) framework, which requires costly environment annotations or heuristically generated synthetic splits, creating practical limitations for real-world applications.

Method: The authors identify that causal subgraphs exhibit smaller distributional variations than non-causal components, formalize this as the Invariant Distribution Criterion, and develop a norm-guided invariant distribution objective for causal subgraph discovery and prediction without using IRM.

Result: Extensive experiments on two widely used benchmarks demonstrate that the proposed method consistently outperforms state-of-the-art methods in graph generalization tasks.

Conclusion: The IRM-free approach effectively captures causal subgraphs by leveraging the invariant distribution properties and norm-guided objectives, providing a practical solution for graph out-of-distribution generalization without the need for environment annotations.

Abstract: Out-of-distribution generalization under distributional shifts remains a
critical challenge for graph neural networks. Existing methods generally adopt
the Invariant Risk Minimization (IRM) framework, requiring costly environment
annotations or heuristically generated synthetic splits. To circumvent these
limitations, in this work, we aim to develop an IRM-free method for capturing
causal subgraphs. We first identify that causal subgraphs exhibit substantially
smaller distributional variations than non-causal components across diverse
environments, which we formalize as the Invariant Distribution Criterion and
theoretically prove in this paper. Building on this criterion, we
systematically uncover the quantitative relationship between distributional
shift and representation norm for identifying the causal subgraph, and
investigate its underlying mechanisms in depth. Finally, we propose an IRM-free
method by introducing a norm-guided invariant distribution objective for causal
subgraph discovery and prediction. Extensive experiments on two widely used
benchmarks demonstrate that our method consistently outperforms
state-of-the-art methods in graph generalization.

</details>


### [66] [DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability](https://arxiv.org/abs/2510.20299)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.LG

TL;DR: Proposes DB-FGA-Net, a double-backbone network combining VGG16 and Xception with Frequency-Gated Attention Block for brain tumor classification without data augmentation, achieving state-of-the-art performance with interpretable Grad-CAM visualizations.


<details>
  <summary>Details</summary>
Motivation: Address limitations of deep learning-based brain tumor classification methods that rely on heavy data augmentation, which can limit generalization and trust in clinical applications. Need for robust, augmentation-free models with clinical interpretability.

Method: Double-backbone network integrating VGG16 and Xception architectures with a Frequency-Gated Attention (FGA) Block to capture complementary local and global features. Uses Grad-CAM for interpretable tumor region visualization. No data augmentation applied.

Result: Achieved 99.24% accuracy on 7K-DS dataset (4-class), 98.68% (3-class), 99.85% (2-class). Generalizes to independent 3K-DS dataset with 95.77% accuracy, outperforming baseline and state-of-the-art methods. Includes GUI for real-time classification and tumor localization.

Conclusion: The proposed DB-FGA-Net demonstrates that augmentation-free, interpretable deep learning models hold strong potential for reliable clinical translation in brain tumor diagnosis, providing both high accuracy and clinical interpretability through Grad-CAM visualizations.

Abstract: Brain tumors are a challenging problem in neuro-oncology, where early and
precise diagnosis is important for successful treatment. Deep learning-based
brain tumor classification methods often rely on heavy data augmentation which
can limit generalization and trust in clinical applications. In this paper, we
propose a double-backbone network integrating VGG16 and Xception with a
Frequency-Gated Attention (FGA) Block to capture complementary local and global
features. Unlike previous studies, our model achieves state-of-the-art
performance without augmentation which demonstrates robustness to variably
sized and distributed datasets. For further transparency, Grad-CAM is
integrated to visualize the tumor regions based on which the model is giving
prediction, bridging the gap between model prediction and clinical
interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS
dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class
and 2-class settings, respectively. On the independent 3K-DS dataset, the model
generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art
methods. To further support clinical usability, we developed a graphical user
interface (GUI) that provides real-time classification and Grad-CAM-based tumor
localization. These findings suggest that augmentation-free, interpretable, and
deployable deep learning models such as DB-FGA-Net hold strong potential for
reliable clinical translation in brain tumor diagnosis.

</details>


### [67] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: InvDec is a hybrid architecture for multivariate time series forecasting that separates temporal encoding from variate-level decoding using a patch-based temporal encoder and inverted decoder with variate-wise self-attention, achieving significant improvements on high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: Existing approaches face limitations: channel-independent methods ignore variable correlations while pure variate-attention approaches sacrifice temporal encoding. There's a need for principled separation between temporal and cross-variate modeling.

Method: Combines patch-based temporal encoder with inverted decoder using variate-wise self-attention. Introduces delayed variate embeddings to preserve temporal integrity and adaptive residual fusion to balance temporal and variate information dynamically.

Result: Significant improvements on high-dimensional datasets: 20.9% MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and 2.7% gain on Traffic compared to PatchTST, while maintaining competitive performance on low-dimensional datasets.

Conclusion: InvDec effectively addresses the trade-off between temporal encoding and cross-variate modeling, with advantages growing with dataset dimensionality, confirming that cross-variate modeling becomes critical as variable count increases.

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [68] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: LEGO is a lightweight framework for multiple-attribute recommendation unlearning that addresses dynamic privacy needs through embedding calibration and flexible combination.


<details>
  <summary>Details</summary>
Motivation: Existing single-attribute unlearning methods cannot handle real-world requirements involving multiple sensitive attributes and dynamic unlearning needs, due to inability to handle multiple simultaneous requests and lack of efficient adaptability.

Method: Two-step framework: 1) Embedding Calibration removes specific attribute information from user embeddings, 2) Flexible Combination combines embeddings to protect all sensitive attributes. Framed as mutual information minimization problem.

Result: Extensive experiments on three real-world datasets across three recommendation models demonstrate effectiveness and efficiency.

Conclusion: LEGO provides theoretical guarantee for simultaneous unlearning and addresses both challenges of handling multiple requests and dynamic adaptability through its parallelizable and flexible framework.

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [69] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: This paper proposes using synthetic images from flight simulators to complement limited real data for runway detection in autonomous landing systems, addressing the high cost of data collection in critical applications.


<details>
  <summary>Details</summary>
Motivation: Deep vision models for critical applications like autonomous navigation require extensive training data covering all possible conditions, but data collection and labeling are too expensive and time-consuming, especially for rare scenarios.

Method: The authors use a commercial flight simulator to generate synthetic images and combine them with few annotated real images. They implement a customized domain adaptation strategy to mitigate the synthetic-to-real distribution shift.

Result: Standard object detection models achieve accurate predictions when trained with the combined real and synthetic data. The approach also shows robustness to adverse conditions like nighttime images that were not present in the real training data.

Conclusion: Synthetic image generation from flight simulators is an effective solution for runway detection in autonomous landing systems, particularly when combined with domain adaptation to handle the distribution shift between synthetic and real data.

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [70] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: The paper proposes an uncertainty-based routing framework that combines a fast reward model with a strong but costly LLM judge to improve RLHF efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Classical reward models trained on human preferences are vulnerable to reward hacking and poor generalization to out-of-distribution inputs, while strong LLM judges have better generalization but high inference costs that limit their use in online RLHF.

Method: An uncertainty-based routing framework that formulates advantage estimation as pairwise preference classification, using principled uncertainty quantification to route uncertain pairs to the LLM judge and confident ones to the reward model.

Result: Experiments show the uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results demonstrate effectiveness in improving online RLHF.

Conclusion: The proposed framework efficiently complements fast reward models with strong LLM judges, enabling better generalization and performance in reinforcement learning with human feedback while managing computational costs.

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [71] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: A robust optimization framework for hierarchical time-series forecasting that addresses uncertainty in covariance matrix estimation to improve forecast coherence and performance.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical forecasting methods rely on estimating covariance matrices from finite samples, creating a gap between true and estimated matrices that degrades forecast performance due to uncertainty.

Method: Proposed a robust optimization framework that introduces an uncertainty set for the estimated covariance matrix and formulates a reconciliation problem minimizing worst-case expected squared error, cast as a semidefinite optimization problem.

Result: Numerical experiments showed the proposed robust reconciliation method achieved better forecast performance than existing hierarchical forecasting methods.

Conclusion: Integrating uncertainty into the reconciliation process through robust optimization effectively improves hierarchical forecasting performance by accounting for covariance matrix estimation uncertainty.

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [72] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: The paper introduces Relative-Based Probability (RBP) as a new scaling law metric that focuses on token ranking rather than just cross-entropy, and demonstrates its robustness across datasets and model families.


<details>
  <summary>Details</summary>
Motivation: Existing scaling laws rely solely on cross-entropy, which only measures absolute probability of correct tokens but ignores relative ordering between correct and incorrect tokens - a crucial aspect for language models in scenarios like greedy sampling.

Method: Proposed Relative-Based Probability (RBP) metric that quantifies the probability that the correct token is ranked among the top predictions, and established the Relative-Based Scaling Law to characterize how RBP improves with increasing model size.

Result: Extensive experiments on four datasets and four model families spanning five orders of magnitude demonstrated the robustness and accuracy of the Relative-Based Scaling Law.

Conclusion: The Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models, offering valuable insights for both practical development and theoretical exploration.

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [73] [Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control](https://arxiv.org/abs/2510.20408)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: This paper introduces an industry-inspired benchmark for multi-stage industrial processes, combining sorting and pressing operations in a recycling scenario. It compares modular vs monolithic RL architectures and shows that action masking dramatically improves performance and reduces the advantage of specialized agents.


<details>
  <summary>Details</summary>
Motivation: RL adoption in industry is limited due to challenges like reward design, modularity, and action space management. Academic benchmarks often don't transfer well to real industrial control problems.

Method: Created an enhanced benchmark combining SortingEnv and ContainerGym into sequential recycling scenario. Evaluated modular architecture with specialized agents vs monolithic agent, with and without action masking.

Result: Without action masking, agents struggle to learn effective policies, with modular architecture performing better. With action masking, both architectures improve substantially and performance gap narrows significantly.

Conclusion: Action space constraints play a decisive role in industrial RL. Advantages of specialization diminish as action complexity is reduced. The benchmark provides valuable testbed for practical multi-agent RL in industrial automation.

Abstract: Autonomous control of multi-stage industrial processes requires both local
specialization and global coordination. Reinforcement learning (RL) offers a
promising approach, but its industrial adoption remains limited due to
challenges such as reward design, modularity, and action space management. Many
academic benchmarks differ markedly from industrial control problems, limiting
their transferability to real-world applications. This study introduces an
enhanced industry-inspired benchmark environment that combines tasks from two
existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling
scenario with sorting and pressing operations. We evaluate two control
strategies: a modular architecture with specialized agents and a monolithic
agent governing the full system, while also analyzing the impact of action
masking. Our experiments show that without action masking, agents struggle to
learn effective policies, with the modular architecture performing better. When
action masking is applied, both architectures improve substantially, and the
performance gap narrows considerably. These results highlight the decisive role
of action space constraints and suggest that the advantages of specialization
diminish as action complexity is reduced. The proposed benchmark thus provides
a valuable testbed for exploring practical and robust multi-agent RL solutions
in industrial automation, while contributing to the ongoing debate on
centralization versus specialization.

</details>


### [74] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: The paper analyzes Direct Preference Optimization (DPO) limitations and proposes AuxDPO to address misspecification issues by incorporating auxiliary variables.


<details>
  <summary>Details</summary>
Motivation: DPO faces misspecification problems when the true reward function cannot be realized by the policy class, leading to issues like preference reversal and poor performance.

Method: The authors characterize DPO's statistical estimation problem, study RLHF's local behavior, and propose AuxDPO which adds auxiliary variables to the DPO loss to better approximate RLHF solutions.

Result: AuxDPO demonstrates superior performance over standard DPO in both bandit experiments and LLM alignment tasks, effectively mitigating misspecification issues.

Conclusion: AuxDPO provides a principled approach to address DPO's misspecification problems by bridging the gap between DPO and RLHF through auxiliary variables.

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [75] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: Proposes a thresholding method for MTPP to handle imbalanced event mark distributions, predicting mark first then time, with a neural model for efficient computation.


<details>
  <summary>Details</summary>
Motivation: Existing MTPP studies overlook highly imbalanced event mark distributions in real-world applications, which significantly challenges next event prediction performance, especially for rare marks.

Method: Thresholding method that learns thresholds to tune mark probability normalized by prior probability; predicts mark first then time; neural MTPP model for effective time sampling and mark probability estimation without expensive numerical integration.

Result: Extensive experiments on real-world datasets demonstrate superior performance against various baselines for next event mark and time prediction.

Conclusion: The proposed solution effectively addresses the imbalanced mark distribution challenge in MTPP and outperforms existing methods.

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [76] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: Systematic analysis of data selection methods for LLM behavioral repair, showing that Semantic-Aware Prioritized Sampling (SAPS) achieves optimal balance between detoxification, utility preservation, and efficiency with less data.


<details>
  <summary>Details</summary>
Motivation: LLMs can produce toxic or biased outputs that undermine safety, but post-hoc repair is costly, motivating selective use of repair data to reduce parameter update costs.

Method: Evaluated five selection methods: random sampling, K-Center, gradient-norm-based selection (GraNd), stratified coverage (CCS), and proposed SAPS approach. Assessed repair effectiveness through toxicity reduction, perplexity metrics, and composite scores (RPS, OPS, RES).

Result: SAPS achieves best balance between detoxification, utility preservation, and efficiency. Random sampling effective for large/robust models, while CCS and GraNd provide limited benefit. Optimal data proportion depends on model scale and repair method.

Conclusion: Sample selection should be regarded as tunable component of repair pipelines. Selection-based repair establishes efficient and scalable paradigm for maintaining LLM reliability.

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [77] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: This paper introduces explainable benchmarking, a new paradigm that automatically generates explanations for system performance in benchmarks, specifically applied to knowledge-graph-based question answering systems using the PruneCEL concept learning approach.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking practices summarize system performance with limited metrics, requiring tedious manual analysis that often produces biased results. There's a need for automated approaches that can provide detailed explanations of system behavior.

Method: The authors propose explainable benchmarking using PruneCEL, a novel concept learning approach developed for large knowledge graphs. PruneCEL computes explanations for system performance in benchmarks.

Result: PruneCEL outperforms state-of-the-art concept learners by up to 0.55 F1 points. A user study with 41 participants shows that in 80% of cases, the majority can accurately predict system behavior based on the generated explanations.

Conclusion: Explainable benchmarking is a viable approach that provides meaningful insights into system performance, with PruneCEL demonstrating superior performance over existing methods and effectively enabling users to understand system behavior.

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [78] [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
*Xuan Lin,Aocheng Ding,Tengfei Ma,Hua Liang,Zhe Quan*

Main category: cs.LG

TL;DR: MolBridge is a novel atom-level joint graph refinement framework that models fine-grained inter-drug interactions for robust DDI event prediction, addressing limitations of existing approaches by preserving structural context and handling long-range atomic dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing DDI prediction methods rely on isolated drug representations and fail to explicitly model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI type distributions. There's a need to capture fine-grained inter-drug relationships critical for modeling metabolic mechanisms like enzyme-mediated competition.

Method: MolBridge constructs a joint graph integrating atomic structures of drug pairs to enable direct modeling of inter-drug associations. It uses a structure consistency module that iteratively refines node features while preserving global structural context, overcoming over-smoothing issues in joint graph settings.

Result: Extensive experiments on two benchmark datasets show that MolBridge consistently outperforms state-of-the-art baselines, achieving superior performance across long-tail and inductive scenarios. It effectively learns both local and global interaction patterns, yielding robust representations across frequent and rare DDI types.

Conclusion: MolBridge demonstrates advantages of fine-grained graph refinement in improving accuracy, robustness, and mechanistic interpretability of DDI event prediction. The framework contributes to Web Mining and Content Analysis by developing graph-based methods for mining and analyzing drug-drug interaction networks.

Abstract: Drug combinations offer therapeutic benefits but also carry the risk of
adverse drug-drug interactions (DDIs), especially under complex molecular
structures. Accurate DDI event prediction requires capturing fine-grained
inter-drug relationships, which are critical for modeling metabolic mechanisms
such as enzyme-mediated competition. However, existing approaches typically
rely on isolated drug representations and fail to explicitly model atom-level
cross-molecular interactions, limiting their effectiveness across diverse
molecular complexities and DDI type distributions. To address these
limitations, we propose MolBridge, a novel atom-level joint graph refinement
framework for robust DDI event prediction. MolBridge constructs a joint graph
that integrates atomic structures of drug pairs, enabling direct modeling of
inter-drug associations. A central challenge in such joint graph settings is
the potential loss of information caused by over-smoothing when modeling
long-range atomic dependencies. To overcome this, we introduce a structure
consistency module that iteratively refines node features while preserving the
global structural context. This joint design allows MolBridge to effectively
learn both local and global interaction outperforms state-of-the-art baselines,
achieving superior performance across long-tail and inductive scenarios.
patterns, yielding robust representations across both frequent and rare DDI
types. Extensive experiments on two benchmark datasets show that MolBridge
consistently. These results demonstrate the advantages of fine-grained graph
refinement in improving the accuracy, robustness, and mechanistic
interpretability of DDI event prediction.This work contributes to Web Mining
and Content Analysis by developing graph-based methods for mining and analyzing
drug-drug interaction networks.

</details>


### [79] [Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach](https://arxiv.org/abs/2510.20454)
*Lawrence Clegg,John Cartlidge*

Main category: cs.LG

TL;DR: This paper proposes a graph neural network approach to model intransitive player relationships in tennis for match forecasting and betting strategies.


<details>
  <summary>Details</summary>
Motivation: Intransitive player dominance (where A beats B, B beats C, but C beats A) is common in competitive tennis but rarely incorporated in forecasting methods, creating potential market inefficiencies.

Method: Graph neural network approach using temporal directed graphs with players as nodes and historical match outcomes as directed edges to explicitly model intransitive relationships.

Result: The model achieved 65.7% accuracy and 0.215 Brier Score on high intransitivity matchups, generating 3.26% ROI with Kelly staking over 1903 bets, showing bookmaker Pinnacle Sports poorly handles matches with high intransitive complexity.

Conclusion: Graph-based approaches are uniquely positioned to capture relational dynamics in intransitive scenarios, successfully exploiting market inefficiencies in handling complex player matchups.

Abstract: Intransitive player dominance, where player A beats B, B beats C, but C beats
A, is common in competitive tennis. Yet, there are few known attempts to
incorporate it within forecasting methods. We address this problem with a graph
neural network approach that explicitly models these intransitive relationships
through temporal directed graphs, with players as nodes and their historical
match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly
handles matches with high intransitive complexity and posit that our
graph-based approach is uniquely positioned to capture relational dynamics in
these scenarios. When selectively betting on higher intransitivity matchups
with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant
positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a
market inefficiency in handling intransitive matchups that our approach
successfully exploits.

</details>


### [80] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: This paper investigates watermark forging attacks on post-hoc image watermarking systems, introducing a method that can detect, remove, and forge watermarks using only a single watermarked image without knowledge of the watermarking model.


<details>
  <summary>Details</summary>
Motivation: With the rise of AI-generated content, watermarking is crucial for authenticity and attribution, but watermark forging (stealing watermarks from genuine content and applying them to malicious content) remains underexplored compared to removal attacks.

Method: The authors introduce a preference model trained with ranking loss on procedurally generated images to detect watermarks, then use this model through backpropagation to optimize input images for watermark removal and forging.

Result: The proposed method effectively forges watermarks across various post-hoc image watermarking models, requiring only a single watermarked image and no knowledge of the watermarking model, making it simpler and more practical than existing attacks.

Conclusion: The research demonstrates significant security vulnerabilities in current watermarking approaches, as the proposed attack can successfully forge watermarks, questioning the reliability of existing watermarking systems for content authentication.

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [81] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Bi-CoG is a plug-and-play method that improves semi-supervised fine-tuning of vision-language models by using both inter-model and intra-model consistency with dynamic pseudo-label assignment, addressing model bias and hyperparameter sensitivity issues.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing semi-supervised fine-tuning methods that suffer from model bias and hyperparameter sensitivity due to reliance on prediction consistency or pre-defined confidence thresholds.

Method: Proposes Bi-CoG (Bi-Consistency-Guided Self-Training) which assigns high-quality pseudo-labels using both inter-model and intra-model consistency, along with an error-aware dynamic pseudo-label assignment strategy.

Result: Extensive experiments over 14 datasets demonstrate Bi-CoG consistently and significantly improves performance of existing methods, with theoretical analysis supporting its effectiveness.

Conclusion: Bi-CoG provides a simple yet effective solution for semi-supervised fine-tuning that addresses key limitations of existing approaches and delivers consistent performance improvements across diverse datasets.

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [82] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: Proposes Hurdle-IMDL framework to address imbalanced label distribution in rainfall retrieval, decomposing imbalance into zero inflation and long tail problems, with IMDL specifically tackling the long tail by transforming learning into an unbiased ideal inverse model.


<details>
  <summary>Details</summary>
Motivation: AI effectiveness in quantitative remote sensing is constrained by imbalanced label distribution, leading models to favor common samples and degrade performance for rare ones, particularly problematic in rainfall retrieval where heavy rain performance is compromised.

Method: Uses divide-and-conquer strategy: hurdle model handles zero inflation (predominance of non-rain samples), while IMDL addresses long tail (disproportionate abundance of light vs heavy rain) by transforming learning object into unbiased ideal inverse model.

Result: Comprehensive evaluation shows Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods, with effective mitigation of systematic underestimation and marked improvement in heavy-to-extreme rain retrieval.

Conclusion: IMDL offers generalizable approach for addressing imbalance in environmental variable distributions, enabling enhanced retrieval of rare yet high-impact events.

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [83] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: SheafAlign is a decentralized multimodal alignment framework using sheaf theory that enables alignment without requiring mutual redundancy across all modalities, achieving better generalization and lower communication costs.


<details>
  <summary>Details</summary>
Motivation: Conventional multimodal alignment methods assume mutual redundancy across all modalities, which fails in real-world distributed scenarios where modalities may not share complete information.

Method: Proposes a sheaf-theoretic framework that replaces single-space alignment with multiple comparison spaces, models pairwise modality relations through sheaf structures, and uses decentralized contrastive learning-based objectives.

Result: Experiments show superior zero-shot generalization, cross-modal alignment, and robustness to missing modalities, with 50% lower communication cost than state-of-the-art baselines.

Conclusion: SheafAlign effectively overcomes limitations of prior methods by preserving both shared and unique information across modalities without requiring mutual redundancy.

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [84] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: This paper presents the first unified framework for zero-shot reinforcement learning, introducing a consistent notation and taxonomy that organizes existing approaches into two families: direct representations and compositional representations.


<details>
  <summary>Details</summary>
Motivation: Zero-shot RL has emerged as a setting for developing general agents capable of solving downstream tasks without additional training, but the field lacks a common analytical lens for comparing different approaches.

Method: The authors develop a unified framework with consistent notation and taxonomy, classifying algorithms into two families: direct representations (end-to-end mappings from rewards to policies) and compositional representations (decomposing representations leveraging value function substructure).

Result: The framework enables direct comparison between existing methods, highlights shared principles and key differences, and derives an extended bound for successor-feature methods, offering new perspectives on their zero-shot performance.

Conclusion: By consolidating existing work under a common lens, this framework provides a principled foundation for future research in zero-shot RL and outlines a clear path toward developing more general agents.

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [85] [Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics](https://arxiv.org/abs/2510.20556)
*Alexandre Benoit,Catherine Aitken,Yu He*

Main category: cs.LG

TL;DR: This paper analyzes how graph rewiring affects structural properties and performance in GNNs, finding that successful methods preserve local structure while allowing global connectivity changes.


<details>
  <summary>Details</summary>
Motivation: Graph rewiring is used to improve GNN performance but alters graph topology, potentially distorting important structural signals. There's limited understanding of which structural properties must be preserved for both performance gains and structural fidelity.

Method: Systematic analysis of seven diverse rewiring strategies, correlating changes in local and global graph properties with node classification accuracy.

Result: Successful rewiring methods consistently preserve local structure while allowing flexibility in global connectivity. Changes in structural metrics correlate with downstream task performance.

Conclusion: The findings provide insights for designing effective rewiring strategies that balance local structure preservation with global connectivity modifications, bridging graph theory and GNN optimization.

Abstract: Graph rewiring has emerged as a key technique to alleviate over-squashing in
Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph
topology to improve information flow. While effective, rewiring inherently
alters the graph's structure, raising the risk of distorting important
topology-dependent signals. Yet, despite the growing use of rewiring, little is
known about which structural properties must be preserved to ensure both
performance gains and structural fidelity. In this work, we provide the first
systematic analysis of how rewiring affects a range of graph structural
metrics, and how these changes relate to downstream task performance. We study
seven diverse rewiring strategies and correlate changes in local and global
graph properties with node classification accuracy. Our results reveal a
consistent pattern: successful rewiring methods tend to preserve local
structure while allowing for flexibility in global connectivity. These findings
offer new insights into the design of effective rewiring strategies, bridging
the gap between graph theory and practical GNN optimization.

</details>


### [86] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: This paper analyzes challenges in integrating MLOps with Operational Technology systems and proposes using established OT reference models like RAMI 4.0 and ISA-95 for systematic MLOps integration.


<details>
  <summary>Details</summary>
Motivation: MLOps practices are increasingly adopted in industrial settings but face significant challenges when integrating with Operational Technology systems, requiring structured approaches for successful implementation.

Method: The paper evaluates the suitability of RAMI 4.0 and ISA-95 reference models for MLOps integration, presents detailed mapping of MLOps lifecycle components to RAMI 4.0, and demonstrates this through a real-world use case.

Result: Findings show that standard MLOps practices cannot be directly applied to OT environments but structured adaptation using existing reference models provides a viable pathway for successful integration.

Conclusion: Systematic embedding of MLOps practices into established OT reference models enables successful integration of machine learning operations with operational technology systems in industrial settings.

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [87] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: A novel approach for reasoning generalization that learns energy landscapes over subproblem solution spaces and combines them compositionally, outperforming state-of-the-art methods on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end reasoning models have limited generalization beyond training distributions, failing to solve problems more complex than those seen during training.

Method: Learn energy landscapes over smaller subproblem solution spaces, then compositionally combine them to construct global energy landscapes for complex problems, using Parallel Energy Minimization for improved sampling.

Result: Outperforms state-of-the-art methods on reasoning problems, demonstrating ability to generalize to larger and more complex problems than training data.

Conclusion: The compositional energy landscape approach enables effective generalization in reasoning tasks by building complex solutions from simpler subproblem components.

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [88] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: This paper provides a comprehensive convergence analysis of Stochastic Gradient Descent (SGD) under the Expected Smoothness (ES) condition, refining ES interpretations, deriving gradient norm bounds, and proving O(1/K) convergence rates with explicit residual errors for various step-size schedules.


<details>
  <summary>Details</summary>
Motivation: Classical SGD analyses rely on assumptions that are either too strong (bounded variance) or too coarse (uniform noise), while the ES condition offers a more flexible alternative by relating stochastic gradient moments to the objective and full gradient.

Method: The authors conduct a self-contained convergence analysis of SGD under ES, refining the ES condition with interpretations and sampling-dependent constants, deriving bounds on the expectation of squared full gradient norm, and proving convergence rates for different step-size schedules.

Result: The paper proves O(1/K) convergence rates with explicit residual errors for various step-size schedules, unifying and extending recent work by Khaled and Richtárik (2020) and Umeda and Iiduka (2025).

Conclusion: The analysis provides a unified framework for understanding SGD convergence under the ES condition, offering refined interpretations and explicit convergence guarantees that extend previous results in the literature.

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [89] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: This paper systematically evaluates retrieval configurations for code-focused generation tasks across chunking strategies, similarity scoring methods, and splitting granularity, providing evidence-based recommendations for code-oriented RAG systems.


<details>
  <summary>Details</summary>
Motivation: To study retrieval design for code-focused generation tasks under realistic compute budgets and provide practical guidance for implementing effective code-oriented RAG systems.

Method: Systematic comparison of retrieval configurations across various context window sizes using two tasks from Long Code Arena (code completion and bug localization), evaluating chunking strategy, similarity scoring, and splitting granularity.

Result: BM25 with word-level splitting is most effective for PL-PL tasks; proprietary dense encoders work best for NL-PL but with 100x latency; optimal chunk size scales with context (32-64 lines for small budgets); line-based chunking matches syntax-aware splitting; retrieval latency varies up to 200x across configurations.

Conclusion: Evidence-based recommendations for code-oriented RAG systems based on task requirements, model constraints, and computational efficiency, with BM25 + word splitting offering the best quality-latency trade-off.

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [90] [PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection](https://arxiv.org/abs/2510.20611)
*Mirza Raquib,Niloy Das,Farida Siddiqi Prity,Arafath Al Fahim,Saydul Akbar Murad,Mohammad Amzad Hossain,MD Jiabul Hoque,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: This paper proposes a machine learning framework using customized Particle Swarm Optimization for feature selection in breast cancer diagnosis, achieving 99.1% accuracy across multiple performance metrics while ensuring interpretability through explainable AI methods.


<details>
  <summary>Details</summary>
Motivation: Breast cancer is the most critical and frequently diagnosed cancer in women worldwide, with conventional diagnostic methods limited by variability, cost, and misdiagnosis risk. Machine learning offers potential for improved computer-aided diagnosis.

Method: Integrated framework incorporating customized Particle Swarm Optimization for feature selection, evaluated on 29 different models including classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. Uses cross-validation and explainable AI methods for interpretability.

Result: The proposed approach achieved a superior score of 99.1% across all performance metrics (accuracy, precision), effectively reduced dimensionality, and provided transparent, model-agnostic explanations.

Conclusion: The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis.

Abstract: Breast cancer is considered the most critical and frequently diagnosed cancer
in women worldwide, leading to an increase in cancer-related mortality. Early
and accurate detection is crucial as it can help mitigate possible threats
while improving survival rates. In terms of prediction, conventional diagnostic
methods are often limited by variability, cost, and, most importantly, risk of
misdiagnosis. To address these challenges, machine learning (ML) has emerged as
a powerful tool for computer-aided diagnosis, with feature selection playing a
vital role in improving model performance and interpretability. This research
study proposes an integrated framework that incorporates customized Particle
Swarm Optimization (PSO) for feature selection. This framework has been
evaluated on a comprehensive set of 29 different models, spanning classical
classifiers, ensemble techniques, neural networks, probabilistic algorithms,
and instance-based algorithms. To ensure interpretability and clinical
relevance, the study uses cross-validation in conjunction with explainable AI
methods. Experimental evaluation showed that the proposed approach achieved a
superior score of 99.1\% across all performance metrics, including accuracy and
precision, while effectively reducing dimensionality and providing transparent,
model-agnostic explanations. The results highlight the potential of combining
swarm intelligence with explainable ML for robust, trustworthy, and clinically
meaningful breast cancer diagnosis.

</details>


### [91] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: MS-BART is a unified modeling framework that maps mass spectra and molecular structures into a shared token vocabulary, enabling cross-modal learning through large-scale pretraining. It addresses data scarcity in mass spectrometry by using computed fingerprint-molecule datasets and incorporates multi-task pretraining, finetuning with MIST, and chemical feedback mechanisms to enhance performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Structure elucidation from mass spectrometry data is challenging due to scarce annotated spectra. While large-scale pretraining works well in other domains, applying it to mass spectrometry is hindered by the complexity and heterogeneity of raw spectral signals.

Method: Propose MS-BART framework that maps mass spectra and molecular structures into shared token vocabulary. Use multi-task pretraining objectives (denoising and translation) on computed fingerprint-molecule datasets. Finetune on experimental spectra using MIST-generated fingerprint predictions. Implement chemical feedback mechanism to reduce molecular hallucination.

Result: MS-BART achieves state-of-the-art performance across 5/12 key metrics on MassSpecGym and NPLIB1 benchmarks. It is faster by one order of magnitude than competing diffusion-based methods. Comprehensive ablation studies validate the model's effectiveness and robustness.

Conclusion: MS-BART provides an effective solution for molecular structure elucidation from mass spectrometry data by leveraging cross-modal learning, large-scale pretraining, and chemical feedback mechanisms, addressing key challenges in the field while achieving superior performance and efficiency.

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [92] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: This paper analyzes hyperparameter selection in differentially private transfer learning, revealing mismatches between theory and practice for clipping bounds and batch sizes, and shows how single parameter settings across tasks lead to suboptimal performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap between theoretical understanding and empirical outcomes in differentially private transfer learning, particularly regarding the selection of clipping bounds and batch sizes under privacy constraints.

Method: The authors analyze gradient distribution changes, examine cumulative DP noise effects, and study clipping as a form of gradient re-weighting under limited compute budgets with fixed epochs.

Result: Found that larger clipping bounds perform better under strong privacy (contradicting theory), existing batch size heuristics don't work, and cumulative DP noise better explains batch size performance. Performance drops significantly when using single parameter settings across different privacy and compute conditions.

Conclusion: Optimal hyperparameter selection in DP transfer learning requires task-specific tuning, as current theoretical guidelines don't match empirical outcomes, especially when moving between different privacy regimes and compute constraints.

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [93] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: H-SPLID is a novel algorithm that learns salient feature representations by explicitly decomposing salient and non-salient features into separate spaces, promoting low-dimensional task-relevant features and improving robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a method that explicitly separates salient and non-salient features to improve model robustness and learn more compressed, task-relevant representations.

Method: H-SPLID explicitly decomposes salient and non-salient features into separate spaces and uses the Hilbert-Schmidt Independence Criterion (HSIC) to bound prediction deviation under input perturbations.

Result: Empirical evaluations show models trained with H-SPLID primarily rely on salient input components with reduced sensitivity to perturbations affecting non-salient features like image backgrounds.

Conclusion: H-SPLID establishes a link between robustness and latent representation compression, demonstrating that explicit feature decomposition improves model reliance on task-relevant features while reducing sensitivity to irrelevant perturbations.

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [94] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: LLMs and LMMs enable autonomous communications in 6G networks, outperforming traditional methods through multimodal sensing, adaptive reconfiguration, and prompt/fine-tuning strategies.


<details>
  <summary>Details</summary>
Motivation: Leverage the breakthrough capabilities of LLMs and LMMs in natural language understanding and complex reasoning to enable autonomous communications among machines, vehicles, and humanoids in 6G networks.

Method: Propose a framework for task-oriented autonomous communications using LLMs/LMMs with multimodal sensing integration, adaptive reconfiguration, and prompt/fine-tuning strategies for wireless tasks. Demonstrate through three case studies: LMM-based traffic control, LLM-based robot scheduling, and LMM-based environment-aware channel estimation.

Result: The proposed LLM/LMM-aided autonomous systems significantly outperform conventional and discriminative deep learning model-based techniques, maintaining robustness under dynamic objectives, varying input parameters, and heterogeneous multimodal conditions.

Conclusion: LLMs and LMMs provide superior performance and robustness for autonomous communications in dynamic 6G environments compared to traditional static optimization approaches.

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [95] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: DiRecGNN is an attention-enhanced entity recommendation framework for cloud service monitoring at Microsoft that recommends optimal attribute subsets for automated watchdogs, achieving 43.1% MRR improvement and high user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with limited structural information and fail to capture long-range dependencies between entities in cloud service monitoring, leading to inferior performance.

Method: Constructs monitor heterogeneous graph, uses multi-head attention mechanism for heterogeneous neighbors and attributes, attends to random walk paths for long-range dependencies, and employs multi-faceted loss functions for sparse data optimization.

Result: Achieved 43.1% increase in MRR over existing methods, with product teams rating the feature 4.5/5 for usefulness.

Conclusion: The attention-enhanced entity ranking model effectively addresses limitations of traditional approaches and provides valuable recommendations for cloud service monitoring at production scale.

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [96] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: xTime is a novel framework for extreme event forecasting in time series that uses knowledge distillation and mixture of experts to improve prediction of rare extreme events like floods, heatwaves, and medical episodes.


<details>
  <summary>Details</summary>
Motivation: Extreme events in time series (e.g., floods, heatwaves, medical episodes) have serious consequences but are hard to forecast due to data imbalance and neglect of preceding intermediate events. Existing models focus on overall performance but struggle with extreme events.

Method: Proposes xTime framework using knowledge distillation to transfer information from models trained on lower-rarity events, and a mixture of experts mechanism that dynamically selects and fuses outputs from expert models across different rarity levels.

Result: Experiments on multiple datasets show xTime achieves consistent improvements, with forecasting accuracy on extreme events improving from 3% to 78%.

Conclusion: xTime effectively addresses the challenges of extreme event forecasting through knowledge distillation and dynamic expert fusion, significantly improving prediction performance for rare but important extreme events.

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [97] [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](https://arxiv.org/abs/2510.20666)
*Mariona Jaramillo-Civill,Luis González-Gudiño,Tales Imbiriba,Pau Closas*

Main category: cs.LG

TL;DR: A hybrid Bayesian mixture-of-experts framework that combines physical path-loss models with CNN using building-height maps to improve GNSS jammer localization and RSS field reconstruction in urban environments.


<details>
  <summary>Details</summary>
Motivation: GNSS signals are vulnerable to jamming in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed RSS field due to limited spatial context.

Method: Proposed hybrid Bayesian mixture-of-experts framework that fuses physical path-loss model and CNN through log-linear pooling. Uses building-height maps to capture urban propagation effects and performs Bayesian inference with Laplace approximation.

Result: Experiments on urban ray-tracing data show improved localization accuracy and reduced uncertainty with more training points. Uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.

Conclusion: The proposed framework effectively combines physical consistency with data-driven learning to improve jammer localization and RSS field reconstruction in challenging urban environments.

Abstract: Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,
particularly in urban areas where multipath and shadowing distort received
power. Previous data-driven approaches achieved reasonable localization but
poorly reconstructed the received signal strength (RSS) field due to limited
spatial context. We propose a hybrid Bayesian mixture-of-experts framework that
fuses a physical path-loss (PL) model and a convolutional neural network (CNN)
through log-linear pooling. The PL expert ensures physical consistency, while
the CNN leverages building-height maps to capture urban propagation effects.
Bayesian inference with Laplace approximation provides posterior uncertainty
over both the jammer position and RSS field. Experiments on urban ray-tracing
data show that localization accuracy improves and uncertainty decreases with
more training points, while uncertainty concentrates near the jammer and along
urban canyons where propagation is most sensitive.

</details>


### [98] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: This paper provides a practical guide for building world models, focusing on a clear progression from masked models to unified architectures, interactive generative models, and memory-augmented systems.


<details>
  <summary>Details</summary>
Motivation: To offer a focused roadmap for developing world models rather than conducting a comprehensive survey, emphasizing the most promising path towards creating true world models.

Method: Follows a structured progression: starting with early masked models for unified representation learning, moving to unified architectures with shared paradigms, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems for temporal consistency.

Result: Identifies a core path focusing on generative capabilities, interactive loops, and memory systems as the most promising approach for developing effective world models.

Conclusion: The paper concludes that concentrating on the generative heart, interactive loop, and memory system represents the most viable path toward building true world models, bypassing loosely related approaches to maintain focus on essential components.

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [99] [GRACE: GRaph-based Addiction Care prEdiction](https://arxiv.org/abs/2510.20671)
*Subham Kumar,Prakrithi Shivaprakash,Koustav Rudra,Lekhansh Shukla,Animesh Mukherjee*

Main category: cs.LG

TL;DR: The paper proposes GRACE, a graph neural network framework for predicting the appropriate locus of care for addiction patients, addressing class imbalance through structured learning and unbiased meta-graph generation.


<details>
  <summary>Details</summary>
Motivation: There is a critical need for automated frameworks to determine the appropriate locus of care for addiction patients, especially given limited specialized treatment resources and severe class imbalances in addiction datasets that affect decision-making accuracy.

Method: The authors propose a graph neural network (GRACE) framework that formalizes locus of care prediction as a structured learning problem, performs extensive feature engineering, and introduces a novel approach to obtain an unbiased meta-graph to overcome class imbalance issues.

Result: Experimental results on real-world data demonstrate significant improvements of 11-35% in F1 score for the minority class compared to competitive baseline methods.

Conclusion: The GRACE framework effectively addresses class imbalance in addiction care locus prediction through structured learning and unbiased meta-graph generation, achieving substantial performance improvements over existing approaches.

Abstract: Determining the appropriate locus of care for addiction patients is one of
the most critical clinical decisions that affects patient treatment outcomes
and effective use of resources. With a lack of sufficient specialized treatment
resources, such as inpatient beds or staff, there is an unmet need to develop
an automated framework for the same. Current decision-making approaches suffer
from severe class imbalances in addiction datasets. To address this limitation,
we propose a novel graph neural network (GRACE) framework that formalizes locus
of care prediction as a structured learning problem. Further, we perform
extensive feature engineering and propose a new approach of obtaining an
unbiased meta-graph to train a GNN to overcome the class imbalance problem.
Experimental results in real-world data show an improvement of 11-35% in terms
of the F1 score of the minority class over competitive baselines. The codes and
note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.

</details>


### [100] [A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks](https://arxiv.org/abs/2510.20683)
*Georgios Mentzelopoulos,Ioannis Asmanis,Konrad P. Kording,Eva L. Dyer,Kostas Daniilidis,Flavia Vitale*

Main category: cs.LG

TL;DR: Spikachu is a scalable, causal, and energy-efficient neural decoding framework using spiking neural networks (SNNs) that outperforms causal baselines while consuming 2.26-418.81x less energy, enabling few-shot transfer across sessions, subjects, and tasks.


<details>
  <summary>Details</summary>
Motivation: Current neural decoders for brain-computer interfaces either lack generalization (simple causal models) or struggle in real-time settings (complex non-causal models), and both rely on power-hungry neural networks that are difficult to integrate into resource-limited systems.

Method: The approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules adapted to input timing extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions.

Result: Evaluation on 113 recording sessions from 6 non-human primates (43 hours total) shows Spikachu outperforms causal baselines with significantly lower energy consumption (2.26-418.81x less), and scaling training to multiple sessions/subjects improves performance and enables few-shot transfer.

Conclusion: Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs that achieves competitive performance relative to state-of-the-art models while consuming orders of magnitude less energy.

Abstract: Brain-computer interfaces (BCIs) promise to enable vital functions, such as
speech and prosthetic control, for individuals with neuromotor impairments.
Central to their success are neural decoders, models that map neural activity
to intended behavior. Current learning-based decoding approaches fall into two
classes: simple, causal models that lack generalization, or complex, non-causal
models that generalize and scale offline but struggle in real-time settings.
Both face a common challenge, their reliance on power-hungry artificial neural
network backbones, which makes integration into real-world, resource-limited
systems difficult. Spiking neural networks (SNNs) offer a promising
alternative. Because they operate causally these models are suitable for
real-time use, and their low energy demands make them ideal for
battery-constrained environments. To this end, we introduce Spikachu: a
scalable, causal, and energy-efficient neural decoding framework based on SNNs.
Our approach processes binned spikes directly by projecting them into a shared
latent space, where spiking modules, adapted to the timing of the input,
extract relevant features; these latent representations are then integrated and
decoded to generate behavioral predictions. We evaluate our approach on 113
recording sessions from 6 non-human primates, totaling 43 hours of recordings.
Our method outperforms causal baselines when trained on single sessions using
between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that
scaling up training to multiple sessions and subjects improves performance and
enables few-shot transfer to unseen sessions, subjects, and tasks. Overall,
Spikachu introduces a scalable, online-compatible neural decoding framework
based on SNNs, whose performance is competitive relative to state-of-the-art
models while consuming orders of magnitude less energy.

</details>


### [101] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: The paper proposes a two-system neural network approach for continual learning: a 'what' system that infers computational context and a 'how' system that implements computations, enabling compositional skill reuse without catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To understand neural mechanisms for continual learning and flexible skill composition, addressing how intelligent systems can learn, retain, and recompose skills efficiently without forgetting previous knowledge.

Method: A two-system RNN approach: (1) a probabilistic generative model ('what' system) that learns task vocabulary incrementally and infers computational context, (2) an RNN ('how' system) with low-rank components composed according to inferred context, enabling unsupervised online learning.

Result: The framework demonstrates effective continual learning without catastrophic forgetting, competitive performance on compositional cognitive tasks, and capabilities for forward/backward transfer and fast generalization to unseen tasks.

Conclusion: The two-system approach provides a viable neural mechanism for continual learning and compositional reuse of computations, offering insights into how biological systems might achieve flexible skill composition and retention.

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [102] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: This study improved fall risk prediction using constrained score optimization on JHFRAT data and EHR variables, achieving better performance than the original tool while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: To better align fall risk prediction with clinically meaningful measures and improve upon the existing Johns Hopkins Fall Risk Assessment Tool through data-driven modeling.

Method: Retrospective analysis of 54,209 inpatient admissions using constrained score optimization models on JHFRAT assessment data and EHR variables, comparing performance against the original JHFRAT and black-box XGBoost models.

Result: The constrained score optimization model significantly outperformed the current JHFRAT (AUC-ROC=0.91 vs 0.86) and demonstrated robustness to risk labeling variations, though XGBoost achieved slightly higher performance (AUC-ROC=0.94).

Conclusion: The evidence-based constrained score optimization approach provides a robust foundation for enhancing inpatient fall prevention protocols and improving risk assessment in healthcare settings while maintaining interpretability.

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [103] [Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2510.20718)
*Daniel Sorensen,Bappaditya Dey,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: This paper proposes two novel approaches for anomaly prediction in semiconductor manufacturing: one using N-BEATS for univariate forecasting assuming variable independence, and another using Graph Neural Networks (GNN) to capture inter-variable relationships. Both methods forecast future values and flag deviations as anomalies, with GNN outperforming N-BEATS in performance while being more computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Semiconductor manufacturing involves complex processes with thousands of interdependent parameters, presenting challenges like high-dimensional sensor data, severe class imbalance due to rare faults, and complex variable interdependencies that complicate anomaly prediction and root-cause analysis.

Method: The framework has two stages: (1) training a forecasting model on anomaly-free data, and (2) performing forecasts on unseen data and flagging deviations beyond threshold as anomalies. Two approaches are compared: N-BEATS for univariate forecasting assuming variable independence, and GNN that captures inter-variable relationships.

Result: Both models demonstrate strong forecasting performance up to 20 time points and stable anomaly prediction up to 50 time points. The GNN consistently outperforms N-BEATS while requiring significantly fewer trainable parameters and lower computational cost.

Conclusion: The GNN approach is positioned as a promising solution for online anomaly forecasting in manufacturing environments, advancing the field from anomaly detection to prediction for real-time process correction and proactive fault prevention.

Abstract: Semiconductor manufacturing is an extremely complex and precision-driven
process, characterized by thousands of interdependent parameters collected
across diverse tools and process steps. Multi-variate time-series analysis has
emerged as a critical field for real-time monitoring and fault detection in
such environments. However, anomaly prediction in semiconductor fabrication
presents several critical challenges, including high dimensionality of sensor
data and severe class imbalance due to the rarity of true faults. Furthermore,
the complex interdependencies between variables complicate both anomaly
prediction and root-cause-analysis. This paper proposes two novel approaches to
advance the field from anomaly detection to anomaly prediction, an essential
step toward enabling real-time process correction and proactive fault
prevention. The proposed anomaly prediction framework contains two main stages:
(a) training a forecasting model on a dataset assumed to contain no anomalies,
and (b) performing forecast on unseen time series data. The forecast is
compared with the forecast of the trained signal. Deviations beyond a
predefined threshold are flagged as anomalies. The two approaches differ in the
forecasting model employed. The first assumes independence between variables by
utilizing the N-BEATS model for univariate time series forecasting. The second
lifts this assumption by utilizing a Graph Neural Network (GNN) to capture
inter-variable relationships. Both models demonstrate strong forecasting
performance up to a horizon of 20 time points and maintain stable anomaly
prediction up to 50 time points. The GNN consistently outperforms the N-BEATS
model while requiring significantly fewer trainable parameters and lower
computational cost. These results position the GNN as promising solution for
online anomaly forecasting to be deployed in manufacturing environments.

</details>


### [104] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: This paper establishes theoretical regret bounds for Thompson sampling in episodic reinforcement learning with Gaussian process priors over rewards and transitions, addressing challenges in analyzing TS for complex temporal structures.


<details>
  <summary>Details</summary>
Motivation: Thompson sampling is widely used in sequential decision-making but lacks strong theoretical foundations, especially in reinforcement learning settings with complex temporal structure. The authors aim to bridge this gap by providing rigorous regret guarantees.

Method: The authors analyze Thompson sampling in episodic RL with joint Gaussian process priors over both rewards and transitions. They extend classical tools like the elliptical potential lemma to handle multi-output settings and address challenges from non-Gaussian value functions and recursive Bellman updates.

Result: The paper proves a regret bound of $\mathcal{\\tilde{O}}(\\sqrt{KH\\Gamma(KH)})$ over $K$ episodes of horizon $H$, where $\\Gamma(\\cdot)$ captures the complexity of the GP model. This provides the first theoretical guarantees for TS in RL with GP priors.

Conclusion: This work advances the theoretical understanding of Thompson sampling in reinforcement learning and demonstrates how structural assumptions and model uncertainty affect performance in finite-horizon Markov Decision Processes.

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [105] [Thought Communication in Multiagent Collaboration](https://arxiv.org/abs/2510.20733)
*Yujia Zheng,Zhuokai Zhao,Zijian Li,Yaqi Xie,Mingze Gao,Lizhu Zhang,Kun Zhang*

Main category: cs.LG

TL;DR: The paper introduces 'thought communication' - a paradigm for direct mind-to-mind interaction between AI agents that goes beyond natural language limitations, enabling more effective collective intelligence.


<details>
  <summary>Details</summary>
Motivation: Natural language is lossy, ambiguous, and indirect, limiting collective intelligence potential. Current LLM-based multi-agent systems rely solely on natural language, which constrains their collaborative capabilities.

Method: Formalize thought communication as a latent variable model where agent states are generated by underlying thoughts. Develop a framework to extract latent thoughts from agents prior to communication and assign relevant thoughts with sharing patterns. The approach works in nonparametric settings without auxiliary information.

Result: Theoretical guarantees for identifying both shared and private latent thoughts between agents and recovering global thought sharing structure. Experiments on synthetic and real-world benchmarks validate the theory and demonstrate collaborative advantages.

Conclusion: Thought communication enables more effective collective intelligence by leveraging hidden generative processes, extending beyond LLMs to all modalities. This approach illuminates the potential of leveraging the hidden world rather than relying solely on surface-level observations.

Abstract: Natural language has long enabled human cooperation, but its lossy,
ambiguous, and indirect nature limits the potential of collective intelligence.
While machines are not subject to these constraints, most LLM-based multi-agent
systems still rely solely on natural language, exchanging tokens or their
embeddings. To go beyond language, we introduce a new paradigm, thought
communication, which enables agents to interact directly mind-to-mind, akin to
telepathy. To uncover these latent thoughts in a principled way, we formalize
the process as a general latent variable model, where agent states are
generated by an unknown function of underlying thoughts. We prove that, in a
nonparametric setting without auxiliary information, both shared and private
latent thoughts between any pair of agents can be identified. Moreover, the
global structure of thought sharing, including which agents share which
thoughts and how these relationships are structured, can also be recovered with
theoretical guarantees. Guided by the established theory, we develop a
framework that extracts latent thoughts from all agents prior to communication
and assigns each agent the relevant thoughts, along with their sharing
patterns. This paradigm naturally extends beyond LLMs to all modalities, as
most observational data arise from hidden generative processes. Experiments on
both synthetic and real-world benchmarks validate the theory and demonstrate
the collaborative advantages of thought communication. We hope this work
illuminates the potential of leveraging the hidden world, as many challenges
remain unsolvable through surface-level observation alone, regardless of
compute or data scale.

</details>


### [106] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: A new Dirichlet Process-driven multimodal learning framework that automatically balances intra-modal representation learning and cross-modal alignment, outperforming competitors on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal fusion approaches over-emphasize cross-modal alignment, which may impose excess regularization and obstruct meaningful representations within each modality. The challenge is preserving feature expressiveness while learning cross-modal interactions.

Method: The framework assumes each modality follows a mixture of multivariate Gaussian distributions and adopts Dirichlet Process to calculate mixture weights, leveraging its richer-gets-richer property to dynamically allocate feature contributions and select prominent features for multimodal fusion.

Result: Extensive experiments on several multimodal datasets demonstrate superior performance over competitors. Ablation analysis validates DP's effectiveness in aligning modality distributions and robustness to hyperparameter changes.

Conclusion: The DP-driven framework successfully achieves optimal balance between prominent intra-modal representation learning and cross-modal alignment, providing an effective solution for multimodal feature fusion.

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [107] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: MEIcoder is a biologically informed decoding method that achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in V1, particularly excelling with small datasets and few neurons.


<details>
  <summary>Details</summary>
Motivation: Visual stimulus decoding from neural activity is crucial for brain understanding and brain-machine interfaces, but biological data is often scarce in primates/humans where high-throughput recording techniques are challenging, posing problems for deep learning decoding methods.

Method: MEIcoder uses neuron-specific most exciting inputs (MEIs), structural similarity index measure loss, and adversarial training to decode visual stimuli from neural population activity.

Result: MEIcoder achieves state-of-the-art performance, can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points, with MEIs identified as the main performance drivers through ablation studies.

Conclusion: The results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications, with a proposed unified benchmark to foster future research.

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [108] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: Transformers trained on chess show compositional generalization by following game rules in out-of-distribution scenarios, perform well on chess puzzles, but struggle with Chess960 compared to symbolic AI algorithms.


<details>
  <summary>Details</summary>
Motivation: To investigate whether decision Transformers truly learn chess rules and exhibit systematic generalization, rather than just memorizing training patterns.

Method: Trained a 270M parameter chess Transformer and tested it on out-of-distribution scenarios including rule extrapolation, chess puzzles, and Chess960 variant to assess generalization capabilities.

Result: Transformers demonstrate strong compositional generalization by consistently choosing valid moves in unfamiliar situations, perform well on OOD puzzles, but show inferior performance in Chess960 compared to symbolic AI algorithms, though the gap is smaller in human gameplay.

Conclusion: Transformers exhibit emergent compositional understanding of chess rules and can generalize systematically, but still lag behind symbolic AI in complex variant scenarios, suggesting room for improvement in reasoning capabilities.

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [109] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: BadGraph is a backdoor attack method targeting latent diffusion models for text-guided graph generation, using textual triggers to poison training data and induce attacker-specified subgraphs during inference while maintaining normal performance on clean inputs.


<details>
  <summary>Details</summary>
Motivation: The rapid progress of graph generation has raised security concerns about backdoor vulnerabilities, particularly in text-guided graph generation which remains largely unexamined compared to image diffusion and unconditional graph generation.

Method: BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs.

Result: Extensive experiments on four benchmark datasets show high effectiveness: less than 10% poisoning rate achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples.

Conclusion: The findings reveal security vulnerabilities in latent diffusion models for text-guided graph generation, highlight serious risks in applications like drug discovery, and underscore the need for robust defenses against backdoor attacks in such diffusion models.

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [110] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: This paper presents a fast adaptation method for LLMs that improves on LASER by eliminating exhaustive layer-by-layer searches, using gradient analysis to identify key matrices, expanding factorization search space, and requiring only 100 samples for evaluation - all without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the impractical overhead of LASER's exhaustive per-matrix search which requires full-dataset forward passes, making it unsuitable for rapid deployment of LLMs to downstream tasks.

Method: Use gradient analysis of singular values to identify key matrices, allow matrices rows to cluster around multiple subspaces for expanded factorization search, and evaluate on only 100 samples for both gradient computation and accuracy measurement.

Result: The method reduces search time significantly, reduces overfitting on original training data, and lifts accuracy by up to 24.6 percentage points compared to the original LASER approach.

Conclusion: Combining these techniques yields a fast and robust adaptation algorithm that can adapt LLMs to new datasets with just a single gradient step on 100 examples and a quick scan of top candidate layers, entirely without fine-tuning.

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [111] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: This paper challenges the common intuition about reverse/forward KL divergence in RL with language models, showing that mode coverage depends on regularization strength and reward scales rather than KL type. The authors develop a simple algorithm that modifies reward magnitudes to optimize for diverse target distributions.


<details>
  <summary>Details</summary>
Motivation: To correct the misconception that reverse KL is mode-seeking and forward KL is mass-covering in RL with language models, and to develop a method that ensures diverse sampling from multiple high-quality modes without external diversity signals.

Method: The authors mathematically analyze how reverse/forward KL regularization affects target distributions, then construct a simple algorithm that minimally modifies reward magnitudes to optimize for target distributions covering all high-quality modes. The method works with both forward and reverse KL.

Result: The proposed algorithm successfully post-trains both Large Language Models and Chemical Language Models, achieving higher solution quality and diversity without external diversity signals. It works with both forward and reverse KL where naive approaches fail.

Conclusion: Mode coverage in RL with language models depends primarily on regularization strength and reward/reference probability scales, not KL divergence type. The developed algorithm provides a theoretically justified, scalable approach for diverse sampling from multiple high-quality modes.

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>
