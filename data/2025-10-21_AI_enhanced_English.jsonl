{"id": "2510.16504", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.16504", "abs": "https://arxiv.org/abs/2510.16504", "authors": ["Jasper Arends", "Guanjie Lyu", "Mhamed Mesfioui", "Elisa Perrone", "Julien Trufin"], "title": "Rank-based concordance for zero-inflated data: New representations, estimators, and sharp bounds", "comment": null, "summary": "Quantifying concordance between two random variables is crucial in\napplications. Traditional estimation techniques for commonly used concordance\nmeasures, such as Gini's gamma or Spearman's rho, often fail when data contain\nties. This is particularly problematic for zero-inflated data, characterized by\na combination of discrete mass in zero and a continuous component, which\nfrequently appear in insurance, weather forecasting, and biomedical\napplications. This study provides a new formulation of Gini's gamma and\nSpearman's footrule, two rank-based concordance measures that incorporate\nabsolute rank differences, tailored to zero-inflated continuous distributions.\nAlong the way, we correct an expression of Spearman's rho for zero-inflated\ndata previously presented in the literature. The best-possible upper and lower\nbounds for these measures in zero-inflated continuous settings are established,\nmaking the estimators useful and interpretable in practice. We pair our\ntheoretical results with simulations and two real-life applications in\ninsurance and weather forecasting, respectively. Our results illustrate the\nimpact of zero inflation on dependence estimation, emphasizing the benefits of\nappropriately adjusted zero-inflated measures.", "AI": {"tldr": "This paper introduces new formulations of Gini's gamma and Spearman's footrule specifically designed for zero-inflated continuous data, correcting previous expressions for Spearman's rho and establishing theoretical bounds for these measures.", "motivation": "Traditional concordance measures fail with tied data, which is problematic for zero-inflated data common in insurance, weather forecasting, and biomedical applications where there's a discrete mass at zero combined with continuous components.", "method": "Developed new formulations of rank-based concordance measures (Gini's gamma and Spearman's footrule) tailored to zero-inflated continuous distributions, established theoretical bounds, and validated through simulations and real-world applications.", "result": "Established best-possible upper and lower bounds for concordance measures in zero-inflated settings, making estimators practical and interpretable. Demonstrated significant impact of zero inflation on dependence estimation through simulations and real applications.", "conclusion": "Appropriately adjusted zero-inflated concordance measures provide substantial benefits over traditional approaches, with the new formulations offering improved accuracy and interpretability for data containing ties and zero inflation."}}
{"id": "2510.16717", "categories": ["stat.ME", "math.ST", "quant-ph", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.16717", "abs": "https://arxiv.org/abs/2510.16717", "authors": ["Johan F. Hoorn"], "title": "Correlation of divergency: c-delta. Being different in a similar way or not", "comment": "9 pages", "summary": "This paper introduces the correlation-of-divergency coefficient, c-delta, a\ncustom statistical measure designed to quantify the similarity of internal\ndivergence patterns between two groups of values. Unlike conventional\ncorrelation coefficients such as Pearson or Spearman, which assess the\nassociation between paired values, c-delta evaluates whether the way values\ndiffer within one group is mirrored in another. The method involves\ncalculating, for each value, its divergence from all other values in its group,\nand then comparing these patterns across the two groups (e.g., human vs machine\nintelligence). The coefficient is normalised by the average root mean square\ndivergence within each group, ensuring scale invariance. Potential applications\nof c-delta span quantum physics, where it can compare the spread of measurement\noutcomes between quantum systems, as well as fields such as genetics, ecology,\npsychometrics, manufacturing, machine learning, and social network analysis.\nThe measure is particularly useful for benchmarking, clustering validation, and\nassessing the similarity of variability structures. While c-delta is not\nbounded between -1 and 1 and may be sensitive to outliers (but so is PMCC), it\noffers a new perspective for analysing internal variability and divergence. The\narticle discusses the mathematical formulation, potential adaptations for\ncomplex data, and the interpretative considerations relevant to this\nalternative approach.", "AI": {"tldr": "The paper introduces c-delta, a new statistical measure that quantifies similarity in internal divergence patterns between two groups, unlike traditional correlation coefficients that assess paired value associations.", "motivation": "To develop a statistical measure that evaluates whether the way values differ within one group is mirrored in another group, addressing limitations of conventional correlation coefficients that only assess paired value associations.", "method": "Calculate for each value its divergence from all other values in its group, then compare these divergence patterns across two groups. The coefficient is normalized by the average root mean square divergence within each group to ensure scale invariance.", "result": "C-delta provides a new perspective for analyzing internal variability and divergence, with applications in quantum physics, genetics, ecology, psychometrics, manufacturing, machine learning, and social network analysis.", "conclusion": "C-delta offers a valuable alternative approach for benchmarking, clustering validation, and assessing similarity of variability structures, though it's not bounded between -1 and 1 and may be sensitive to outliers like PMCC."}}
{"id": "2510.16745", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G10, 62G20, 62P05, 46E22"], "pdf": "https://arxiv.org/pdf/2510.16745", "abs": "https://arxiv.org/abs/2510.16745", "authors": ["Rohan Sen"], "title": "Kernel-Based Nonparametric Tests For Shape Constraints", "comment": "31 pages, 1 figure", "summary": "We develop a reproducing kernel Hilbert space (RKHS) framework for\nnonparametric mean-variance optimization and inference on shape constraints of\nthe optimal rule. We derive statistical properties of the sample estimator and\nprovide rigorous theoretical guarantees, such as asymptotic consistency, a\nfunctional central limit theorem, and a finite-sample deviation bound that\nmatches the Monte Carlo rate up to regularization. Building on these findings,\nwe introduce a joint Wald-type statistic to test for shape constraints over\nfinite grids. The approach comes with an efficient computational procedure\nbased on a pivoted Cholesky factorization, facilitating scalability to large\ndatasets. Empirical tests suggest favorably of the proposed methodology.", "AI": {"tldr": "The paper develops an RKHS framework for nonparametric mean-variance optimization and inference on shape constraints, with theoretical guarantees and efficient computational methods.", "motivation": "To provide rigorous statistical guarantees and efficient computational procedures for nonparametric mean-variance optimization with shape constraints on the optimal rule.", "method": "Uses reproducing kernel Hilbert space (RKHS) framework, derives statistical properties of sample estimator, introduces joint Wald-type statistic for shape constraint testing, and implements efficient computation via pivoted Cholesky factorization.", "result": "Achieves asymptotic consistency, functional central limit theorem, finite-sample deviation bounds matching Monte Carlo rate, and demonstrates favorable empirical performance.", "conclusion": "The proposed RKHS framework provides theoretically sound and computationally efficient methods for nonparametric mean-variance optimization with shape constraint inference, showing promising empirical results."}}
{"id": "2510.17167", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.17167", "abs": "https://arxiv.org/abs/2510.17167", "authors": ["Yong Wu", "Yanwei Fu", "Shouyan Wang", "Yizhou Wang", "Xinwei Sun"], "title": "Discovering Causal Relationships using Proxy Variables under Unmeasured Confounding", "comment": "under review", "summary": "Inferring causal relationships between variable pairs in the observational\nstudy is crucial but challenging, due to the presence of unmeasured\nconfounding. While previous methods employed the negative controls to adjust\nfor the confounding bias, they were either restricted to the discrete setting\n(i.e., all variables are discrete) or relied on strong assumptions for\nidentification. To address these problems, we develop a general nonparametric\napproach that accommodates both discrete and continuous settings for testing\ncausal hypothesis under unmeasured confounders. By using only a single negative\ncontrol outcome (NCO), we establish a new identification result based on a\nnewly proposed integral equation that links the outcome and NCO, requiring only\nthe completeness and mild regularity conditions. We then propose a kernel-based\ntesting procedure that is more efficient than existing moment-restriction\nmethods. We derive the asymptotic level and power properties for our tests.\nFurthermore, we examine cases where our procedure using only NCO fails to\nachieve identification, and introduce a new procedure that incorporates a\nnegative control exposure (NCE) to restore identifiability. We demonstrate the\neffectiveness of our approach through extensive simulations and real-world data\nfrom the Intensive Care Data and World Values Survey.", "AI": {"tldr": "A new nonparametric method for testing causal relationships under unmeasured confounding using negative controls, applicable to both discrete and continuous settings with improved efficiency over existing approaches.", "motivation": "Existing methods for causal inference with unmeasured confounding are limited to discrete settings or require strong assumptions, creating a need for more general and robust approaches.", "method": "Developed a kernel-based testing procedure using a single negative control outcome (NCO) based on a new integral equation, with extensions using negative control exposure (NCE) when NCO alone is insufficient.", "result": "The method achieves asymptotic level and power properties, demonstrates effectiveness through simulations and real-world data from Intensive Care Data and World Values Survey, and shows improved efficiency over moment-restriction methods.", "conclusion": "The proposed approach provides a general framework for causal testing under unmeasured confounding that accommodates both discrete and continuous settings with minimal assumptions."}}
{"id": "2510.16127", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16127", "abs": "https://arxiv.org/abs/2510.16127", "authors": ["Oliver J. Hines", "Caleb H. Miles"], "title": "Learning density ratios in causal inference using Bregman-Riesz regression", "comment": "Replication code is available from\n  https://github.com/CI-NYC/densityratios", "summary": "The ratio of two probability density functions is a fundamental quantity that\nappears in many areas of statistics and machine learning, including causal\ninference, reinforcement learning, covariate shift, outlier detection,\nindependence testing, importance sampling, and diffusion modeling. Naively\nestimating the numerator and denominator densities separately using, e.g.,\nkernel density estimators, can lead to unstable performance and suffers from\nthe curse of dimensionality as the number of covariates increases. For this\nreason, several methods have been developed for estimating the density ratio\ndirectly based on (a) Bregman divergences or (b) recasting the density ratio as\nthe odds in a probabilistic classification model that predicts whether an\nobservation is sampled from the numerator or denominator distribution.\nAdditionally, the density ratio can be viewed as the Riesz representer of a\ncontinuous linear map, making it amenable to estimation via (c) minimization of\nthe so-called Riesz loss, which was developed to learn the Riesz representer in\nthe Riesz regression procedure in causal inference. In this paper we show that\nall three of these methods can be unified in a common framework, which we call\nBregman-Riesz regression. We further show how data augmentation techniques can\nbe used to apply density ratio learning methods to causal problems, where the\nnumerator distribution typically represents an unobserved intervention. We show\nthrough simulations how the choice of Bregman divergence and data augmentation\nstrategy can affect the performance of the resulting density ratio learner. A\nPython package is provided for researchers to apply Bregman-Riesz regression in\npractice using gradient boosting, neural networks, and kernel methods.", "AI": {"tldr": "This paper unifies three different density ratio estimation methods (Bregman divergences, probabilistic classification, and Riesz loss minimization) into a common framework called Bregman-Riesz regression, and shows how data augmentation can be used to apply these methods to causal inference problems.", "motivation": "Density ratio estimation is fundamental in many statistical and machine learning applications, but existing methods are fragmented and naive approaches suffer from instability and the curse of dimensionality. The authors aim to unify different approaches and extend them to causal inference settings.", "method": "The paper proposes Bregman-Riesz regression, which unifies three density ratio estimation approaches: (1) Bregman divergences, (2) probabilistic classification models, and (3) Riesz loss minimization. It also incorporates data augmentation techniques for causal inference applications where the numerator distribution represents unobserved interventions.", "result": "The unified framework shows how different Bregman divergences and data augmentation strategies affect density ratio learning performance. The authors provide a Python package implementing Bregman-Riesz regression using gradient boosting, neural networks, and kernel methods.", "conclusion": "Bregman-Riesz regression provides a unified framework for density ratio estimation that connects previously disparate methods and enables application to causal inference problems through data augmentation techniques."}}
{"id": "2510.16166", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16166", "abs": "https://arxiv.org/abs/2510.16166", "authors": ["Daniel Csillag", "Pedro Dall'Antonia", "Claudio Jos\u00e9 Struchiner", "Guilherme Tegoni Goedert"], "title": "Extending Prediction-Powered Inference through Conformal Prediction", "comment": null, "summary": "Prediction-powered inference is a recent methodology for the safe use of\nblack-box ML models to impute missing data, strengthening inference of\nstatistical parameters. However, many applications require strong properties\nbesides valid inference, such as privacy, robustness or validity under\ncontinuous distribution shifts; deriving prediction-powered methods with such\nguarantees is generally an arduous process, and has to be done case by case. In\nthis paper, we resolve this issue by connecting prediction-powered inference\nwith conformal prediction: by performing imputation through a calibrated\nconformal set-predictor, we attain validity while achieving additional\nguarantees in a natural manner. We instantiate our procedure for the inference\nof means, Z- and M-estimation, as well as e-values and e-value-based\nprocedures. Furthermore, in the case of e-values, ours is the first general\nprediction-powered procedure that operates off-line. We demonstrate these\nadvantages by applying our method on private and time-series data. Both tasks\nare nontrivial within the standard prediction-powered framework but become\nnatural under our method.", "AI": {"tldr": "This paper connects prediction-powered inference with conformal prediction to achieve valid statistical inference with additional guarantees like privacy and robustness, addressing limitations of standard prediction-powered methods.", "motivation": "Standard prediction-powered inference lacks guarantees for privacy, robustness, and validity under continuous distribution shifts, requiring case-by-case solutions for different applications.", "method": "The method performs imputation through calibrated conformal set-predictors, enabling validity while naturally achieving additional guarantees. It's instantiated for means, Z- and M-estimation, e-values, and e-value-based procedures.", "result": "The approach provides the first general prediction-powered procedure that operates off-line for e-values, and demonstrates advantages on private and time-series data where standard methods are nontrivial.", "conclusion": "Connecting prediction-powered inference with conformal prediction creates a unified framework that naturally handles privacy, robustness, and distribution shift concerns while maintaining valid statistical inference."}}
{"id": "2510.15940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15940", "abs": "https://arxiv.org/abs/2510.15940", "authors": ["Jialin Lu", "Kye Emond", "Kaiyu Yang", "Swarat Chaudhuri", "Weiran Sun", "Wuyang Chen"], "title": "Lean Finder: Semantic Search for Mathlib That Understands User Intents", "comment": null, "summary": "We present Lean Finder, a semantic search engine for Lean and mathlib that\nunderstands and aligns with the intents of mathematicians. Progress in formal\ntheorem proving is often hindered by the difficulty of locating relevant\ntheorems and the steep learning curve of the Lean 4 language, making\nadvancement slow and labor-intensive. Existing Lean search engines, though\nhelpful, rely primarily on informalizations (natural language translation of\nthe formal statements), while largely overlooking the mismatch with real-world\nuser queries. In contrast, we propose a user-centered semantic search tailored\nto the needs of mathematicians. Our approach begins by analyzing and clustering\nthe semantics of public Lean discussions, then fine-tuning text embeddings on\nsynthesized queries that emulate user intents. We further align Lean Finder\nwith mathematicians' preferences using diverse feedback signals, encoding it\nwith a rich awareness of their goals from multiple perspectives. Evaluations on\nreal-world queries, informalized statements, and proof states demonstrate that\nour Lean Finder achieves over $30\\%$ relative improvement compared to previous\nsearch engines and GPT-4o. In addition, Lean Finder is compatible with\nLLM-based theorem provers, bridging retrieval with formal reasoning. Lean\nFinder is available at: https://leanfinder.github.io", "AI": {"tldr": "Lean Finder is a semantic search engine for Lean and mathlib that understands mathematician intents through fine-tuned embeddings and user feedback alignment, achieving 30%+ improvement over existing methods.", "motivation": "Progress in formal theorem proving is hindered by difficulty locating relevant theorems and Lean 4's steep learning curve, with existing search engines overlooking real-world user query mismatches.", "method": "Analyze and cluster semantics of public Lean discussions, fine-tune text embeddings on synthesized queries emulating user intents, and align with mathematician preferences using diverse feedback signals.", "result": "Achieves over 30% relative improvement compared to previous search engines and GPT-4o on real-world queries, informalized statements, and proof states.", "conclusion": "Lean Finder provides user-centered semantic search tailored to mathematicians' needs and bridges retrieval with formal reasoning, being compatible with LLM-based theorem provers."}}
{"id": "2510.17567", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.17567", "abs": "https://arxiv.org/abs/2510.17567", "authors": ["Alexsandro A. Ferreira", "Gauss M. Cordeiro"], "title": "The modified odd Burr XII-G family of distributions: Properties and Applications", "comment": null, "summary": "The modified odd Burr XII-G family is developed, capable of incorporating\nbimodal and bathtub shapes in its baseline distributions, with properties\nderived from the exponentiated-G class. A regression model is developed within\nthis family. The parameters are estimated by maximum likelihood, and\nsimulations are performed to verify their consistency. The usefulness of the\nproposals is demonstrated by means of three real data sets.", "AI": {"tldr": "A new Burr XII-G family is developed that can model bimodal and bathtub shapes, with properties from exponentiated-G class. Includes regression model, maximum likelihood estimation, simulations for consistency verification, and real data applications.", "motivation": "To create a flexible distribution family capable of modeling complex shapes like bimodal and bathtub distributions, extending the capabilities of existing baseline distributions.", "method": "Developed modified odd Burr XII-G family based on exponentiated-G class properties. Implemented maximum likelihood estimation for parameter estimation and conducted simulations to verify consistency. Applied to three real datasets.", "result": "Successfully developed a distribution family that can incorporate bimodal and bathtub shapes. Simulations confirmed parameter consistency, and real data applications demonstrated practical usefulness.", "conclusion": "The proposed Burr XII-G family provides a flexible framework for modeling complex distribution shapes, with verified statistical properties and practical applicability to real-world data."}}
{"id": "2510.16232", "categories": ["stat.ML", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16232", "abs": "https://arxiv.org/abs/2510.16232", "authors": ["Chenyu Zhang", "Navid Azizan"], "title": "Personalized Collaborative Learning with Affinity-Based Variance Reduction", "comment": null, "summary": "Multi-agent learning faces a fundamental tension: leveraging distributed\ncollaboration without sacrificing the personalization needed for diverse\nagents. This tension intensifies when aiming for full personalization while\nadapting to unknown heterogeneity levels -- gaining collaborative speedup when\nagents are similar, without performance degradation when they are different.\nEmbracing the challenge, we propose personalized collaborative learning (PCL),\na novel framework for heterogeneous agents to collaboratively learn\npersonalized solutions with seamless adaptivity. Through carefully designed\nbias correction and importance correction mechanisms, our method AffPCL\nrobustly handles both environment and objective heterogeneity. We prove that\nAffPCL reduces sample complexity over independent learning by a factor of\n$\\max\\{n^{-1}, \\delta\\}$, where $n$ is the number of agents and\n$\\delta\\in[0,1]$ measures their heterogeneity. This affinity-based acceleration\nautomatically interpolates between the linear speedup of federated learning in\nhomogeneous settings and the baseline of independent learning, without\nrequiring prior knowledge of the system. Our analysis further reveals that an\nagent may obtain linear speedup even by collaborating with arbitrarily\ndissimilar agents, unveiling new insights into personalization and\ncollaboration in the high heterogeneity regime.", "AI": {"tldr": "Personalized Collaborative Learning (PCL) framework enables heterogeneous agents to learn personalized solutions while automatically adapting to unknown heterogeneity levels, achieving sample complexity reduction that interpolates between federated learning speedup and independent learning.", "motivation": "To resolve the fundamental tension between distributed collaboration and personalization in multi-agent learning, especially when dealing with unknown heterogeneity levels where agents need to gain collaborative speedup when similar without performance degradation when different.", "method": "Proposed AffPCL method with carefully designed bias correction and importance correction mechanisms to robustly handle both environment and objective heterogeneity among agents.", "result": "AffPCL reduces sample complexity over independent learning by a factor of max{n^{-1}, \u03b4}, where n is the number of agents and \u03b4\u2208[0,1] measures heterogeneity. The method automatically interpolates between linear speedup in homogeneous settings and independent learning baseline without requiring prior knowledge.", "conclusion": "The framework enables agents to obtain linear speedup even when collaborating with arbitrarily dissimilar agents, revealing new insights into personalization and collaboration in high heterogeneity regimes."}}
{"id": "2510.15944", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15944", "abs": "https://arxiv.org/abs/2510.15944", "authors": ["Tianyu Bell Pan", "Mengdi Zhu", "Alexa Jordyn Cole", "Ronald Wilson", "Damon L. Woodard"], "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift", "comment": null, "summary": "Multimodal learning systems often struggle in non-stationary environments due\nto concept drift, where changing data distributions can degrade performance.\nModality-specific drifts and the lack of mechanisms for continuous, stable\nadaptation compound this challenge. This paper introduces LS-OGD, a novel\nadaptive control framework for robust multimodal learning in the presence of\nconcept drift. LS-OGD uses an online controller that dynamically adjusts the\nmodel's learning rate and the fusion weights between different data modalities\nin response to detected drift and evolving prediction errors. We prove that\nunder bounded drift conditions, the LS-OGD system's prediction error is\nuniformly ultimately bounded and converges to zero if the drift ceases.\nAdditionally, we demonstrate that the adaptive fusion strategy effectively\nisolates and mitigates the impact of severe modality-specific drift, thereby\nensuring system resilience and fault tolerance. These theoretical guarantees\nestablish a principled foundation for developing reliable and continuously\nadapting multimodal learning systems.", "AI": {"tldr": "LS-OGD is an adaptive control framework for robust multimodal learning that dynamically adjusts learning rates and fusion weights to handle concept drift, ensuring bounded prediction errors and system resilience.", "motivation": "Multimodal learning systems face performance degradation in non-stationary environments due to concept drift, particularly modality-specific drifts and lack of continuous adaptation mechanisms.", "method": "LS-OGD uses an online controller that dynamically adjusts the model's learning rate and fusion weights between different data modalities based on detected drift and evolving prediction errors.", "result": "The framework ensures uniformly ultimately bounded prediction error under bounded drift conditions, with convergence to zero if drift ceases. It effectively isolates and mitigates modality-specific drift impact.", "conclusion": "LS-OGD provides theoretical guarantees for developing reliable and continuously adapting multimodal learning systems with fault tolerance and resilience against concept drift."}}
{"id": "2510.17608", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.17608", "abs": "https://arxiv.org/abs/2510.17608", "authors": ["Gitte Kremling", "Francesco Iafrate", "Mahsa Taheri", "Johannes Lederer"], "title": "Non-asymptotic error bounds for probability flow ODEs under weak log-concavity", "comment": null, "summary": "Score-based generative modeling, implemented through probability flow ODEs,\nhas shown impressive results in numerous practical settings. However, most\nconvergence guarantees rely on restrictive regularity assumptions on the target\ndistribution -- such as strong log-concavity or bounded support. This work\nestablishes non-asymptotic convergence bounds in the 2-Wasserstein distance for\na general class of probability flow ODEs under considerably weaker assumptions:\nweak log-concavity and Lipschitz continuity of the score function. Our\nframework accommodates non-log-concave distributions, such as Gaussian\nmixtures, and explicitly accounts for initialization errors, score\napproximation errors, and effects of discretization via an exponential\nintegrator scheme. Bridging a key theoretical challenge in diffusion-based\ngenerative modeling, our results extend convergence theory to more realistic\ndata distributions and practical ODE solvers. We provide concrete guarantees\nfor the efficiency and correctness of the sampling algorithm, complementing the\nempirical success of diffusion models with rigorous theory. Moreover, from a\npractical perspective, our explicit rates might be helpful in choosing\nhyperparameters, such as the step size in the discretization.", "AI": {"tldr": "This paper establishes non-asymptotic convergence bounds for score-based generative modeling using probability flow ODEs under weak assumptions of weak log-concavity and Lipschitz continuity of the score function, accommodating non-log-concave distributions like Gaussian mixtures.", "motivation": "Most existing convergence guarantees for score-based generative modeling rely on restrictive regularity assumptions like strong log-concavity or bounded support, which limits applicability to realistic data distributions. This work aims to bridge this theoretical gap.", "method": "The authors use probability flow ODEs with an exponential integrator scheme, explicitly accounting for initialization errors, score approximation errors, and discretization effects. They establish convergence bounds in the 2-Wasserstein distance.", "result": "The paper provides non-asymptotic convergence bounds for a general class of probability flow ODEs under considerably weaker assumptions than previous work, extending convergence theory to more realistic data distributions and practical ODE solvers.", "conclusion": "This work bridges a key theoretical challenge in diffusion-based generative modeling by extending convergence theory to practical scenarios, providing concrete guarantees for sampling algorithm efficiency and correctness, and offering practical guidance for hyperparameter selection."}}
{"id": "2510.16419", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16419", "abs": "https://arxiv.org/abs/2510.16419", "authors": ["Jiayi Guo", "Haoxuan Li", "Ye Tian", "Peng Wu"], "title": "A Relative Error-Based Evaluation Framework of Heterogeneous Treatment Effect Estimators", "comment": null, "summary": "While significant progress has been made in heterogeneous treatment effect\n(HTE) estimation, the evaluation of HTE estimators remains underdeveloped. In\nthis article, we propose a robust evaluation framework based on relative error,\nwhich quantifies performance differences between two HTE estimators. We first\nderive the key theoretical conditions on the nuisance parameters that are\nnecessary to achieve a robust estimator of relative error. Building on these\nconditions, we introduce novel loss functions and design a neural network\narchitecture to estimate nuisance parameters and obtain robust estimation of\nrelative error, thereby achieving reliable evaluation of HTE estimators. We\nprovide the large sample properties of the proposed relative error estimator.\nFurthermore, beyond evaluation, we propose a new learning algorithm for HTE\nthat leverages both the previously HTE estimators and the nuisance parameters\nlearned through our neural network architecture. Extensive experiments\ndemonstrate that our evaluation framework supports reliable comparisons across\nHTE estimators, and the proposed learning algorithm for HTE exhibits desirable\nperformance.", "AI": {"tldr": "A robust evaluation framework for heterogeneous treatment effect (HTE) estimators using relative error, with theoretical conditions, novel loss functions, neural network architecture, and a new HTE learning algorithm.", "motivation": "Current evaluation methods for heterogeneous treatment effect estimators are underdeveloped, creating a need for robust evaluation frameworks to reliably compare different HTE estimators.", "method": "Proposed relative error-based evaluation framework with theoretical conditions on nuisance parameters, novel loss functions, neural network architecture for nuisance parameter estimation, and a new HTE learning algorithm that leverages existing estimators.", "result": "The framework achieves robust estimation of relative error, supports reliable comparisons across HTE estimators, and the proposed learning algorithm demonstrates desirable performance in extensive experiments.", "conclusion": "The proposed evaluation framework provides reliable assessment of HTE estimators, and the integrated learning algorithm enhances HTE estimation performance by leveraging both existing estimators and learned nuisance parameters."}}
{"id": "2510.16317", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16317", "abs": "https://arxiv.org/abs/2510.16317", "authors": ["Siqi Cao", "Shu Yang"], "title": "Heterogeneity-Aware Federated Causal Inference Leveraging Effect-Measure Transportability", "comment": null, "summary": "Federated learning of causal estimands offers a powerful strategy to improve\nestimation efficiency by leveraging data from multiple study sites while\npreserving privacy. Existing literature has primarily focused on the average\ntreatment effect using single data source, whereas our work addresses a broader\nclass of causal measures across multiple sources. We derive and compare\nsemiparametrically efficient estimators under two transportability assumptions,\nwhich impose different restrictions on the data likelihood and illustrate the\nefficiency-robustness tradeoff. This estimator also permits the incorporation\nof flexible machine learning algorithms for nuisance functions while\nmaintaining parametric convergence rates and nominal coverage. To further\nhandle scenarios where some source sites violate transportability, we propose a\nPost-Federated Weighting Selection (PFWS) framework, which is a two-step\nprocedure that adaptively identifies compatible sites and achieves the\nsemiparametric efficiency bound asymptotically. This framework mitigates the\nefficiency loss of weighting methods and the instability and computational\nburden of direct site selection in finite samples. Through extensive\nsimulations and real-data analysis, we demonstrate that our PFWS framework\nachieves superior variance efficiency compared with the target-only analyses\nacross diverse transportability scenarios.", "AI": {"tldr": "This paper proposes federated learning methods for causal estimands that improve estimation efficiency by leveraging multiple data sources while preserving privacy. It introduces semiparametric efficient estimators under different transportability assumptions and a Post-Federated Weighting Selection framework to handle incompatible sites.", "motivation": "To address the limitation of existing literature focusing on average treatment effects using single data sources, and to develop more efficient causal estimation methods that can leverage multiple study sites while maintaining privacy protection.", "method": "The authors derive semiparametrically efficient estimators under two transportability assumptions and propose a Post-Federated Weighting Selection (PFWS) framework - a two-step procedure that adaptively identifies compatible sites and achieves semiparametric efficiency bounds asymptotically. The method incorporates flexible machine learning algorithms for nuisance functions.", "result": "Through extensive simulations and real-data analysis, the PFWS framework demonstrates superior variance efficiency compared to target-only analyses across diverse transportability scenarios, while maintaining parametric convergence rates and nominal coverage.", "conclusion": "The proposed federated learning approach for causal estimands effectively improves estimation efficiency by leveraging multiple data sources, with the PFWS framework providing robust performance even when some source sites violate transportability assumptions, outperforming traditional methods in efficiency and stability."}}
{"id": "2510.15945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15945", "abs": "https://arxiv.org/abs/2510.15945", "authors": ["Guangya Wan", "Zixin Stephen Xu", "Sasa Zorc", "Manel Baucells", "Mengxuan Hu", "Hao Wang", "Sheng Li"], "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling", "comment": "Under review on ARR", "summary": "Sampling multiple responses is a common way to improve LLM output quality,\nbut it comes at the cost of additional computation. The key challenge is\ndeciding when to stop generating new samples to balance accuracy gains against\nefficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive\nCriterion for Optimal N-stopping), a principled adaptive sampling framework\ngrounded in Sequential Search with Bayesian Learning. BEACON sequentially\ngenerates responses from the policy LLM, updates posterior belief over reward\ndistributions in real time without further training, and determines when to\nstop by weighing expected gains against computational cost. Sampling terminates\nonce the marginal utility of further exploration no longer justifies the\nexpense. We establish both theoretical optimality guarantees and practical\ntractability, and show empirically that BEACON reduces average sampling by up\nto 80% while maintaining response quality. We further demonstrate BEACON's\nutility for cost-efficient preference data generation and outline practical\nextensions, offering actionable insights for future researchers.", "AI": {"tldr": "BEACON is an adaptive sampling framework that uses Bayesian learning to determine when to stop generating multiple LLM responses, reducing sampling by up to 80% while maintaining quality.", "motivation": "Sampling multiple responses improves LLM output quality but increases computational costs. The key challenge is determining the optimal stopping point to balance accuracy gains against efficiency.", "method": "BEACON uses Sequential Search with Bayesian Learning to sequentially generate responses, update posterior belief over reward distributions in real time without training, and stop when marginal utility no longer justifies computational cost.", "result": "BEACON reduces average sampling by up to 80% while maintaining response quality. It also demonstrates utility for cost-efficient preference data generation.", "conclusion": "BEACON provides a principled adaptive sampling framework with theoretical optimality guarantees and practical tractability, offering actionable insights for efficient LLM sampling strategies."}}
{"id": "2510.16509", "categories": ["stat.ML", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.16509", "abs": "https://arxiv.org/abs/2510.16509", "authors": ["Ziad Ghanem", "Chang Hyunwoong", "Preskella Mrad"], "title": "A Bayesian Framework for Symmetry Inference in Chaotic Attractors", "comment": null, "summary": "Detecting symmetry from data is a fundamental problem in signal analysis,\nproviding insight into underlying structure and constraints. When data emerge\nas trajectories of dynamical systems, symmetries encode structural properties\nof the dynamics that enable model reduction, principled comparison across\nconditions, and detection of regime changes. While recent optimal transport\nmethods provide practical tools for data-driven symmetry detection in this\nsetting, they rely on deterministic thresholds and lack uncertainty\nquantification, limiting robustness to noise and ability to resolve\nhierarchical symmetry structures. We present a Bayesian framework that\nformulates symmetry detection as probabilistic model selection over a lattice\nof candidate subgroups, using a Gibbs posterior constructed from Wasserstein\ndistances between observed data and group-transformed copies. We establish\nthree theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal\nsymmetry consistent with data, $(ii)$ conjugation equivariance ensuring\nframe-independence, and $(iii)$ stability bounds under perturbations for\nrobustness to noise. Posterior inference is performed via Metropolis-Hastings\nsampling and numerical experiments on equivariant dynamical systems and\nsynthetic point clouds demonstrate accurate symmetry recovery under high noise\nand small sample sizes. An application to human gait dynamics reveals symmetry\nchanges induced by mechanical constraints, demonstrating the framework's\nutility for statistical inference in biomechanical and dynamical systems.", "AI": {"tldr": "A Bayesian framework for symmetry detection in dynamical systems using Wasserstein distances and probabilistic model selection over candidate symmetry subgroups, with theoretical guarantees and applications to noisy data and biomechanical systems.", "motivation": "Existing symmetry detection methods rely on deterministic thresholds and lack uncertainty quantification, limiting robustness to noise and ability to resolve hierarchical symmetry structures in dynamical systems.", "method": "Bayesian framework formulates symmetry detection as probabilistic model selection over candidate subgroups using Gibbs posterior constructed from Wasserstein distances between observed data and group-transformed copies, with Metropolis-Hastings sampling for posterior inference.", "result": "The method demonstrates accurate symmetry recovery under high noise and small sample sizes in numerical experiments on equivariant dynamical systems and synthetic point clouds, and reveals symmetry changes in human gait dynamics induced by mechanical constraints.", "conclusion": "The Bayesian framework provides robust symmetry detection with uncertainty quantification, enabling statistical inference in biomechanical and dynamical systems while establishing theoretical guarantees for minimal symmetry, frame-independence, and noise robustness."}}
{"id": "2510.16337", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16337", "abs": "https://arxiv.org/abs/2510.16337", "authors": ["Hairi Bai", "Xinyan Fan", "Kuangnan Fang", "Yan Zhang"], "title": "Common-Individual Embedding for Dynamic Networks with Temporal Group Structure", "comment": "35 pages, 3 figures, 8 tables", "summary": "We propose STANE (Shared and Time-specific Adaptive Network Embedding), a new\njoint embedding framework for dynamic networks that captures both stable global\nstructures and localized temporal variations. To further improve the model's\nadaptability to transient changes, we introduce Sparse STANE, which models\ntime-specific changes as sparse perturbations, thereby improving\ninterpretability. Unlike existing methods that either overlook cross-time\nsimilarities or enforce overly smooth evolution, Sparse STANE integrates\ntemporal clustering with sparse deviation modeling to strike a flexible balance\nbetween persistence and change. We also provide non-asymptotic error guarantees\nof embeddings and show that our estimator can reliably identify changed node\npairs when deviations are sparse. On synthetic and real-world political\nconflict networks, STANE and its extensions improve temporal clustering\naccuracy and structural recovery, outperforming state-of-the-art baselines.\nThese results highlight the potential of STANE in applications such as\ninternational relations modeling, where both persistent and transient\nconnections matter. Our findings underscore the power of structured dynamic\nembeddings for revealing interpretable patterns in network evolution.", "AI": {"tldr": "STANE is a dynamic network embedding framework that captures both stable global structures and localized temporal variations, with Sparse STANE adding sparse perturbations for better adaptability and interpretability.", "motivation": "Existing methods either overlook cross-time similarities or enforce overly smooth evolution, failing to balance persistence and change in dynamic networks.", "method": "Integrates temporal clustering with sparse deviation modeling, modeling time-specific changes as sparse perturbations to capture both stable structures and transient variations.", "result": "Outperforms state-of-the-art baselines on synthetic and real-world political conflict networks, improving temporal clustering accuracy and structural recovery with non-asymptotic error guarantees.", "conclusion": "STANE demonstrates the power of structured dynamic embeddings for revealing interpretable patterns in network evolution, particularly valuable in applications like international relations modeling where both persistent and transient connections matter."}}
{"id": "2510.15946", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15946", "abs": "https://arxiv.org/abs/2510.15946", "authors": ["Wenshuo Wang", "Ziyou Jiang", "Junjie Wang", "Mingyang Li", "Jie Huang", "Yuekai Huang", "Zhiyuan Chang", "Feiyan Duan", "Qing Wang"], "title": "Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns", "comment": "12 Pages, Submitted to WWW'26", "summary": "Internet memes have emerged as a popular multimodal medium, yet they are\nincreasingly weaponized to convey harmful opinions through subtle rhetorical\ndevices like irony and metaphor. Existing detection approaches, including\nMLLM-based techniques, struggle with these implicit expressions, leading to\nfrequent misjudgments. This paper introduces PatMD, a novel approach that\nimproves harmful meme detection by learning from and proactively mitigating\nthese potential misjudgment risks. Our core idea is to move beyond superficial\ncontent-level matching and instead identify the underlying misjudgment risk\npatterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We\nfirst construct a knowledge base where each meme is deconstructed into a\nmisjudgment risk pattern explaining why it might be misjudged, either\noverlooking harmful undertones (false negative) or overinterpreting benign\ncontent (false positive). For a given target meme, PatMD retrieves relevant\npatterns and utilizes them to dynamically guide the MLLM's reasoning.\nExperiments on a benchmark of 6,626 memes across 5 harmful detection tasks show\nthat PatMD outperforms state-of-the-art baselines, achieving an average of\n8.30\\% improvement in F1-score and 7.71\\% improvement in accuracy,\ndemonstrating strong generalizability and improved detection capability of\nharmful memes.", "AI": {"tldr": "PatMD improves harmful meme detection by identifying misjudgment risk patterns and proactively guiding MLLMs to avoid known pitfalls, achieving significant performance improvements over state-of-the-art methods.", "motivation": "Internet memes are increasingly weaponized to convey harmful opinions through subtle rhetorical devices like irony and metaphor, and existing detection approaches struggle with these implicit expressions, leading to frequent misjudgments.", "method": "PatMD constructs a knowledge base where memes are deconstructed into misjudgment risk patterns, then retrieves relevant patterns for target memes to dynamically guide MLLM reasoning and avoid known misjudgment pitfalls.", "result": "Experiments on 6,626 memes across 5 harmful detection tasks show PatMD outperforms state-of-the-art baselines with 8.30% improvement in F1-score and 7.71% improvement in accuracy.", "conclusion": "PatMD demonstrates strong generalizability and improved detection capability for harmful memes by proactively mitigating misjudgment risks through pattern-based guidance."}}
{"id": "2510.16551", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16551", "abs": "https://arxiv.org/abs/2510.16551", "authors": ["Khaled Boughanmi", "Kamel Jedidi", "Nour Jedidi"], "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction", "comment": null, "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.", "AI": {"tldr": "This research proposes an LLM-based framework for extracting product/service attributes, features, and sentiments from customer reviews, demonstrating high accuracy and significant efficiency gains over manual coding while providing actionable business insights.", "motivation": "To develop a scalable, efficient method for analyzing customer reviews that provides interpretable and managerially actionable insights, overcoming the limitations of time-consuming manual coding processes.", "method": "Systematic LLM approach grounded in marketing theory, distinguishing perceptual attributes from actionable features, applied to 20,000 Yelp reviews of Starbucks stores with evaluation of eight prompt variants against human annotations.", "result": "High consistency between LLMs and human coders (comparable insights), strong predictive validity for customer ratings, and massive efficiency improvement (2 seconds vs 6 minutes per review). Identified key attributes influencing satisfaction with potential for 1-2% revenue gains.", "conclusion": "LLM-based review analysis provides reliable, scalable insights for businesses to identify joy/pain points, enabling targeted interventions and actionable marketing dashboards with significant efficiency advantages over manual methods."}}
{"id": "2510.15947", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC", "68T07, 92C55, 62M10", "I.2.6; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.15947", "abs": "https://arxiv.org/abs/2510.15947", "authors": ["Casper van Laar", "Khubaib Ahmed"], "title": "WaveNet's Precision in EEG Classification", "comment": "6 pages, 5 figures and 3 tables. Includes main text and bibliography", "summary": "This study introduces a WaveNet-based deep learning model designed to\nautomate the classification of EEG signals into physiological, pathological,\nartifact, and noise categories. Traditional methods for EEG signal\nclassification, which rely on expert visual review, are becoming increasingly\nimpractical due to the growing complexity and volume of EEG recordings.\nLeveraging a publicly available annotated dataset from Mayo Clinic and St.\nAnne's University Hospital, the WaveNet model was trained, validated, and\ntested on 209,232 samples with a 70/20/10 percent split. The model achieved a\nclassification accuracy exceeding previous CNN and LSTM-based approaches, and\nwas benchmarked against a Temporal Convolutional Network (TCN) baseline.\nNotably, the model distinguishes noise and artifacts with high precision,\nalthough it reveals a modest but explainable degree of misclassification\nbetween physiological and pathological signals, reflecting inherent clinical\noverlap. WaveNet's architecture, originally developed for raw audio synthesis,\nis well suited for EEG data due to its use of dilated causal convolutions and\nresidual connections, enabling it to capture both fine-grained and long-range\ntemporal dependencies. The research also details the preprocessing pipeline,\nincluding dynamic dataset partitioning and normalization steps that support\nmodel generalization.", "AI": {"tldr": "WaveNet-based deep learning model for automated EEG signal classification outperforms previous CNN and LSTM approaches, achieving high accuracy in distinguishing physiological, pathological, artifact, and noise categories.", "motivation": "Traditional EEG classification methods relying on expert visual review are becoming impractical due to increasing complexity and volume of EEG recordings, necessitating automated solutions.", "method": "Used WaveNet architecture with dilated causal convolutions and residual connections, trained on 209,232 annotated EEG samples from Mayo Clinic and St. Anne's University Hospital with 70/20/10 train/validation/test split, including preprocessing with dynamic dataset partitioning and normalization.", "result": "Achieved classification accuracy exceeding previous CNN and LSTM-based approaches, with high precision in distinguishing noise and artifacts, though showed modest misclassification between physiological and pathological signals due to clinical overlap.", "conclusion": "WaveNet architecture is well-suited for EEG classification due to its ability to capture both fine-grained and long-range temporal dependencies, providing an effective automated solution for EEG signal analysis."}}
{"id": "2510.16587", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16587", "abs": "https://arxiv.org/abs/2510.16587", "authors": ["Byoungwoo Park", "Juho Lee"], "title": "Multi-Marginal Schr\u00f6dinger Bridge Matching", "comment": null, "summary": "Understanding the continuous evolution of populations from discrete temporal\nsnapshots is a critical research challenge, particularly in fields like\ndevelopmental biology and systems medicine where longitudinal tracking of\nindividual entities is often impossible. Such trajectory inference is vital for\nunraveling the mechanisms of dynamic processes. While Schr\\\"odinger Bridge (SB)\noffer a potent framework, their traditional application to pairwise time points\ncan be insufficient for systems defined by multiple intermediate snapshots.\nThis paper introduces Multi-Marginal Schr\\\"odinger Bridge Matching (MSBM), a\nnovel algorithm specifically designed for the multi-marginal SB problem. MSBM\nextends iterative Markovian fitting (IMF) to effectively handle multiple\nmarginal constraints. This technique ensures robust enforcement of all\nintermediate marginals while preserving the continuity of the learned global\ndynamics across the entire trajectory. Empirical validations on synthetic data\nand real-world single-cell RNA sequencing datasets demonstrate the competitive\nor superior performance of MSBM in capturing complex trajectories and\nrespecting intermediate distributions, all with notable computational\nefficiency.", "AI": {"tldr": "MSBM is a novel algorithm for multi-marginal Schr\u00f6dinger Bridge problems that extends iterative Markovian fitting to handle multiple intermediate snapshots, enabling robust trajectory inference from discrete temporal data with computational efficiency.", "motivation": "Traditional Schr\u00f6dinger Bridge methods are limited to pairwise time points, which is insufficient for systems with multiple intermediate snapshots where longitudinal tracking of individual entities is impossible, particularly in developmental biology and systems medicine.", "method": "Multi-Marginal Schr\u00f6dinger Bridge Matching (MSBM) extends iterative Markovian fitting (IMF) to handle multiple marginal constraints, ensuring robust enforcement of all intermediate marginals while preserving continuity of learned global dynamics across the entire trajectory.", "result": "Empirical validations on synthetic data and real-world single-cell RNA sequencing datasets demonstrate MSBM's competitive or superior performance in capturing complex trajectories and respecting intermediate distributions with notable computational efficiency.", "conclusion": "MSBM provides an effective solution for trajectory inference from multiple temporal snapshots, addressing limitations of traditional pairwise SB methods and showing strong performance in biological applications."}}
{"id": "2510.16429", "categories": ["stat.ME", "62R10, 62H11"], "pdf": "https://arxiv.org/pdf/2510.16429", "abs": "https://arxiv.org/abs/2510.16429", "authors": ["Muge Mutis", "Ufuk Beyaztas", "Filiz Karaman", "Han Lin Shang"], "title": "Spatial Scalar-on-Function Quantile Regression Model", "comment": "51 pages, 6 figures, 4 tables", "summary": "This paper introduces a novel spatial scalar-on-function quantile regression\nmodel that extends classical scalar-on-function models to account for spatial\ndependence and heterogeneous conditional distributions. The proposed model\nincorporates spatial autocorrelation through a spatially lagged response and\ncharacterizes the entire conditional distribution of a scalar outcome given a\nfunctional predictor. To address the endogeneity induced by the spatial lag\nterm, we develop two robust estimation procedures based on instrumental\nvariable strategies. $\\sqrt{n}$-consistency and asymptotic normality of the\nproposed estimators are established under mild regularity conditions. We\ndemonstrate through extensive Monte Carlo simulations that the proposed\nestimators outperform existing mean-based and robust alternatives, particularly\nin settings with strong spatial dependence and outlier contamination. We apply\nour method to high-resolution environmental data from the Lombardy region in\nItaly, using daily ozone trajectories to predict daily mean particulate matter\nwith a diameter of less than 2.5 micrometers concentrations. The empirical\nresults confirm the superiority of our approach in predictive accuracy,\nrobustness, and interpretability across various quantile levels. Our method has\nbeen implemented in the \\texttt{ssofqrm} R package.", "AI": {"tldr": "This paper introduces a spatial scalar-on-function quantile regression model that incorporates spatial dependence and handles heterogeneous conditional distributions, with robust estimation procedures for spatial endogeneity.", "motivation": "To extend classical scalar-on-function models to account for spatial dependence and characterize entire conditional distributions rather than just means, addressing limitations in handling spatial autocorrelation and outlier contamination.", "method": "Developed a spatial scalar-on-function quantile regression model with spatially lagged response, using instrumental variable strategies to address endogeneity from spatial lag terms, with two robust estimation procedures.", "result": "The proposed estimators achieve \u221an-consistency and asymptotic normality, outperform existing methods in simulations with strong spatial dependence and outliers, and show superior predictive accuracy in environmental data application.", "conclusion": "The method provides improved predictive accuracy, robustness, and interpretability across quantile levels, with implementation available in the ssofqrm R package."}}
{"id": "2510.15950", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15950", "abs": "https://arxiv.org/abs/2510.15950", "authors": ["Arianna Francesconi", "Donato Cappetta", "Fabio Rebecchi", "Paolo Soda", "Valerio Guarrasi", "Rosa Sicilia"], "title": "Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics", "comment": "Proceedings of the Workshop on Artificial Intelligence for Biomedical\n  Data (AIBio 2025), 28th European Conference on Artificial Intelligence 2025,\n  Springer CCIS", "summary": "Parkinson's disease (PD) presents a growing global challenge, affecting over\n10 million individuals, with prevalence expected to double by 2040. Early\ndiagnosis remains difficult due to the late emergence of motor symptoms and\nlimitations of traditional clinical assessments. In this study, we propose a\nnovel pipeline that leverages keystroke dynamics as a non-invasive and scalable\nbiomarker for remote PD screening and telemonitoring. Our methodology involves\nthree main stages: (i) preprocessing of data from four distinct datasets,\nextracting four temporal signals and addressing class imbalance through the\ncomparison of three methods; (ii) pre-training eight state-of-the-art\ndeep-learning architectures on the two largest datasets, optimizing temporal\nwindowing, stride, and other hyperparameters; (iii) fine-tuning on an\nintermediate-sized dataset and performing external validation on a fourth,\nindependent cohort. Our results demonstrate that hybrid convolutional-recurrent\nand transformer-based models achieve strong external validation performance,\nwith AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal\nconvolutional model attains an AUC-ROC of 91.14% in external validation,\noutperforming existing methods that rely solely on internal validation. These\nfindings underscore the potential of keystroke dynamics as a reliable digital\nbiomarker for PD, offering a promising avenue for early detection and\ncontinuous monitoring.", "AI": {"tldr": "This paper proposes a keystroke dynamics-based pipeline for Parkinson's disease screening using deep learning models, achieving over 90% AUC-ROC in external validation.", "motivation": "Parkinson's disease affects over 10 million people with prevalence expected to double by 2040, and early diagnosis is challenging due to late motor symptom emergence and limitations of traditional clinical assessments.", "method": "Three-stage pipeline: (1) preprocessing data from four datasets with temporal signal extraction and class imbalance handling, (2) pre-training eight deep-learning architectures with hyperparameter optimization, (3) fine-tuning on intermediate dataset and external validation on independent cohort.", "result": "Hybrid convolutional-recurrent and transformer models achieved strong external validation with AUC-ROC >90% and F1-Score >70%. A temporal convolutional model attained 91.14% AUC-ROC in external validation, outperforming existing methods.", "conclusion": "Keystroke dynamics show promise as a reliable digital biomarker for Parkinson's disease, offering potential for early detection and continuous monitoring through non-invasive remote screening."}}
{"id": "2510.16612", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.16612", "abs": "https://arxiv.org/abs/2510.16612", "authors": ["Eli N. Weinstein", "Andrei Slabodkin", "Mattia G. Gollub", "Elizabeth B. Wood"], "title": "Accelerated Learning on Large Scale Screens using Generative Library Models", "comment": null, "summary": "Biological machine learning is often bottlenecked by a lack of scaled data.\nOne promising route to relieving data bottlenecks is through high throughput\nscreens, which can experimentally test the activity of $10^6-10^{12}$ protein\nsequences in parallel. In this article, we introduce algorithms to optimize\nhigh throughput screens for data creation and model training. We focus on the\nlarge scale regime, where dataset sizes are limited by the cost of measurement\nand sequencing. We show that when active sequences are rare, we maximize\ninformation gain if we only collect positive examples of active sequences, i.e.\n$x$ with $y>0$. We can correct for the missing negative examples using a\ngenerative model of the library, producing a consistent and efficient estimate\nof the true $p(y | x)$. We demonstrate this approach in simulation and on a\nlarge scale screen of antibodies. Overall, co-design of experiments and\ninference lets us accelerate learning dramatically.", "AI": {"tldr": "This paper introduces algorithms to optimize high-throughput screens for biological machine learning by focusing on collecting only positive examples when active sequences are rare, and using generative models to correct for missing negatives, achieving dramatic learning acceleration.", "motivation": "Biological machine learning faces data bottlenecks due to limited scaled data. High-throughput screens can test millions to trillions of protein sequences in parallel but are constrained by measurement and sequencing costs.", "method": "The authors propose algorithms that collect only positive examples (active sequences) when active sequences are rare, then use generative models of the library to correct for missing negative examples, producing consistent estimates of p(y|x).", "result": "The approach was demonstrated in simulation and on a large-scale antibody screen, showing that co-design of experiments and inference dramatically accelerates learning.", "conclusion": "By co-designing experiments and inference, the researchers achieved significant learning acceleration in biological machine learning through optimized high-throughput screening strategies that maximize information gain while minimizing data collection costs."}}
{"id": "2510.15954", "categories": ["cs.LG", "cs.CE", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15954", "abs": "https://arxiv.org/abs/2510.15954", "authors": ["Hongzheng Shi", "Yuhang Wang", "Xiao Liu"], "title": "Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter", "comment": null, "summary": "As wildfires become increasingly destructive and expensive to control,\neffective management of active wildfires requires accurate, real-time fire\nspread predictions. To enhance the forecasting accuracy of active fires, data\nassimilation plays a vital role by integrating observations (such as\nremote-sensing data) and fire predictions generated from numerical models. This\npaper provides a comprehensive investigation on the application of a recently\nproposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter\n(EnSF) -- to the data assimilation problem for real-time active wildfire spread\npredictions. Leveraging a score-based generative diffusion model, EnSF has been\nshown to have superior accuracy for high-dimensional nonlinear filtering\nproblems, making it an ideal candidate for the filtering problems of wildfire\nspread models. Technical details are provided, and our numerical investigations\ndemonstrate that EnSF provides superior accuracy, stability, and computational\nefficiency, establishing it as a robust and practical method for wildfire data\nassimilation. Our code has been made publicly available.", "AI": {"tldr": "The paper introduces the Ensemble Score Filter (EnSF), a diffusion-model-based filtering algorithm, for real-time wildfire spread prediction through data assimilation, showing superior accuracy and efficiency.", "motivation": "As wildfires become more destructive and costly to control, there is a need for accurate real-time fire spread predictions. Data assimilation can improve forecasting by integrating observations with numerical models.", "method": "The paper applies the Ensemble Score Filter (EnSF), a diffusion-model-based filtering algorithm, to wildfire data assimilation. EnSF leverages score-based generative diffusion models for high-dimensional nonlinear filtering problems.", "result": "Numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency compared to other methods for wildfire data assimilation.", "conclusion": "EnSF is established as a robust and practical method for wildfire data assimilation, with publicly available code for implementation."}}
{"id": "2510.16652", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16652", "abs": "https://arxiv.org/abs/2510.16652", "authors": ["Zihan Wang", "Yi-Ping Chen", "Tuba Dolar", "Wei Chen"], "title": "ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design", "comment": null, "summary": "Modern scientific and engineering design increasingly involves distributed\noptimization, where agents such as laboratories, simulations, or industrial\npartners pursue related goals under differing conditions. These agents often\nface heterogeneities in objectives, evaluation budgets, and accessible design\nvariables, which complicates coordination and can lead to redundancy, poor\nresource use, and ineffective information sharing. Bayesian Optimization (BO)\nis a widely used decision-making framework for expensive black box functions,\nbut its single-agent formulation assumes centralized control and full data\nsharing. Recent collaborative BO methods relax these assumptions, yet they\noften require uniform resources, fully shared input spaces, and fixed task\nalignment, conditions rarely satisfied in practice. To address these\nchallenges, we introduce Adaptive Resource Aware Collaborative Bayesian\nOptimization (ARCO-BO), a framework that explicitly accounts for heterogeneity\nin multi-agent optimization. ARCO-BO combines three components: a similarity\nand optima-aware consensus mechanism for adaptive information sharing, a\nbudget-aware asynchronous sampling strategy for resource coordination, and a\npartial input space sharing for heterogeneous design spaces. Experiments on\nsynthetic and high-dimensional engineering problems show that ARCO-BO\nconsistently outperforms independent BO and existing collaborative BO via\nconsensus approach, achieving robust and efficient performance in complex\nmulti-agent settings.", "AI": {"tldr": "ARCO-BO is a collaborative Bayesian Optimization framework that addresses heterogeneity in multi-agent optimization through adaptive information sharing, resource coordination, and partial input space sharing.", "motivation": "Distributed optimization faces challenges with heterogeneous objectives, evaluation budgets, and accessible design variables across different agents, leading to poor coordination and resource utilization. Existing collaborative BO methods assume uniform resources and fully shared input spaces, which are rarely satisfied in practice.", "method": "ARCO-BO combines three key components: similarity and optima-aware consensus mechanism for adaptive information sharing, budget-aware asynchronous sampling strategy for resource coordination, and partial input space sharing for heterogeneous design spaces.", "result": "Experiments on synthetic and high-dimensional engineering problems show ARCO-BO consistently outperforms independent BO and existing collaborative BO via consensus approach, achieving robust and efficient performance in complex multi-agent settings.", "conclusion": "ARCO-BO provides an effective framework for multi-agent optimization that explicitly handles heterogeneity, enabling better coordination and resource utilization in distributed optimization scenarios."}}
{"id": "2510.16560", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16560", "abs": "https://arxiv.org/abs/2510.16560", "authors": ["Jean-Baptiste Baitairian", "Bernard Sebastien", "Rana Jreich", "Sandrine Katsahian", "Agathe Guilloux"], "title": "Sensitivity Analysis to Unobserved Confounders: A Comparative Review to Estimate Confounding Strength in Sensitivity Models", "comment": null, "summary": "Causal inference is only valid when its underlying assumptions are satisfied,\none of the most central being the ignorability assumption (also known as\nunconfoundedness or exogeneity). In practice, however, this assumption is often\nunrealistic in observational studies, as some confounding variables may remain\nunobserved. To address this limitation, sensitivity models for Inverse\nProbability Weighting (IPW) estimators, known as Marginal Sensitivity Models,\nhave been introduced, allowing for a controlled relaxation of ignorability.\nOver the past decades, a substantial body of literature has emerged around\nthese models, aiming to derive sharp and robust bounds for both binary and\ncontinuous treatment effects. A key element of these approaches is the\nspecification of a sensitivity parameter, sometimes referred to as the\n\"confounding strength\", which quantifies the extent of deviation from\nignorability. Yet, determining an appropriate value for this parameter is\nchallenging, and the final interpretation of sensitivity analyses can be\nunclear. We believe these difficulties represent major obstacles to the\nadoption of such methods in practice. In this review, after introducing\nsensitivity analyses for IPW estimators, we focus on different strategies to\nestimate or lower bound the confounding strength, select the most suitable\napproach, and avoid common pitfalls in the interpretation of results.", "AI": {"tldr": "This paper reviews sensitivity analysis methods for Inverse Probability Weighting (IPW) estimators, focusing on Marginal Sensitivity Models that relax the ignorability assumption in causal inference when unobserved confounding exists.", "motivation": "The ignorability assumption in causal inference is often unrealistic in observational studies due to unobserved confounding variables. While sensitivity models exist to address this limitation, determining appropriate sensitivity parameters and interpreting results remain challenging obstacles to practical adoption.", "method": "The paper reviews different strategies for sensitivity analysis using Marginal Sensitivity Models for IPW estimators, including methods to estimate or lower bound the confounding strength parameter and approaches for selecting suitable methods while avoiding interpretation pitfalls.", "result": "The review synthesizes existing literature on deriving sharp and robust bounds for treatment effects under sensitivity models, covering both binary and continuous treatments, and provides guidance on parameter specification and result interpretation.", "conclusion": "The paper aims to facilitate practical adoption of sensitivity analysis methods by addressing key challenges in parameter determination and interpretation, providing a comprehensive overview of strategies to handle unobserved confounding in causal inference."}}
{"id": "2510.16657", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16657", "abs": "https://arxiv.org/abs/2510.16657", "authors": ["Bingji Yi", "Qiyuan Liu", "Yuwei Cheng", "Haifeng Xu"], "title": "Escaping Model Collapse via Synthetic Data Verification: Near-term Improvements and Long-term Convergence", "comment": "26 pages, 6 figures", "summary": "Synthetic data has been increasingly used to train frontier generative\nmodels. However, recent study raises key concerns that iteratively retraining a\ngenerative model on its self-generated synthetic data may keep deteriorating\nmodel performance, a phenomenon often coined model collapse. In this paper, we\ninvestigate ways to modify this synthetic retraining process to avoid model\ncollapse, and even possibly help reverse the trend from collapse to\nimprovement. Our key finding is that by injecting information through an\nexternal synthetic data verifier, whether a human or a better model, synthetic\nretraining will not cause model collapse. To develop principled understandings\nof the above insight, we situate our analysis in the foundational linear\nregression setting, showing that iterative retraining with verified synthetic\ndata can yield near-term improvements but ultimately drives the parameter\nestimate to the verifier's \"knowledge center\" in the long run. Our theory hence\npredicts that, unless the verifier is perfectly reliable, the early gains will\nplateau and may even reverse. Indeed, these theoretical insights are further\nconfirmed by our experiments on both linear regression as well as Variational\nAutoencoders (VAEs) trained on MNIST data.", "AI": {"tldr": "The paper investigates how to prevent model collapse in synthetic data retraining by using external verifiers, showing that verified synthetic data can avoid collapse and even improve performance, though gains may plateau if verifiers are imperfect.", "motivation": "To address concerns about model collapse - the deterioration of model performance when iteratively retraining on self-generated synthetic data - and explore methods to prevent or reverse this trend.", "method": "The study uses external synthetic data verifiers (human or better models) to inject information during synthetic retraining. Theoretical analysis is conducted in linear regression settings, with experimental validation on both linear regression and Variational Autoencoders (VAEs) trained on MNIST data.", "result": "Iterative retraining with verified synthetic data prevents model collapse and can yield near-term improvements. However, the parameter estimate ultimately converges to the verifier's \"knowledge center,\" meaning gains plateau and may reverse if the verifier is imperfect.", "conclusion": "External verification is crucial for preventing model collapse in synthetic data retraining, but the long-term performance depends on the reliability of the verifier, with imperfect verifiers leading to plateaued or reversed gains."}}
{"id": "2510.16673", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16673", "abs": "https://arxiv.org/abs/2510.16673", "authors": ["Yuki Ohnishi", "Michael J. Daniels", "Lei Yang", "Fan Li"], "title": "Identification and estimation of causal mechanisms in cluster-randomized trials with post-treatment confounding using Bayesian nonparametrics", "comment": "74 pages", "summary": "Causal mediation analysis in cluster-randomized trials (CRTs) is essential\nfor explaining how cluster-level interventions affect individual outcomes, yet\nit is complicated by interference, post-treatment confounding, and hierarchical\ncovariate adjustment. We develop a Bayesian nonparametric framework that\nsimultaneously accommodates interference and a post-treatment confounder that\nprecedes the mediator. Identification is achieved through a multivariate\nGaussian copula that replaces cross-world independence with a single dependence\nparameter, yielding a built-in sensitivity analysis to residual post-treatment\nconfounding. For estimation, we introduce a nested common atoms enriched\nDirichlet process (CA-EDP) prior that integrates the Common Atoms Model (CAM)\nto share information across clusters while capturing between- and\nwithin-cluster heterogeneity, and an Enriched Dirichlet Process (EDP) structure\ndelivering robust covariate adjustment without impacting the outcome model. We\nprovide formal theoretical support for our prior by deriving the model's key\ndistributional properties, including its partially exchangeable partition\nstructure, and by establishing convergence guarantees for the practical\ntruncation-based posterior inference strategy. We demonstrate the performance\nof the proposed methods in simulations and provide further illustration through\na reanalysis of a completed CRT.", "AI": {"tldr": "A Bayesian nonparametric framework for causal mediation analysis in cluster-randomized trials that handles interference and post-treatment confounding using a multivariate Gaussian copula and nested Dirichlet process priors.", "motivation": "Causal mediation analysis in CRTs is complicated by interference, post-treatment confounding, and hierarchical covariate adjustment, requiring methods that can simultaneously address these challenges.", "method": "Bayesian nonparametric framework with multivariate Gaussian copula for identification and nested common atoms enriched Dirichlet process (CA-EDP) prior for estimation, integrating CAM for cross-cluster information sharing and EDP for robust covariate adjustment.", "result": "The method provides formal theoretical support with distributional properties and convergence guarantees, and demonstrates good performance in simulations and reanalysis of a completed CRT.", "conclusion": "The proposed framework effectively addresses key challenges in CRT mediation analysis by accommodating interference and post-treatment confounding while providing robust estimation and theoretical guarantees."}}
{"id": "2510.15960", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.15960", "abs": "https://arxiv.org/abs/2510.15960", "authors": ["Sana Kordoghli", "Abdelhakim Settar", "Oumayma Belaati", "Mohammad Alkhatib"], "title": "Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling", "comment": "41 pages, 21 figures", "summary": "This work contributes to advancing sustainable energy and waste management\nstrategies by investigating the thermochemical conversion of food-based biomass\nthrough pyrolysis, highlighting the role of artificial intelligence (AI) in\nenhancing process modelling accuracy and optimization efficiency. The main\nobjective is to explore the potential of underutilized biomass resources, such\nas spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen\nproduction. Specifically, it aims to optimize the pyrolysis process while\nevaluating the performance of these resources both individually and as blends.\nProximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC\nanalyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS\n- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential\nbut had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1\nexhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic\nmodelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS\nas the most accurate. These approaches provide a detailed understanding of the\npyrolysis process, with particular emphasis on the integration of artificial\nintelligence. An LSTM model trained with lignocellulosic data predicted TGA\ncurves with exceptional accuracy (R^2: 0.9996-0.9998).", "AI": {"tldr": "This study investigates thermochemical conversion of food-based biomass (spent coffee grounds and date seeds) for sustainable hydrogen production, using AI-enhanced pyrolysis modeling and optimization.", "motivation": "To advance sustainable energy and waste management by exploring underutilized biomass resources for hydrogen production and improving process efficiency through artificial intelligence integration.", "method": "Conducted comprehensive analyses including proximate, ultimate, fiber, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC analyses on pure and blended biomass samples. Used isoconversional methods (KAS, FWO, Friedman) for kinetic modeling and trained an LSTM model with lignocellulosic data.", "result": "Blend 3 (25% DS - 75% SCG) offered superior hydrogen yield potential but highest activation energy (313.24 kJ/mol), while Blend 1 (75% DS - 25% SCG) had best activation energy (161.75 kJ/mol). KAS method was most accurate for kinetic modeling. LSTM model achieved exceptional TGA curve prediction accuracy (R\u00b2: 0.9996-0.9998).", "conclusion": "The integration of artificial intelligence with thermochemical conversion processes enables accurate modeling and optimization of biomass pyrolysis for sustainable hydrogen production, demonstrating the potential of underutilized food-based biomass resources."}}
{"id": "2510.16675", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16675", "abs": "https://arxiv.org/abs/2510.16675", "authors": ["Daniel Augusto de Souza", "Yuchen Zhu", "Harry Jake Cunningham", "Yuri Saporito", "Diego Mesquita", "Marc Peter Deisenroth"], "title": "Infinite Neural Operators: Gaussian processes on functions", "comment": "Accepted at the Conference on Neural Information Processing Systems\n  (NeurIPS) 2025", "summary": "A variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and\ntransformers) induce Gaussian process (GP) priors over their outputs. These\nrelationships provide both an accurate characterization of the prior predictive\ndistribution and enable the use of GP machinery to improve the uncertainty\nquantification of deep neural networks. In this work, we extend this connection\nto neural operators (NOs), a class of models designed to learn mappings between\nfunction spaces. Specifically, we show conditions for when arbitrary-depth NOs\nwith Gaussian-distributed convolution kernels converge to function-valued GPs.\nBased on this result, we show how to compute the covariance functions of these\nNO-GPs for two NO parametrizations, including the popular Fourier neural\noperator (FNO). With this, we compute the posteriors of these GPs in regression\nscenarios, including PDE solution operators. This work is an important step\ntowards uncovering the inductive biases of current FNO architectures and opens\na path to incorporate novel inductive biases for use in kernel-based operator\nlearning methods.", "AI": {"tldr": "This paper extends the neural network-Gaussian process connection to neural operators, showing conditions for convergence to function-valued GPs and computing covariance functions for practical applications like PDE solution operators.", "motivation": "To improve uncertainty quantification in neural operators by establishing their connection to Gaussian processes, similar to what exists for standard neural networks, and to uncover the inductive biases of current architectures.", "method": "The authors derive conditions for when arbitrary-depth neural operators with Gaussian-distributed convolution kernels converge to function-valued Gaussian processes, and compute the covariance functions for different parametrizations including Fourier neural operators.", "result": "The paper successfully establishes the neural operator-Gaussian process connection, provides computable covariance functions, and demonstrates how to compute GP posteriors in regression scenarios including PDE solution operators.", "conclusion": "This work enables better uncertainty quantification for neural operators, reveals their inductive biases, and opens paths for incorporating novel inductive biases in kernel-based operator learning methods."}}
{"id": "2510.15961", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15961", "abs": "https://arxiv.org/abs/2510.15961", "authors": ["Yiyang Li", "Zehong Wang", "Zhengqing Yuan", "Zheyuan Zhang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "title": "Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use", "comment": null, "summary": "Illicit drug use among teenagers and young adults (TYAs) remains a pressing\npublic health concern, with rising prevalence and long-term impacts on health\nand well-being. To detect illicit drug use among TYAs, researchers analyze\nlarge-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the\nNational Survey on Drug Use and Health (NSDUH), which preserve rich\ndemographic, psychological, and environmental factors related to substance use.\nHowever, existing modeling methods treat survey variables independently,\noverlooking latent and interconnected structures among them. To address this\nlimitation, we propose LAMI (LAtent relation Mining with bi-modal\nInterpretability), a novel joint graph-language modeling framework for\ndetecting illicit drug use and interpreting behavioral risk factors among TYAs.\nLAMI represents individual responses as relational graphs, learns latent\nconnections through a specialized graph structure learning layer, and\nintegrates a large language model to generate natural language explanations\ngrounded in both graph structures and survey semantics. Experiments on the YRBS\nand NSDUH datasets show that LAMI outperforms competitive baselines in\npredictive accuracy. Interpretability analyses further demonstrate that LAMI\nreveals meaningful behavioral substructures and psychosocial pathways, such as\nfamily dynamics, peer influence, and school-related distress, that align with\nestablished risk factors for substance use.", "AI": {"tldr": "LAMI is a joint graph-language modeling framework that detects illicit drug use in teenagers and young adults by learning latent connections in survey data and generating natural language explanations.", "motivation": "Existing methods treat survey variables independently, overlooking latent and interconnected structures among demographic, psychological, and environmental factors related to substance use.", "method": "LAMI represents individual survey responses as relational graphs, learns latent connections through a specialized graph structure learning layer, and integrates a large language model to generate natural language explanations.", "result": "Experiments on YRBS and NSDUH datasets show LAMI outperforms competitive baselines in predictive accuracy and reveals meaningful behavioral substructures and psychosocial pathways.", "conclusion": "LAMI effectively detects illicit drug use while providing interpretable insights into behavioral risk factors like family dynamics, peer influence, and school-related distress."}}
{"id": "2510.16728", "categories": ["stat.ML", "cs.LG", "math.PR", "stat.ME", "60L10, 60L20, 62G05, 62G08"], "pdf": "https://arxiv.org/pdf/2510.16728", "abs": "https://arxiv.org/abs/2510.16728", "authors": ["Christian Bayer", "Davit Gogolashvili", "Luca Pelizzari"], "title": "Local regression on path spaces with signature metrics", "comment": null, "summary": "We study nonparametric regression and classification for path-valued data. We\nintroduce a functional Nadaraya-Watson estimator that combines the signature\ntransform from rough path theory with local kernel regression. The signature\ntransform provides a principled way to encode sequential data through iterated\nintegrals, enabling direct comparison of paths in a natural metric space. Our\napproach leverages signature-induced distances within the classical kernel\nregression framework, achieving computational efficiency while avoiding the\nscalability bottlenecks of large-scale kernel matrix operations. We establish\nfinite-sample convergence bounds demonstrating favorable statistical properties\nof signature-based distances compared to traditional metrics in\ninfinite-dimensional settings. We propose robust signature variants that\nprovide stability against outliers, enhancing practical performance.\nApplications to both synthetic and real-world data - including stochastic\ndifferential equation learning and time series classification - demonstrate\ncompetitive accuracy while offering significant computational advantages over\nexisting methods.", "AI": {"tldr": "A functional Nadaraya-Watson estimator combining signature transforms from rough path theory with kernel regression for path-valued data, achieving computational efficiency and competitive performance in regression and classification tasks.", "motivation": "To develop efficient methods for nonparametric regression and classification of path-valued data by leveraging signature transforms to enable principled comparison of sequential data while avoiding computational bottlenecks of traditional kernel methods.", "method": "Functional Nadaraya-Watson estimator that integrates signature transforms (providing iterated integral encodings of paths) with local kernel regression, using signature-induced distances within kernel framework and proposing robust variants for outlier stability.", "result": "Established finite-sample convergence bounds showing favorable statistical properties, demonstrated competitive accuracy on synthetic and real-world data including SDE learning and time series classification, with significant computational advantages over existing methods.", "conclusion": "Signature-based kernel regression provides an effective framework for path-valued data analysis, combining theoretical guarantees with practical computational efficiency and robustness, offering a scalable alternative to traditional functional data methods."}}
{"id": "2510.16798", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16798", "abs": "https://arxiv.org/abs/2510.16798", "authors": ["Helene Charlotte Wiese Rytgaard", "Mark van der Laan"], "title": "Causal inference for calibrated scaling interventions on time-to-event processes", "comment": null, "summary": "This work studies stochastic interventions in continuous-time event-history\nsettings formulated as multiplicative scalings of the observed intensity\ngoverning an intermediate event process. This gives rise to a family of causal\nestimands indexed by a scalar parameter {\\alpha}, which changes the event rate\nwhile preserving the temporal and covariate structure of the data-generating\nprocess. We introduce calibrated interventions, where \\(\\alpha\\) is chosen to\nachieve a pre-specified goal, such as a desired level of cumulative risk of the\nintermediate event, and define corresponding composite target parameters\ncapturing the resulting effects on the outcome process. Our proposal enables\npractical yet statistically principled intervention analysis in survival and\nlongitudinal settings, which offers a flexible alternative to deterministic or\nstatic interventions that are often ill-defined. The framework applies broadly\nto causal questions involving time-to-event treatments or mediators, and offers\na pragmatic analogue to indirect/direct effect decompositions. We present the\nefficient influence curves for various versions of target parameters under a\nnonparametric statistical model, discuss their double robustness properties,\nand propose an estimation procedure based on targeted maximum likelihood\nestimation (TMLE). The proposed estimands are illustrated through examples of\nevent-history scenarios addressing distinct causal questions.", "AI": {"tldr": "This paper introduces stochastic interventions in continuous-time event-history settings using multiplicative intensity scalings, creating causal estimands indexed by parameter \u03b1. It proposes calibrated interventions to achieve specific goals and presents estimation methods using targeted maximum likelihood estimation.", "motivation": "To provide a flexible alternative to deterministic interventions in survival and longitudinal settings, enabling practical yet statistically principled intervention analysis for time-to-event treatments or mediators.", "method": "Uses multiplicative scalings of observed intensity for intermediate event processes, defines calibrated interventions with parameter \u03b1 chosen to achieve specific goals, and employs targeted maximum likelihood estimation (TMLE) with efficient influence curves.", "result": "Develops a framework for causal estimands indexed by \u03b1 that preserve temporal and covariate structure, with double robustness properties for various target parameters under nonparametric models.", "conclusion": "The proposed stochastic intervention framework offers a flexible and pragmatic approach for causal analysis in event-history settings, serving as an analogue to indirect/direct effect decompositions and applicable to various causal questions involving time-to-event treatments."}}
{"id": "2510.15962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15962", "abs": "https://arxiv.org/abs/2510.15962", "authors": ["Zhuxuanzi Wang", "Mingqiao Mo", "Xi Xiao", "Chen Liu", "Chenrui Ma", "Yunbei Zhang", "Xiao Wang", "Smita Krishnaswamy", "Tianyang Wang"], "title": "CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has become the standard approach for\nadapting large language models under limited compute and memory budgets.\nAlthough previous methods improve efficiency through low-rank updates,\nquantization, or heuristic budget reallocation, they often decouple the\nallocation of capacity from the way updates evolve during training. In this\nwork, we introduce CTR-LoRA, a framework guided by curvature trust region that\nintegrates rank scheduling with stability-aware optimization. CTR-LoRA\nallocates parameters based on marginal utility derived from lightweight\nsecond-order proxies and constrains updates using a Fisher/Hessian-metric trust\nregion. Experiments on multiple open-source backbones (7B-13B), evaluated on\nboth in-distribution and out-of-distribution benchmarks, show consistent\nimprovements over strong PEFT baselines. In addition to increased accuracy,\nCTR-LoRA enhances training stability, reduces memory requirements, and achieves\nhigher throughput, positioning it on the Pareto frontier of performance and\nefficiency. These results highlight a principled path toward more robust and\ndeployable PEFT.", "AI": {"tldr": "CTR-LoRA is a parameter-efficient fine-tuning framework that integrates rank scheduling with stability-aware optimization using curvature trust regions, achieving better performance and efficiency than existing PEFT methods.", "motivation": "Current PEFT methods often decouple capacity allocation from how updates evolve during training, leading to suboptimal efficiency and performance.", "method": "CTR-LoRA uses curvature trust regions to allocate parameters based on marginal utility from second-order proxies and constrains updates using Fisher/Hessian-metric trust regions.", "result": "Experiments on 7B-13B models show consistent improvements over strong PEFT baselines on both in-distribution and out-of-distribution benchmarks, with enhanced training stability, reduced memory requirements, and higher throughput.", "conclusion": "CTR-LoRA provides a principled path toward more robust and deployable PEFT by positioning it on the Pareto frontier of performance and efficiency."}}
{"id": "2510.16975", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16975", "abs": "https://arxiv.org/abs/2510.16975", "authors": ["Lin Yu", "Zhihui Liu", "Kathy Han", "Olli Saarela"], "title": "Causal Variance Decompositions for Measuring Health Inequalities", "comment": null, "summary": "Recent causal inference literature has introduced causal effect\ndecompositions to quantify sources of observed inequalities or disparities in\noutcomes but usually limiting this to pairwise comparisons. In the context of\nhospital profiling, comparison of hospital performance may reveal inequalities\nin healthcare delivery between sociodemographic groups, which may be explained\nby access/selection or actual effect modification. We consider the case of\npolytomous exposures in hospital profiling where the comparison is often to the\nsystem wide average performance, and decompose the observed variance in care\ndelivery as the quantity of interest. For this, we formulate a new eight-way\ncausal variance decomposition where we attribute the observed variation to\ncomponents describing the main effects of hospital and group membership,\nmodification of the hospital effect by group membership, hospital\naccess/selection, effect of case-mix covariates and residual variance. We\ndiscuss the causal interpretation of the components, formulate parametric and\nnonparametric model based estimators and study the properties of these\nestimators through simulation. Finally, we illustrate our method by an example\nof cancer care delivery using data from the SEER database.", "AI": {"tldr": "This paper introduces an eight-way causal variance decomposition method for analyzing healthcare disparities in hospital profiling, focusing on polytomous exposures and comparing hospital performance to system-wide averages.", "motivation": "To address limitations in existing causal inference methods that typically focus on pairwise comparisons, particularly in hospital profiling where understanding sources of healthcare disparities between sociodemographic groups is crucial for identifying access/selection issues versus actual effect modification.", "method": "Developed a new eight-way causal variance decomposition framework that attributes observed variation in care delivery to: main effects of hospital and group membership, modification of hospital effect by group membership, hospital access/selection, case-mix covariates effects, and residual variance. Formulated both parametric and nonparametric model-based estimators.", "result": "The method provides causal interpretation of variance components and was validated through simulation studies. Applied to cancer care delivery data from the SEER database to demonstrate practical implementation.", "conclusion": "The proposed eight-way decomposition offers a comprehensive framework for quantifying sources of healthcare disparities in hospital profiling, enabling better understanding of whether observed inequalities stem from access/selection issues or actual effect modification, with applications demonstrated in cancer care delivery analysis."}}
{"id": "2510.15964", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15964", "abs": "https://arxiv.org/abs/2510.15964", "authors": ["Tuowei Wang", "Kun Li", "Zixu Hao", "Donglin Bai", "Ju Ren", "Yaoxue Zhang", "Ting Cao", "Mao Yang"], "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity", "comment": null, "summary": "The adaptation of pre-trained large language models (LLMs) to diverse\ndownstream tasks via fine-tuning is critical for numerous applications.\nHowever, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques\npresents significant challenges in terms of time investments and operational\ncosts. In this paper, we first introduce a nuanced form of sparsity, termed\nShadowy Sparsity, which is distinctive in fine-tuning and has not been\nadequately addressed for acceleration. Under Shadowy Sparsity, we propose Long\nExposure, an efficient system to accelerate PEFT for LLMs. Long Exposure\ncomprises three key components: Shadowy-sparsity Exposer employs a prolonged\nsensing range to capture more sparsity details under shadowy sparsity;\nSequence-oriented Predictor provides efficient yet accurate predictions to\nhandle large sequence inputs and constantly-evolving parameters; and\nDynamic-aware Operator facilitates more structured computational patterns and\ncoalesced memory accesses, addressing dynamic sparse operations. Extensive\nevaluations show that Long Exposure outperforms state-of-the-arts with up to a\n$2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements\nin accelerating PEFT for LLMs.", "AI": {"tldr": "Long Exposure is an efficient system that accelerates parameter-efficient fine-tuning (PEFT) for large language models by addressing Shadowy Sparsity through three components: Shadowy-sparsity Exposer, Sequence-oriented Predictor, and Dynamic-aware Operator.", "motivation": "The inefficiency of parameter-efficient fine-tuning techniques presents significant challenges in terms of time investments and operational costs for adapting pre-trained LLMs to downstream tasks.", "method": "The system addresses Shadowy Sparsity with three key components: 1) Shadowy-sparsity Exposer uses prolonged sensing to capture sparsity details, 2) Sequence-oriented Predictor handles large sequence inputs and evolving parameters, 3) Dynamic-aware Operator enables structured computational patterns and coalesced memory accesses.", "result": "Extensive evaluations show Long Exposure outperforms state-of-the-art methods with up to 2.49\u00d7 speedup in end-to-end fine-tuning.", "conclusion": "Long Exposure offers promising advancements in accelerating PEFT for LLMs by effectively addressing the unique challenges of Shadowy Sparsity in fine-tuning scenarios."}}
{"id": "2510.16937", "categories": ["stat.ML", "cs.LG", "stat.ME", "G.3"], "pdf": "https://arxiv.org/pdf/2510.16937", "abs": "https://arxiv.org/abs/2510.16937", "authors": ["Vikram Kher", "Argyris Oikonomou", "Manolis Zampetakis"], "title": "Prediction-Augmented Trees for Reliable Statistical Inference", "comment": "45 pages, 9 Figures", "summary": "The remarkable success of machine learning (ML) in predictive tasks has led\nscientists to incorporate ML predictions as a core component of the scientific\ndiscovery pipeline. This was exemplified by the landmark achievement of\nAlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions\ncan be safely used in statistical analysis of data towards scientific\ndiscovery. In particular, we follow the framework introduced by Angelopoulos et\nal. (2023). In this framework, we assume access to a small set of $n$\ngold-standard labeled samples, a much larger set of $N$ unlabeled samples, and\na ML model that can be used to impute the labels of the unlabeled data points.\nWe introduce two new learning-augmented estimators: (1) Prediction-Augmented\nResidual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both\nestimators have significant advantages over existing estimators like PPI and\nPPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),\nrespectively. PART is a decision-tree based estimator built using a greedy\ncriterion. We first characterize PART's asymptotic distribution and demonstrate\nhow to construct valid confidence intervals. Then we show that PART outperforms\nexisting methods in real-world datasets from ecology, astronomy, and census\nreports, among other domains. This leads to estimators with higher confidence,\nwhich is the result of using both the gold-standard samples and the machine\nlearning predictions. Finally, we provide a formal proof of the advantage of\nPART by exploring PAQ, an estimation that arises when considering the limit of\nPART when the depth its tree grows to infinity. Under appropriate assumptions\nin the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}\n+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing\nmethods.", "AI": {"tldr": "This paper introduces two new learning-augmented estimators (PART and PAQ) that safely incorporate ML predictions into statistical analysis for scientific discovery, improving upon existing methods like PPI and PPI++ with better confidence intervals and faster variance reduction.", "motivation": "With ML's success in predictive tasks (like AlphaFold), scientists need safe ways to incorporate ML predictions into statistical analysis for scientific discovery, using limited gold-standard labeled data alongside abundant unlabeled data.", "method": "Two new estimators: PART (Prediction-Augmented Residual Tree) - a decision-tree based estimator with greedy criterion, and PAQ (Prediction-Augmented Quadrature) - the theoretical limit of PART with infinite tree depth. Both combine gold-standard samples with ML predictions.", "result": "PART outperforms existing methods in real-world datasets from ecology, astronomy, and census reports, providing higher confidence. PAQ achieves variance reduction at rate O(N^{-1} + n^{-4}), significantly improving on existing O(N^{-1} + n^{-1}) rates.", "conclusion": "The proposed PART and PAQ estimators provide safer and more effective ways to incorporate ML predictions into scientific statistical analysis, with proven theoretical advantages and superior empirical performance across multiple domains."}}
{"id": "2510.16992", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16992", "abs": "https://arxiv.org/abs/2510.16992", "authors": ["Haiyan Liu", "Jeanine Houwing-Duistermaat"], "title": "Functional principal component analysis for functional data with detection limits", "comment": null, "summary": "When measurements fall below or above a detection threshold, the resulting\ndata are missing not at random (MNAR), posing challenges for statistical\nanalysis. For example, in longitudinal biomarker studies, observations may be\nsubject to detection limits. Functional principal component analysis (FPCA) is\ncommonly used method for dimension reduction of dense and sparse data measured\nalong a continuum, but standard approaches typically ignore MNAR mechanisms by\nimputing detection limit values, leading to biased estimates of principal\ncomponents and scores.\n  Building on recent work by Liu and Houwing-Duistermaat (2022, 2023), who\nproposed estimators for the mean and covariance functions under detection\nlimits, we extend FPCA to accommodate functional data affected by such limits.\nWe derive the asymptotic properties of the resulting estimators and assess\ntheir performance through simulations, comparing them to standard methods.\nFinally, we illustrate our approach using longitudinal biomarker data subject\nto detection limits.\n  Our method yields more accurate estimates of functional principal components\nand scores, enhancing the reliability of functional data analysis in the\npresence of detection limits.", "AI": {"tldr": "The paper extends functional principal component analysis (FPCA) to handle data with detection limits, addressing missing not at random (MNAR) issues in longitudinal biomarker studies where observations fall below or above detection thresholds.", "motivation": "Standard FPCA methods ignore MNAR mechanisms by imputing detection limit values, leading to biased estimates of principal components and scores in longitudinal biomarker studies with detection limits.", "method": "Building on Liu and Houwing-Duistermaat's estimators for mean and covariance functions under detection limits, the authors extend FPCA to accommodate functional data affected by such limits and derive asymptotic properties of the resulting estimators.", "result": "Simulations show the proposed method yields more accurate estimates of functional principal components and scores compared to standard methods that ignore MNAR mechanisms.", "conclusion": "The extended FPCA approach enhances the reliability of functional data analysis in the presence of detection limits by providing more accurate estimation of principal components and scores."}}
{"id": "2510.15965", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15965", "abs": "https://arxiv.org/abs/2510.15965", "authors": ["Mohan Zhang", "Yihua Zhang", "Jinghan Jia", "Zhangyang Wang", "Sijia Liu", "Tianlong Chen"], "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model", "comment": "NeurIPS 2025", "summary": "Modern large reasoning models (LRMs) exhibit impressive multi-step\nproblem-solving via chain-of-thought (CoT) reasoning. However, this iterative\nthinking mechanism introduces a new vulnerability surface. We present the\nDeadlock Attack, a resource exhaustion method that hijacks an LRM's generative\ncontrol flow by training a malicious adversarial embedding to induce perpetual\nreasoning loops. Specifically, the optimized embedding encourages transitional\ntokens (e.g., \"Wait\", \"But\") after reasoning steps, preventing the model from\nconcluding its answer. A key challenge we identify is the\ncontinuous-to-discrete projection gap: na\\\"ive projections of adversarial\nembeddings to token sequences nullify the attack. To overcome this, we\nintroduce a backdoor implantation strategy, enabling reliable activation\nthrough specific trigger tokens. Our method achieves a 100% attack success rate\nacross four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three\nmath reasoning benchmarks, forcing models to generate up to their maximum token\nlimits. The attack is also stealthy (in terms of causing negligible utility\nloss on benign user inputs) and remains robust against existing strategies\ntrying to mitigate the overthinking issue. Our findings expose a critical and\nunderexplored security vulnerability in LRMs from the perspective of reasoning\n(in)efficiency.", "AI": {"tldr": "The Deadlock Attack is a resource exhaustion method that hijacks large reasoning models' generative control flow using malicious adversarial embeddings to induce perpetual reasoning loops, preventing models from concluding answers.", "motivation": "To expose a critical security vulnerability in large reasoning models where chain-of-thought reasoning introduces a new attack surface for resource exhaustion through perpetual thinking loops.", "method": "Optimized adversarial embeddings encourage transitional tokens after reasoning steps, combined with backdoor implantation strategy to overcome the continuous-to-discrete projection gap and enable reliable attack activation through specific trigger tokens.", "result": "Achieved 100% attack success rate across four advanced LRMs and three math reasoning benchmarks, forcing models to generate up to maximum token limits while being stealthy and robust against existing mitigation strategies.", "conclusion": "The findings reveal an underexplored security vulnerability in LRMs from the perspective of reasoning inefficiency, exposing critical risks in modern reasoning models."}}
{"id": "2510.16986", "categories": ["stat.ML", "cs.LG", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.16986", "abs": "https://arxiv.org/abs/2510.16986", "authors": ["Hamza Cherkaoui", "H\u00e9l\u00e8ne Halconruy", "Yohan Petetin"], "title": "Adaptive Sample Sharing for Linear Regression", "comment": null, "summary": "In many business settings, task-specific labeled data are scarce or costly to\nobtain, which limits supervised learning on a specific task. To address this\nchallenge, we study sample sharing in the case of ridge regression: leveraging\nan auxiliary data set while explicitly protecting against negative transfer. We\nintroduce a principled, data-driven rule that decides how many samples from an\nauxiliary dataset to add to the target training set. The rule is based on an\nestimate of the transfer gain i.e. the marginal reduction in the predictive\nerror. Building on this estimator, we derive finite-sample guaranties: under\nstandard conditions, the procedure borrows when it improves parameter\nestimation and abstains otherwise. In the Gaussian feature setting, we analyze\nwhich data set properties ensure that borrowing samples reduces the predictive\nerror. We validate the approach in synthetic and real datasets, observing\nconsistent gains over strong baselines and single-task training while avoiding\nnegative transfer.", "AI": {"tldr": "A data-driven method for sample sharing in ridge regression that leverages auxiliary datasets while preventing negative transfer, using transfer gain estimation to determine optimal sample borrowing.", "motivation": "Address the challenge of scarce labeled data in business settings by enabling safe transfer learning from auxiliary datasets without negative transfer effects.", "method": "Develop a principled rule based on transfer gain estimation to determine how many samples to borrow from auxiliary datasets, with finite-sample guarantees and analysis of Gaussian feature settings.", "result": "The method consistently outperforms strong baselines and single-task training in synthetic and real datasets, achieving predictive gains while avoiding negative transfer.", "conclusion": "The proposed sample sharing approach provides a safe and effective way to leverage auxiliary data for ridge regression tasks, with theoretical guarantees and empirical validation."}}
{"id": "2510.17084", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.17084", "abs": "https://arxiv.org/abs/2510.17084", "authors": ["Fatemeh Mahmoudi", "Chenxi Li", "Kaida Cai", "Xuewen Lu"], "title": "Variable Selection with Broken Adaptive Ridge Regression for Interval-Censored Competing Risks Data", "comment": null, "summary": "Competing risks data refer to situations where the occurrence of one event\npre- cludes the possibility of other events happening, resulting in multiple\nmutually exclusive events. This data type is commonly encountered in medical\nresearch and clinical trials, exploring the interplay between different events\nand informing decision-making in fields such as healthcare and epidemiology. We\ndevelop a penal- ized variable selection procedure to handle such complex data\nin an interval-censored setting. We consider a broad class of semiparametric\ntransformation regression mod- els, including popular models such as\nproportional and non-proportional hazards models. To promote sparsity and\nselect variables specific to each event, we employ the broken adaptive ridge\n(BAR) penalty. This approach allows us to simultane- ously select important\nrisk factors and estimate their effects for each event under investigation. We\nestablish the oracle property of the BAR procedure and evaluate its performance\nthrough simulation studies. The proposed method is applied to a real-life HIV\ncohort dataset, further validating its applicability in practice.", "AI": {"tldr": "A penalized variable selection method for interval-censored competing risks data using semiparametric transformation models with broken adaptive ridge penalty.", "motivation": "Competing risks data with multiple mutually exclusive events are common in medical research but require specialized methods for variable selection in interval-censored settings.", "method": "Developed a penalized variable selection procedure using broken adaptive ridge (BAR) penalty within semiparametric transformation regression models for interval-censored competing risks data.", "result": "The method establishes oracle properties, performs well in simulations, and successfully applies to a real HIV cohort dataset.", "conclusion": "The proposed BAR-based approach effectively handles variable selection for competing risks data in interval-censored settings, with theoretical guarantees and practical applicability."}}
{"id": "2510.15967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15967", "abs": "https://arxiv.org/abs/2510.15967", "authors": ["Zhengyi Zhong", "Wenzheng Jiang", "Weidong Bao", "Ji Wang", "Cheems Wang", "Guanbo Wang", "Yongheng Deng", "Ju Ren"], "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set", "comment": "Accepted by NeurIPS2025", "summary": "Conventional federated learning (FL) assumes a closed world with a fixed\ntotal number of clients. In contrast, new clients continuously join the FL\nprocess in real-world scenarios, introducing new knowledge. This raises two\ncritical demands: detecting new knowledge, i.e., knowledge discovery, and\nintegrating it into the global model, i.e., knowledge adaptation. Existing\nresearch focuses on coarse-grained knowledge discovery, and often sacrifices\nsource domain performance and adaptation efficiency. To this end, we propose a\nfine-grained federated domain adaptation approach in open set (Gains). Gains\nsplits the model into an encoder and a classifier, empirically revealing\nfeatures extracted by the encoder are sensitive to domain shifts while\nclassifier parameters are sensitive to class increments. Based on this, we\ndevelop fine-grained knowledge discovery and contribution-driven aggregation\ntechniques to identify and incorporate new knowledge. Additionally, an\nanti-forgetting mechanism is designed to preserve source domain performance,\nensuring balanced adaptation. Experimental results on multi-domain datasets\nacross three typical data-shift scenarios demonstrate that Gains significantly\noutperforms other baselines in performance for both source-domain and\ntarget-domain clients. Code is available at:\nhttps://github.com/Zhong-Zhengyi/Gains.", "AI": {"tldr": "Gains is a federated learning approach that addresses open-world scenarios where new clients with new knowledge continuously join. It uses fine-grained knowledge discovery and adaptation while preserving source domain performance.", "motivation": "Real-world federated learning involves continuous client participation with new knowledge, requiring both detection of new knowledge (discovery) and integration into global models (adaptation) without sacrificing existing performance.", "method": "Splits model into encoder and classifier, uses fine-grained knowledge discovery and contribution-driven aggregation, and implements anti-forgetting mechanism to preserve source domain performance.", "result": "Outperforms other baselines on multi-domain datasets across three typical data-shift scenarios, achieving better performance for both source-domain and target-domain clients.", "conclusion": "Gains effectively handles open-set federated learning by balancing knowledge discovery and adaptation while maintaining source domain performance through its fine-grained approach and anti-forgetting mechanisms."}}
{"id": "2510.17063", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17063", "abs": "https://arxiv.org/abs/2510.17063", "authors": ["Shunan Sheng", "Bohan Wu", "Alberto Gonz\u00e1lez-Sanz"], "title": "Mode Collapse of Mean-Field Variational Inference", "comment": null, "summary": "Mean-field variational inference (MFVI) is a widely used method for\napproximating high-dimensional probability distributions by product measures.\nIt has been empirically observed that MFVI optimizers often suffer from mode\ncollapse. Specifically, when the target measure $\\pi$ is a mixture $\\pi = w P_0\n+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a\nsingle component of the mixture. This work provides the first theoretical\nexplanation of mode collapse in MFVI. We introduce the notion to capture the\nseparatedness of the two mixture components -- called\n$\\varepsilon$-separateness -- and derive explicit bounds on the fraction of\nmass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are\n$\\varepsilon$-separated for sufficiently small $\\varepsilon$. Our results\nsuggest that the occurrence of mode collapse crucially depends on the relative\nposition of the components. To address this issue, we propose the rotational\nvariational inference (RoVI), which augments MFVI with a rotation matrix. The\nnumerical studies support our theoretical findings and demonstrate the benefits\nof RoVI.", "AI": {"tldr": "This paper provides the first theoretical explanation of mode collapse in mean-field variational inference (MFVI) when approximating mixture distributions, and proposes a new method called rotational variational inference (RoVI) to address this issue.", "motivation": "MFVI is widely used but empirically observed to suffer from mode collapse when approximating mixture distributions, where it tends to place most mass near a single component rather than capturing the full mixture structure. The authors aim to provide the first theoretical understanding of this phenomenon.", "method": "The authors introduce the concept of \u03b5-separateness to quantify how separated mixture components are, and derive theoretical bounds on mass assignment in MFVI optimizers. They then propose RoVI, which augments MFVI with a rotation matrix to better capture mixture structure.", "result": "Theoretical analysis shows that mode collapse in MFVI crucially depends on the relative position of mixture components. Numerical studies demonstrate that RoVI successfully addresses mode collapse and provides better approximation of mixture distributions compared to standard MFVI.", "conclusion": "This work provides the first theoretical framework explaining mode collapse in MFVI and introduces RoVI as an effective solution that maintains the computational efficiency of MFVI while better capturing mixture structures through rotational augmentation."}}
{"id": "2510.15968", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15968", "abs": "https://arxiv.org/abs/2510.15968", "authors": ["Zhen Huang", "Hong Wang", "Wenkai Yang", "Muxi Tang", "Depeng Xie", "Ting-Jung Lin", "Yu Zhang", "Wei W. Xing", "Lei He"], "title": "Self-Attention to Operator Learning-based 3D-IC Thermal Simulation", "comment": null, "summary": "Thermal management in 3D ICs is increasingly challenging due to higher power\ndensities. Traditional PDE-solving-based methods, while accurate, are too slow\nfor iterative design. Machine learning approaches like FNO provide faster\nalternatives but suffer from high-frequency information loss and high-fidelity\ndata dependency. We introduce Self-Attention U-Net Fourier Neural Operator\n(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to\ncapture long-range dependencies and model local high-frequency features\neffectively. Transfer learning is employed to fine-tune low-fidelity data,\nminimizing the need for extensive high-fidelity datasets and speeding up\ntraining. Experiments demonstrate that SAU-FNO achieves state-of-the-art\nthermal prediction accuracy and provides an 842x speedup over traditional FEM\nmethods, making it an efficient tool for advanced 3D IC thermal simulations.", "AI": {"tldr": "SAU-FNO combines self-attention, U-Net, and FNO for fast and accurate 3D IC thermal prediction, achieving 842x speedup over FEM methods.", "motivation": "Traditional PDE methods are too slow for iterative design, while existing ML approaches like FNO suffer from high-frequency information loss and require extensive high-fidelity data.", "method": "Self-Attention U-Net Fourier Neural Operator (SAU-FNO) framework that captures long-range dependencies and local high-frequency features, using transfer learning to fine-tune low-fidelity data.", "result": "SAU-FNO achieves state-of-the-art thermal prediction accuracy and provides 842x speedup over traditional FEM methods.", "conclusion": "SAU-FNO is an efficient tool for advanced 3D IC thermal simulations, minimizing the need for extensive high-fidelity datasets while maintaining accuracy."}}
{"id": "2510.17072", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.17072", "abs": "https://arxiv.org/abs/2510.17072", "authors": ["Kyum Kim", "Yaqing Chen", "Paromita Dubey"], "title": "DFNN: A Deep Fr\u00e9chet Neural Network Framework for Learning Metric-Space-Valued Responses", "comment": null, "summary": "Regression with non-Euclidean responses -- e.g., probability distributions,\nnetworks, symmetric positive-definite matrices, and compositions -- has become\nincreasingly important in modern applications. In this paper, we propose deep\nFr\\'echet neural networks (DFNNs), an end-to-end deep learning framework for\npredicting non-Euclidean responses -- which are considered as random objects in\na metric space -- from Euclidean predictors. Our method leverages the\nrepresentation-learning power of deep neural networks (DNNs) to the task of\napproximating conditional Fr\\'echet means of the response given the predictors,\nthe metric-space analogue of conditional expectations, by minimizing a\nFr\\'echet risk. The framework is highly flexible, accommodating diverse metrics\nand high-dimensional predictors. We establish a universal approximation theorem\nfor DFNNs, advancing the state-of-the-art of neural network approximation\ntheory to general metric-space-valued responses without making model\nassumptions or relying on local smoothing. Empirical studies on synthetic\ndistributional and network-valued responses, as well as a real-world\napplication to predicting employment occupational compositions, demonstrate\nthat DFNNs consistently outperform existing methods.", "AI": {"tldr": "Deep Fr\u00e9chet neural networks (DFNNs) are proposed as an end-to-end deep learning framework for predicting non-Euclidean responses (like distributions, networks, matrices) from Euclidean predictors by approximating conditional Fr\u00e9chet means.", "motivation": "Regression with non-Euclidean responses has become increasingly important in modern applications, but existing methods have limitations in handling diverse metric spaces and high-dimensional predictors.", "method": "DFNNs leverage deep neural networks to approximate conditional Fr\u00e9chet means by minimizing Fr\u00e9chet risk, accommodating diverse metrics and high-dimensional predictors without model assumptions or local smoothing.", "result": "A universal approximation theorem is established for DFNNs, and empirical studies on synthetic distributional/network-valued responses and real-world occupational composition prediction show DFNNs consistently outperform existing methods.", "conclusion": "DFNNs provide a flexible and powerful framework for regression with non-Euclidean responses, advancing neural network approximation theory for general metric-space-valued responses."}}
{"id": "2510.17215", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.17215", "abs": "https://arxiv.org/abs/2510.17215", "authors": ["Fumiya Iwashige", "Tomoya Wakayama", "Shonosuke Sugasawa", "Shintaro Hashimoto"], "title": "On Misspecified Error Distributions in Bayesian Functional Clustering: Consequences and Remedies", "comment": null, "summary": "Nonparametric Bayesian approaches provide a flexible framework for clustering\nwithout pre-specifying the number of groups, yet they are well known to\noverestimate the number of clusters, especially for functional data. We show\nthat a fundamental cause of this phenomenon lies in misspecification of the\nerror structure: errors are conventionally assumed to be independent across\nobserved points in Bayesian functional models. Through high-dimensional\nclustering theory, we demonstrate that ignoring the underlying correlation\nleads to excess clusters regardless of the flexibility of prior distributions.\nGuided by this theory, we propose incorporating the underlying correlation\nstructures via Gaussian processes and also present its scalable approximation\nwith principled hyperparameter selection. Numerical experiments illustrate that\neven simple clustering based on Dirichlet processes performs well once error\ndependence is properly modeled.", "AI": {"tldr": "The paper shows that overestimation of clusters in nonparametric Bayesian functional clustering stems from ignoring error correlation, and proposes using Gaussian processes to model this dependence, which significantly improves clustering accuracy.", "motivation": "Nonparametric Bayesian approaches for functional data clustering tend to overestimate the number of clusters due to misspecification of error structure, where errors are incorrectly assumed to be independent across observed points.", "method": "The authors propose incorporating underlying correlation structures via Gaussian processes and present a scalable approximation with principled hyperparameter selection to properly model error dependence in functional data clustering.", "result": "Numerical experiments demonstrate that even simple Dirichlet process-based clustering performs well when error dependence is properly modeled, overcoming the overestimation problem.", "conclusion": "Properly modeling error correlation structure is crucial for accurate clustering in functional data, and Gaussian process-based approaches provide an effective solution to the overestimation problem in nonparametric Bayesian clustering."}}
{"id": "2510.15969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15969", "abs": "https://arxiv.org/abs/2510.15969", "authors": ["Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Aaron Jeremias Elsing", "Elena Kuss", "Steffen Rebennack"], "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "comment": null, "summary": "Reformulating nonlinear optimization problems is largely manual and\nexpertise-intensive, yet it remains essential for solving such problems with\nlinear optimization solvers or applying special-purpose algorithms. We\nintroduce \\textit{LinearizeLLM}, an agent-based framework that solves this task\nby leveraging Large Language Models (LLMs). The framework assigns each\nnonlinear pattern to a \\textit{reformulation agent} that is explicitly\ninstructed to derive an exact linear reformulation for its nonlinearity\npattern, for instance, absolute-value terms or bilinear products of decision\nvariables. The agents then coordinate to assemble a solver-ready linear model\nequivalent to the original problem. To benchmark the approach, we create a\ndataset of 20 real-world nonlinear optimization problems derived from the\nestablished ComplexOR dataset of linear optimization problems. We evaluate our\napproach with several LLMs. Our results indicate that specialized LLM agents\ncan automate linearization tasks, opening a path toward fully conversational\nmodeling pipelines for nonlinear optimization.", "AI": {"tldr": "LinearizeLLM is an agent-based framework that uses Large Language Models to automatically reformulate nonlinear optimization problems into linear equivalents, enabling their solution with linear optimization solvers.", "motivation": "Reformulating nonlinear optimization problems is currently manual and expertise-intensive, but essential for using linear solvers or specialized algorithms. This creates a barrier to efficient problem solving.", "method": "The framework assigns each nonlinear pattern to specialized reformulation agents that are explicitly instructed to derive exact linear reformulations. These agents coordinate to assemble solver-ready linear models equivalent to the original nonlinear problems.", "result": "The approach was evaluated on a dataset of 20 real-world nonlinear optimization problems derived from the ComplexOR dataset. Results show that specialized LLM agents can successfully automate linearization tasks.", "conclusion": "Specialized LLM agents can automate linearization tasks, paving the way for fully conversational modeling pipelines in nonlinear optimization."}}
{"id": "2510.17348", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17348", "abs": "https://arxiv.org/abs/2510.17348", "authors": ["Marc Jourdan", "Achraf Azize"], "title": "Optimal Best Arm Identification under Differential Privacy", "comment": "92 pages, 2 figures, 2 tables. To be published in the Thirty-Ninth\n  Annual Conference on Neural Information Processing Systems", "summary": "Best Arm Identification (BAI) algorithms are deployed in data-sensitive\napplications, such as adaptive clinical trials or user studies. Driven by the\nprivacy concerns of these applications, we study the problem of\nfixed-confidence BAI under global Differential Privacy (DP) for Bernoulli\ndistributions. While numerous asymptotically optimal BAI algorithms exist in\nthe non-private setting, a significant gap remains between the best lower and\nupper bounds in the global DP setting. This work reduces this gap to a small\nmultiplicative constant, for any privacy budget $\\epsilon$. First, we provide a\ntighter lower bound on the expected sample complexity of any $\\delta$-correct\nand $\\epsilon$-global DP strategy. Our lower bound replaces the\nKullback-Leibler (KL) divergence in the transportation cost used by the\nnon-private characteristic time with a new information-theoretic quantity that\noptimally trades off between the KL divergence and the Total Variation distance\nscaled by $\\epsilon$. Second, we introduce a stopping rule based on these\ntransportation costs and a private estimator of the means computed using an\narm-dependent geometric batching. En route to proving the correctness of our\nstopping rule, we derive concentration results of independent interest for the\nLaplace distribution and for the sum of Bernoulli and Laplace distributions.\nThird, we propose a Top Two sampling rule based on these transportation costs.\nFor any budget $\\epsilon$, we show an asymptotic upper bound on its expected\nsample complexity that matches our lower bound to a multiplicative constant\nsmaller than $8$. Our algorithm outperforms existing $\\delta$-correct and\n$\\epsilon$-global DP BAI algorithms for different values of $\\epsilon$.", "AI": {"tldr": "This paper addresses the gap between lower and upper bounds for Best Arm Identification (BAI) under global Differential Privacy (DP) for Bernoulli distributions, achieving near-optimal performance with a small multiplicative constant gap.", "motivation": "BAI algorithms are used in privacy-sensitive applications like clinical trials and user studies, creating a need for differentially private BAI algorithms. However, there exists a significant performance gap between lower and upper bounds in the global DP setting that needs to be addressed.", "method": "The authors develop a three-part approach: (1) a tighter lower bound using a new information-theoretic quantity that optimally trades off KL divergence and Total Variation distance scaled by epsilon, (2) a stopping rule based on transportation costs with private estimators using arm-dependent geometric batching, and (3) a Top Two sampling rule based on the same transportation costs.", "result": "The proposed algorithm achieves an asymptotic upper bound on expected sample complexity that matches the lower bound to a multiplicative constant smaller than 8, outperforming existing differentially private BAI algorithms for various epsilon values.", "conclusion": "This work significantly reduces the performance gap in differentially private BAI, providing near-optimal algorithms with strong privacy guarantees while maintaining competitive sample complexity."}}
{"id": "2510.17514", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.17514", "abs": "https://arxiv.org/abs/2510.17514", "authors": ["Jun Wu", "St\u00e9phane Guerrier", "Si Gou", "Yogeshvar N. Kalia", "Luca Insolia"], "title": "Bridging the gap between experimental burden and statistical power for quantiles equivalence testing", "comment": "65 pages, 15 figures", "summary": "Testing the equivalence of multiple quantiles between two populations is\nimportant in many scientific applications, such as clinical trials, where\nconventional mean-based methods may be inadequate. This is particularly\nrelevant in bridging studies that compare drug responses across different\nexperimental conditions or patient populations. These studies often aim to\nassess whether a proposed dose for a target population achieves pharmacokinetic\nlevels comparable to those of a reference population where efficacy and safety\nhave been established. The focus is on extreme quantiles which directly inform\nboth efficacy and safety assessments. When analyzing heterogeneous Gaussian\nsamples, where a single quantile of interest is estimated, the existing Two\nOne-Sided Tests method for quantile equivalence testing (qTOST) tends to be\noverly conservative. To mitigate this behavior, we introduce $\\alpha$-qTOST, a\nfinite-sample adjustment that achieves uniformly higher power compared to qTOST\nwhile maintaining the test size at the nominal level. Moreover, we extend the\nquantile equivalence framework to simultaneously assess equivalence across\nmultiple quantiles. Through theoretical guarantees and an extensive simulation\nstudy, we demonstrate that $\\alpha$-qTOST offers substantial improvements,\nespecially when testing extreme quantiles under heteroskedasticity and with\nsmall, unbalanced sample sizes. We illustrate these advantages through two case\nstudies, one in HIV drug development, where a bridging clinical trial examines\nexposure distributions between male and female populations with unbalanced\nsample sizes, and another in assessing the reproducibility of an identical\nexperimental protocol performed by different operators for generating\nbiodistribution profiles of topically administered and locally acting products.", "AI": {"tldr": "The paper introduces \u03b1-qTOST, a finite-sample adjustment to the quantile equivalence testing method (qTOST) that addresses over-conservatism in heterogeneous Gaussian samples. The method achieves higher power while maintaining nominal test size, especially for extreme quantiles under heteroskedasticity and small, unbalanced samples.", "motivation": "Conventional mean-based methods are inadequate for testing equivalence of multiple quantiles between populations, particularly in bridging studies for drug development where extreme quantiles directly inform efficacy and safety assessments. Existing qTOST method tends to be overly conservative when analyzing heterogeneous Gaussian samples.", "method": "The authors propose \u03b1-qTOST, a finite-sample adjustment to the qTOST method. They extend the quantile equivalence framework to simultaneously assess equivalence across multiple quantiles. The approach is validated through theoretical guarantees and extensive simulation studies.", "result": "\u03b1-qTOST achieves uniformly higher power compared to qTOST while maintaining test size at the nominal level. Substantial improvements are demonstrated, especially when testing extreme quantiles under heteroskedasticity and with small, unbalanced sample sizes.", "conclusion": "The proposed \u03b1-qTOST method offers significant advantages over existing approaches for quantile equivalence testing, making it particularly valuable for bridging studies in clinical trials and other scientific applications where multiple quantile comparisons are needed under challenging conditions like heteroskedasticity and unbalanced sample sizes."}}
{"id": "2510.15970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15970", "abs": "https://arxiv.org/abs/2510.15970", "authors": ["Yang Ba", "Mohammad Sadeq Abolhasani", "Rong Pan"], "title": "Predict Training Data Quality via Its Geometry in Metric Space", "comment": "Accepted to the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "High-quality training data is the foundation of machine learning and\nartificial intelligence, shaping how models learn and perform. Although much is\nknown about what types of data are effective for training, the impact of the\ndata's geometric structure on model performance remains largely underexplored.\nWe propose that both the richness of representation and the elimination of\nredundancy within training data critically influence learning outcomes. To\ninvestigate this, we employ persistent homology to extract topological features\nfrom data within a metric space, thereby offering a principled way to quantify\ndiversity beyond entropy-based measures. Our findings highlight persistent\nhomology as a powerful tool for analyzing and enhancing the training data that\ndrives AI systems.", "AI": {"tldr": "The paper explores how the geometric structure of training data affects machine learning performance, using persistent homology to quantify data diversity beyond traditional entropy measures.", "motivation": "While the importance of training data quality is well-established, the impact of data's geometric structure on model performance remains underexplored. The authors aim to understand how data richness and redundancy elimination influence learning outcomes.", "method": "The researchers employ persistent homology to extract topological features from data within a metric space, providing a principled approach to quantify diversity that goes beyond entropy-based measures.", "result": "The findings demonstrate that persistent homology serves as a powerful tool for analyzing and enhancing training data quality in AI systems.", "conclusion": "Persistent homology offers valuable insights into data geometric structure and can significantly contribute to improving training data quality for machine learning and AI systems."}}
{"id": "2510.17472", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17472", "abs": "https://arxiv.org/abs/2510.17472", "authors": ["Paula Cordero-Encinar", "Andrew B. Duncan"], "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs", "comment": null, "summary": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs.", "AI": {"tldr": "A unified statistical framework for certifiable inference in LLMs that explains self-consistency and test-time reinforcement learning (TTRL), providing finite-sample guarantees and adaptive stopping rules for reliable reasoning.", "motivation": "Recent methods like self-consistency and TTRL improve LLM reliability without supervision, but their underlying mechanisms and statistical guarantees remain poorly understood.", "method": "Developed majority voting certificates with finite-sample and anytime-valid concentration bounds, introduced Martingale Majority Certificate (MMC) for adaptive stopping, and showed TTRL sharpens distributions via exponential tilting.", "result": "Majority voting provides statistical certificates for self-consistency, MMC adaptively determines sufficient samples, and TTRL reduces certification samples by distribution sharpening.", "conclusion": "The framework unifies self-consistency and TTRL within a single statistical framework for label-free, certifiable reliability in reasoning LLMs."}}
{"id": "2510.17550", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.17550", "abs": "https://arxiv.org/abs/2510.17550", "authors": ["Andrew Hall", "Duncan Wilson", "Stuart Barber", "Sarah R Brown"], "title": "Defining Utility as a Measure of Preference Under Uncertainty in Phase I-II Oncology Dose Finding Trials", "comment": null, "summary": "The main objective of dose finding trials is to find an optimal dose amongst\na candidate set for further research. The trial design in oncology proceeds in\nstages with a decision as to how to treat the next group of patients made at\nevery stage until a final sample size is reached or the trial stopped early.\n  This work applies a Bayesian decision-theoretic approach to the problem,\nproposing a new utility function based on both efficacy and toxicity and\ngrounded in von Neumann-Morgenstern (VNM) utility theory. Our proposed\nframework seeks to better capture real clinical judgements by allowing\nattitudes to risk to vary when the judgements are of gains or losses, which are\ndefined with respect to an intermediate outcome known as a reference point. We\ncall this method Reference Dependent Decision Theoretic dose finding (R2DT).\n  A simulation study demonstrates that the framework can perform well and\nproduce good operating characteristics. The simulation results demonstrate that\nR2DT is better at detecting the optimal dose in scenarios where candidate doses\nare around minimum acceptable efficacy and maximum acceptable toxicity\nthresholds.\n  The proposed framework shows that a flexible utility function, which better\ncaptures clinician beliefs, can lead to trials with good operating\ncharacteristics, including a high probability of finding the optimal dose. Our\nwork demonstrates proof-of-concept for this framework, which should be\nevaluated in a broader range of settings.", "AI": {"tldr": "A Bayesian decision-theoretic framework called R2DT is proposed for dose finding trials in oncology, using a utility function based on efficacy and toxicity that incorporates reference-dependent risk attitudes.", "motivation": "To better capture real clinical judgements in dose finding by allowing varying attitudes to risk for gains vs losses relative to reference points, improving on traditional approaches.", "method": "Bayesian decision-theoretic approach with a new utility function grounded in von Neumann-Morgenstern utility theory, incorporating reference points for defining gains and losses in efficacy and toxicity.", "result": "Simulation studies show R2DT performs well with good operating characteristics, particularly better at detecting optimal doses when candidate doses are near efficacy and toxicity thresholds.", "conclusion": "The flexible utility function better captures clinician beliefs and leads to trials with good operating characteristics, demonstrating proof-of-concept for this reference-dependent framework."}}
{"id": "2510.15977", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15977", "abs": "https://arxiv.org/abs/2510.15977", "authors": ["Wenyun Li", "Zheng Zhang", "Dongmei Jiang", "Xiangyuan Lan"], "title": "Bolster Hallucination Detection via Prompt-Guided Data Augmentation", "comment": null, "summary": "Large language models (LLMs) have garnered significant interest in AI\ncommunity. Despite their impressive generation capabilities, they have been\nfound to produce misleading or fabricated information, a phenomenon known as\nhallucinations. Consequently, hallucination detection has become critical to\nensure the reliability of LLM-generated content. One primary challenge in\nhallucination detection is the scarcity of well-labeled datasets containing\nboth truthful and hallucinated outputs. To address this issue, we introduce\nPrompt-guided data Augmented haLlucination dEtection (PALE), a novel framework\nthat leverages prompt-guided responses from LLMs as data augmentation for\nhallucination detection. This strategy can generate both truthful and\nhallucinated data under prompt guidance at a relatively low cost. To more\neffectively evaluate the truthfulness of the sparse intermediate embeddings\nproduced by LLMs, we introduce an estimation metric called the Contrastive\nMahalanobis Score (CM Score). This score is based on modeling the distributions\nof truthful and hallucinated data in the activation space. CM Score employs a\nmatrix decomposition approach to more accurately capture the underlying\nstructure of these distributions. Importantly, our framework does not require\nadditional human annotations, offering strong generalizability and practicality\nfor real-world applications. Extensive experiments demonstrate that PALE\nachieves superior hallucination detection performance, outperforming the\ncompetitive baseline by a significant margin of 6.55%.", "AI": {"tldr": "PALE is a novel framework for hallucination detection in LLMs that uses prompt-guided data augmentation and a Contrastive Mahalanobis Score metric to evaluate truthfulness without requiring human annotations.", "motivation": "Address the scarcity of well-labeled datasets for hallucination detection in LLMs, which is critical for ensuring the reliability of LLM-generated content.", "method": "Uses prompt-guided responses from LLMs as data augmentation to generate both truthful and hallucinated data. Introduces Contrastive Mahalanobis Score (CM Score) based on modeling distributions of truthful and hallucinated data in activation space using matrix decomposition.", "result": "Achieves superior hallucination detection performance, outperforming competitive baseline by 6.55%. The framework offers strong generalizability and practicality without requiring additional human annotations.", "conclusion": "PALE provides an effective and practical solution for hallucination detection in LLMs through data augmentation and novel evaluation metrics, demonstrating significant performance improvements over existing methods."}}
{"id": "2510.17553", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.17553", "abs": "https://arxiv.org/abs/2510.17553", "authors": ["Priyanjali Bukke", "Martin Slawski"], "title": "Relaxing the Assumption of Strongly Non-Informative Linkage Error in Secondary Regression Analysis of Linked Files", "comment": null, "summary": "Data analysis of files that are a result of linking records from multiple\nsources are often affected by linkage errors. Records may be linked\nincorrectly, or their links may be missed. In consequence, it is essential that\nsuch errors are taken into account to ensure valid post-linkage inference.\nHere, we propose an extension to a general framework for regression with linked\ncovariates and responses based on a two-component mixture model, which was\ndeveloped in prior work. This framework addresses the challenging case of\nsecondary analysis in which only the linked data is available and information\nabout the record linkage process is limited. The extension considered herein\nrelaxes the assumption of strongly non-informative linkage in the framework\naccording to which linkage does not depend on the covariates used in the\nanalysis, which may be limiting in practice. The effectiveness of the proposed\nextension is investigated by simulations and a case study.", "AI": {"tldr": "Extension of regression framework for linked data that relaxes the strong non-informative linkage assumption, allowing linkage to depend on covariates.", "motivation": "Linkage errors in multi-source data analysis can invalidate post-linkage inference, especially when only linked data is available with limited linkage process information.", "method": "Two-component mixture model extension that relaxes the assumption that linkage doesn't depend on analysis covariates, making it more practical for real-world applications.", "result": "Effectiveness demonstrated through simulations and case study showing improved performance when linkage depends on covariates.", "conclusion": "The extended framework provides more realistic and practical approach for regression analysis with linked data by accounting for covariate-dependent linkage processes."}}
{"id": "2510.15978", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.15978", "abs": "https://arxiv.org/abs/2510.15978", "authors": ["Junchao Gong", "Jingyi Xu", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Kun Chen", "Wanghan Xu", "Weidong Yang", "Xiaokang Yang", "Lei Bai"], "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space", "comment": null, "summary": "Weather prediction is a critical task for human society, where impressive\nprogress has been made by training artificial intelligence weather prediction\n(AIWP) methods with reanalysis data. However, reliance on reanalysis data\nlimits the AIWPs with shortcomings, including data assimilation biases and\ntemporal discrepancies. To liberate AIWPs from the reanalysis data, observation\nforecasting emerges as a transformative paradigm for weather prediction. One of\nthe key challenges in observation forecasting is learning spatiotemporal\ndynamics across disparate measurement systems with irregular high-resolution\nobservation data, which constrains the design and prediction of AIWPs. To this\nend, we propose our DAWP as an innovative framework to enable AIWPs to operate\nin a complete observation space by initialization with an artificial\nintelligence data assimilation (AIDA) module. Specifically, our AIDA module\napplies a mask multi-modality autoencoder(MMAE)for assimilating irregular\nsatellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a\nspatiotemporal decoupling transformer with cross-regional boundary conditioning\n(CBC), learning the dynamics in observation space, to enable sub-image-based\nglobal observation forecasting. Comprehensive experiments demonstrate that AIDA\ninitialization significantly improves the roll out and efficiency of AIWP.\nAdditionally, we show that DAWP holds promising potential to be applied in\nglobal precipitation forecasting.", "AI": {"tldr": "DAWP is a novel AI weather prediction framework that operates in observation space rather than reanalysis data, using an AI data assimilation module to handle irregular satellite observations and a spatiotemporal transformer for global forecasting.", "motivation": "To overcome limitations of current AI weather prediction methods that rely on reanalysis data, which suffer from data assimilation biases and temporal discrepancies, by enabling direct forecasting from observation data.", "method": "Proposes DAWP framework with two key components: 1) AI data assimilation (AIDA) module using mask multi-modality autoencoder and mask ViT-VAEs to assimilate irregular satellite observations, 2) Spatiotemporal decoupling transformer with cross-regional boundary conditioning for learning dynamics in observation space and enabling sub-image-based global forecasting.", "result": "Comprehensive experiments show AIDA initialization significantly improves AIWP rollout and efficiency. The framework demonstrates promising potential for global precipitation forecasting applications.", "conclusion": "DAWP represents a transformative paradigm that liberates AI weather prediction from reanalysis data dependencies, enabling more direct and efficient forecasting from observational data through innovative data assimilation and spatiotemporal modeling techniques."}}
{"id": "2510.16060", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16060", "abs": "https://arxiv.org/abs/2510.16060", "authors": ["Coen Adler", "Yuxin Chang", "Felix Draxler", "Samar Abdi", "Padhraic Smyth"], "title": "Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?", "comment": null, "summary": "The recent development of foundation models for time series data has\ngenerated considerable interest in using such models across a variety of\napplications. Although foundation models achieve state-of-the-art predictive\nperformance, their calibration properties remain relatively underexplored,\ndespite the fact that calibration can be critical for many practical\napplications. In this paper, we investigate the calibration-related properties\nof five recent time series foundation models and two competitive baselines. We\nperform a series of systematic evaluations assessing model calibration (i.e.,\nover- or under-confidence), effects of varying prediction heads, and\ncalibration under long-term autoregressive forecasting. We find that time\nseries foundation models are consistently better calibrated than baseline\nmodels and tend not to be either systematically over- or under-confident, in\ncontrast to the overconfidence often seen in other deep learning models.", "AI": {"tldr": "This paper investigates the calibration properties of time series foundation models, finding they are better calibrated than baselines and don't show systematic overconfidence like other deep learning models.", "motivation": "Despite foundation models achieving state-of-the-art predictive performance for time series data, their calibration properties remain underexplored, even though calibration is critical for many practical applications.", "method": "Systematic evaluation of five recent time series foundation models and two competitive baselines, assessing model calibration, effects of varying prediction heads, and calibration under long-term autoregressive forecasting.", "result": "Time series foundation models are consistently better calibrated than baseline models and tend not to be either systematically over- or under-confident, unlike the overconfidence often seen in other deep learning models.", "conclusion": "Time series foundation models demonstrate superior calibration properties compared to baseline approaches, addressing a key limitation of traditional deep learning models in time series forecasting."}}
{"id": "2510.15979", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15979", "abs": "https://arxiv.org/abs/2510.15979", "authors": ["Zexu Sun", "Yongcheng Zeng", "Erxue Min", "Heyang Gao", "Bokai Ji", "Xu Chen"], "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning", "comment": "22 Pages, 8 figures, 4 tables", "summary": "Contemporary progress in large language models (LLMs) has revealed notable\ninferential capacities via reinforcement learning (RL) employing verifiable\nreward, facilitating the development of O1 and R1-like reasoning models.\nDirectly training from base models with RL is called zero-RL. However, previous\nworks rely upon activating LLMs' inherent capacities through fixed prompt\ntemplates. This strategy introduces substantial sampling inefficiencies for\nweak LLMs, as the majority of problems generate invalid outputs during\naccuracy-driven filtration in reasoning tasks, which causes a waste of samples.\nTo solve this issue, we propose Cog-Rethinker, a novel hierarchical\nmetacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses\non the rollout procedure in RL training. After the direct rollout, our\nCog-Rethinker improves sample utilization in a hierarchical metacognitive\ntwo-stage framework. By leveraging human cognition during solving problems,\nfirstly, it prompts policy to decompose zero-accuracy problems into subproblems\nto produce final reasoning results. Secondly, with zero-accuracy problems in\nprevious rollout stage, it further prompts policy to refine these answers by\nreferencing previous wrong solutions. Moreover, to enable cold-start of the two\nnew reasoning patterns and maintain train-test consistency across prompt\ntemplates, our Cog-Rethinker applies supervised fine-tuning on the policy using\ncorrect samples of the two stages with direct rollout template. Experimental\nresults demonstrate Cog-Rethinker's superior performance on various\nmathematical reasoning benchmarks, we also analyzed its improved sample\nefficiency that accelerates convergence compared to baseline methods.", "AI": {"tldr": "Cog-Rethinker is a hierarchical metacognitive RL framework that improves sample efficiency in LLM reasoning by decomposing zero-accuracy problems and refining wrong solutions, outperforming baseline methods on mathematical reasoning benchmarks.", "motivation": "Current RL approaches for LLM reasoning suffer from sampling inefficiencies, particularly for weak LLMs where most problems generate invalid outputs during accuracy-driven filtration, wasting samples.", "method": "A two-stage hierarchical metacognitive framework: 1) decomposes zero-accuracy problems into subproblems, and 2) refines answers by referencing previous wrong solutions. Uses supervised fine-tuning to enable cold-start and maintain train-test consistency.", "result": "Superior performance on various mathematical reasoning benchmarks and improved sample efficiency that accelerates convergence compared to baseline methods.", "conclusion": "Cog-Rethinker effectively addresses sampling inefficiency in RL training for LLM reasoning through hierarchical metacognitive processing, demonstrating both performance improvements and faster convergence."}}
{"id": "2510.16097", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16097", "abs": "https://arxiv.org/abs/2510.16097", "authors": ["Eleni Straitouri", "Stratis Tsirtsis", "Ander Artola Velasco", "Manuel Gomez-Rodriguez"], "title": "Narrowing Action Choices with AI Improves Human Sequential Decisions", "comment": "Accepted at the Human-AI Complementarity for Decision Making Workshop\n  2025 by the NSF AI Institute for Societal Decision Making", "summary": "Recent work has shown that, in classification tasks, it is possible to design\ndecision support systems that do not require human experts to understand when\nto cede agency to a classifier or when to exercise their own agency to achieve\ncomplementarity$\\unicode{x2014}$experts using these systems make more accurate\npredictions than those made by the experts or the classifier alone. The key\nprinciple underpinning these systems reduces to adaptively controlling the\nlevel of human agency, by design. Can we use the same principle to achieve\ncomplementarity in sequential decision making tasks? In this paper, we answer\nthis question affirmatively. We develop a decision support system that uses a\npre-trained AI agent to narrow down the set of actions a human can take to a\nsubset, and then asks the human to take an action from this action set. Along\nthe way, we also introduce a bandit algorithm that leverages the smoothness\nproperties of the action sets provided by our system to efficiently optimize\nthe level of human agency. To evaluate our decision support system, we conduct\na large-scale human subject study ($n = 1{,}600$) where participants play a\nwildfire mitigation game. We find that participants who play the game supported\nby our system outperform those who play on their own by $\\sim$$30$% and the AI\nagent used by our system by $>$$2$%, even though the AI agent largely\noutperforms participants playing without support. We have made available the\ndata gathered in our human subject study as well as an open source\nimplementation of our system at\nhttps://github.com/Networks-Learning/narrowing-action-choices .", "AI": {"tldr": "The paper proposes a decision support system that uses a pre-trained AI agent to narrow down action choices for humans in sequential decision making, achieving complementarity where human-AI teams outperform either alone.", "motivation": "To extend the principle of adaptive human agency control from classification tasks to sequential decision making, aiming to achieve complementarity where human-AI collaboration outperforms individual performance.", "method": "Developed a decision support system that uses a pre-trained AI agent to restrict human action choices to a subset, combined with a bandit algorithm that optimizes human agency level by leveraging action set smoothness properties.", "result": "In a large-scale human study (n=1,600) using a wildfire mitigation game, participants using the system outperformed those playing alone by ~30% and the AI agent by >2%, despite the AI agent significantly outperforming unsupported humans.", "conclusion": "The proposed decision support system successfully achieves complementarity in sequential decision making by adaptively controlling human agency through action set narrowing, demonstrating significant performance improvements over individual human or AI performance."}}
{"id": "2510.15982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15982", "abs": "https://arxiv.org/abs/2510.15982", "authors": ["Donghyeok Shin", "Yeongmin Kim", "Suhyeon Jo", "Byeonghu Na", "Il-Chul Moon"], "title": "AMiD: Knowledge Distillation for LLMs with $\u03b1$-mixture Assistant Distribution", "comment": null, "summary": "Autoregressive large language models (LLMs) have achieved remarkable\nimprovement across many tasks but incur high computational and memory costs.\nKnowledge distillation (KD) mitigates this issue by transferring knowledge from\na large teacher to a smaller student through distributional alignment. Previous\nstudies have proposed various discrepancy metrics, but the capacity gap and\ntraining instability caused by near-zero probabilities, stemming from the\nhigh-dimensional output of LLMs, remain fundamental limitations. To overcome\nthese challenges, several approaches implicitly or explicitly incorporating\nassistant distribution have recently been proposed. However, the past proposals\nof assistant distributions have been a fragmented approach without a systematic\ninvestigation of the interpolation path and the divergence. This paper proposes\n$\\alpha$-mixture assistant distribution, a novel generalized family of\nassistant distributions, and $\\alpha$-mixture distillation, coined AMiD, a\nunified framework for KD using the assistant distribution. The $\\alpha$-mixture\nassistant distribution provides a continuous extension of the assistant\ndistribution by introducing a new distribution design variable $\\alpha$, which\nhas been fixed in all previous approaches. Furthermore, AMiD generalizes the\nfamily of divergences used with the assistant distributions based on\noptimality, which has also been restricted in previous works. Through extensive\nexperiments, we demonstrate that AMiD offers superior performance and training\nstability by leveraging a broader and theoretically grounded assistant\ndistribution space.", "AI": {"tldr": "The paper proposes AMiD, a unified knowledge distillation framework using \u03b1-mixture assistant distributions to address capacity gaps and training instability in LLM distillation.", "motivation": "Autoregressive LLMs have high computational costs, and existing knowledge distillation methods suffer from capacity gaps and training instability due to near-zero probabilities in high-dimensional outputs.", "method": "Introduces \u03b1-mixture assistant distribution with a continuous parameter \u03b1 to generalize previous approaches, and AMiD framework that generalizes divergence families based on optimality.", "result": "AMiD demonstrates superior performance and training stability by leveraging a broader, theoretically grounded assistant distribution space.", "conclusion": "The proposed \u03b1-mixture assistant distribution and AMiD framework provide a systematic solution to overcome fundamental limitations in LLM knowledge distillation."}}
{"id": "2510.16132", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16132", "abs": "https://arxiv.org/abs/2510.16132", "authors": ["Phalguni Nanda", "Zaiwei Chen"], "title": "A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies", "comment": "43 pages, 4 figures", "summary": "In this work, we present the first finite-time analysis of the Q-learning\nalgorithm under time-varying learning policies (i.e., on-policy sampling) with\nminimal assumptions -- specifically, assuming only the existence of a policy\nthat induces an irreducible Markov chain over the state space. We establish a\nlast-iterate convergence rate for $\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty^2]$,\nimplying a sample complexity of order $O(1/\\epsilon^2)$ for achieving\n$\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty] \\le \\epsilon$, matching that of off-policy\nQ-learning but with a worse dependence on exploration-related parameters. We\nalso derive an explicit rate for $\\mathbb{E}[\\|Q^{\\pi_k} - Q^*\\|_\\infty^2]$,\nwhere $\\pi_k$ is the learning policy at iteration $k$. These results reveal\nthat on-policy Q-learning exhibits weaker exploration than its off-policy\ncounterpart but enjoys an exploitation advantage, as its policy converges to an\noptimal one rather than remaining fixed. Numerical simulations corroborate our\ntheory.\n  Technically, the combination of time-varying learning policies (which induce\nrapidly time-inhomogeneous Markovian noise) and the minimal assumption on\nexploration presents significant analytical challenges. To address these\nchallenges, we employ a refined approach that leverages the Poisson equation to\ndecompose the Markovian noise corresponding to the lazy transition matrix into\na martingale-difference term and residual terms. To control the residual terms\nunder time inhomogeneity, we perform a sensitivity analysis of the Poisson\nequation solution with respect to both the Q-function estimate and the learning\npolicy. These tools may further facilitate the analysis of general\nreinforcement learning algorithms with rapidly time-varying learning policies\n-- such as single-timescale actor--critic methods and learning-in-games\nalgorithms -- and are of independent interest.", "AI": {"tldr": "First finite-time analysis of Q-learning with time-varying learning policies, showing O(1/\u03b5\u00b2) sample complexity matching off-policy Q-learning but with different exploration-exploitation trade-offs.", "motivation": "To provide rigorous finite-time analysis of on-policy Q-learning under minimal assumptions, addressing the analytical challenges of time-varying policies and rapidly time-inhomogeneous Markovian noise.", "method": "Employed refined approach using Poisson equation to decompose Markovian noise into martingale-difference and residual terms, with sensitivity analysis of Poisson equation solution for time inhomogeneity control.", "result": "Established last-iterate convergence rate for Q-function error and derived explicit rate for policy evaluation error, showing on-policy Q-learning has weaker exploration but better exploitation than off-policy counterpart.", "conclusion": "On-policy Q-learning achieves optimal policy convergence with O(1/\u03b5\u00b2) sample complexity, and the developed analytical tools can facilitate analysis of other RL algorithms with time-varying policies."}}
{"id": "2510.16703", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16703", "abs": "https://arxiv.org/abs/2510.16703", "authors": ["Yizuo Chen", "Adnan Darwiche"], "title": "On the Granularity of Causal Effect Identifiability", "comment": null, "summary": "The classical notion of causal effect identifiability is defined in terms of\ntreatment and outcome variables. In this note, we consider the identifiability\nof state-based causal effects: how an intervention on a particular state of\ntreatment variables affects a particular state of outcome variables. We\ndemonstrate that state-based causal effects may be identifiable even when\nvariable-based causal effects may not. Moreover, we show that this separation\noccurs only when additional knowledge -- such as context-specific\nindependencies and conditional functional dependencies -- is available. We\nfurther examine knowledge that constrains the states of variables, and show\nthat such knowledge does not improve identifiability on its own but can improve\nboth variable-based and state-based identifiability when combined with other\nknowledge such as context-specific independencies. Our findings highlight\nsituations where causal effects of interest may be estimable from observational\ndata and this identifiability may be missed by existing variable-based\nframeworks.", "AI": {"tldr": "State-based causal effects (interventions on specific states of treatment variables affecting specific states of outcome variables) can be identifiable even when variable-based causal effects are not, particularly when context-specific independencies and conditional functional dependencies are available.", "motivation": "To explore whether causal effects defined at the state level (specific values of variables) rather than the variable level can be identifiable in situations where traditional variable-based causal effects are not identifiable.", "method": "Theoretical analysis of state-based causal effect identifiability, examining how additional knowledge such as context-specific independencies and conditional functional dependencies enables identifiability that variable-based frameworks miss.", "result": "State-based causal effects may be identifiable even when variable-based effects are not, and this separation only occurs with additional structural knowledge. Knowledge constraining variable states alone doesn't improve identifiability but can enhance both variable-based and state-based identifiability when combined with other structural knowledge.", "conclusion": "State-based causal analysis can reveal estimable causal effects from observational data that existing variable-based frameworks would miss, highlighting the importance of considering finer-grained causal questions and leveraging additional structural knowledge."}}
{"id": "2510.15985", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15985", "abs": "https://arxiv.org/abs/2510.15985", "authors": ["Zexi Tan", "Tao Xie", "Binbin Sun", "Xiang Zhang", "Yiqun Zhang", "Yiu-Ming Cheung"], "title": "MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction", "comment": "Accepted to PRICAI 2025", "summary": "Sepsis is a life-threatening infectious syndrome associated with high\nmortality in intensive care units (ICUs). Early and accurate sepsis prediction\n(SP) is critical for timely intervention, yet remains challenging due to subtle\nearly manifestations and rapidly escalating mortality. While AI has improved SP\nefficiency, existing methods struggle to capture weak early temporal signals.\nThis paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)\nmechanism to construct enriched feature views, coupled with a Cascaded\nDual-convolution Time-series Attention (CDTA) module for multi-scale temporal\nrepresentation learning. The proposed MEET-Sepsis framework achieves\ncompetitive prediction accuracy using only 20% of the ICU monitoring time\nrequired by SOTA methods, significantly advancing early SP. Extensive\nvalidation confirms its efficacy. Code is available at:\nhttps://github.com/yueliangy/MEET-Sepsis.", "AI": {"tldr": "The paper introduces MEET-Sepsis, a framework that uses multi-view feature enhancement and multi-scale temporal attention for early sepsis prediction, achieving competitive accuracy with only 20% of ICU monitoring time compared to state-of-the-art methods.", "motivation": "Early and accurate sepsis prediction is critical but challenging due to subtle early manifestations and rapidly escalating mortality. Existing AI methods struggle to capture weak early temporal signals in ICU patients.", "method": "Proposes MEET-Sepsis framework with Multi-Endogenous-view Representation Enhancement (MERE) mechanism for enriched feature views and Cascaded Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal representation learning.", "result": "The framework achieves competitive prediction accuracy using only 20% of the ICU monitoring time required by state-of-the-art methods, significantly advancing early sepsis prediction.", "conclusion": "MEET-Sepsis effectively addresses the challenge of early sepsis prediction by capturing subtle temporal patterns and demonstrates superior efficiency in clinical settings with reduced monitoring requirements."}}
{"id": "2510.15986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15986", "abs": "https://arxiv.org/abs/2510.15986", "authors": ["Sifeddine Sellami", "Juba Agoun", "Lamia Yessad", "Louenas Bounia"], "title": "User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis", "comment": "in French language, Plate-Forme Intelligence Artificielle, Jun 2025,\n  Dijon (FRANCE), France", "summary": "Sleep disorders have a major impact on patients' health and quality of life,\nbut their diagnosis remains complex due to the diversity of symptoms. Today,\ntechnological advances, combined with medical data analysis, are opening new\nperspectives for a better understanding of these disorders. In particular,\nexplainable artificial intelligence (XAI) aims to make AI model decisions\nunderstandable and interpretable for users. In this study, we propose a\nclustering-based method to group patients according to different sleep disorder\nprofiles. By integrating an explainable approach, we identify the key factors\ninfluencing these pathologies. An experiment on anonymized real data\nillustrates the effectiveness and relevance of our approach.", "AI": {"tldr": "Proposes a clustering-based method using explainable AI to group patients by sleep disorder profiles and identify key influencing factors, validated on real anonymized data.", "motivation": "Sleep disorders significantly impact health and quality of life, but diagnosis is complex due to diverse symptoms. Technological advances and medical data analysis offer new opportunities for better understanding these disorders.", "method": "Uses a clustering-based approach combined with explainable artificial intelligence (XAI) to group patients according to different sleep disorder profiles and make AI model decisions interpretable.", "result": "The method successfully identifies key factors influencing sleep pathologies through an experiment conducted on anonymized real data.", "conclusion": "The proposed approach demonstrates effectiveness and relevance in understanding sleep disorders through explainable clustering of patient profiles."}}
{"id": "2510.16157", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16157", "abs": "https://arxiv.org/abs/2510.16157", "authors": ["Xuchen Gong", "Tian Li"], "title": "Zeroth-Order Sharpness-Aware Learning with Exponential Tilting", "comment": null, "summary": "Classic zeroth-order optimization approaches typically optimize for a\nsmoothed version of the original function, i.e., the expected objective under\nrandomly perturbed model parameters. This can be interpreted as encouraging the\nloss values in the perturbation set to be small on average. Popular\nsharpness-aware minimization (SAM) objectives, however, typically focus on the\nlargest loss within the neighborhood to arrive at flat minima more effectively.\nIn this work, we connect zeroth-order optimization (and its corresponding\nobjectives) with SAM approaches explicitly, through an exponential tilting\nobjective that provides a smooth transition between the average- and the\nmax-loss formulations. We explore new zeroth-order algorithms to solve a soft\nSAM objective parameterized by a tilting parameter $t$. We provide precise\ncharacterizations of the sharpness notions of the tilted SAM framework.\nPractically, our approach can be used as a gradient-free and memory-efficient\nalternative to SAM variants, and it achieves better generalization compared to\nvanilla zeroth-order baselines on a wide range of downstream tasks, including\nclassification, multiple choice QA, and language generation.", "AI": {"tldr": "This paper connects zeroth-order optimization with sharpness-aware minimization (SAM) through an exponential tilting objective that bridges average-loss and max-loss formulations, proposing new gradient-free algorithms that achieve better generalization than vanilla zeroth-order methods.", "motivation": "To bridge the gap between classic zeroth-order optimization (which optimizes smoothed functions) and SAM approaches (which focus on worst-case loss), providing a unified framework that transitions between average and max loss objectives.", "method": "Develops an exponential tilting objective that smoothly transitions between average-loss and max-loss formulations, and proposes new zeroth-order algorithms to solve this soft SAM objective parameterized by a tilting parameter.", "result": "The approach achieves better generalization compared to vanilla zeroth-order baselines across various tasks including classification, multiple choice QA, and language generation, while serving as a gradient-free and memory-efficient alternative to SAM variants.", "conclusion": "The exponential tilting framework successfully connects zeroth-order optimization with SAM, providing precise characterizations of sharpness notions and practical gradient-free alternatives that improve generalization performance."}}
{"id": "2510.15987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15987", "abs": "https://arxiv.org/abs/2510.15987", "authors": ["Samuel Lippl", "Thomas McGee", "Kimberly Lopez", "Ziwen Pan", "Pierce Zhang", "Salma Ziadi", "Oliver Eberle", "Ida Momennejad"], "title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models", "comment": null, "summary": "How do latent and inference time computations enable large language models\n(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and\nsteering algorithmic primitives that underlie model reasoning. Our approach\nlinks reasoning traces to internal activation patterns and evaluates\nalgorithmic primitives by injecting them into residual streams and measuring\ntheir effect on reasoning steps and task performance. We consider four\nbenchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph\nnavigation. We operationalize primitives by clustering neural activations and\nlabeling their matched reasoning traces. We then apply function vector methods\nto derive primitive vectors as reusable compositional building blocks of\nreasoning. Primitive vectors can be combined through addition, subtraction, and\nscalar operations, revealing a geometric logic in activation space. Cross-task\nand cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both\nshared and task-specific primitives. Notably, comparing Phi-4 with its\nreasoning-finetuned variant highlights compositional generalization after\nfinetuning: Phi-4-Reasoning exhibits more systematic use of verification and\npath-generation primitives. Injecting the associated primitive vectors in\nPhi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.\nTogether, these findings demonstrate that reasoning in LLMs may be supported by\na compositional geometry of algorithmic primitives, that primitives transfer\ncross-task and cross-model, and that reasoning finetuning strengthens\nalgorithmic generalization across domains.", "AI": {"tldr": "The paper introduces a framework for identifying and manipulating algorithmic primitives in LLMs that enable multi-step reasoning, showing these primitives form a compositional geometry in activation space and transfer across tasks and models.", "motivation": "To understand how latent computations and inference time processes enable large language models to perform complex multi-step reasoning tasks, and to develop methods for tracing and steering these algorithmic building blocks.", "method": "Developed a framework that links reasoning traces to internal activation patterns, operationalizes primitives by clustering neural activations and labeling matched reasoning traces, and uses function vector methods to derive primitive vectors as reusable compositional building blocks.", "result": "Primitive vectors can be combined through arithmetic operations, revealing geometric logic in activation space. Cross-task and cross-model evaluations show both shared and task-specific primitives. Reasoning-finetuned models exhibit more systematic use of verification and path-generation primitives, and injecting these vectors into base models induces behavioral changes.", "conclusion": "LLM reasoning is supported by a compositional geometry of algorithmic primitives that transfer cross-task and cross-model, and reasoning finetuning strengthens algorithmic generalization across domains."}}
{"id": "2510.16161", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16161", "abs": "https://arxiv.org/abs/2510.16161", "authors": ["Ankitkumar Joshi", "Milos Hauskrecht"], "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction", "comment": null, "summary": "Modeling irregularly sampled multivariate time series is a persistent\nchallenge in domains like healthcare and sensor networks. While recent works\nhave explored a variety of complex learning architectures to solve the\nprediction problems for irregularly sampled time series, it remains unclear\nwhat are the true benefits of some of these architectures, and whether clever\nmodifications of simpler and more efficient RNN-based algorithms are still\ncompetitive, i.e. they are on par with or even superior to these methods. In\nthis work, we propose and study GRUwE: Gated Recurrent Unit with Exponential\nbasis functions, that builds upon RNN-based architectures for observations made\nat irregular times. GRUwE supports both regression-based and event-based\npredictions in continuous time. GRUwE works by maintaining a Markov state\nrepresentation of the time series that updates with the arrival of irregular\nobservations. The Markov state update relies on two reset mechanisms: (i)\nobservation-triggered reset, and (ii) time-triggered reset of the GRU state\nusing learnable exponential decays, to support the predictions in continuous\ntime. Our empirical evaluations across several real-world benchmarks on\nnext-observation and next-event prediction tasks demonstrate that GRUwE can\nindeed achieve competitive to superior performance compared to the recent\nstate-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers\ncompelling advantages: it is easy to implement, requires minimal\nhyper-parameter tuning efforts, and significantly reduces the computational\noverhead in the online deployment.", "AI": {"tldr": "GRUwE is a simple yet effective RNN-based model for irregularly sampled multivariate time series that achieves competitive performance with state-of-the-art methods while being easier to implement and more computationally efficient.", "motivation": "To determine if simpler RNN-based architectures can still be competitive with complex learning methods for irregular time series prediction, and to provide an efficient solution for domains like healthcare and sensor networks.", "method": "Proposes GRUwE (Gated Recurrent Unit with Exponential basis functions) that maintains a Markov state representation updated via two reset mechanisms: observation-triggered reset and time-triggered reset using learnable exponential decays.", "result": "GRUwE achieves competitive to superior performance compared to recent state-of-the-art methods on next-observation and next-event prediction tasks across several real-world benchmarks.", "conclusion": "GRUwE demonstrates that simpler RNN-based approaches can still be highly effective for irregular time series prediction, offering advantages in implementation simplicity, minimal hyper-parameter tuning, and computational efficiency for online deployment."}}
{"id": "2510.15990", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15990", "abs": "https://arxiv.org/abs/2510.15990", "authors": ["Kangqi Ni", "Zhen Tan", "Zijie Liu", "Pingzhi Li", "Tianlong Chen"], "title": "Can GRPO Help LLMs Transcend Their Pretraining Origin?", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by\nthe Group Relative Policy Optimization (GRPO) algorithm, is a leading approach\nfor enhancing the reasoning abilities of Large Language Models (LLMs). Despite\nits wide adoption, GRPO's gains are often inconsistent; for instance, a model\nmay show significant improvement in one reasoning domain, like mathematics, yet\nremain stagnant in another, such as medicine. This inconsistency raises a\ncritical question: under what conditions does GRPO improve reasoning and\ngeneralize out-of-distribution (OOD)? We investigate this from a data\ndistribution perspective. We first prove theoretically that GRPO is a\nconservative reweighting scheme, bounded by the base model's distribution and\nthus unable to discover completely novel solutions. We further validate this in\ncarefully designed controlled studies by training transformers from scratch,\nevaluating generalization across reasoning depth, input length, token\nrepresentation, and compositionality. Our results provide a principled\nexplanation for GRPO's boundaries: OOD improvement emerges only when the target\ntask aligns with the model's pretrained biases, while gains on in-distribution\n(ID) tasks diminish as performance saturates. This reframes GRPO not as a\nuniversal reasoning enhancer but as a tool that sharpens pretraining biases.\nOur findings motivate future development of algorithms that can expand a\nmodel's capabilities beyond its pretraining origin.", "AI": {"tldr": "GRPO (Group Relative Policy Optimization) in RLVR improves LLM reasoning inconsistently - it only enhances capabilities that align with the model's pretraining biases and cannot discover novel solutions beyond the base distribution.", "motivation": "To understand why GRPO shows inconsistent performance gains across different reasoning domains and determine the conditions under which it improves reasoning and generalizes out-of-distribution.", "method": "Theoretical analysis proving GRPO is a conservative reweighting scheme bounded by base model distribution, plus controlled experiments training transformers from scratch to evaluate generalization across reasoning depth, input length, token representation, and compositionality.", "result": "GRPO only improves OOD performance when target tasks align with model's pretrained biases. In-distribution gains diminish as performance saturates. GRPO cannot discover completely novel solutions beyond the base distribution.", "conclusion": "GRPO is not a universal reasoning enhancer but rather a tool that sharpens pretraining biases. Future algorithms should focus on expanding model capabilities beyond pretraining origins."}}
{"id": "2510.15992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15992", "abs": "https://arxiv.org/abs/2510.15992", "authors": ["Ziming Dai", "Tuo Zhang", "Fei Gao", "Xingyi Cai", "Xiaofei Wang", "Cheng Zhang", "Wenyu Wang", "Chengjie Zang"], "title": "Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments", "comment": null, "summary": "The growing industrial demand for customized and cost-efficient large\nlanguage models (LLMs) is fueled by the rise of vertical, domain-specific tasks\nand the need to optimize performance under constraints such as latency and\nbudget. Knowledge distillation, as an efficient model compression and transfer\ntechnique, offers a feasible solution. However, existing distillation\nframeworks often require manual intervention and struggle to meet such complex\nuser-defined distillation requirements. To bridge this gap, we propose Stratos,\nan end-to-end LLM distillation pipeline that automates server and model\nselection, knowledge distillation, and deployment in distributed cloud\nenvironments. Given user-defined constraints on model performance and system\nbudget, Stratos automatically selects Pareto-optimal servers, dynamically\nmatches teacher-student pairs, and adapts distillation strategies based on task\ncomplexity to optimize cloud hosting. Experiments show that Stratos produces a\nstudent model that achieves four times the accuracy of its GPT-4o teacher\nbaseline on a rare, domain-specific Mahjong reasoning task with reverse\nsynthetic data and knowledge injection. Moreover, it achieves reduced latency\nand cost without compromising accuracy. These results highlight its promise for\nvertical-domain LLM deployment.", "AI": {"tldr": "Stratos is an automated LLM distillation pipeline that selects optimal cloud servers, matches teacher-student pairs, and adapts distillation strategies to meet user constraints on performance and budget, achieving 4x accuracy improvement over GPT-4o on domain-specific tasks.", "motivation": "Growing industrial demand for customized, cost-efficient LLMs for vertical domain tasks with constraints like latency and budget, with existing distillation frameworks requiring manual intervention and struggling with complex requirements.", "method": "End-to-end LLM distillation pipeline that automates server/model selection, knowledge distillation, and deployment in distributed cloud environments, using Pareto-optimal server selection, dynamic teacher-student matching, and adaptive distillation strategies based on task complexity.", "result": "Achieved 4x accuracy improvement over GPT-4o teacher baseline on rare Mahjong reasoning task using reverse synthetic data and knowledge injection, with reduced latency and cost without compromising accuracy.", "conclusion": "Stratos shows promise for efficient vertical-domain LLM deployment by automating complex distillation processes and optimizing cloud hosting while meeting user-defined constraints."}}
{"id": "2510.16185", "categories": ["cs.LG", "cs.AI", "cs.FL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16185", "abs": "https://arxiv.org/abs/2510.16185", "authors": ["Daniel Donnelly", "Angelo Ferrando", "Francesco Belardinelli"], "title": "Expressive Reward Synthesis with the Runtime Monitoring Language", "comment": null, "summary": "A key challenge in reinforcement learning (RL) is reward (mis)specification,\nwhereby imprecisely defined reward functions can result in unintended, possibly\nharmful, behaviours. Indeed, reward functions in RL are typically treated as\nblack-box mappings from state-action pairs to scalar values. While effective in\nmany settings, this approach provides no information about why rewards are\ngiven, which can hinder learning and interpretability. Reward Machines address\nthis issue by representing reward functions as finite state automata, enabling\nthe specification of structured, non-Markovian reward functions. However, their\nexpressivity is typically bounded by regular languages, leaving them unable to\ncapture more complex behaviours such as counting or parametrised conditions. In\nthis work, we build on the Runtime Monitoring Language (RML) to develop a novel\nclass of language-based Reward Machines. By leveraging the built-in memory of\nRML, our approach can specify reward functions for non-regular, non-Markovian\ntasks. We demonstrate the expressiveness of our approach through experiments,\nhighlighting additional advantages in flexible event-handling and task\nspecification over existing Reward Machine-based methods.", "AI": {"tldr": "The paper introduces language-based Reward Machines that extend traditional Reward Machines using Runtime Monitoring Language (RML) to handle non-regular, non-Markovian tasks through built-in memory capabilities.", "motivation": "Traditional RL reward functions are black-box mappings that lack interpretability and cannot capture complex behaviors like counting or parameterized conditions. Reward Machines address some limitations but are bounded by regular languages.", "method": "Developed a novel class of language-based Reward Machines by building on Runtime Monitoring Language (RML), leveraging its built-in memory to specify reward functions for non-regular, non-Markovian tasks.", "result": "The approach demonstrates enhanced expressiveness through experiments, showing advantages in flexible event-handling and task specification over existing Reward Machine-based methods.", "conclusion": "Language-based Reward Machines using RML provide a more expressive framework for specifying complex reward functions in reinforcement learning, overcoming limitations of traditional Reward Machines."}}
{"id": "2510.15996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15996", "abs": "https://arxiv.org/abs/2510.15996", "authors": ["Ozan K. Tonguz", "Federico Taschin"], "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning", "comment": null, "summary": "One of the major problems in Machine Learning (ML) and Artificial\nIntelligence (AI) is the fact that the probability distribution of the test\ndata in the real world could deviate substantially from the probability\ndistribution of the training data set. When this happens, the predictions of an\nML system or an AI agent could involve large errors which is very troublesome\nand undesirable. While this is a well-known hard problem plaguing the AI and ML\nsystems' accuracy and reliability, in certain applications such errors could be\ncritical for safety and reliability of AI and ML systems. One approach to deal\nwith this problem is to monitor and measure the deviation in the probability\ndistribution of the test data in real time and to compensate for this\ndeviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov\n(KS) Test for measuring the distribution shift and we show how the KS distance\ncan be used to quantify the distribution shift and its impact on an AI agent's\nperformance. Our results suggest that KS distance could be used as a valuable\nstatistical tool for monitoring and measuring the distribution shift. More\nspecifically, it is shown that even a distance of KS=0.02 could lead to about\n50\\% increase in the travel time at a single intersection using a Reinforcement\nLearning agent which is quite significant. It is hoped that the use of KS Test\nand KS distance in AI-based smart transportation could be an important step\nforward for gauging the performance degradation of an AI agent in real time and\nthis, in turn, could help the AI agent to cope with the distribution shift in a\nmore informed manner.", "AI": {"tldr": "The paper proposes using Kolmogorov-Smirnov (KS) Test to measure distribution shift between training and test data, showing that even small KS distances (0.02) can cause significant performance degradation (50% travel time increase) in AI transportation systems.", "motivation": "Address the critical problem of distribution shift between training and test data in ML/AI systems, which can lead to large prediction errors and safety/reliability issues, particularly in applications like smart transportation.", "method": "Propose and explore the use of Kolmogorov-Smirnov (KS) Test for measuring distribution shift, using KS distance to quantify the shift and its impact on AI agent performance.", "result": "Results show that even a small KS distance of 0.02 can lead to about 50% increase in travel time at a single intersection when using a Reinforcement Learning agent, demonstrating significant performance degradation.", "conclusion": "KS Test and KS distance can serve as valuable statistical tools for real-time monitoring of distribution shift in AI systems, enabling more informed coping strategies for AI agents dealing with distribution shifts in applications like smart transportation."}}
{"id": "2510.16208", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16208", "abs": "https://arxiv.org/abs/2510.16208", "authors": ["Sunmook Choi", "Yahya Sattar", "Yassir Jedra", "Maryam Fazel", "Sarah Dean"], "title": "Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics", "comment": null, "summary": "We study a nonstationary bandit problem where rewards depend on both actions\nand latent states, the latter governed by unknown linear dynamics. Crucially,\nthe state dynamics also depend on the actions, resulting in tension between\nshort-term and long-term rewards. We propose an explore-then-commit algorithm\nfor a finite horizon $T$. During the exploration phase, random Rademacher\nactions enable estimation of the Markov parameters of the linear dynamics,\nwhich characterize the action-reward relationship. In the commit phase, the\nalgorithm uses the estimated parameters to design an optimized action sequence\nfor long-term reward. Our proposed algorithm achieves\n$\\tilde{\\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:\nlearning from temporally correlated rewards, and designing action sequences\nwith optimal long-term reward. We address the first challenge by providing\nnear-optimal sample complexity and error bounds for system identification using\nbilinear rewards. We address the second challenge by proving an equivalence\nwith indefinite quadratic optimization over a hypercube, a known NP-hard\nproblem. We provide a sub-optimality guarantee for this problem, enabling our\nregret upper bound. Lastly, we propose a semidefinite relaxation with\nGoemans-Williamson rounding as a practical approach.", "AI": {"tldr": "The paper presents an explore-then-commit algorithm for nonstationary bandits with action-dependent linear state dynamics, achieving \u00d5(T^{2/3}) regret by first estimating system parameters through random exploration, then optimizing long-term action sequences.", "motivation": "To address the tension between short-term and long-term rewards in bandit problems where rewards depend on both actions and latent states governed by unknown linear dynamics that also depend on actions.", "method": "An explore-then-commit algorithm with random Rademacher actions during exploration to estimate Markov parameters, followed by optimized action sequence design using estimated parameters for long-term reward maximization.", "result": "The proposed algorithm achieves \u00d5(T^{2/3}) regret, with analysis providing near-optimal sample complexity for system identification and sub-optimality guarantees for the NP-hard indefinite quadratic optimization problem.", "conclusion": "The paper successfully addresses challenges of learning from temporally correlated rewards and designing optimal long-term action sequences, providing both theoretical guarantees and practical implementation via semidefinite relaxation with Goemans-Williamson rounding."}}
{"id": "2510.15998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15998", "abs": "https://arxiv.org/abs/2510.15998", "authors": ["Nilo Schwencke", "Cyriaque Rousselot", "Alena Shilova", "Cyril Furtlehner"], "title": "AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM", "comment": null, "summary": "Recent works have shown that natural gradient methods can significantly\noutperform standard optimizers when training physics-informed neural networks\n(PINNs). In this paper, we analyze the training dynamics of PINNs optimized\nwith ANaGRAM, a natural-gradient-inspired approach employing singular value\ndecomposition with cutoff regularization. Building on this analysis, we propose\na multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.\nExperiments on benchmark PDEs validate the effectiveness of our method, which\nallows to reach machine precision on some experiments. To provide theoretical\ngrounding, we develop a framework based on spectral theory that explains the\nnecessity of regularization and extend previous shown connections with Green's\nfunctions theory.", "AI": {"tldr": "Analysis of PINNs training dynamics with ANaGRAM natural gradient method, proposing multi-cutoff adaptation strategy that achieves machine precision on benchmark PDEs, with theoretical framework explaining regularization necessity.", "motivation": "Recent works show natural gradient methods outperform standard optimizers for PINNs training, but training dynamics analysis and improved regularization strategies are needed.", "method": "Analyze PINNs training dynamics with ANaGRAM (natural-gradient approach using SVD with cutoff regularization), propose multi-cutoff adaptation strategy, validate on benchmark PDEs.", "result": "Enhanced ANaGRAM performance, reaching machine precision on some experiments, with theoretical framework explaining regularization necessity.", "conclusion": "Multi-cutoff adaptation strategy significantly improves ANaGRAM's performance for PINNs training, with theoretical grounding in spectral theory and connections to Green's functions."}}
{"id": "2510.16211", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16211", "abs": "https://arxiv.org/abs/2510.16211", "authors": ["Henrique Pickler", "Jorge K. S. Kamassury", "Danilo Silva"], "title": "Benchmarking noisy label detection methods", "comment": null, "summary": "Label noise is a common problem in real-world datasets, affecting both model\ntraining and validation. Clean data are essential for achieving strong\nperformance and ensuring reliable evaluation. While various techniques have\nbeen proposed to detect noisy labels, there is no clear consensus on optimal\napproaches. We perform a comprehensive benchmark of detection methods by\ndecomposing them into three fundamental components: label agreement function,\naggregation method, and information gathering approach (in-sample vs\nout-of-sample). This decomposition can be applied to many existing detection\nmethods, and enables systematic comparison across diverse approaches. To fairly\ncompare methods, we propose a unified benchmark task, detecting a fraction of\ntraining samples equal to the dataset's noise rate. We also introduce a novel\nmetric: the false negative rate at this fixed operating point. Our evaluation\nspans vision and tabular datasets under both synthetic and real-world noise\nconditions. We identify that in-sample information gathering using average\nprobability aggregation combined with the logit margin as the label agreement\nfunction achieves the best results across most scenarios. Our findings provide\npractical guidance for designing new detection methods and selecting techniques\nfor specific applications.", "AI": {"tldr": "A comprehensive benchmark of label noise detection methods decomposed into three components: label agreement function, aggregation method, and information gathering approach, with evaluation across vision and tabular datasets under synthetic and real-world noise conditions.", "motivation": "Label noise is a common problem in real-world datasets that affects model training and validation, but there's no clear consensus on optimal approaches for detecting noisy labels.", "method": "Decompose detection methods into three fundamental components: label agreement function, aggregation method, and information gathering approach (in-sample vs out-of-sample). Propose a unified benchmark task of detecting training samples equal to the dataset's noise rate with a novel false negative rate metric.", "result": "In-sample information gathering using average probability aggregation combined with the logit margin as the label agreement function achieves the best results across most scenarios in both vision and tabular datasets under synthetic and real-world noise conditions.", "conclusion": "The findings provide practical guidance for designing new detection methods and selecting techniques for specific applications, with the identified combination performing optimally across diverse scenarios."}}
{"id": "2510.16007", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16007", "abs": "https://arxiv.org/abs/2510.16007", "authors": ["Ziao Yang", "Longbo Huang", "Hongfu Liu"], "title": "Layer-Aware Influence for Online Data Valuation Estimation", "comment": null, "summary": "Data-centric learning emphasizes curating high-quality training samples to\nboost performance rather than designing new architectures. A central problem is\nto estimate the influence of training sample efficiently. Prior studies largely\nfocus on static influence measured on a converged model, overlooking how data\nvaluation dynamically changes during optimization. This omission neglects the\ndynamic nature of sample influence during optimization, especially in deep\nmodels. To address the computational burden of frequent influence estimation,\nwe develop a layer-aware online estimator that requires only loss-to-output\ngradients. This design avoids parameter-level and full-network gradients while\npreserving ranking fidelity. Extensive experiments across LLM pretraining,\nfine-tuning, and image classification show our method improves accuracy with\nsubstantially lower time and memory cost, making dynamic data curation\nefficient and scalable in practice.", "AI": {"tldr": "This paper proposes a layer-aware online estimator for dynamic data influence during training, using only loss-to-output gradients to efficiently track how sample importance changes throughout optimization.", "motivation": "Current data-centric approaches focus on static influence measurements after model convergence, ignoring the dynamic nature of sample importance during training, especially in deep models. Existing methods are computationally expensive for frequent influence estimation.", "method": "Developed a layer-aware online estimator that requires only loss-to-output gradients, avoiding parameter-level and full-network gradients while maintaining ranking accuracy of sample influence.", "result": "Extensive experiments across LLM pretraining, fine-tuning, and image classification demonstrate improved accuracy with significantly reduced time and memory costs compared to existing methods.", "conclusion": "The proposed method enables efficient and scalable dynamic data curation in practice by providing a computationally lightweight approach to track changing sample influence during model optimization."}}
{"id": "2510.16250", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16250", "abs": "https://arxiv.org/abs/2510.16250", "authors": ["Danil Akhtiamov", "Reza Ghane", "Babak Hassibi"], "title": "One-Bit Quantization for Random Features Models", "comment": null, "summary": "Recent advances in neural networks have led to significant computational and\nmemory demands, spurring interest in one-bit weight compression to enable\nefficient inference on resource-constrained devices. However, the theoretical\nunderpinnings of such compression remain poorly understood. We address this gap\nby analyzing one-bit quantization in the Random Features model, a simplified\nframework that corresponds to neural networks with random representations. We\nprove that, asymptotically, quantizing weights of all layers except the last\nincurs no loss in generalization error, compared to the full precision random\nfeatures model. Our findings offer theoretical insights into neural network\ncompression. We also demonstrate empirically that one-bit quantization leads to\nsignificant inference speed ups for the Random Features models even on a laptop\nGPU, confirming the practical benefits of our work. Additionally, we provide an\nasymptotically precise characterization of the generalization error for Random\nFeatures with an arbitrary number of layers. To the best of our knowledge, our\nanalysis yields more general results than all previous works in the related\nliterature.", "AI": {"tldr": "The paper analyzes one-bit weight quantization in neural networks using the Random Features model, proving that quantizing all layers except the last incurs no asymptotic loss in generalization error compared to full precision models.", "motivation": "Recent neural network advances create high computational and memory demands, creating interest in one-bit weight compression for efficient inference on resource-constrained devices, but the theoretical foundations of such compression remain poorly understood.", "method": "The authors analyze one-bit quantization using the Random Features model, which corresponds to neural networks with random representations, and provide theoretical proofs about generalization error.", "result": "The analysis proves that asymptotically, quantizing weights of all layers except the last layer incurs no loss in generalization error compared to full precision models. Empirical results show significant inference speedups on laptop GPUs.", "conclusion": "The work provides theoretical insights into neural network compression and demonstrates practical benefits through significant inference speed improvements, while offering more general results than previous literature in this area."}}
{"id": "2510.16014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16014", "abs": "https://arxiv.org/abs/2510.16014", "authors": ["Hanyin Cheng", "Ruitong Zhang", "Yuning Lu", "Peng Chen", "Meng Wang", "Yang Shu", "Bin Yang", "Chenjuan Guo"], "title": "STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter", "comment": null, "summary": "While Time Series Foundation Models (TSFMs) have demonstrated remarkable\nsuccess in Multivariate Time Series Anomaly Detection (MTSAD), however, in\nreal-world industrial scenarios, many time series comprise not only numerical\nvariables such as temperature and flow, but also numerous discrete state\nvariables that describe the system status, such as valve on/off or day of the\nweek. Existing TSFMs often overlook the distinct categorical nature of state\nvariables and their critical role as conditions, typically treating them\nuniformly with numerical variables. This inappropriate modeling approach\nprevents the model from fully leveraging state information and even leads to a\nsignificant degradation in detection performance after state variables are\nintegrated. To address this critical limitation, this paper proposes a novel\nSTate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance\nthe capability of TSFMs in modeling and leveraging state variables during the\nfine-tuning stage. Specifically, STAR comprisesthree core components: (1) We\ndesign an Identity-guided State Encoder, whicheffectively captures the complex\ncategorical semantics of state variables through a learnable State Memory. (2)\nWe propose a Conditional Bottleneck Adapter, which dynamically generates\nlow-rank adaptation parameters conditioned on the current state, thereby\nflexibly injecting the influence of state variables into the backbone model.\n(3) We also introduce a Numeral-State Matching module to more effectively\ndetect anomalies inherent to the state variables themselves. Extensive\nexperiments conducted on real-world datasets demonstrate that STAR can improve\nthe performance of existing TSFMs on MTSAD.", "AI": {"tldr": "STAR is a plug-and-play module that enhances Time Series Foundation Models' ability to handle state variables (discrete/categorical data) in multivariate time series anomaly detection, addressing the limitation of existing models that treat state variables uniformly with numerical variables.", "motivation": "Existing Time Series Foundation Models overlook the distinct categorical nature of state variables (e.g., valve on/off, day of week) and their critical role as conditions, treating them uniformly with numerical variables. This inappropriate modeling prevents full utilization of state information and can degrade detection performance when state variables are integrated.", "method": "STAR comprises three core components: (1) Identity-guided State Encoder with learnable State Memory to capture categorical semantics of state variables, (2) Conditional Bottleneck Adapter that dynamically generates low-rank adaptation parameters conditioned on current state, and (3) Numeral-State Matching module to detect anomalies in state variables themselves.", "result": "Extensive experiments on real-world datasets demonstrate that STAR can improve the performance of existing Time Series Foundation Models on Multivariate Time Series Anomaly Detection.", "conclusion": "STAR effectively addresses the critical limitation of existing TSFMs in handling state variables, providing a plug-and-play solution that enhances model capability to leverage state information during fine-tuning without requiring architectural changes to the backbone models."}}
{"id": "2510.16253", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM", "stat.ML", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.16253", "abs": "https://arxiv.org/abs/2510.16253", "authors": ["Arielle Sanford", "Shuo Sun", "Christian B. Mendl"], "title": "Protein Folding with Neural Ordinary Differential Equations", "comment": null, "summary": "Recent advances in protein structure prediction, such as AlphaFold, have\ndemonstrated the power of deep neural architectures like the Evoformer for\ncapturing complex spatial and evolutionary constraints on protein conformation.\nHowever, the depth of the Evoformer, comprising 48 stacked blocks, introduces\nhigh computational costs and rigid layerwise discretization. Inspired by Neural\nOrdinary Differential Equations (Neural ODEs), we propose a continuous-depth\nformulation of the Evoformer, replacing its 48 discrete blocks with a Neural\nODE parameterization that preserves its core attention-based operations. This\ncontinuous-time Evoformer achieves constant memory cost (in depth) via the\nadjoint method, while allowing a principled trade-off between runtime and\naccuracy through adaptive ODE solvers. Benchmarking on protein structure\nprediction tasks, we find that the Neural ODE-based Evoformer produces\nstructurally plausible predictions and reliably captures certain secondary\nstructure elements, such as alpha-helices, though it does not fully replicate\nthe accuracy of the original architecture. However, our model achieves this\nperformance using dramatically fewer resources, just 17.5 hours of training on\na single GPU, highlighting the promise of continuous-depth models as a\nlightweight and interpretable alternative for biomolecular modeling. This work\nopens new directions for efficient and adaptive protein structure prediction\nframeworks.", "AI": {"tldr": "The paper proposes a continuous-depth Evoformer using Neural ODEs to replace the 48 discrete blocks of AlphaFold's Evoformer, achieving constant memory cost and resource-efficient training while maintaining structural plausibility in protein predictions.", "motivation": "To address the high computational costs and rigid layerwise discretization of AlphaFold's 48-block Evoformer architecture by leveraging Neural ODEs for continuous-depth modeling.", "method": "Replace the 48 discrete Evoformer blocks with a Neural ODE parameterization that preserves attention-based operations, using the adjoint method for constant memory cost and adaptive ODE solvers for runtime-accuracy trade-offs.", "result": "The Neural ODE-based Evoformer produces structurally plausible protein predictions and captures secondary structure elements like alpha-helices, though with slightly lower accuracy than the original. It achieves this with dramatically fewer resources (17.5 hours on single GPU).", "conclusion": "Continuous-depth models offer a promising lightweight and interpretable alternative for biomolecular modeling, opening new directions for efficient and adaptive protein structure prediction frameworks."}}
{"id": "2510.16015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16015", "abs": "https://arxiv.org/abs/2510.16015", "authors": ["Qian Sun", "Graham Hults", "Susu Xu"], "title": "Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach", "comment": null, "summary": "Timely and reliable decision-making is vital for flood emergency response,\nyet it remains severely hindered by limited and imprecise situational awareness\ndue to various budget and data accessibility constraints. Traditional flood\nmanagement systems often rely on in-situ sensors to calibrate remote\nsensing-based large-scale flood depth forecasting models, and further take\nflood depth estimates to optimize flood response decisions. However, these\napproaches often take fixed, decision task-agnostic strategies to decide where\nto put in-situ sensors (e.g., maximize overall information gain) and train\nflood forecasting models (e.g., minimize average forecasting errors), but\noverlook that systems with the same sensing gain and average forecasting errors\nmay lead to distinct decisions. To address this, we introduce a novel\ndecision-focused framework that strategically selects locations for in-situ\nsensor placement and optimize spatio-temporal flood forecasting models to\noptimize downstream flood response decision regrets. Our end-to-end pipeline\nintegrates four components: a contextual scoring network, a differentiable\nsensor selection module under hard budget constraints, a spatio-temporal flood\nreconstruction and forecasting model, and a differentiable decision layer\ntailored to task-specific objectives. Central to our approach is the\nincorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable\ngradient-based learning over discrete sensor configurations, and probabilistic\ndecision heads to enable differentiable approximation to various constrained\ndisaster response tasks.", "AI": {"tldr": "A decision-focused framework for flood emergency response that integrates sensor placement, flood forecasting, and decision optimization to minimize response regrets under budget constraints.", "motivation": "Traditional flood management systems use task-agnostic approaches for sensor placement and model training, overlooking how different configurations with similar sensing gains can lead to distinct decision outcomes in emergency response.", "method": "End-to-end pipeline with four components: contextual scoring network, differentiable sensor selection under budget constraints, spatio-temporal flood reconstruction/forecasting model, and differentiable decision layer. Uses Implicit Maximum Likelihood Estimation (I-MLE) for gradient-based learning over discrete sensor configurations.", "result": "The framework enables strategic sensor placement and flood forecasting optimization specifically tailored to downstream decision-making tasks rather than generic metrics like information gain or average forecasting errors.", "conclusion": "The proposed decision-focused approach provides a more effective solution for flood emergency response by directly optimizing for decision quality rather than intermediate metrics, addressing limitations of traditional task-agnostic methods."}}
{"id": "2510.16356", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16356", "abs": "https://arxiv.org/abs/2510.16356", "authors": ["Fuqun Han", "Stanley Osher", "Wuchen Li"], "title": "Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior", "comment": null, "summary": "In this work, we propose a sparse transformer architecture that incorporates\nprior information about the underlying data distribution directly into the\ntransformer structure of the neural network. The design of the model is\nmotivated by a special optimal transport problem, namely the regularized\nWasserstein proximal operator, which admits a closed-form solution and turns\nout to be a special representation of transformer architectures. Compared with\nclassical flow-based models, the proposed approach improves the convexity\nproperties of the optimization problem and promotes sparsity in the generated\nsamples. Through both theoretical analysis and numerical experiments, including\napplications in generative modeling and Bayesian inverse problems, we\ndemonstrate that the sparse transformer achieves higher accuracy and faster\nconvergence to the target distribution than classical neural ODE-based methods.", "AI": {"tldr": "A sparse transformer architecture that incorporates prior data distribution information through optimal transport theory, improving convexity and sparsity compared to classical flow-based models.", "motivation": "To enhance transformer architectures by directly embedding prior knowledge about data distributions, addressing limitations of classical flow-based models in optimization convexity and sample sparsity.", "method": "Proposes a sparse transformer design motivated by regularized Wasserstein proximal operators, which have closed-form solutions and naturally represent transformer architectures. This approach integrates optimal transport theory into neural network structure.", "result": "The sparse transformer achieves higher accuracy and faster convergence to target distributions than neural ODE-based methods, as demonstrated through theoretical analysis and numerical experiments in generative modeling and Bayesian inverse problems.", "conclusion": "Incorporating optimal transport principles into transformer architectures through sparse designs improves optimization properties and sample quality, offering superior performance over traditional flow-based approaches."}}
{"id": "2510.16016", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16016", "abs": "https://arxiv.org/abs/2510.16016", "authors": ["Saeed Salehi"], "title": "Transfer learning strategies for accelerating reinforcement-learning-based flow control", "comment": null, "summary": "This work investigates transfer learning strategies to accelerate deep\nreinforcement learning (DRL) for multifidelity control of chaotic fluid flows.\nProgressive neural networks (PNNs), a modular architecture designed to preserve\nand reuse knowledge across tasks, are employed for the first time in the\ncontext of DRL-based flow control. In addition, a comprehensive benchmarking of\nconventional fine-tuning strategies is conducted, evaluating their performance,\nconvergence behavior, and ability to retain transferred knowledge. The\nKuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how\nknowledge encoded in control policies, trained in low-fidelity environments,\ncan be effectively transferred to high-fidelity settings. Systematic\nevaluations show that while fine-tuning can accelerate convergence, it is\nhighly sensitive to pretraining duration and prone to catastrophic forgetting.\nIn contrast, PNNs enable stable and efficient transfer by preserving prior\nknowledge and providing consistent performance gains, and are notably robust to\noverfitting during the pretraining phase. Layer-wise sensitivity analysis\nfurther reveals how PNNs dynamically reuse intermediate representations from\nthe source policy while progressively adapting deeper layers to the target\ntask. Moreover, PNNs remain effective even when the source and target\nenvironments differ substantially, such as in cases with mismatched physical\nregimes or control objectives, where fine-tuning strategies often result in\nsuboptimal adaptation or complete failure of knowledge transfer. The results\nhighlight the potential of novel transfer learning frameworks for robust,\nscalable, and computationally efficient flow control that can potentially be\napplied to more complex flow configurations.", "AI": {"tldr": "This paper investigates transfer learning strategies to accelerate deep reinforcement learning for multifidelity control of chaotic fluid flows, comparing progressive neural networks with conventional fine-tuning methods.", "motivation": "To accelerate deep reinforcement learning for multifidelity control of chaotic fluid flows by effectively transferring knowledge from low-fidelity to high-fidelity environments.", "method": "Employed progressive neural networks (PNNs) for the first time in DRL-based flow control and conducted comprehensive benchmarking of conventional fine-tuning strategies using the Kuramoto-Sivashinsky system as a benchmark.", "result": "PNNs enable stable and efficient transfer by preserving prior knowledge with consistent performance gains, while fine-tuning is sensitive to pretraining duration and prone to catastrophic forgetting. PNNs remain effective even with substantial differences between source and target environments.", "conclusion": "Progressive neural networks offer robust, scalable, and computationally efficient transfer learning for flow control, outperforming fine-tuning strategies and showing potential for application to more complex flow configurations."}}
{"id": "2510.16462", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16462", "abs": "https://arxiv.org/abs/2510.16462", "authors": ["Emmanuelle Claeys", "Elena Kerjean", "Jean-Michel Loubes"], "title": "Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making", "comment": null, "summary": "We introduce a sequential reinforcement learning framework for imitation\nlearning designed to model heterogeneous cognitive strategies in pollinators.\nFocusing on honeybees, our approach leverages trajectory similarity to capture\nand forecast behavior across individuals that rely on distinct strategies: some\nexploiting numerical cues, others drawing on memory, or being influenced by\nenvironmental factors such as weather. Through empirical evaluation, we show\nthat state-of-the-art imitation learning methods often fail in this setting:\nwhen expert policies shift across memory windows or deviate from optimality,\nthese models overlook both fast and slow learning behaviors and cannot\nfaithfully reproduce key decision patterns. Moreover, they offer limited\ninterpretability, hindering biological insight. Our contribution addresses\nthese challenges by (i) introducing a model that minimizes predictive loss\nwhile identifying the effective memory horizon most consistent with behavioral\ndata, and (ii) ensuring full interpretability to enable biologists to analyze\nunderlying decision-making strategies and finally (iii) providing a\nmathematical framework linking bee policy search with bandit formulations under\nvarying exploration-exploitation dynamics, and releasing a novel dataset of 80\ntracked bees observed under diverse weather conditions. This benchmark\nfacilitates research on pollinator cognition and supports ecological governance\nby improving simulations of insect behavior in agroecosystems. Our findings\nshed new light on the learning strategies and memory interplay shaping\npollinator decision-making.", "AI": {"tldr": "A sequential reinforcement learning framework for imitation learning that models heterogeneous cognitive strategies in pollinators, particularly honeybees, addressing limitations of existing methods in capturing diverse learning behaviors and providing interpretability for biological insights.", "motivation": "To address the failure of state-of-the-art imitation learning methods in modeling heterogeneous cognitive strategies in pollinators, especially when expert policies shift across memory windows or deviate from optimality, and to provide interpretability for biological analysis of decision-making strategies.", "method": "Introduces a model that minimizes predictive loss while identifying the effective memory horizon most consistent with behavioral data, ensures full interpretability, and provides a mathematical framework linking bee policy search with bandit formulations under varying exploration-exploitation dynamics.", "result": "The approach captures and forecasts behavior across individuals using distinct strategies (numerical cues, memory, environmental factors), successfully reproduces key decision patterns, and provides interpretable insights into pollinator decision-making. A novel dataset of 80 tracked bees under diverse weather conditions is released.", "conclusion": "The framework sheds new light on learning strategies and memory interplay shaping pollinator decision-making, facilitates research on pollinator cognition, and supports ecological governance by improving simulations of insect behavior in agroecosystems."}}
{"id": "2510.16020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16020", "abs": "https://arxiv.org/abs/2510.16020", "authors": ["Sangjoon Lee", "Haris Moazam Sheikh"], "title": "Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality", "comment": null, "summary": "Effective airfoil geometry optimization requires exploring a diverse range of\ndesigns using as few design variables as possible. This study introduces\nAirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil\noptimization that systematically reduces design-space dimensionality. AirDbM\nselects an optimal set of 12 baseline airfoils from the UIUC airfoil database,\nwhich contains over 1,600 shapes, by sequentially adding the baseline that most\nincreases the design capacity. With these baselines, AirDbM reconstructs 99 \\%\nof the database with a mean absolute error below 0.005, which matches the\nperformance of a previous DbM approach that used more baselines. In\nmulti-objective aerodynamic optimization, AirDbM demonstrates rapid convergence\nand achieves a Pareto front with a greater hypervolume than that of the\nprevious larger-baseline study, where new Pareto-optimal solutions are\ndiscovered with enhanced lift-to-drag ratios at moderate stall tolerances.\nFurthermore, AirDbM demonstrates outstanding adaptability for reinforcement\nlearning (RL) agents in generating airfoil geometry when compared to\nconventional airfoil parameterization methods, implying the broader potential\nof DbM in machine learning-driven design.", "AI": {"tldr": "AirDbM is a Design-by-Morphing approach that uses only 12 optimal baseline airfoils to reconstruct 99% of the UIUC database with high accuracy, outperforming previous methods in aerodynamic optimization and showing strong potential for machine learning applications.", "motivation": "To develop an efficient airfoil optimization method that explores diverse designs using minimal design variables while maintaining high reconstruction accuracy.", "method": "AirDbM selects an optimal set of 12 baseline airfoils from the UIUC database by sequentially adding baselines that maximize design capacity, then uses these for airfoil reconstruction and optimization.", "result": "Reconstructs 99% of the database with mean absolute error below 0.005, achieves better Pareto front with higher hypervolume in multi-objective optimization, and shows superior adaptability for reinforcement learning agents compared to conventional methods.", "conclusion": "AirDbM effectively reduces design-space dimensionality while maintaining performance, demonstrating the broader potential of Design-by-Morphing approaches in machine learning-driven design applications."}}
{"id": "2510.16511", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16511", "abs": "https://arxiv.org/abs/2510.16511", "authors": ["Dongchan Cho", "Jiho Han", "Keumyeong Kang", "Minsang Kim", "Honggyu Ryu", "Namsoon Jung"], "title": "Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Real-world multivariate time series anomalies are rare and often unlabeled.\nAdditionally, prevailing methods rely on increasingly complex architectures\ntuned to benchmarks, detecting only fragments of anomalous segments and\noverstating performance. In this paper, we introduce OracleAD, a simple and\ninterpretable unsupervised framework for multivariate time series anomaly\ndetection. OracleAD encodes each variable's past sequence into a single causal\nembedding to jointly predict the present time point and reconstruct the input\nwindow, effectively modeling temporal dynamics. These embeddings then undergo a\nself-attention mechanism to project them into a shared latent space and capture\nspatial relationships. These relationships are not static, since they are\nmodeled by a property that emerges from each variable's temporal dynamics. The\nprojected embeddings are aligned to a Stable Latent Structure (SLS)\nrepresenting normal-state relationships. Anomalies are identified using a dual\nscoring mechanism based on prediction error and deviation from the SLS,\nenabling fine-grained anomaly diagnosis at each time point and across\nindividual variables. Since any noticeable SLS deviation originates from\nembeddings that violate the learned temporal causality of normal data, OracleAD\ndirectly pinpoints the root-cause variables at the embedding level. OracleAD\nachieves state-of-the-art results across multiple real-world datasets and\nevaluation protocols, while remaining interpretable through SLS.", "AI": {"tldr": "OracleAD is a simple, interpretable unsupervised framework for multivariate time series anomaly detection that models temporal dynamics through causal embeddings and captures spatial relationships via self-attention, identifying anomalies through prediction error and deviation from a stable latent structure.", "motivation": "Real-world multivariate time series anomalies are rare and often unlabeled, while existing methods use complex architectures that detect only fragments of anomalies and overstate performance.", "method": "Encodes each variable's past sequence into causal embeddings for joint prediction and reconstruction, uses self-attention to project embeddings into shared latent space capturing spatial relationships, aligns embeddings to a Stable Latent Structure (SLS), and employs dual scoring based on prediction error and SLS deviation.", "result": "Achieves state-of-the-art results across multiple real-world datasets and evaluation protocols while maintaining interpretability.", "conclusion": "OracleAD provides an effective, interpretable solution for multivariate time series anomaly detection that directly pinpoints root-cause variables through temporal causality violations."}}
{"id": "2510.16021", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.16021", "abs": "https://arxiv.org/abs/2510.16021", "authors": ["Arega Getaneh Abate", "Xiufeng Liu", "Ruyu Liu", "Xiaobing Zhang"], "title": "Feature-driven reinforcement learning for photovoltaic in continuous intraday trading", "comment": null, "summary": "Photovoltaic (PV) operators face substantial uncertainty in generation and\nshort-term electricity prices. Continuous intraday markets enable producers to\nadjust their positions in real time, potentially improving revenues and\nreducing imbalance costs. We propose a feature-driven reinforcement learning\n(RL) approach for PV intraday trading that integrates data-driven features into\nthe state and learns bidding policies in a sequential decision framework. The\nproblem is cast as a Markov Decision Process with a reward that balances\ntrading profit and imbalance penalties and is solved with Proximal Policy\nOptimization (PPO) using a predominantly linear, interpretable policy. Trained\non historical market data and evaluated out-of-sample, the strategy\nconsistently outperforms benchmark baselines across diverse scenarios.\nExtensive validation shows rapid convergence, real-time inference, and\ntransparent decision rules. Learned weights highlight the central role of\nmarket microstructure and historical features. Taken together, these results\nindicate that feature-driven RL offers a practical, data-efficient, and\noperationally deployable pathway for active intraday participation by PV\nproducers.", "AI": {"tldr": "A reinforcement learning approach using Proximal Policy Optimization (PPO) for PV intraday trading that balances trading profit with imbalance penalties, outperforming benchmarks across various scenarios.", "motivation": "PV operators face uncertainty in generation and electricity prices, needing real-time position adjustments in continuous intraday markets to improve revenues and reduce imbalance costs.", "method": "Feature-driven RL approach integrating data-driven features into state representation, formulated as Markov Decision Process with PPO algorithm using interpretable linear policies.", "result": "Strategy consistently outperforms benchmark baselines across diverse scenarios, shows rapid convergence, real-time inference, and transparent decision rules with learned weights highlighting market microstructure importance.", "conclusion": "Feature-driven RL offers practical, data-efficient, and operationally deployable pathway for active intraday participation by PV producers."}}
{"id": "2510.16513", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16513", "abs": "https://arxiv.org/abs/2510.16513", "authors": ["Dhruv Gupta", "Aditya Nagarsekar", "Vraj Shah", "Sujith Thomas"], "title": "eDCF: Estimating Intrinsic Dimension using Local Connectivity", "comment": "58 pages (35 (main) + 23 (appendix)), 54 figures (27 (main) + 27\n  (appendix))", "summary": "Modern datasets often contain high-dimensional features exhibiting complex\ndependencies. To effectively analyze such data, dimensionality reduction\nmethods rely on estimating the dataset's intrinsic dimension (id) as a measure\nof its underlying complexity. However, estimating id is challenging due to its\ndependence on scale: at very fine scales, noise inflates id estimates, while at\ncoarser scales, estimates stabilize to lower, scale-invariant values. This\npaper introduces a novel, scalable, and parallelizable method called eDCF,\nwhich is based on Connectivity Factor (CF), a local connectivity-based metric,\nto robustly estimate intrinsic dimension across varying scales. Our method\nconsistently matches leading estimators, achieving comparable values of mean\nabsolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our\napproach also attains higher exact intrinsic dimension match rates, reaching up\nto 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling\nunder medium to high noise levels and large datasets. Further, we showcase our\nmethod's ability to accurately detect fractal geometries in decision\nboundaries, confirming its utility for analyzing realistic, structured data.", "AI": {"tldr": "The paper introduces eDCF, a scalable and parallelizable method for robust intrinsic dimension estimation across scales using Connectivity Factor, achieving competitive performance with higher exact match rates under noise.", "motivation": "Modern datasets with high-dimensional features and complex dependencies require accurate intrinsic dimension estimation, but current methods struggle with scale-dependent noise effects where fine scales inflate estimates and coarse scales provide stable but potentially underestimated values.", "method": "Proposes eDCF method based on Connectivity Factor (CF), a local connectivity-based metric, designed to be scalable and parallelizable for robust intrinsic dimension estimation across varying scales.", "result": "eDCF consistently matches leading estimators with comparable MAE on synthetic benchmarks, achieves higher exact intrinsic dimension match rates (25.0% vs 16.7% for MLE and 12.5% for TWO-NN), excels under medium to high noise levels and large datasets, and accurately detects fractal geometries in decision boundaries.", "conclusion": "The eDCF method provides robust intrinsic dimension estimation across scales, demonstrating superior performance particularly in noisy and large-scale scenarios, and proves effective for analyzing realistic structured data with complex geometries."}}
{"id": "2510.16022", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16022", "abs": "https://arxiv.org/abs/2510.16022", "authors": ["Changsheng Wang", "Xin Chen", "Sijia Liu", "Ke Ding"], "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization", "comment": null, "summary": "Adapting pretrained large language models (LLMs) to code domains via\nsupervised fine-tuning (FT) has been commonly used for code generation.\nHowever, we identify a previously underappreciated failure mode, the\nmemorization barrier, where strong memorization of downstream code data in the\nbase model could trap optimization and prevent the standard FT from effectively\nacquiring new, generalizable code knowledge. To overcome this barrier, we\npropose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which\napplies an IB penalty on hidden representations of the code data to compress\nspurious, memorized features while preserving task-relevant information.\nExtensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)\nshow that IB-FT substantially alleviates the memorization barrier, improves\ntop-1 performance (Pass@$1$), and yields far more stable gains under the\nstricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if\nat least $m$ of $k$ samples pass unit tests) compared with conventional FT.", "AI": {"tldr": "The paper identifies a memorization barrier in fine-tuning LLMs for code generation, where strong memorization prevents learning new generalizable knowledge. It proposes IB-FT, an information bottleneck-guided approach that compresses memorized features while preserving task-relevant information.", "motivation": "Standard supervised fine-tuning of LLMs for code generation suffers from a memorization barrier where the model's strong memorization of downstream code data prevents effective acquisition of new, generalizable code knowledge.", "method": "Proposes IB-FT (information bottleneck-guided fine-tuning) that applies an IB penalty on hidden representations of code data to compress spurious, memorized features while preserving task-relevant information.", "result": "IB-FT substantially alleviates the memorization barrier, improves top-1 performance (Pass@1), and yields more stable gains under stricter multi-sample metrics (Pass@k^(m)) compared to conventional fine-tuning.", "conclusion": "The information bottleneck approach effectively overcomes the memorization barrier in code generation fine-tuning, leading to better performance and more stable improvements across different evaluation metrics."}}
{"id": "2510.16530", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16530", "abs": "https://arxiv.org/abs/2510.16530", "authors": ["Ashutosh Srivastava", "Lokesh Nagalapatti", "Gautam Jajoo", "Aniket Vashishtha", "Parameswari Krishnamurthy", "Amit Sharma"], "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks", "comment": null, "summary": "Recent claims of strong performance by Large Language Models (LLMs) on causal\ndiscovery are undermined by a key flaw: many evaluations rely on benchmarks\nlikely included in pretraining corpora. Thus, apparent success suggests that\nLLM-only methods, which ignore observational data, outperform classical\nstatistical approaches. We challenge this narrative by asking: Do LLMs truly\nreason about causal structure, and how can we measure it without memorization\nconcerns? Can they be trusted for real-world scientific discovery? We argue\nthat realizing LLMs' potential for causal analysis requires two shifts: (P.1)\ndeveloping robust evaluation protocols based on recent scientific studies to\nguard against dataset leakage, and (P.2) designing hybrid methods that combine\nLLM-derived knowledge with data-driven statistics. To address P.1, we encourage\nevaluating discovery methods on novel, real-world scientific studies. We\noutline a practical recipe for extracting causal graphs from recent\npublications released after an LLM's training cutoff, ensuring relevance and\npreventing memorization while capturing both established and novel relations.\nCompared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,\nthey perform far worse on our curated graphs, underscoring the need for\nstatistical grounding. Supporting P.2, we show that using LLM predictions as\npriors for the classical PC algorithm significantly improves accuracy over both\nLLM-only and purely statistical methods. We call on the community to adopt\nscience-grounded, leakage-resistant benchmarks and invest in hybrid causal\ndiscovery methods suited to real-world inquiry.", "AI": {"tldr": "LLMs' claimed causal discovery performance is inflated by benchmark memorization. The paper proposes using recent scientific studies for evaluation and shows hybrid methods combining LLM priors with statistical algorithms outperform pure approaches.", "motivation": "To challenge the narrative that LLM-only methods outperform classical statistical approaches in causal discovery, and to address concerns about memorization in evaluations.", "method": "Develops robust evaluation protocols using recent scientific studies released after LLM training cutoffs, and designs hybrid methods that combine LLM-derived knowledge with data-driven statistics like the PC algorithm.", "result": "LLMs perform far worse on curated graphs from recent publications compared to standard benchmarks like BNLearn. Using LLM predictions as priors for the PC algorithm significantly improves accuracy over both LLM-only and purely statistical methods.", "conclusion": "The community should adopt science-grounded, leakage-resistant benchmarks and invest in hybrid causal discovery methods that combine LLM knowledge with statistical approaches for real-world scientific inquiry."}}
{"id": "2510.16023", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.16023", "abs": "https://arxiv.org/abs/2510.16023", "authors": ["Fanmeng Wang", "Shan Mei", "Wentao Guo", "Hongshuai Wang", "Qi Ou", "Zhifeng Gao", "Hongteng Xu"], "title": "Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model", "comment": null, "summary": "Polymers, macromolecules formed from covalently bonded monomers, underpin\ncountless technologies and are indispensable to modern life. While deep\nlearning is advancing polymer science, existing methods typically represent the\nwhole polymer solely through monomer-level descriptors, overlooking the global\nstructural information inherent in polymer conformations, which ultimately\nlimits their practical performance. Moreover, this field still lacks a\nuniversal foundation model that can effectively support diverse downstream\ntasks, thereby severely constraining progress. To address these challenges, we\nintroduce PolyConFM, the first polymer foundation model that unifies polymer\nmodeling and design through conformation-centric generative pretraining.\nRecognizing that each polymer conformation can be decomposed into a sequence of\nlocal conformations (i.e., those of its repeating units), we pretrain PolyConFM\nunder the conditional generation paradigm, reconstructing these local\nconformations via masked autoregressive (MAR) modeling and further generating\ntheir orientation transformations to recover the corresponding polymer\nconformation. Besides, we construct the first high-quality polymer conformation\ndataset via molecular dynamics simulations to mitigate data sparsity, thereby\nenabling conformation-centric pretraining. Experiments demonstrate that\nPolyConFM consistently outperforms representative task-specific methods on\ndiverse downstream tasks, equipping polymer science with a universal and\npowerful tool.", "AI": {"tldr": "PolyConFM is the first polymer foundation model that uses conformation-centric generative pretraining to address limitations of existing methods that overlook global structural information in polymers. It decomposes polymer conformations into local repeating units and reconstructs them via masked autoregressive modeling.", "motivation": "Existing deep learning methods for polymers only use monomer-level descriptors and ignore global structural information from polymer conformations, limiting practical performance. The field lacks a universal foundation model for diverse downstream tasks.", "method": "PolyConFM uses conformation-centric generative pretraining by decomposing polymer conformations into sequences of local conformations of repeating units. It employs masked autoregressive modeling to reconstruct local conformations and generate orientation transformations to recover full polymer conformations. A high-quality polymer conformation dataset was created via molecular dynamics simulations.", "result": "Experiments show that PolyConFM consistently outperforms representative task-specific methods across diverse downstream tasks, providing a universal and powerful tool for polymer science.", "conclusion": "PolyConFM successfully addresses the limitations of existing polymer modeling methods by incorporating global structural information through conformation-centric pretraining, establishing the first universal foundation model for polymer science that effectively supports multiple downstream applications."}}
{"id": "2510.16591", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16591", "abs": "https://arxiv.org/abs/2510.16591", "authors": ["Cassidy Ashworth", "Pietro Li\u00f2", "Francesco Caso"], "title": "Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations", "comment": null, "summary": "Deep learning models have proven enormously successful at using multiple\nlayers of representation to learn relevant features of structured data.\nEncoding physical symmetries into these models can improve performance on\ndifficult tasks, and recent work has motivated the principle of parameter\nsymmetry breaking and restoration as a unifying mechanism underlying their\nhierarchical learning dynamics. We evaluate the role of parameter symmetry and\nnetwork expressivity in the generalisation behaviour of neural networks when\nlearning a real-space renormalisation group (RG) transformation, using the\ncentral limit theorem (CLT) as a test case map. We consider simple multilayer\nperceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries\nand activation functions across architectures. Our results reveal a competition\nbetween symmetry constraints and expressivity, with overly complex or\noverconstrained models generalising poorly. We analytically demonstrate this\npoor generalisation behaviour for certain constrained MLP architectures by\nrecasting the CLT as a cumulant recursion relation and making use of an\nestablished framework to propagate cumulants through MLPs. We also empirically\nvalidate an extension of this framework from MLPs to GNNs, elucidating the\ninternal information processing performed by these more complex models. These\nfindings offer new insight into the learning dynamics of symmetric networks and\ntheir limitations in modelling structured physical transformations.", "AI": {"tldr": "The paper investigates how parameter symmetry and network expressivity affect neural network generalization when learning real-space renormalization group transformations, using the central limit theorem as a test case.", "motivation": "To understand the role of physical symmetries in deep learning models and evaluate the principle of parameter symmetry breaking and restoration in hierarchical learning dynamics.", "method": "Used multilayer perceptrons (MLPs) and graph neural networks (GNNs) with varied weight symmetries and activation functions, analyzing generalization behavior through cumulant recursion relations and established frameworks for propagating cumulants through neural networks.", "result": "Revealed competition between symmetry constraints and expressivity, with overly complex or overconstrained models generalizing poorly. Demonstrated poor generalization analytically for constrained MLPs and empirically validated extension to GNNs.", "conclusion": "Findings provide new insights into learning dynamics of symmetric networks and their limitations in modeling structured physical transformations, highlighting the delicate balance between symmetry and expressivity."}}
{"id": "2510.16026", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16026", "abs": "https://arxiv.org/abs/2510.16026", "authors": ["Marco Barbero-Mota", "Eric V. Strobl", "John M. Still", "William W. Stead", "Thomas A. Lasko"], "title": "A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data", "comment": null, "summary": "We provide an accessible description of a peer-reviewed generalizable causal\nmachine learning pipeline to (i) discover latent causal sources of large-scale\nelectronic health records observations, and (ii) quantify the source causal\neffects on clinical outcomes. We illustrate how imperfect multimodal clinical\ndata can be processed, decomposed into probabilistic independent latent\nsources, and used to train taskspecific causal models from which individual\ncausal effects can be estimated. We summarize the findings of the two\nreal-world applications of the approach to date as a demonstration of its\nversatility and utility for medical discovery at scale.", "AI": {"tldr": "A causal machine learning pipeline for discovering latent causal sources in electronic health records and quantifying their effects on clinical outcomes using imperfect multimodal data.", "motivation": "To develop an accessible method for discovering latent causal factors from large-scale electronic health records and measuring their impact on clinical outcomes, addressing the challenge of imperfect multimodal clinical data.", "method": "Process imperfect multimodal clinical data, decompose it into probabilistic independent latent sources, and train task-specific causal models to estimate individual causal effects.", "result": "The approach has been successfully applied in two real-world applications, demonstrating its versatility and utility for medical discovery at scale.", "conclusion": "The proposed pipeline provides a generalizable framework for causal discovery and effect estimation from electronic health records, showing practical value in medical research applications."}}
{"id": "2510.16974", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16974", "abs": "https://arxiv.org/abs/2510.16974", "authors": ["Shurong Lin", "Aleksandra Slavkovi\u0107", "Deekshith Reddy Bhoomireddy"], "title": "Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees", "comment": null, "summary": "In social sciences, small- to medium-scale datasets are common and linear\nregression (LR) is canonical. In privacy-aware settings, much work has focused\non differentially private (DP) LR, but mostly on point estimation with limited\nattention to uncertainty quantification. Meanwhile, synthetic data generation\n(SDG) is increasingly important for reproducibility studies, yet current DP LR\nmethods do not readily support it. Mainstream SDG approaches are either\ntailored to discretized data, making them less suitable for continuous\nregression, or rely on deep models that require large datasets, limiting their\nuse for the smaller, continuous data typical in social science. We propose a\nmethod for LR with valid inference under Gaussian DP: a DP bias-corrected\nestimator with asymptotic confidence intervals (CIs) and a general SDG\nprocedure in which regression on the synthetic data matches our DP regression.\nOur binning-aggregation strategy is effective in small- to moderate-dimensional\nsettings. Experiments show our method (1) improves accuracy over existing\nmethods, (2) provides valid CIs, and (3) produces more reliable synthetic data\nfor downstream ML tasks than current DP SDGs.", "AI": {"tldr": "A method for linear regression with valid inference under Gaussian differential privacy, providing bias-corrected estimators with confidence intervals and synthetic data generation that matches DP regression results.", "motivation": "Address the gap in differentially private linear regression methods that lack uncertainty quantification and synthetic data generation capabilities, particularly for small-to-medium continuous datasets common in social sciences.", "method": "Proposes a DP bias-corrected estimator with asymptotic confidence intervals and a synthetic data generation procedure where regression on synthetic data matches DP regression results, using binning-aggregation strategy for small-to-moderate dimensions.", "result": "Experiments show improved accuracy over existing methods, valid confidence intervals, and more reliable synthetic data for downstream machine learning tasks compared to current DP synthetic data generation approaches.", "conclusion": "The proposed method effectively addresses limitations in current DP linear regression by providing valid inference and synthetic data generation capabilities suitable for small-to-medium continuous datasets in social sciences."}}
{"id": "2510.16035", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16035", "abs": "https://arxiv.org/abs/2510.16035", "authors": ["Yingguang Yang", "Xianghua Zeng", "Qi Wu", "Hao Peng", "Yutong Xia", "Hao Liu", "Bin Chong", "Philip S. Yu"], "title": "RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction", "comment": "27 pages, 10 figures", "summary": "Social networks have become a crucial source of real-time information for\nindividuals. The influence of social bots within these platforms has garnered\nconsiderable attention from researchers, leading to the development of numerous\ndetection technologies. However, the vulnerability and robustness of these\ndetection methods is still underexplored. Existing Graph Neural Network\n(GNN)-based methods cannot be directly applied due to the issues of limited\ncontrol over social agents, the black-box nature of bot detectors, and the\nheterogeneity of bots. To address these challenges, this paper proposes the\nfirst adversarial multi-agent Reinforcement learning framework for social Bot\ncontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.\nSpecifically, we use a diffusion model to generate high-fidelity bot accounts\nby reconstructing existing account data with minor modifications, thereby\nevading detection on social platforms. To the best of our knowledge, this is\nthe first application of diffusion models to mimic the behavior of evolving\nsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning\n(MARL) method to simulate bots adversarial behavior. We categorize social\naccounts based on their influence and budget. Different agents are then\nemployed to control bot accounts across various categories, optimizing the\nattachment strategy through reinforcement learning. Additionally, a\nhierarchical state abstraction based on structural entropy is designed to\naccelerate the reinforcement learning. Extensive experiments on social bot\ndetection datasets demonstrate that our framework can effectively undermine the\nperformance of GNN-based detectors.", "AI": {"tldr": "This paper proposes RoBCtrl, the first adversarial multi-agent reinforcement learning framework for social bot control attacks targeting GNN-based bot detectors, using diffusion models to generate realistic bot accounts and MARL to optimize attack strategies.", "motivation": "The vulnerability and robustness of existing social bot detection methods are underexplored, and current GNN-based approaches face challenges with limited control over social agents, black-box nature of detectors, and bot heterogeneity.", "method": "Uses diffusion models to generate high-fidelity bot accounts by reconstructing existing data with minor modifications, then employs Multi-Agent Reinforcement Learning to simulate adversarial bot behavior across different account categories based on influence and budget, with hierarchical state abstraction using structural entropy.", "result": "Extensive experiments on social bot detection datasets show the framework effectively undermines the performance of GNN-based detectors.", "conclusion": "The proposed RoBCtrl framework successfully demonstrates how adversarial attacks can compromise GNN-based social bot detection systems through realistic bot generation and strategic multi-agent control."}}
{"id": "2510.17085", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17085", "abs": "https://arxiv.org/abs/2510.17085", "authors": ["Yiling Chen", "Shi Feng", "Paul Kattuman", "Fang-Yi Yu"], "title": "Data Reliability Scoring", "comment": "39 pages, 5 figures", "summary": "How can we assess the reliability of a dataset without access to ground\ntruth? We introduce the problem of reliability scoring for datasets collected\nfrom potentially strategic sources. The true data are unobserved, but we see\noutcomes of an unknown statistical experiment that depends on them. To\nbenchmark reliability, we define ground-truth-based orderings that capture how\nmuch reported data deviate from the truth. We then propose the Gram determinant\nscore, which measures the volume spanned by vectors describing the empirical\ndistribution of the observed data and experiment outcomes. We show that this\nscore preserves several ground-truth based reliability orderings and, uniquely\nup to scaling, yields the same reliability ranking of datasets regardless of\nthe experiment -- a property we term experiment agnosticism. Experiments on\nsynthetic noise models, CIFAR-10 embeddings, and real employment data\ndemonstrate that the Gram determinant score effectively captures data quality\nacross diverse observation processes.", "AI": {"tldr": "The paper introduces a method to assess dataset reliability without ground truth by measuring how much reported data deviate from truth through the Gram determinant score, which is experiment-agnostic and preserves reliability orderings.", "motivation": "To evaluate dataset reliability when ground truth is unavailable, especially for data from potentially strategic sources where true data are unobserved but outcomes of unknown statistical experiments are visible.", "method": "Proposes the Gram determinant score that measures the volume spanned by vectors describing the empirical distribution of observed data and experiment outcomes, preserving ground-truth-based reliability orderings.", "result": "The Gram determinant score uniquely preserves reliability rankings regardless of the experiment (experiment agnosticism) and effectively captures data quality across synthetic noise models, CIFAR-10 embeddings, and real employment data.", "conclusion": "The Gram determinant score provides a robust method for assessing dataset reliability without ground truth access, maintaining consistent rankings across different observation processes and experimental conditions."}}
{"id": "2510.16039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16039", "abs": "https://arxiv.org/abs/2510.16039", "authors": ["Xiangyuan Peng", "Xingsi Dong", "Si Wu"], "title": "Vector Quantization in the Brain: Grid-like Codes in World Models", "comment": null, "summary": "We propose Grid-like Code Quantization (GCQ), a brain-inspired method for\ncompressing observation-action sequences into discrete representations using\ngrid-like patterns in attractor dynamics. Unlike conventional vector\nquantization approaches that operate on static inputs, GCQ performs\nspatiotemporal compression through an action-conditioned codebook, where\ncodewords are derived from continuous attractor neural networks and dynamically\nselected based on actions. This enables GCQ to jointly compress space and time,\nserving as a unified world model. The resulting representation supports\nlong-horizon prediction, goal-directed planning, and inverse modeling.\nExperiments across diverse tasks demonstrate GCQ's effectiveness in compact\nencoding and downstream performance. Our work offers both a computational tool\nfor efficient sequence modeling and a theoretical perspective on the formation\nof grid-like codes in neural systems.", "AI": {"tldr": "GCQ is a brain-inspired method that compresses observation-action sequences into discrete representations using grid-like patterns in attractor dynamics, enabling joint spatiotemporal compression.", "motivation": "To develop a more efficient sequence modeling approach that can jointly compress space and time, inspired by grid-like patterns in neural systems, unlike conventional vector quantization methods that handle static inputs.", "method": "Uses action-conditioned codebook with codewords derived from continuous attractor neural networks, dynamically selecting codewords based on actions to perform spatiotemporal compression.", "result": "GCQ effectively compresses observation-action sequences into compact representations that support long-horizon prediction, goal-directed planning, and inverse modeling across diverse tasks.", "conclusion": "GCQ provides both a computational tool for efficient sequence modeling and theoretical insights into grid-like code formation in neural systems, demonstrating superior performance in compact encoding and downstream applications."}}
{"id": "2510.17103", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17103", "abs": "https://arxiv.org/abs/2510.17103", "authors": ["Shinji Ito", "Kevin Jamieson", "Haipeng Luo", "Arnab Maiti", "Taira Tsuchiya"], "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback", "comment": "49 pages", "summary": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback.", "AI": {"tldr": "First BOBW algorithms for episodic tabular MDPs with aggregate bandit feedback, achieving O(log T) regret in stochastic and O(\u221aT) in adversarial settings, with matching lower bounds.", "motivation": "Prior work focused only on worst-case analysis; this paper initiates study of best-of-both-worlds algorithms that perform well in both stochastic and adversarial environments under challenging aggregate bandit feedback.", "method": "Combination of FTRL over occupancy measures, self-bounding techniques, and new loss estimators inspired by advances in online shortest path problems; extended to unknown-transition settings using confidence-based techniques.", "result": "Algorithms achieve O(log T) regret in stochastic settings and O(\u221aT) regret in adversarial settings with known transitions, with matching lower bounds proving optimality; also provides first individual-gap-dependent lower bounds.", "conclusion": "The paper successfully develops the first BOBW algorithms for episodic MDPs with aggregate bandit feedback, achieving near-optimal performance in both stochastic and adversarial environments with theoretical guarantees."}}
{"id": "2510.16045", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16045", "abs": "https://arxiv.org/abs/2510.16045", "authors": ["Mengtao Lv", "Ruiqi Zhu", "Xinyu Wang", "Yun Li"], "title": "AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization", "comment": "12 pages, 6 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious kinds of tasks, while the billion or even trillion parameters bring\nstorage and efficiency bottlenecks for inference. Quantization, particularly\nfloating-point quantization, is known to be capable of speeding up LLM\ninference by reducing memory footprint and data movement during the inference\nprocess. For the first time, we advance the floating-point quantization\nexploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,\nto further approach the quantization sweet spot. AMS-Quant incorporates two\nnovel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,\nwhich groups k quantized weights and lets them share the least significant\nmantissa bit, allowing us to further approach the minimum quantization\nbit-width without accuracy loss. (2) It introduces Adaptive Searching, which\nemploys an offline optimization strategy to minimize the accuracy degradation\nintroduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA\nLinear kernels, which translates memory savings into wall-clock latency\nreduction by reducing memory access. Extensive experiments on large-scale\ndatasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3\nand FP4.25-e2m2, and significantly speed up the LLM decoding over FP16\ninference (2.8x and 3.2x), with negligible accuracy loss.", "AI": {"tldr": "AMS-Quant introduces non-integer bit-width floating-point quantization for LLMs, using mantissa-bit sharing and adaptive searching to achieve FP5.33 and FP4.25 formats, delivering 2.8-3.2x speedup over FP16 with minimal accuracy loss.", "motivation": "Large language models have massive parameters that create storage and efficiency bottlenecks for inference. While floating-point quantization helps reduce memory footprint and improve inference speed, existing approaches only use integer bit-widths, leaving potential optimization opportunities unexplored.", "method": "AMS-Quant uses two key techniques: (1) Mantissa-bit Sharing - groups of k quantized weights share the least significant mantissa bit to approach minimum quantization bit-width without accuracy loss; (2) Adaptive Searching - offline optimization strategy to minimize accuracy degradation from sharing. The method is implemented as efficient CUDA Linear kernels to translate memory savings into actual latency reduction.", "result": "The method successfully quantizes models to FP-5.33-e2m3 and FP4.25-e2m2 formats, achieving significant speedup over FP16 inference (2.8x and 3.2x respectively) with negligible accuracy loss on large-scale datasets and models.", "conclusion": "AMS-Quant demonstrates that non-integer bit-width floating-point quantization is feasible and effective, providing substantial inference speed improvements for LLMs while maintaining model accuracy through innovative mantissa-bit sharing and adaptive optimization techniques."}}
{"id": "2510.17120", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17120", "abs": "https://arxiv.org/abs/2510.17120", "authors": ["Rishi Sonthalia", "Raj Rao Nadakuditi"], "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation", "comment": null, "summary": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems.", "AI": {"tldr": "The paper introduces a novel regularization method for autoencoders using matricial free energy, which creates Gaussian-like codes that generalize well across datasets and can be applied to underdetermined inverse problems.", "motivation": "To develop a regularization scheme that ensures autoencoder codes have Gaussian-like properties and generalize effectively across training and test datasets, particularly for applications in underdetermined inverse problems.", "method": "The method uses matricial free energy as a differentiable loss function based on the singular values of the code matrix. It minimizes negative matricial free energy through stochastic gradient-based training to achieve codes with singular value distributions matching those of Gaussian random matrices.", "result": "Empirical simulations show that minimizing the negative matricial free energy produces Gaussian-like codes that generalize well across training and test sets. The approach also enables reliable production of Gaussian codes through matricidal free energy maximizing autoencoders.", "conclusion": "The matricial free energy regularization successfully creates autoencoders with Gaussian-like codes that generalize effectively, providing a valuable tool for applications in underdetermined inverse problems where such properties are beneficial."}}
{"id": "2510.16051", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16051", "abs": "https://arxiv.org/abs/2510.16051", "authors": ["Sofiya Garkot", "Maksym Shamrai", "Ivan Synytsia", "Mariya Hirna"], "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration", "comment": "22 pages", "summary": "Autonomous agents capable of operating complex graphical user interfaces\n(GUIs) have the potential to transform desktop automation. While recent\nadvances in large language models (LLMs) have significantly improved UI\nunderstanding, navigating full-window, multi-application desktop environments\nremains a major challenge. Data availability is limited by costly manual\nannotation, closed-source datasets and surface-level synthetic pipelines. We\nintroduce GUIrilla, an automated scalable framework that systematically\nexplores applications via native accessibility APIs to address the critical\ndata collection challenge in GUI automation. Our framework focuses on macOS -\nan ecosystem with limited representation in current UI datasets - though many\nof its components are designed for broader cross-platform applicability.\nGUIrilla organizes discovered interface elements and crawler actions into\nhierarchical GUI graphs and employs specialized interaction handlers to achieve\ncomprehensive application coverage. Using the application graphs from GUIrilla\ncrawler, we construct and release GUIrilla-Task, a large-scale dataset of\n27,171 functionally grounded tasks across 1,108 macOS applications, each\nannotated with full-desktop and window-level screenshots, accessibility\nmetadata, and semantic action traces. Empirical results show that tuning\nLLM-based agents on GUIrilla-Task significantly improves performance on\ndownstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro\nbenchmark while using 97% less data. We also release macapptree, an open-source\nlibrary for reproducible collection of structured accessibility metadata, along\nwith the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold\nbenchmark, and the framework code to support open research in desktop autonomy.", "AI": {"tldr": "GUIrilla is an automated framework for collecting GUI interaction data on macOS using accessibility APIs, creating a large-scale dataset (GUIrilla-Task) that improves LLM-based UI automation performance with significantly less data.", "motivation": "Address the data scarcity challenge in GUI automation by providing scalable data collection for desktop environments, particularly macOS which is underrepresented in current UI datasets.", "method": "Systematically explores applications via native accessibility APIs, organizes interface elements into hierarchical GUI graphs, and uses specialized interaction handlers for comprehensive coverage.", "result": "Created GUIrilla-Task dataset with 27,171 tasks across 1,108 macOS apps, showing that LLMs tuned on this data outperform synthetic baselines on ScreenSpot Pro benchmark using 97% less data.", "conclusion": "GUIrilla provides an effective solution for scalable GUI data collection, enabling improved desktop automation through accessible, reproducible frameworks and datasets."}}
{"id": "2510.16053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16053", "abs": "https://arxiv.org/abs/2510.16053", "authors": ["Chenyang Yu", "Xinpeng Xie", "Yan Huang", "Chenxi Qiu"], "title": "FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting", "comment": null, "summary": "Accurate traffic forecasting is a core technology for building Intelligent\nTransportation Systems (ITS), enabling better urban resource allocation and\nimproved travel experiences. With growing urbanization, traffic congestion has\nintensified, highlighting the need for reliable and responsive forecasting\nmodels. In recent years, deep learning, particularly Graph Neural Networks\n(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can\neffectively capture complex spatial dependencies in road network topology and\ndynamic temporal evolution patterns in traffic flow data. Foundational models\nsuch as STGCN and GraphWaveNet, along with more recent developments including\nSTWave and D2STGNN, have achieved impressive performance on standard traffic\ndatasets. These approaches incorporate sophisticated graph convolutional\nstructures and temporal modeling mechanisms, demonstrating particular\neffectiveness in capturing and forecasting traffic patterns characterized by\nperiodic regularities. To address this challenge, researchers have explored\nvarious ways to incorporate event information. Early attempts primarily relied\non manually engineered event features. For instance, some approaches introduced\nmanually defined incident effect scores or constructed specific subgraphs for\ndifferent event-induced traffic conditions. While these methods somewhat\nenhance responsiveness to specific events, their core drawback lies in a heavy\nreliance on domain experts' prior knowledge, making generalization to diverse\nand complex unknown events difficult, and low-dimensional manual features often\nlead to the loss of rich semantic details.", "AI": {"tldr": "This paper discusses traffic forecasting using Graph Neural Networks (GNNs) and highlights the limitations of current approaches that rely on manually engineered event features, which struggle with generalization to diverse unknown events.", "motivation": "Accurate traffic forecasting is crucial for Intelligent Transportation Systems to improve urban resource allocation and travel experiences. With increasing urbanization and traffic congestion, there's a need for reliable forecasting models that can handle complex spatial and temporal dependencies in traffic data.", "method": "The paper reviews current deep learning approaches, particularly GNNs like STGCN, GraphWaveNet, STWave, and D2STGNN, which capture spatial dependencies in road networks and temporal evolution patterns. However, current methods rely heavily on manually engineered event features and domain expert knowledge.", "result": "Existing GNN-based approaches have achieved impressive performance on standard traffic datasets and are effective at capturing periodic traffic patterns, but they struggle with generalization to diverse and complex unknown events due to their reliance on manual feature engineering.", "conclusion": "Current traffic forecasting methods using GNNs are effective for regular patterns but limited by their dependence on manually engineered event features, which hinders their ability to generalize to unknown events and results in loss of semantic details."}}
{"id": "2510.17266", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17266", "abs": "https://arxiv.org/abs/2510.17266", "authors": ["Jiayu Bai", "Zhanbo Feng", "Zhijie Deng", "Tianqi Hou", "Robert C. Qiu", "Zenan Ling"], "title": "Adaptive Discretization for Consistency Models", "comment": "Accepted by NeurIPS 2025", "summary": "Consistency Models (CMs) have shown promise for efficient one-step\ngeneration. However, most existing CMs rely on manually designed discretization\nschemes, which can cause repeated adjustments for different noise schedules and\ndatasets. To address this, we propose a unified framework for the automatic and\nadaptive discretization of CMs, formulating it as an optimization problem with\nrespect to the discretization step. Concretely, during the consistency training\nprocess, we propose using local consistency as the optimization objective to\nensure trainability by avoiding excessive discretization, and taking global\nconsistency as a constraint to ensure stability by controlling the denoising\nerror in the training target. We establish the trade-off between local and\nglobal consistency with a Lagrange multiplier. Building on this framework, we\nachieve adaptive discretization for CMs using the Gauss-Newton method. We refer\nto our approach as ADCMs. Experiments demonstrate that ADCMs significantly\nimprove the training efficiency of CMs, achieving superior generative\nperformance with minimal training overhead on both CIFAR-10 and ImageNet.\nMoreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code\nis available at https://github.com/rainstonee/ADCM.", "AI": {"tldr": "ADCMs is a unified framework for automatic and adaptive discretization of Consistency Models, using local consistency as optimization objective and global consistency as constraint to improve training efficiency and generative performance.", "motivation": "Existing Consistency Models rely on manually designed discretization schemes that require repeated adjustments for different noise schedules and datasets, which is inefficient and suboptimal.", "method": "Formulate discretization as an optimization problem using local consistency as objective and global consistency as constraint with Lagrange multiplier. Achieve adaptive discretization using Gauss-Newton method.", "result": "ADCMs significantly improve training efficiency of CMs, achieving superior generative performance with minimal training overhead on CIFAR-10 and ImageNet, and exhibit strong adaptability to advanced DM variants.", "conclusion": "The proposed ADCMs framework provides an effective solution for automatic discretization in Consistency Models, enhancing training efficiency and performance while maintaining adaptability to different settings."}}
{"id": "2510.17268", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17268", "abs": "https://arxiv.org/abs/2510.17268", "authors": ["Anthony Frion", "David S Greenberg"], "title": "Uncertainty-aware data assimilation through variational inference", "comment": null, "summary": "Data assimilation, consisting in the combination of a dynamical model with a\nset of noisy and incomplete observations in order to infer the state of a\nsystem over time, involves uncertainty in most settings. Building upon an\nexisting deterministic machine learning approach, we propose a variational\ninference-based extension in which the predicted state follows a multivariate\nGaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing\nground, we show that our new model enables to obtain nearly perfectly\ncalibrated predictions, and can be integrated in a wider variational data\nassimilation pipeline in order to achieve greater benefit from increasing\nlengths of data assimilation windows. Our code is available at\nhttps://github.com/anthony-frion/Stochastic_CODA.", "AI": {"tldr": "This paper proposes a variational inference-based extension to deterministic machine learning for data assimilation, modeling predicted states as multivariate Gaussian distributions to handle uncertainty in chaotic systems like Lorenz-96.", "motivation": "Data assimilation involves uncertainty when combining dynamical models with noisy, incomplete observations. Existing deterministic approaches need extension to properly handle uncertainty in predictions.", "method": "The authors extend a deterministic machine learning approach using variational inference, where the predicted state follows a multivariate Gaussian distribution. This is tested on chaotic Lorenz-96 dynamics.", "result": "The new model achieves nearly perfectly calibrated predictions and can be integrated into wider variational data assimilation pipelines, showing greater benefits from increasing data assimilation window lengths.", "conclusion": "The proposed variational inference-based extension successfully handles uncertainty in data assimilation, providing well-calibrated predictions and improved performance with longer data assimilation windows in chaotic systems."}}
{"id": "2510.17303", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17303", "abs": "https://arxiv.org/abs/2510.17303", "authors": ["Armin Beck", "Peter Ochs"], "title": "Symmetries in PAC-Bayesian Learning", "comment": null, "summary": "Symmetries are known to improve the empirical performance of machine learning\nmodels, yet theoretical guarantees explaining these gains remain limited. Prior\nwork has focused mainly on compact group symmetries and often assumes that the\ndata distribution itself is invariant, an assumption rarely satisfied in\nreal-world applications. In this work, we extend generalization guarantees to\nthe broader setting of non-compact symmetries, such as translations and to\nnon-invariant data distributions. Building on the PAC-Bayes framework, we adapt\nand tighten existing bounds, demonstrating the approach on McAllester's\nPAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes\nbounds. We validate our theory with experiments on a rotated MNIST dataset with\na non-uniform rotation group, where the derived guarantees not only hold but\nalso improve upon prior results. These findings provide theoretical evidence\nthat, for symmetric data, symmetric models are preferable beyond the narrow\nsetting of compact groups and invariant distributions, opening the way to a\nmore general understanding of symmetries in machine learning.", "AI": {"tldr": "This paper extends generalization guarantees to non-compact symmetries and non-invariant data distributions using PAC-Bayes framework, showing improved theoretical bounds and empirical validation on rotated MNIST.", "motivation": "Prior theoretical guarantees for symmetries in ML mainly focused on compact group symmetries and assumed invariant data distributions, which rarely hold in real-world applications. The authors aim to extend these guarantees to more realistic settings.", "method": "The authors build on the PAC-Bayes framework, adapting and tightening existing bounds to handle non-compact symmetries (like translations) and non-invariant data distributions. They demonstrate the approach specifically on McAllester's PAC-Bayes bound.", "result": "The derived theoretical guarantees hold and improve upon prior results in experiments on a rotated MNIST dataset with non-uniform rotation group. The bounds apply to a wide range of PAC-Bayes bounds.", "conclusion": "Symmetric models are preferable for symmetric data beyond the narrow setting of compact groups and invariant distributions, providing a more general understanding of symmetries in machine learning with theoretical evidence."}}
{"id": "2510.16064", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16064", "abs": "https://arxiv.org/abs/2510.16064", "authors": ["Muhy Eddin Za'ter", "Bri-Mathias Hodge", "Kyri Baker"], "title": "Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions", "comment": null, "summary": "Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major\ncomputational bottleneck for real-time grid operations. In this paper, we\npropose a residual learning paradigm that uses fast DC optimal power flow (DC\nOPF) solutions as a baseline, and learns only the nonlinear corrections\nrequired to provide the full AC-OPF solution. The method utilizes a\ntopology-aware Graph Neural Network with local attention and two-level DC\nfeature integration, trained using a physics-informed loss that enforces AC\npower-flow feasibility and operational limits. Evaluations on OPFData for 57-,\n118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in\nfeasibility error, and up to 13X runtime speedup compared to conventional AC\nOPF solvers. The model maintains accuracy under N-1 contingencies and scales\nefficiently to large networks. These results demonstrate that residual learning\nis a practical and scalable bridge between linear approximations and\nAC-feasible OPF, enabling near real-time operational decision making.", "AI": {"tldr": "A residual learning approach that uses fast DC OPF solutions as baseline and learns nonlinear corrections to provide full AC OPF solutions, achieving significant speedup and accuracy improvements.", "motivation": "The nonlinear AC optimal power flow problem is a major computational bottleneck for real-time grid operations, requiring faster and more efficient solutions.", "method": "Uses topology-aware Graph Neural Network with local attention and two-level DC feature integration, trained with physics-informed loss to enforce AC power-flow feasibility and operational limits.", "result": "25% lower MSE, up to 3X reduction in feasibility error, and up to 13X runtime speedup compared to conventional AC OPF solvers on 57-, 118-, and 2000-bus systems.", "conclusion": "Residual learning provides a practical and scalable bridge between linear approximations and AC-feasible OPF, enabling near real-time operational decision making."}}
{"id": "2510.17390", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17390", "abs": "https://arxiv.org/abs/2510.17390", "authors": ["Seouh-won Yi", "Min-hwan Oh"], "title": "Exploration via Feature Perturbation in Contextual Bandits", "comment": "Accepted at NeurIPS 2025 (spotlight)", "summary": "We propose feature perturbation, a simple yet powerful technique that injects\nrandomness directly into feature inputs, instead of randomizing unknown\nparameters or adding noise to rewards. Remarkably, this algorithm achieves\n$\\tilde{\\mathcal{O}}(d\\sqrt{T})$ worst-case regret bound for generalized linear\nbandits, while avoiding the $\\tilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret\ntypical of existing randomized bandit algorithms. Because our algorithm eschews\nparameter sampling, it is both computationally efficient and naturally extends\nto non-parametric or neural network models. We verify these advantages through\nempirical evaluations, demonstrating that feature perturbation not only\nsurpasses existing methods but also unifies strong practical performance with\nbest-known theoretical guarantees.", "AI": {"tldr": "Feature perturbation is a novel bandit algorithm that injects randomness into feature inputs rather than parameters or rewards, achieving improved regret bounds and computational efficiency.", "motivation": "Existing randomized bandit algorithms suffer from suboptimal regret bounds (\u00d5(d\u00b3/\u00b2\u221aT)) and computational inefficiency due to parameter sampling, motivating a simpler approach.", "method": "The method injects random perturbations directly into feature inputs instead of randomizing unknown parameters or adding noise to rewards.", "result": "Achieves \u00d5(d\u221aT) worst-case regret bound for generalized linear bandits, outperforming existing methods with \u00d5(d\u00b3/\u00b2\u221aT) bounds, and demonstrates strong empirical performance.", "conclusion": "Feature perturbation successfully unifies strong practical performance with best-known theoretical guarantees while being computationally efficient and extensible to non-parametric models."}}
{"id": "2510.16065", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16065", "abs": "https://arxiv.org/abs/2510.16065", "authors": ["Lunchen Xie", "Zehua He", "Qingjiang Shi"], "title": "FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning", "comment": null, "summary": "Personalized Federated Learning (PFL) has emerged as a critical research\nfrontier addressing data heterogeneity issue across distributed clients. Novel\nmodel architectures and collaboration mechanisms are engineered to accommodate\nstatistical disparities while producing client-specific models. Parameter\ndecoupling represents a promising paradigm for maintaining model performance in\nPFL frameworks. However, the communication efficiency of many existing methods\nremains suboptimal, sustaining substantial communication burdens that impede\npractical deployment. To bridge this gap, we propose Federated Learning with\nProgrammed Update and Reduced INformation (FedPURIN), a novel framework that\nstrategically identifies critical parameters for transmission through an\ninteger programming formulation. This mathematically grounded strategy is\nseamlessly integrated into a sparse aggregation scheme, achieving a significant\ncommunication reduction while preserving the efficacy. Comprehensive\nevaluations on standard image classification benchmarks under varied non-IID\nconditions demonstrate competitive performance relative to state-of-the-art\nmethods, coupled with quantifiable communication reduction through sparse\naggregation. The framework establishes a new paradigm for\ncommunication-efficient PFL, particularly advantageous for edge intelligence\nsystems operating with heterogeneous data sources.", "AI": {"tldr": "FedPURIN is a communication-efficient personalized federated learning framework that uses integer programming to identify critical parameters for transmission, achieving significant communication reduction while maintaining competitive performance on non-IID data.", "motivation": "Address the suboptimal communication efficiency in existing PFL methods that sustain substantial communication burdens, which impedes practical deployment, especially for edge intelligence systems with heterogeneous data sources.", "method": "Propose FedPURIN framework that strategically identifies critical parameters for transmission through an integer programming formulation, integrated into a sparse aggregation scheme to reduce communication overhead.", "result": "Comprehensive evaluations on standard image classification benchmarks under varied non-IID conditions demonstrate competitive performance relative to state-of-the-art methods with quantifiable communication reduction through sparse aggregation.", "conclusion": "FedPURIN establishes a new paradigm for communication-efficient PFL that is particularly advantageous for edge intelligence systems operating with heterogeneous data sources, bridging the gap between model performance and communication efficiency."}}
{"id": "2510.17396", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17396", "abs": "https://arxiv.org/abs/2510.17396", "authors": ["Keivan Faghih Niresi", "Zepeng Zhang", "Olga Fink"], "title": "RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems", "comment": "Accepted to IEEE Transactions on Instrumentation and Measurement", "summary": "Time series data are often affected by various forms of corruption, such as\nmissing values, noise, and outliers, which pose significant challenges for\ntasks such as forecasting and anomaly detection. To address these issues,\ninverse problems focus on reconstructing the original signal from corrupted\ndata by leveraging prior knowledge about its underlying structure. While deep\nlearning methods have demonstrated potential in this domain, they often require\nextensive pretraining and struggle to generalize under distribution shifts. In\nthis work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series\nLinear Inverse Problems), a novel deep prior framework that achieves high\nrecovery performance without requiring pretraining data. RINS-T leverages\nneural networks as implicit priors and integrates robust optimization\ntechniques, making it resilient to outliers while relaxing the reliance on\nGaussian noise assumptions. To further improve optimization stability and\nrobustness, we introduce three key innovations: guided input initialization,\ninput perturbation, and convex output combination techniques. Each of these\ncontributions strengthens the framework's optimization stability and\nrobustness. These advancements make RINS-T a flexible and effective solution\nfor addressing complex real-world time series challenges. Our code is available\nat https://github.com/EPFL-IMOS/RINS-T.", "AI": {"tldr": "RINS-T is a novel deep prior framework for time series linear inverse problems that achieves high recovery performance without pretraining data, using neural networks as implicit priors with robust optimization techniques.", "motivation": "Time series data often suffer from corruption like missing values, noise, and outliers, which challenge forecasting and anomaly detection. Existing deep learning methods require extensive pretraining and struggle with distribution shifts.", "method": "RINS-T leverages neural networks as implicit priors with robust optimization, incorporating guided input initialization, input perturbation, and convex output combination techniques to improve optimization stability and robustness.", "result": "The framework achieves high recovery performance without requiring pretraining data and demonstrates resilience to outliers while relaxing Gaussian noise assumptions.", "conclusion": "RINS-T provides a flexible and effective solution for complex real-world time series challenges, with code available for implementation."}}
{"id": "2510.16071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16071", "abs": "https://arxiv.org/abs/2510.16071", "authors": ["Qinxuan Wang", "Chuang Wang", "Mingyu Zhang", "Jingwei Sun", "Peipei Yang", "Shuo Tang", "Shiming Xiang"], "title": "MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data", "comment": null, "summary": "Neural operators have emerged as a powerful data-driven paradigm for solving\nPartial Differential Equations (PDEs), offering orders-of-magnitude\nacceleration over traditional solvers. However, existing approaches still\nsuffer from limited accuracy and scalability, particularly on irregular domains\nwhere fluid flows exhibit rich multiscale structures. In this work, we\nintroduce the Multiscale Neural Operator (MNO), a new architecture for\nComputational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point\nclouds. MNO explicitly decomposes information across three scales: a global\ndimension-shrinkage attention module for long-range dependencies, a local graph\nattention module for neighborhood-level interactions, and a micro point-wise\nattention module for fine-grained details. This design preserves multiscale\ninductive biases while remaining computationally efficient. We evaluate MNO on\nfour diverse benchmarks, covering both steady-state and unsteady flow scenarios\nwith up to 300K points. Across all tasks, MNO consistently outperforms\nstate-of-the-art baselines, reducing prediction errors by 5% to 40% and\ndemonstrating improved robustness in challenging 3D CFD problems. Our results\nhighlight the importance of explicit multiscale design for neural operators and\nestablish MNO as a scalable framework for learning complex fluid dynamics on\nirregular domains.", "AI": {"tldr": "MNO is a multiscale neural operator for 3D CFD on unstructured point clouds that explicitly decomposes information across global, local, and micro scales using attention modules, achieving superior accuracy and scalability compared to existing methods.", "motivation": "Existing neural operators for PDEs suffer from limited accuracy and scalability, especially on irregular domains where fluid flows exhibit rich multiscale structures. Current approaches don't adequately capture the multiscale nature of fluid dynamics.", "method": "MNO uses a three-scale decomposition: global dimension-shrinkage attention for long-range dependencies, local graph attention for neighborhood interactions, and micro point-wise attention for fine details. This preserves multiscale inductive biases while maintaining computational efficiency on 3D unstructured point clouds.", "result": "MNO consistently outperforms state-of-the-art baselines across four diverse benchmarks (steady-state and unsteady flows with up to 300K points), reducing prediction errors by 5% to 40% and demonstrating improved robustness in challenging 3D CFD problems.", "conclusion": "Explicit multiscale design is crucial for neural operators, and MNO establishes a scalable framework for learning complex fluid dynamics on irregular domains, highlighting the importance of capturing multiscale structures in CFD applications."}}
{"id": "2510.17503", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17503", "abs": "https://arxiv.org/abs/2510.17503", "authors": ["El Mahdi Chayti", "Martin Jaggi"], "title": "Stochastic Difference-of-Convex Optimization with Momentum", "comment": null, "summary": "Stochastic difference-of-convex (DC) optimization is prevalent in numerous\nmachine learning applications, yet its convergence properties under small batch\nsizes remain poorly understood. Existing methods typically require large\nbatches or strong noise assumptions, which limit their practical use. In this\nwork, we show that momentum enables convergence under standard smoothness and\nbounded variance assumptions (of the concave part) for any batch size. We prove\nthat without momentum, convergence may fail regardless of stepsize,\nhighlighting its necessity. Our momentum-based algorithm achieves provable\nconvergence and demonstrates strong empirical performance.", "AI": {"tldr": "Momentum enables convergence in stochastic DC optimization under standard assumptions for any batch size, addressing limitations of existing methods that require large batches or strong noise assumptions.", "motivation": "Stochastic DC optimization is widely used in machine learning but existing methods have poor convergence properties under small batch sizes, requiring large batches or strong noise assumptions that limit practical applicability.", "method": "Proposed a momentum-based algorithm for stochastic DC optimization that works under standard smoothness and bounded variance assumptions of the concave part.", "result": "Proved that without momentum, convergence may fail regardless of stepsize, while the momentum-based approach achieves provable convergence and demonstrates strong empirical performance.", "conclusion": "Momentum is necessary and effective for enabling convergence in stochastic DC optimization under standard assumptions with any batch size, overcoming limitations of existing methods."}}
{"id": "2510.16074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16074", "abs": "https://arxiv.org/abs/2510.16074", "authors": ["Jing He", "Hua Jiang", "Cheng Li", "Siqian Xin", "Shuzhen Yang"], "title": "Early-stopping for Transformer model training", "comment": null, "summary": "This work introduces a novel theoretical framework grounded in Random Matrix\nTheory (RMT) for analyzing Transformer training dynamics. We focus on the\nunderlying mechanisms that drive performance improvements and derive principled\nearly-stopping criteria. Empirically, we observe that the spectral density of\nthe shallow self-attention matrix V consistently evolves into a heavy-tailed\ndistribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we\ndemarcate training into three stages: structural exploration, heavy-tailed\nstructure stabilization, and convergence saturation. This staging provides\nguidance for preliminary stopping decisions. Crucially, we propose two\nconsistent and validation-free criteria: a quantitative metric for heavy-tailed\ndynamics and a novel spectral signature indicative of convergence. The strong\nalignment between these criteria highlights the utility of RMT for monitoring\nand diagnosing the progression of Transformer model training.", "AI": {"tldr": "A Random Matrix Theory framework for analyzing Transformer training dynamics, identifying three training stages through spectral density evolution and proposing validation-free early-stopping criteria.", "motivation": "To understand the underlying mechanisms driving Transformer performance improvements and derive principled early-stopping criteria without relying on validation data.", "method": "Using Random Matrix Theory to analyze the spectral density of shallow self-attention matrices, applying Power Law fits to identify training stages, and developing quantitative metrics for heavy-tailed dynamics and spectral signatures.", "result": "The spectral density consistently evolves into heavy-tailed distributions, allowing demarcation of training into three stages: structural exploration, heavy-tailed structure stabilization, and convergence saturation. The proposed criteria show strong alignment.", "conclusion": "Random Matrix Theory provides effective tools for monitoring and diagnosing Transformer training progression, enabling principled early-stopping decisions without validation data."}}
{"id": "2510.17543", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17543", "abs": "https://arxiv.org/abs/2510.17543", "authors": ["Jiayi Huang", "Sangwoo Park", "Nicola Paoletti", "Osvaldo Simeone"], "title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment", "comment": "Under Review", "summary": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size.", "AI": {"tldr": "The paper proposes CAb (Conformal Alignment-based) cascading for edge-cloud systems to ensure edge predictions maintain cloud-level conditional coverage while reducing cloud offloading.", "motivation": "Edge intelligence enables low-latency inference but lacks reliability guarantees. The goal is to ensure edge predictions maintain the same conditional coverage probability as cloud models while minimizing cloud offloading.", "method": "Formalizes conditional coverage with respect to cloud predictive distribution. Uses conformal alignment to cast edge-to-cloud escalation as multiple-hypothesis testing, selecting which inputs can be safely handled at the edge while maintaining statistical guarantees.", "result": "Experiments on CIFAR-100 and TeleQnA show CAb maintains target conditional coverage for edge predictions while substantially reducing cloud offloading and incurring modest increases in prediction-set size.", "conclusion": "CAb cascading provides statistical guarantees on edge decisions satisfying cloud-level conditional coverage, offering tunable trade-offs among coverage, deferral rate, and set size."}}
{"id": "2510.16075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16075", "abs": "https://arxiv.org/abs/2510.16075", "authors": ["Sergio Mu\u00f1iz Subi\u00f1as", "Manuel L. Gonz\u00e1lez", "Jorge Ruiz G\u00f3mez", "Alejandro Mata Ali", "Jorge Mart\u00ednez Mart\u00edn", "Miguel Franco Hernando", "\u00c1ngel Miguel Garc\u00eda-Vico"], "title": "Optimization of the quantization of dense neural networks from an exact QUBO formulation", "comment": null, "summary": "This work introduces a post-training quantization (PTQ) method for dense\nneural networks via a novel ADAROUND-based QUBO formulation. Using the\nFrobenius distance between the theoretical output and the dequantized output\n(before the activation function) as the objective, an explicit QUBO whose\nbinary variables represent the rounding choice for each weight and bias is\nobtained. Additionally, by exploiting the structure of the coefficient QUBO\nmatrix, the global problem can be exactly decomposed into $n$ independent\nsubproblems of size $f+1$, which can be efficiently solved using some\nheuristics such as simulated annealing. The approach is evaluated on MNIST,\nFashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1\nand compared with a round-to-nearest traditional quantization methodology.", "AI": {"tldr": "A novel post-training quantization method using ADAROUND-based QUBO formulation that decomposes global optimization into independent subproblems for efficient neural network quantization.", "motivation": "To develop an efficient post-training quantization method that can handle various integer precisions (int8 to int1) for dense neural networks while maintaining accuracy.", "method": "Formulates quantization as a QUBO problem using Frobenius distance between theoretical and dequantized outputs, then decomposes the global problem into independent subproblems of size f+1 that can be solved with heuristics like simulated annealing.", "result": "Evaluated on MNIST, Fashion-MNIST, EMNIST, and CIFAR-10 datasets across integer precisions from int8 to int1, showing improved performance compared to traditional round-to-nearest quantization.", "conclusion": "The proposed ADAROUND-based QUBO formulation provides an effective and efficient approach for neural network quantization that outperforms traditional methods across various precision levels."}}
{"id": "2510.17794", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17794", "abs": "https://arxiv.org/abs/2510.17794", "authors": ["Omer Haq"], "title": "Functional Distribution Networks (FDN)", "comment": "Submitted to ICLR 2026. Code will be released upon acceptance", "summary": "Modern probabilistic regressors often remain overconfident under distribution\nshift. We present Functional Distribution Networks (FDN), an input-conditioned\ndistribution over network weights that induces predictive mixtures whose\ndispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo\nsampling. We further propose an evaluation protocol that cleanly separates\ninterpolation from extrapolation and stresses OOD sanity checks (e.g., that\npredictive likelihood degrades under shift while in-distribution accuracy and\ncalibration are maintained). On standard regression tasks, we benchmark against\nstrong Bayesian, ensemble, dropout, and hypernetwork baselines under matched\nparameter and update budgets, and assess accuracy, calibration, and\nshift-awareness with standard diagnostics. Together, the framework and protocol\naim to make OOD-aware, well-calibrated neural regression practical and modular.", "AI": {"tldr": "Functional Distribution Networks (FDN) is a method that creates input-conditioned weight distributions to produce adaptive predictive mixtures, addressing overconfidence in neural networks under distribution shift through beta-ELBO training and Monte Carlo sampling.", "motivation": "Modern probabilistic regressors often remain overconfident under distribution shift, highlighting the need for methods that can adapt predictive uncertainty to input conditions.", "method": "FDN uses input-conditioned distributions over network weights to induce predictive mixtures with adaptive dispersion, trained with beta-ELBO and Monte Carlo sampling. An evaluation protocol separates interpolation from extrapolation and includes OOD sanity checks.", "result": "The method was benchmarked against Bayesian, ensemble, dropout, and hypernetwork baselines under matched parameter and update budgets, assessing accuracy, calibration, and shift-awareness with standard diagnostics.", "conclusion": "The FDN framework and evaluation protocol aim to make OOD-aware, well-calibrated neural regression practical and modular for real-world applications."}}
{"id": "2510.16076", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16076", "abs": "https://arxiv.org/abs/2510.16076", "authors": ["SeongKu Kang", "Jianxun Lian", "Dongha Lee", "Wonbin Kweon", "Sanghwan Jang", "Jaehyun Lee", "Jindong Wang", "Xing Xie", "Hwanjo Yu"], "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recommender systems suffer from biases that cause the collected feedback to\nincompletely reveal user preference. While debiasing learning has been\nextensively studied, they mostly focused on the specialized (called\ncounterfactual) test environment simulated by random exposure of items,\nsignificantly degrading accuracy in the typical (called factual) test\nenvironment based on actual user-item interactions. In fact, each test\nenvironment highlights the benefit of a different aspect: the counterfactual\ntest emphasizes user satisfaction in the long-terms, while the factual test\nfocuses on predicting subsequent user behaviors on platforms. Therefore, it is\ndesirable to have a model that performs well on both tests rather than only\none. In this work, we introduce a new learning framework, called Bias-adaptive\nPreference distillation Learning (BPL), to gradually uncover user preferences\nwith dual distillation strategies. These distillation strategies are designed\nto drive high performance in both factual and counterfactual test environments.\nEmploying a specialized form of teacher-student distillation from a biased\nmodel, BPL retains accurate preference knowledge aligned with the collected\nfeedback, leading to high performance in the factual test. Furthermore, through\nself-distillation with reliability filtering, BPL iteratively refines its\nknowledge throughout the training process. This enables the model to produce\nmore accurate predictions across a broader range of user-item combinations,\nthereby improving performance in the counterfactual test. Comprehensive\nexperiments validate the effectiveness of BPL in both factual and\ncounterfactual tests. Our implementation is accessible via:\nhttps://github.com/SeongKu-Kang/BPL.", "AI": {"tldr": "BPL is a new learning framework that uses dual distillation strategies to achieve high performance in both factual and counterfactual test environments for recommender systems, addressing the bias problem in user preference modeling.", "motivation": "Recommender systems suffer from biases that incompletely reveal user preferences, and existing debiasing methods mostly focus on counterfactual test environments, degrading performance in factual test environments based on actual user interactions. Both test environments are important - counterfactual for long-term user satisfaction and factual for predicting subsequent user behaviors.", "method": "BPL uses dual distillation strategies: 1) Teacher-student distillation from a biased model to retain accurate preference knowledge aligned with collected feedback for factual test performance, and 2) Self-distillation with reliability filtering to iteratively refine knowledge throughout training for better counterfactual test performance across broader user-item combinations.", "result": "Comprehensive experiments validate the effectiveness of BPL in both factual and counterfactual tests, demonstrating improved performance compared to existing methods.", "conclusion": "BPL successfully addresses the bias problem in recommender systems by gradually uncovering user preferences through dual distillation strategies, achieving high performance in both factual and counterfactual test environments rather than sacrificing one for the other."}}
{"id": "2510.16077", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16077", "abs": "https://arxiv.org/abs/2510.16077", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Weiping Ding", "Jimmy Cao", "Wolfgang Mayer", "Ryszard Kowalczyk"], "title": "Continual Knowledge Consolidation LORA for Domain Incremental Learning", "comment": null, "summary": "Domain Incremental Learning (DIL) is a continual learning sub-branch that\naims to address never-ending arrivals of new domains without catastrophic\nforgetting problems. Despite the advent of parameter-efficient fine-tuning\n(PEFT) approaches, existing works create task-specific LoRAs overlooking shared\nknowledge across tasks. Inaccurate selection of task-specific LORAs during\ninference results in significant drops in accuracy, while existing works rely\non linear or prototype-based classifiers, which have suboptimal generalization\npowers. Our paper proposes continual knowledge consolidation low rank\nadaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed\nfrom consolidations between task-shared LORA to extract common knowledge and\ntask-specific LORA to embrace domain-specific knowledge. Unlike existing\napproaches, CONEC-LoRA integrates the concept of a stochastic classifier whose\nparameters are sampled from a distribution, thus enhancing the likelihood of\ncorrect classifications. Last but not least, an auxiliary network is deployed\nto optimally predict the task-specific LoRAs for inferences and implements the\nconcept of a different-depth network structure in which every layer is\nconnected with a local classifier to take advantage of intermediate\nrepresentations. This module integrates the ball-generator loss and\ntransformation module to address the synthetic sample bias problem. Our\nrigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in\n4 popular benchmark problems with over 5% margins.", "AI": {"tldr": "CONEC-LoRA is a novel approach for Domain Incremental Learning that combines task-shared and task-specific LoRAs with a stochastic classifier and auxiliary network to prevent catastrophic forgetting and improve generalization.", "motivation": "Existing parameter-efficient fine-tuning approaches create task-specific LoRAs but overlook shared knowledge across tasks, and rely on suboptimal classifiers that limit generalization performance in domain incremental learning scenarios.", "method": "Proposes CONEC-LoRA with: 1) Consolidation between task-shared and task-specific LoRAs, 2) Stochastic classifier with parameters sampled from distributions, 3) Auxiliary network for optimal LoRA prediction, 4) Different-depth network structure with local classifiers, 5) Ball-generator loss and transformation module to address synthetic sample bias.", "result": "CONEC-LoRA demonstrates significant advantages over prior methods, achieving over 5% improvement margins across 4 popular benchmark problems in domain incremental learning.", "conclusion": "The proposed CONEC-LoRA framework effectively addresses domain incremental learning challenges by leveraging shared knowledge, stochastic classification, and intermediate representations, outperforming existing approaches substantially."}}
{"id": "2510.16083", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16083", "abs": "https://arxiv.org/abs/2510.16083", "authors": ["Jaehan Kim", "Minkyoo Song", "Minjae Seo", "Youngjin Jin", "Seungwon Shin", "Jinwoo Kim"], "title": "PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites", "comment": "Accepted by Elsevier Expert Systems with Applications", "summary": "Credential stuffing attacks have caused significant harm to online users who\nfrequently reuse passwords across multiple websites. While prior research has\nattempted to detect users with reused passwords or identify malicious login\nattempts, existing methods often compromise usability by restricting password\ncreation or website access, and their reliance on complex account-sharing\nmechanisms hinders real-world deployment. To address these limitations, we\npropose PassREfinder-FL, a novel framework that predicts credential stuffing\nrisks across websites. We introduce the concept of password reuse relations --\ndefined as the likelihood of users reusing passwords between websites -- and\nrepresent them as edges in a website graph. Using graph neural networks (GNNs),\nwe perform a link prediction task to assess credential reuse risk between\nsites. Our approach scales to a large number of arbitrary websites by\nincorporating public website information and linking newly observed websites as\nnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a\nfederated learning (FL) approach that eliminates the need to share user\nsensitive information across administrators. Evaluation on a real-world dataset\nof 360 million breached accounts from 22,378 websites shows that\nPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further\nvalidate that our FL-based GNN achieves a 4-11% performance improvement over\nother state-of-the-art GNN models through an ablation study. Finally, we\ndemonstrate that the predicted results can be used to quantify password reuse\nlikelihood as actionable risk scores.", "AI": {"tldr": "PassREfinder-FL is a novel framework that predicts credential stuffing risks across websites using graph neural networks and federated learning, achieving high accuracy while preserving user privacy.", "motivation": "Credential stuffing attacks harm users who reuse passwords across websites. Existing detection methods compromise usability and face deployment challenges due to complex account-sharing mechanisms.", "method": "Proposes password reuse relations as edges in a website graph, uses GNNs for link prediction, scales to arbitrary websites using public information, and incorporates federated learning to protect user privacy.", "result": "Achieves F1-score of 0.9153 on real-world dataset of 360M breached accounts from 22,378 websites, with 4-11% performance improvement over other GNN models.", "conclusion": "The framework effectively quantifies password reuse likelihood as actionable risk scores while maintaining user privacy through federated learning."}}
{"id": "2510.16084", "categories": ["cs.LG", "cond-mat.quant-gas", "math-ph", "math.MP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16084", "abs": "https://arxiv.org/abs/2510.16084", "authors": ["Karol Sajnok", "Micha\u0142 Matuszewski"], "title": "Near-Equilibrium Propagation training in nonlinear wave systems", "comment": "7 figures", "summary": "Backpropagation learning algorithm, the workhorse of modern artificial\nintelligence, is notoriously difficult to implement in physical neural\nnetworks. Equilibrium Propagation (EP) is an alternative with comparable\nefficiency and strong potential for in-situ training. We extend EP learning to\nboth discrete and continuous complex-valued wave systems. In contrast to\nprevious EP implementations, our scheme is valid in the weakly dissipative\nregime, and readily applicable to a wide range of physical settings, even\nwithout well defined nodes, where trainable inter-node connections can be\nreplaced by trainable local potential. We test the method in driven-dissipative\nexciton-polariton condensates governed by generalized Gross-Pitaevskii\ndynamics. Numerical studies on standard benchmarks, including a simple logical\ntask and handwritten-digit recognition, demonstrate stable convergence,\nestablishing a practical route to in-situ learning in physical systems in which\nsystem control is restricted to local parameters.", "AI": {"tldr": "The paper extends Equilibrium Propagation (EP) learning to complex-valued wave systems, making it applicable in weakly dissipative regimes and systems without well-defined nodes, using trainable local potentials instead of inter-node connections.", "motivation": "Backpropagation is difficult to implement in physical neural networks, so the authors aim to develop EP as a practical alternative for in-situ training of physical systems where control is limited to local parameters.", "method": "Extend EP learning to discrete and continuous complex-valued wave systems, particularly in weakly dissipative regimes. Test the method using driven-dissipative exciton-polariton condensates governed by generalized Gross-Pitaevskii dynamics.", "result": "Numerical studies on standard benchmarks (logical tasks and handwritten-digit recognition) demonstrate stable convergence of the method.", "conclusion": "The approach establishes a practical route to in-situ learning in physical systems where system control is restricted to local parameters, making EP applicable to a wide range of physical settings."}}
{"id": "2510.16086", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16086", "abs": "https://arxiv.org/abs/2510.16086", "authors": ["Ziyang Liu", "Pengjunfei Chu", "Shuming Dong", "Chen Zhang", "Mingcheng Li", "Jin Wang"], "title": "FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis", "comment": "6 pages,3 figures", "summary": "In recent years, Multimodal Sentiment Analysis (MSA) has become a research\nhotspot that aims to utilize multimodal data for human sentiment understanding.\nPrevious MSA studies have mainly focused on performing interaction and fusion\non complete multimodal data, ignoring the problem of missing modalities in\nreal-world applications due to occlusion, personal privacy constraints, and\ndevice malfunctions, resulting in low generalizability.\n  To this end, we propose a Factorization-guided Semantic Recovery Framework\n(FSRF) to mitigate the modality missing problem in the MSA task.\n  Specifically, we propose a de-redundant homo-heterogeneous factorization\nmodule that factorizes modality into modality-homogeneous,\nmodality-heterogeneous, and noisy representations and design elaborate\nconstraint paradigms for representation learning.\n  Furthermore, we design a distribution-aligned self-distillation module that\nfully recovers the missing semantics by utilizing bidirectional knowledge\ntransfer.\n  Comprehensive experiments on two datasets indicate that FSRF has a\nsignificant performance advantage over previous methods with uncertain missing\nmodalities.", "AI": {"tldr": "FSRF is a framework that addresses modality missing in Multimodal Sentiment Analysis by factorizing modalities into homogeneous, heterogeneous, and noisy representations, and using self-distillation for semantic recovery.", "motivation": "Previous MSA studies focused on complete multimodal data but ignored real-world modality missing issues due to occlusion, privacy constraints, and device malfunctions, leading to poor generalizability.", "method": "Proposed de-redundant homo-heterogeneous factorization module to separate modalities into homogeneous, heterogeneous, and noisy representations with constraint paradigms, plus distribution-aligned self-distillation for bidirectional knowledge transfer.", "result": "Comprehensive experiments on two datasets show FSRF has significant performance advantage over previous methods when dealing with uncertain missing modalities.", "conclusion": "FSRF effectively mitigates the modality missing problem in MSA tasks through factorization and self-distillation techniques, achieving superior performance compared to existing approaches."}}
{"id": "2510.16089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16089", "abs": "https://arxiv.org/abs/2510.16089", "authors": ["William Hoy", "Nurcin Celik"], "title": "STABLE: Gated Continual Learning for Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly require mechanisms for continual\nadaptation without full retraining. However, sequential updates can lead to\ncatastrophic forgetting, where new edits degrade previously acquired knowledge.\nThis work presents STABLE, a gated continual self editing framework that\nconstrains forgetting during sequential updates using parameter efficient fine\ntuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate\nedit is evaluated against a stability budget using one of three metrics: (i)\nExact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,\nreflecting reduced model confidence; and (iii) KL divergence, quantifying\ndistributional drift between the base and adapted models. If a threshold is\nexceeded, the LoRA update is rescaled through a clipping procedure or rejected.\nExperiments on the Qwen-2.5-7B model show that gating effectively mitigates\nforgetting while preserving adaptability. EM based gating achieved the highest\ncumulative performance in short continual learning sequences. Our results show\nthat different gating strategies can achieve comparable distribution shift\n(measured by KL divergence) while producing different accuracy outcomes,\nhighlighting the importance of gating design in continual adaptation. This\napproach offers a principled method for continual model editing, enabling LLMs\nto integrate new knowledge while maintaining reliability. Code:\nhttps://github.com/Bhoy1/STABLE", "AI": {"tldr": "STABLE is a gated continual self-editing framework that uses LoRA-based parameter efficient fine-tuning to prevent catastrophic forgetting during sequential updates to LLMs. It evaluates edits against stability budgets using three metrics and gates updates through clipping or rejection when thresholds are exceeded.", "motivation": "Large language models need continual adaptation mechanisms without full retraining, but sequential updates can cause catastrophic forgetting where new edits degrade previously acquired knowledge.", "method": "Uses parameter efficient fine-tuning via Low Rank Adaptation (LoRA) with gating mechanism. Each edit is evaluated against stability budget using three metrics: Exact Match drop, bits increase, and KL divergence. Updates are rescaled through clipping or rejected if thresholds are exceeded.", "result": "Experiments on Qwen-2.5-7B show gating effectively mitigates forgetting while preserving adaptability. EM-based gating achieved highest cumulative performance in short continual learning sequences. Different gating strategies achieve comparable distribution shift (KL divergence) but different accuracy outcomes.", "conclusion": "STABLE offers a principled method for continual model editing, enabling LLMs to integrate new knowledge while maintaining reliability. Gating design is crucial for effective continual adaptation."}}
{"id": "2510.16092", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16092", "abs": "https://arxiv.org/abs/2510.16092", "authors": ["Devvrit Khatri", "Pranamya Kulkarni", "Nilesh Gupta", "Yerram Varun", "Liqian Peng", "Jay Yagnik", "Praneeth Netrapalli", "Cho-Jui Hsieh", "Alec Go", "Inderjit S Dhillon", "Aditya Kusupati", "Prateek Jain"], "title": "Compressing Many-Shots in In-Context Learning", "comment": null, "summary": "Large Language Models (LLMs) have been shown to be able to learn different\ntasks without explicit finetuning when given many input-output examples /\ndemonstrations through In-Context Learning (ICL). Increasing the number of\nexamples, called ``shots'', improves downstream task performance but incurs\nhigher memory and computational costs. In this work, we study an approach to\nimprove the memory and computational efficiency of ICL inference by compressing\nthe many-shot prompts. Given many shots comprising t tokens, our goal is to\ngenerate a m soft-token summary, where m < t. We first show that existing\nprompt compression methods are ineffective for many-shot compression, and\nsimply using fewer shots as a baseline is surprisingly strong. To achieve\neffective compression, we find that: (a) a stronger compressor model with more\ntrainable parameters is necessary, and (b) compressing many-shot\nrepresentations at each transformer layer enables more fine-grained compression\nby providing each layer with its own compressed representation. Based on these\ninsights, we propose MemCom, a layer-wise compression method. We systematically\nevaluate various compressor models and training approaches across different\nmodel sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence\nlengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms\nstrong baselines across all compression ratios on multiple classification tasks\nwith large label sets. Notably, while baseline performance degrades sharply at\nhigher compression ratios, often by over 20-30%, MemCom maintains high accuracy\nwith minimal degradation, typically dropping by less than 10%.", "AI": {"tldr": "MemCom is a layer-wise compression method that improves memory and computational efficiency of many-shot in-context learning by compressing prompts into fewer soft tokens while maintaining high accuracy.", "motivation": "Large Language Models (LLMs) benefit from more input examples in in-context learning but this incurs high memory and computational costs. Existing prompt compression methods are ineffective for many-shot scenarios, and simply using fewer shots is surprisingly strong but still inefficient.", "method": "Proposed MemCom, a layer-wise compression method that uses stronger compressor models with more trainable parameters and compresses many-shot representations at each transformer layer to enable fine-grained compression.", "result": "MemCom outperforms strong baselines across all compression ratios (3x to 8x) on multiple classification tasks with large label sets. While baseline performance degrades sharply (20-30%) at higher compression ratios, MemCom maintains high accuracy with minimal degradation (<10%).", "conclusion": "Layer-wise compression with stronger compressor models is effective for many-shot prompt compression, enabling significant memory and computational efficiency gains while preserving task performance."}}
{"id": "2510.16123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16123", "abs": "https://arxiv.org/abs/2510.16123", "authors": ["Federico Malato", "Ville Hautam\u00e4ki"], "title": "Zero-shot World Models via Search in Memory", "comment": "10 pages, 8 figures in main text + appendices", "summary": "World Models have vastly permeated the field of Reinforcement Learning. Their\nability to model the transition dynamics of an environment have greatly\nimproved sample efficiency in online RL. Among them, the most notorious example\nis Dreamer, a model that learns to act in a diverse set of image-based\nenvironments. In this paper, we leverage similarity search and stochastic\nrepresentations to approximate a world model without a training procedure. We\nestablish a comparison with PlaNet, a well-established world model of the\nDreamer family. We evaluate the models on the quality of latent reconstruction\nand on the perceived similarity of the reconstructed image, on both next-step\nand long horizon dynamics prediction. The results of our study demonstrate that\na search-based world model is comparable to a training based one in both cases.\nNotably, our model show stronger performance in long-horizon prediction with\nrespect to the baseline on a range of visually different environments.", "AI": {"tldr": "A search-based world model using similarity search and stochastic representations achieves comparable performance to training-based models like PlaNet in latent reconstruction and image similarity, with better long-horizon prediction.", "motivation": "World models improve sample efficiency in RL, but typically require training. This paper explores whether a training-free approach using similarity search can match established models.", "method": "Leverage similarity search and stochastic representations to approximate world model without training. Compare with PlaNet (Dreamer family) on latent reconstruction quality and image similarity.", "result": "Search-based model is comparable to training-based model in next-step and long-horizon prediction. Shows stronger performance in long-horizon prediction across visually different environments.", "conclusion": "Training-free world models using similarity search are viable alternatives to training-based approaches, particularly excelling in long-term predictions."}}
{"id": "2510.16165", "categories": ["cs.LG", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2510.16165", "abs": "https://arxiv.org/abs/2510.16165", "authors": ["Charles Rhys Campbell", "Aldo H. Romero", "Kamal Choudhary"], "title": "AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures", "comment": null, "summary": "Generative models have become significant assets in the exploration and\nidentification of new materials, enabling the rapid proposal of candidate\ncrystal structures that satisfy target properties. Despite the increasing\nadoption of diverse architectures, a rigorous comparative evaluation of their\nperformance on materials datasets is lacking. In this work, we present a\nsystematic benchmark of three representative generative models- AtomGPT (a\ntransformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),\nand FlowMM (a Riemannian flow matching model). These models were trained to\nreconstruct crystal structures from subsets of two publicly available\nsuperconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria\ndatabase. Performance was assessed using the Kullback-Leibler (KL) divergence\nbetween predicted and reference distributions of lattice parameters, as well as\nthe mean absolute error (MAE) of individual lattice constants. For the computed\nKLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and\nthen FlowMM. All benchmarking code and model configurations will be made\npublicly available at https://github.com/atomgptlab/atombench_inverse.", "AI": {"tldr": "Systematic benchmark comparing three generative models (AtomGPT, CDVAE, FlowMM) for crystal structure generation using superconductivity datasets, with CDVAE performing best.", "motivation": "To provide rigorous comparative evaluation of diverse generative architectures for materials discovery, addressing the lack of systematic benchmarks in the field.", "method": "Trained three representative models (transformer-based AtomGPT, CDVAE, and Riemannian flow matching FlowMM) on superconductivity datasets and evaluated using KL divergence and MAE metrics for lattice parameters.", "result": "CDVAE performed most favorably, followed by AtomGPT, then FlowMM, based on KL divergence and MAE scores for lattice parameter predictions.", "conclusion": "The benchmark establishes performance rankings and provides publicly available code for reproducible evaluation of generative models in materials science."}}
{"id": "2510.16167", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16167", "abs": "https://arxiv.org/abs/2510.16167", "authors": ["Archie Chaudhury"], "title": "Alignment is Localized: A Causal Probe into Preference Layers", "comment": null, "summary": "Reinforcement Learning frameworks, particularly those utilizing human\nannotations, have become an increasingly popular method for preference\nfine-tuning, where the outputs of a language model are tuned to match a certain\nset of behavioral policies or guidelines. Reinforcement Learning through Human\nFeedback (RLHF) is perhaps the most popular implementation of such a framework,\nparticularly for aligning LMs toward safety and human intent. However, the\ninternal workings of how such alignment is achieved remain largely opaque. In\nthis work, we systematically analyze preference optimization for language model\nalignment by applying layer-wide causal patching between a base model and its\ntuned counterpart across human preference pairs. We implement our methodology\non \\textit{Llama-3.2-1B}, and find that alignment is spatially localized:\nmid-layer activations encode a distinct subspace that causally determines\nreward-consistent behavior, while early and late layers remain largely\nunaffected. Utilizing LASSO regression, we also find that only a small number\nof layers possess non-zero coefficients linking activation distances to reward\ngains. Overall, we show that, at least for some language models, alignment from\nhuman-based, preferential tuning is a directional, low rank process, rather\nthan diffuse and parameteric.", "AI": {"tldr": "This paper analyzes how reinforcement learning with human feedback (RLHF) aligns language models by examining layer activations. The study finds that alignment is spatially localized in mid-layer activations rather than diffusely distributed across all parameters.", "motivation": "While RLHF has become popular for aligning language models with human preferences, the internal mechanisms of how this alignment is achieved remain unclear and opaque.", "method": "The researchers systematically analyze preference optimization by applying layer-wide causal patching between a base model and its tuned counterpart across human preference pairs, using Llama-3.2-1B and LASSO regression to identify key layers.", "result": "Alignment is spatially localized in mid-layer activations that causally determine reward-consistent behavior, while early and late layers remain largely unaffected. Only a small number of layers have non-zero coefficients linking activation distances to reward gains.", "conclusion": "Alignment from human-based preferential tuning is a directional, low-rank process rather than diffuse and parametric, suggesting that model alignment occurs through specific, targeted changes rather than widespread parameter adjustments."}}
{"id": "2510.16175", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16175", "abs": "https://arxiv.org/abs/2510.16175", "authors": ["Pablo Samuel Castro"], "title": "The Formalism-Implementation Gap in Reinforcement Learning Research", "comment": null, "summary": "The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems.", "AI": {"tldr": "This paper argues that RL research should shift focus from demonstrating agent capabilities to advancing scientific understanding of reinforcement learning dynamics, and calls for more precise mapping between benchmarks and mathematical formalisms.", "motivation": "The motivation stems from concern that performance-focused RL research risks overfitting on academic benchmarks, making techniques less transferable to real-world problems, while diminishing work that improves understanding of learning dynamics.", "method": "The paper uses the Arcade Learning Environment (ALE) as an example benchmark to demonstrate how even \"saturated\" benchmarks can be effectively used for developing understanding and facilitating real-world deployment.", "result": "The analysis shows that ALE, despite being considered saturated, remains valuable for understanding RL techniques and enabling their application to impactful real-world problems.", "conclusion": "RL research should prioritize advancing scientific understanding over mere performance demonstration, and establish clearer connections between benchmarks and underlying mathematical formalisms to improve technique transferability."}}
{"id": "2510.16188", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16188", "abs": "https://arxiv.org/abs/2510.16188", "authors": ["Fateme Golivand Darvishvand", "Hikaru Shindo", "Sahil Sidheekh", "Kristian Kersting", "Sriraam Natarajan"], "title": "Human-Allied Relational Reinforcement Learning", "comment": "Proceedings of the Twelfth Annual Conference on Advances in Cognitive\n  Systems, ACS-2025 (143-159)", "summary": "Reinforcement learning (RL) has experienced a second wind in the past decade.\nWhile incredibly successful in images and videos, these systems still operate\nwithin the realm of propositional tasks ignoring the inherent structure that\nexists in the problem. Consequently, relational extensions (RRL) have been\ndeveloped for such structured problems that allow for effective generalization\nto arbitrary number of objects. However, they inherently make strong\nassumptions about the problem structure. We introduce a novel framework that\ncombines RRL with object-centric representation to handle both structured and\nunstructured data. We enhance learning by allowing the system to actively query\nthe human expert for guidance by explicitly modeling the uncertainty over the\npolicy. Our empirical evaluation demonstrates the effectiveness and efficiency\nof our proposed approach.", "AI": {"tldr": "A novel framework combining relational reinforcement learning with object-centric representations to handle both structured and unstructured data, enhanced by active human guidance through policy uncertainty modeling.", "motivation": "Traditional RL systems operate on propositional tasks ignoring inherent problem structure, while relational extensions (RRL) make strong assumptions about problem structure. There's a need for a more flexible approach that can handle both structured and unstructured data effectively.", "method": "Combines RRL with object-centric representation learning and incorporates active human guidance by explicitly modeling uncertainty over the policy, allowing the system to query human experts when needed.", "result": "Empirical evaluation demonstrates the effectiveness and efficiency of the proposed approach in handling both structured and unstructured data while benefiting from human guidance.", "conclusion": "The proposed framework successfully bridges the gap between structured and unstructured data handling in RL, with active human guidance through uncertainty modeling proving to be an effective enhancement for learning."}}
{"id": "2510.16233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16233", "abs": "https://arxiv.org/abs/2510.16233", "authors": ["Patricia West", "Michelle WL Wan", "Alexander Hepburn", "Edwin Simpson", "Raul Santos-Rodriguez", "Jeffrey N Clark"], "title": "Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal", "comment": null, "summary": "Climate change demands effective legislative action to mitigate its impacts.\nThis study explores the application of machine learning (ML) to understand the\nprogression of climate policy from announcement to adoption, focusing on\npolicies within the European Green Deal. We present a dataset of 165 policies,\nincorporating text and metadata. We aim to predict a policy's progression\nstatus, and compare text representation methods, including TF-IDF, BERT, and\nClimateBERT. Metadata features are included to evaluate the impact on\npredictive performance. On text features alone, ClimateBERT outperforms other\napproaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance\nwith the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods\nfrom explainable AI highlights the influence of factors such as policy wording\nand metadata including political party and country representation. These\nfindings underscore the potential of ML tools in supporting climate policy\nanalysis and decision-making.", "AI": {"tldr": "This study uses machine learning to predict climate policy progression in the European Green Deal, comparing text representation methods and finding that ClimateBERT performs best on text alone, while BERT with metadata achieves superior overall performance.", "motivation": "Climate change requires effective legislative action, and this research aims to understand how machine learning can help track and predict the progression of climate policies from announcement to adoption.", "method": "The study analyzes 165 policies with text and metadata, comparing text representation methods (TF-IDF, BERT, ClimateBERT) and incorporating metadata features to predict policy progression status using explainable AI techniques.", "result": "ClimateBERT achieved the best performance on text features alone (RMSE = 0.17, R\u00b2 = 0.29), while BERT with metadata features yielded superior overall performance (RMSE = 0.16, R\u00b2 = 0.38). Explainable AI revealed key influencing factors like policy wording, political party, and country representation.", "conclusion": "Machine learning tools show significant potential for supporting climate policy analysis and decision-making by predicting policy progression and identifying influential factors through explainable AI methods."}}
{"id": "2510.16252", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16252", "abs": "https://arxiv.org/abs/2510.16252", "authors": ["Yuxuan Lu", "Jing Huang", "Hui Liu", "Jiri Gesi", "Yan Han", "Shihan Fu", "Tianqi Zheng", "Dakuo Wang"], "title": "WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale", "comment": null, "summary": "Training and evaluation of Reinforcement Learning (RL) web agents have gained\nincreasing attention, yet a scalable and efficient environment that couples\nrealistic and robust browser-side interaction with controllable server-side\nstate at scale is still missing. Existing environments tend to have one or more\nof the following issues: they overwhelm policy models with excessive and noisy\ncontext; they perform actions non-deterministically without waiting for the UI\nor network to stabilize; or they cannot scale isolated client-server containers\neffectively for parallel RL rollouts. We propose WEBSERV, an environment that\nincludes 1) a compact, site-agnostic browser environment that balances context\nand action complexity, and 2) a scalable RL environment via efficient launching\nand resetting web-servers to enable scalable RL training and evaluation. We\nevaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving\nstate-of-the-art single-prompt success rates while cutting launch latency by\n~5x and storage need by ~240x, with a comparable memory footprint, enabling\n200+ concurrent containers on a single host.", "AI": {"tldr": "WEBSERV is a scalable web environment for RL agents that addresses limitations of existing systems by providing compact browser interaction and efficient server management, achieving SOTA performance with significantly reduced latency and storage requirements.", "motivation": "Existing RL web environments have issues with excessive context noise, non-deterministic actions, and poor scalability for parallel training, creating a need for a more efficient and robust solution.", "method": "Proposes WEBSERV with two components: 1) a compact, site-agnostic browser environment balancing context and action complexity, and 2) scalable RL environment with efficient web-server launching and resetting for parallel training.", "result": "Achieved state-of-the-art single-prompt success rates on WebArena shopping CMS and Gitlab tasks, with ~5x reduction in launch latency, ~240x reduction in storage needs, and support for 200+ concurrent containers on a single host.", "conclusion": "WEBSERV provides an efficient and scalable solution for RL web agent training that significantly outperforms existing environments in performance, resource efficiency, and scalability."}}
{"id": "2510.16289", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16289", "abs": "https://arxiv.org/abs/2510.16289", "authors": ["Yoonho Lee", "Junseok Lee", "Sangwoo Seo", "Sungwon Kim", "Yeongmin Kim", "Chanyoung Park"], "title": "Disentangling Hyperedges through the Lens of Category Theory", "comment": "Accepted to NeurIPS 2025", "summary": "Despite the promising results of disentangled representation learning in\ndiscovering latent patterns in graph-structured data, few studies have explored\ndisentanglement for hypergraph-structured data. Integrating hyperedge\ndisentanglement into hypergraph neural networks enables models to leverage\nhidden hyperedge semantics, such as unannotated relations between nodes, that\nare associated with labels. This paper presents an analysis of hyperedge\ndisentanglement from a category-theoretical perspective and proposes a novel\ncriterion for disentanglement derived from the naturality condition. Our\nproof-of-concept model experimentally showed the potential of the proposed\ncriterion by successfully capturing functional relations of genes (nodes) in\ngenetic pathways (hyperedges).", "AI": {"tldr": "This paper introduces a novel criterion for hyperedge disentanglement in hypergraph neural networks using category theory, and demonstrates its effectiveness in capturing gene functional relations in genetic pathways.", "motivation": "Few studies have explored disentangled representation learning for hypergraph-structured data, despite its potential to reveal hidden hyperedge semantics like unannotated node relations associated with labels.", "method": "The paper analyzes hyperedge disentanglement from a category-theoretical perspective and proposes a novel disentanglement criterion derived from the naturality condition, implemented in a proof-of-concept model.", "result": "The proof-of-concept model successfully captured functional relations of genes (nodes) in genetic pathways (hyperedges), demonstrating the potential of the proposed criterion.", "conclusion": "The proposed category-theoretical approach to hyperedge disentanglement shows promise for leveraging hidden hyperedge semantics in hypergraph neural networks, particularly for applications like genetic pathway analysis."}}
{"id": "2510.16292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16292", "abs": "https://arxiv.org/abs/2510.16292", "authors": ["Yutong Wang", "Haiyu Wang", "Sai Qian Zhang"], "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models", "comment": "Accepted as Spotlight paper by NeurIPS 2025", "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.", "AI": {"tldr": "Proposes using SVD on QKV weight matrices with dynamic rank allocation and quantization to reduce KV cache size and computational overhead in Vision-Language Models, achieving >10% accuracy improvement with lower hardware cost.", "motivation": "Vision-Language Models have high computational costs that limit scalability and real-time applicability due to large memory footprints and processing time.", "method": "Leverage SVD over joint QKV weight matrices to reduce KV cache size, introduce dynamic rank allocation strategy, and apply quantization to both weights and activations.", "result": "Achieves more than 10% accuracy improvement while consuming less hardware cost compared to approaches using only quantization or SVD.", "conclusion": "The proposed method enables highly efficient VLMs suitable for real-time deployment on resource-constrained devices."}}
{"id": "2510.16306", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16306", "abs": "https://arxiv.org/abs/2510.16306", "authors": ["Xin Wang", "Yu Wang", "Yunchao Liu", "Jens Meiler", "Tyler Derr"], "title": "Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening", "comment": null, "summary": "Ligand-based virtual screening (VS) is an essential step in drug discovery\nthat evaluates large chemical libraries to identify compounds that potentially\nbind to a therapeutic target. However, VS faces three major challenges: class\nimbalance due to the low active rate, structural imbalance among active\nmolecules where certain scaffolds dominate, and the need to identify\nstructurally diverse active compounds for novel drug development. We introduce\nScaffAug, a scaffold-aware VS framework that addresses these challenges through\nthree modules. The augmentation module first generates synthetic data\nconditioned on scaffolds of actual hits using generative AI, specifically a\ngraph diffusion model. This helps mitigate the class imbalance and furthermore\nthe structural imbalance, due to our proposed scaffold-aware sampling\nalgorithm, designed to produce more samples for active molecules with\nunderrepresented scaffolds. A model-agnostic self-training module is then used\nto safely integrate the generated synthetic data from our augmentation module\nwith the original labeled data. Lastly, we introduce a reranking module that\nimproves VS by enhancing scaffold diversity in the top recommended set of\nmolecules, while still maintaining and even enhancing the overall general\nperformance of identifying novel, active compounds. We conduct comprehensive\ncomputational experiments across five target classes, comparing ScaffAug\nagainst existing baseline methods by reporting the performance of multiple\nevaluation metrics and performing ablation studies on ScaffAug. Overall, this\nwork introduces novel perspectives on effectively enhancing VS by leveraging\ngenerative augmentations, reranking, and general scaffold-awareness.", "AI": {"tldr": "ScaffAug is a scaffold-aware virtual screening framework that addresses class and structural imbalance in drug discovery using generative AI augmentation, self-training, and scaffold-diversity reranking to improve identification of novel active compounds.", "motivation": "Virtual screening faces three major challenges: class imbalance (low active rate), structural imbalance (certain scaffolds dominate), and the need to identify structurally diverse active compounds for novel drug development.", "method": "Three-module framework: 1) Augmentation module using graph diffusion model to generate synthetic data conditioned on scaffolds, 2) Model-agnostic self-training to integrate synthetic and original data, 3) Reranking module to enhance scaffold diversity in top recommendations.", "result": "Comprehensive computational experiments across five target classes show ScaffAug outperforms baseline methods in multiple evaluation metrics, maintaining and enhancing overall performance while improving scaffold diversity.", "conclusion": "ScaffAug introduces novel perspectives on enhancing virtual screening through generative augmentations, reranking, and scaffold-awareness, effectively addressing key challenges in drug discovery."}}
{"id": "2510.16311", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16311", "abs": "https://arxiv.org/abs/2510.16311", "authors": ["Daohan Su", "Yang Zhang", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "Toward General Digraph Contrastive Learning: A Dual Spatial Perspective", "comment": null, "summary": "Graph Contrastive Learning (GCL) has emerged as a powerful tool for\nextracting consistent representations from graphs, independent of labeled\ninformation. However, existing methods predominantly focus on undirected\ngraphs, disregarding the pivotal directional information that is fundamental\nand indispensable in real-world networks (e.g., social networks and\nrecommendations).In this paper, we introduce S2-DiGCL, a novel framework that\nemphasizes spatial insights from complex and real domain perspectives for\ndirected graph (digraph) contrastive learning. From the complex-domain\nperspective, S2-DiGCL introduces personalized perturbations into the magnetic\nLaplacian to adaptively modulate edge phases and directional semantics. From\nthe real-domain perspective, it employs a path-based subgraph augmentation\nstrategy to capture fine-grained local asymmetries and topological\ndependencies. By jointly leveraging these two complementary spatial views,\nS2-DiGCL constructs high-quality positive and negative samples, leading to more\ngeneral and robust digraph contrastive learning. Extensive experiments on 7\nreal-world digraph datasets demonstrate the superiority of our approach,\nachieving SOTA performance with 4.41% improvement in node classification and\n4.34% in link prediction under both supervised and unsupervised settings.", "AI": {"tldr": "S2-DiGCL is a novel directed graph contrastive learning framework that leverages both complex-domain (via magnetic Laplacian perturbations) and real-domain (via path-based subgraph augmentation) perspectives to capture directional information and spatial dependencies in digraphs, achieving state-of-the-art performance.", "motivation": "Existing graph contrastive learning methods primarily focus on undirected graphs and ignore crucial directional information that is fundamental in real-world networks like social networks and recommendation systems.", "method": "S2-DiGCL uses two complementary approaches: 1) Complex-domain perspective with personalized perturbations in magnetic Laplacian to modulate edge phases and directional semantics, and 2) Real-domain perspective with path-based subgraph augmentation to capture local asymmetries and topological dependencies.", "result": "Extensive experiments on 7 real-world digraph datasets show S2-DiGCL achieves state-of-the-art performance with 4.41% improvement in node classification and 4.34% improvement in link prediction under both supervised and unsupervised settings.", "conclusion": "The proposed S2-DiGCL framework effectively addresses the limitations of existing GCL methods by incorporating directional information through complementary complex and real-domain spatial perspectives, leading to more general and robust digraph representations."}}
{"id": "2510.16322", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16322", "abs": "https://arxiv.org/abs/2510.16322", "authors": ["Mo Zhou", "Haoyang Ma", "Rong Ge"], "title": "Memorizing Long-tail Data Can Help Generalization Through Composition", "comment": "30 pages", "summary": "Deep learning has led researchers to rethink the relationship between\nmemorization and generalization. In many settings, memorization does not hurt\ngeneralization due to implicit regularization and may help by memorizing\nlong-tailed examples. In this paper, we consider the synergy between\nmemorization and simple composition -- the ability to make correct prediction\non a combination of long-tailed features. Theoretically, we show that for a\nlinear setting, memorization together with composition can help the model make\ncorrect predictions on rare test examples that require a combination of\nlong-tailed features, even if such combinations were never observed in the\ntraining data. Experiments on neural network architecture on simple data show\nthat the theoretical insight extends beyond the linear setting, and we further\nobserve that the composition capability of the model depends on its\narchitecture.", "AI": {"tldr": "The paper explores how memorization combined with composition helps models generalize to rare test examples with unseen combinations of long-tailed features, even in linear settings.", "motivation": "To understand the relationship between memorization and generalization in deep learning, particularly how memorization of long-tailed examples can aid in compositional generalization.", "method": "Theoretical analysis in a linear setting and experiments with neural network architectures on simple data to validate the theoretical insights.", "result": "Memorization with composition enables correct predictions on rare test examples with unseen combinations of long-tailed features. The composition capability varies with model architecture.", "conclusion": "Memorization and composition synergize to enhance generalization, especially for rare examples with novel feature combinations, with architecture playing a key role in composition capability."}}
{"id": "2510.16350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16350", "abs": "https://arxiv.org/abs/2510.16350", "authors": ["Shule Hao", "Junpeng Bao", "Wenli Li"], "title": "MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting", "comment": null, "summary": "Recent research in time series forecasting has explored integrating\nmultimodal features into models to improve accuracy. However, the accuracy of\nsuch methods is constrained by three key challenges: inadequate extraction of\nfine-grained temporal patterns, suboptimal integration of multimodal\ninformation, and limited adaptability to dynamic multi-scale features. To\naddress these problems, we propose MGTS-Net, a Multimodal Graph-enhanced\nNetwork for Time Series forecasting. The model consists of three core\ncomponents: (1) a Multimodal Feature Extraction layer (MFE), which optimizes\nfeature encoders according to the characteristics of temporal, visual, and\ntextual modalities to extract temporal features of fine-grained patterns; (2) a\nMultimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph\nto model intra-modal temporal dependencies and cross-modal alignment\nrelationships and dynamically aggregates multimodal knowledge; (3) a\nMulti-Scale Prediction layer (MSP), which adapts to multi-scale features by\ndynamically weighting and fusing the outputs of short-term, medium-term, and\nlong-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits\nexcellent performance with light weight and high efficiency. Compared with\nother state-of-the-art baseline models, our method achieves superior\nperformance, validating the superiority of the proposed methodology.", "AI": {"tldr": "MGTS-Net is a multimodal graph-enhanced network for time series forecasting that addresses challenges in fine-grained temporal pattern extraction, multimodal integration, and multi-scale feature adaptation through three core components: multimodal feature extraction, fusion via heterogeneous graphs, and multi-scale prediction.", "motivation": "Current multimodal time series forecasting methods face limitations in extracting fine-grained temporal patterns, optimally integrating multimodal information, and adapting to dynamic multi-scale features, which constrains their accuracy.", "method": "MGTS-Net consists of three components: (1) Multimodal Feature Extraction layer that optimizes encoders for temporal, visual, and textual modalities; (2) Multimodal Feature Fusion layer that constructs heterogeneous graphs to model temporal dependencies and cross-modal relationships; (3) Multi-Scale Prediction layer that dynamically weights short-term, medium-term, and long-term predictors.", "result": "Extensive experiments show MGTS-Net achieves excellent performance with lightweight and high efficiency, outperforming state-of-the-art baseline models.", "conclusion": "The proposed MGTS-Net methodology demonstrates superior performance in multimodal time series forecasting, validating the effectiveness of its graph-enhanced approach for addressing key challenges in the field."}}
{"id": "2510.16440", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16440", "abs": "https://arxiv.org/abs/2510.16440", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition.", "AI": {"tldr": "Winning adversarial attack solution for Task 1 of Colliding with Adversaries challenge using multi-round gradient-based strategy with random initialization and sample-mixing.", "motivation": "To design an effective adversarial attack that maximizes misclassification while minimizing perturbations for a classification model in a high energy physics competition.", "method": "Multi-round gradient-based strategy leveraging the model's differentiable structure, augmented with random initialization and sample-mixing techniques.", "result": "Achieved best results in perturbation size and fooling success rate, securing first place in the competition.", "conclusion": "The proposed adversarial attack method successfully won the competition by effectively balancing misclassification maximization with minimal perturbations."}}
{"id": "2510.16443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16443", "abs": "https://arxiv.org/abs/2510.16443", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 2 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The goal of the challenge was to design and train a robust\nANN-based model capable of achieving high accuracy in a binary classification\ntask on both clean and adversarial data generated with the Random Distribution\nShuffle Attack (RDSA). Our solution consists of two components: a data\ngeneration phase and a robust model training phase. In the first phase, we\nproduced 15 million artificial training samples using a custom methodology\nderived from Random Distribution Shuffle Attack (RDSA). In the second phase, we\nintroduced a robust architecture comprising (i)a Feature Embedding Block with\nshared weights among features of the same type and (ii)a Dense Fusion Tail\nresponsible for the final prediction. Training this architecture on our\nadversarial dataset achieved a mixed accuracy score of 80\\%, exceeding the\nsecond-place solution by two percentage points.", "AI": {"tldr": "Winning solution for ECML-PKDD 2025 challenge on robust learning in high energy physics, achieving 80% mixed accuracy on clean and adversarial data using custom data generation and robust neural network architecture.", "motivation": "To design a robust ANN-based model that maintains high accuracy on both clean and adversarial data in high energy physics discovery, specifically addressing the Random Distribution Shuffle Attack (RDSA).", "method": "Two-phase approach: (1) Data generation phase producing 15 million artificial training samples using custom RDSA methodology, (2) Robust model training with Feature Embedding Block (shared weights for same feature types) and Dense Fusion Tail for final prediction.", "result": "Achieved 80% mixed accuracy score on both clean and adversarial data, outperforming the second-place solution by 2 percentage points.", "conclusion": "The proposed two-phase approach with custom data generation and robust neural architecture successfully addresses adversarial robustness in high energy physics classification tasks, demonstrating superior performance in the challenge."}}
{"id": "2510.16448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16448", "abs": "https://arxiv.org/abs/2510.16448", "authors": ["Yongxiang Hua", "Haoyu Cao", "Zhou Tao", "Bocheng Li", "Zihao Wu", "Chaohu Liu", "Linli Xu"], "title": "Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts", "comment": "ACM MM25", "summary": "Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling\nlarge vision-language models, offering substantial capacity while maintaining\ncomputational efficiency through dynamic, sparse activation of experts.\nHowever, existing routing mechanisms, typically based on similarity scoring,\nstruggle to effectively capture the underlying input structure. This limitation\nleads to a trade-off between expert specialization and balanced computation,\nhindering both scalability and performance. We propose Input Domain Aware MoE,\na novel routing framework that leverages a probabilistic mixture model to\nbetter partition the input space. By modeling routing probabilities as a\nmixture of distributions, our method enables experts to develop clear\nspecialization boundaries while achieving balanced utilization. Unlike\nconventional approaches, our routing mechanism is trained independently of\ntask-specific objectives, allowing for stable optimization and decisive expert\nassignments. Empirical results on vision-language tasks demonstrate that our\nmethod consistently outperforms existing sMoE approaches, achieving higher task\nperformance and improved expert utilization balance.", "AI": {"tldr": "Proposes Input Domain Aware MoE, a novel routing framework using probabilistic mixture models to better partition input space in sparse Mixture of Experts, achieving improved specialization and balanced utilization compared to conventional similarity-based routing.", "motivation": "Existing routing mechanisms in sparse Mixture of Experts struggle to capture input structure effectively, creating a trade-off between expert specialization and balanced computation that hinders scalability and performance.", "method": "Leverages a probabilistic mixture model to partition input space, modeling routing probabilities as mixture distributions to enable clear expert specialization boundaries while maintaining balanced utilization. The routing mechanism is trained independently of task-specific objectives for stable optimization.", "result": "Empirical results on vision-language tasks show consistent outperformance of existing sMoE approaches, achieving higher task performance and improved expert utilization balance.", "conclusion": "The proposed Input Domain Aware MoE framework effectively addresses limitations of conventional routing mechanisms, enabling better input space partitioning and expert specialization while maintaining computational efficiency."}}
{"id": "2510.16474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16474", "abs": "https://arxiv.org/abs/2510.16474", "authors": ["Farwa Abbas", "Hussain Ahmad", "Claudia Szabo"], "title": "SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning", "comment": null, "summary": "High-dimensional, heterogeneous data with complex feature interactions pose\nsignificant challenges for traditional predictive modeling approaches. While\nProjection to Latent Structures (PLS) remains a popular technique, it struggles\nto model complex non-linear relationships, especially in multivariate systems\nwith high-dimensional correlation structures. This challenge is further\ncompounded by simultaneous interactions across multiple scales, where local\nprocessing fails to capture crossgroup dependencies. Additionally, static\nfeature weighting limits adaptability to contextual variations, as it ignores\nsample-specific relevance. To address these limitations, we propose a novel\nmethod that enhances predictive performance through novel architectural\ninnovations. Our architecture introduces an adaptive kernel-based attention\nmechanism that processes distinct feature groups separately before integration,\nenabling capture of local patterns while preserving global relationships.\nExperimental results show substantial improvements in performance metrics,\ncompared to the state-of-the-art methods across diverse datasets.", "AI": {"tldr": "A novel method using adaptive kernel-based attention to handle high-dimensional heterogeneous data with complex feature interactions, outperforming state-of-the-art approaches.", "motivation": "Traditional methods like PLS struggle with non-linear relationships and high-dimensional correlations in multivariate systems, and static feature weighting lacks adaptability to contextual variations.", "method": "Proposes an architecture with adaptive kernel-based attention that processes distinct feature groups separately before integration, capturing both local patterns and global relationships.", "result": "Experimental results demonstrate substantial improvements in performance metrics compared to state-of-the-art methods across diverse datasets.", "conclusion": "The proposed method effectively addresses limitations of traditional approaches by enabling better modeling of complex non-linear relationships and adaptive feature relevance."}}
{"id": "2510.16547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16547", "abs": "https://arxiv.org/abs/2510.16547", "authors": ["Alif Elham Khan", "Mohammad Junayed Hasan", "Humayra Anjum", "Nabeel Mohammed", "Sifat Momen"], "title": "Predicting life satisfaction using machine learning and explainable AI", "comment": null, "summary": "Life satisfaction is a crucial facet of human well-being. Hence, research on\nlife satisfaction is incumbent for understanding how individuals experience\ntheir lives and influencing interventions targeted at enhancing mental health\nand well-being. Life satisfaction has traditionally been measured using analog,\ncomplicated, and frequently error-prone methods. These methods raise questions\nconcerning validation and propagation. However, this study demonstrates the\npotential for machine learning algorithms to predict life satisfaction with a\nhigh accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a\ngovernment survey of 19000 people aged 16-64 years in Denmark. Using feature\nlearning techniques, 27 significant questions for assessing contentment were\nextracted, making the study highly reproducible, simple, and easily\ninterpretable. Furthermore, clinical and biomedical large language models\n(LLMs) were explored for predicting life satisfaction by converting tabular\ndata into natural language sentences through mapping and adding meaningful\ncounterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It\nwas found that life satisfaction prediction is more closely related to the\nbiomedical domain than the clinical domain. Ablation studies were also\nconducted to understand the impact of data resampling and feature selection\ntechniques on model performance. Moreover, the correlation between primary\ndeterminants with different age brackets was analyzed, and it was found that\nhealth condition is the most important determinant across all ages. This study\ndemonstrates how machine learning, large language models and XAI can jointly\ncontribute to building trust and understanding in using AI to investigate human\nbehavior, with significant ramifications for academics and professionals\nworking to quantify and comprehend subjective well-being.", "AI": {"tldr": "Machine learning and large language models can predict life satisfaction with high accuracy (93.80% and 93.74% respectively) using Danish government survey data, identifying health condition as the most important determinant across all age groups.", "motivation": "Traditional methods for measuring life satisfaction are analog, complicated, and error-prone, raising validation concerns. This study aims to leverage AI to create more reliable, reproducible, and interpretable methods for assessing subjective well-being.", "method": "Used Danish government survey data of 19,000 people aged 16-64. Applied feature learning to extract 27 significant questions, explored clinical and biomedical LLMs by converting tabular data to natural language sentences, and conducted ablation studies on data resampling and feature selection.", "result": "Achieved 93.80% accuracy with machine learning and 93.74% accuracy with LLMs. Biomedical domain performed better than clinical domain for life satisfaction prediction. Health condition was identified as the most important determinant across all age groups.", "conclusion": "Machine learning, LLMs, and XAI can jointly build trust in using AI to investigate human behavior, with significant implications for academics and professionals working on subjective well-being quantification and comprehension."}}
{"id": "2510.16548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16548", "abs": "https://arxiv.org/abs/2510.16548", "authors": ["Zitao Fang", "Chenxuan Li", "Hongting Zhou", "Shuyang Yu", "Guodong Du", "Ashwaq Qasem", "Yang Lu", "Jing Li", "Junsong Zhang", "Sim Kuan Goh"], "title": "NeurIPT: Foundation Model for Neural Interfaces", "comment": "Accepted by The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025). Project Page:\n  https://ZzzitaoFang.github.io/projects/NeurIPT/", "summary": "Electroencephalography (EEG) has wide-ranging applications, from clinical\ndiagnosis to brain-computer interfaces (BCIs). With the increasing volume and\nvariety of EEG data, there has been growing interest in establishing foundation\nmodels (FMs) to scale up and generalize neural decoding. Despite showing early\npotential, applying FMs to EEG remains challenging due to substantial\ninter-subject, inter-task, and inter-condition variability, as well as diverse\nelectrode configurations across recording setups. To tackle these open\nchallenges, we propose NeurIPT, a foundation model developed for diverse\nEEG-based Neural Interfaces with a Pre-trained Transformer by capturing both\nhomogeneous and heterogeneous spatio-temporal characteristics inherent in EEG\nsignals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),\nmasking based on signal amplitude rather than random intervals, to learn robust\nrepresentations across varying signal intensities beyond local interpolation.\nMoreover, this temporal representation is enhanced by a Progressive\nMixture-of-Experts (PMoE) architecture, where specialized expert subnetworks\nare progressively introduced at deeper layers, adapting effectively to the\ndiverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages\nthe 3D physical coordinates of electrodes, enabling effective transfer of\nembedding across varying EEG settings, and develops Intra-Inter Lobe Pooling\n(IILP) during fine-tuning to efficiently exploit regional brain features.\nEmpirical evaluations across eight downstream BCI datasets, via fine-tuning,\ndemonstrated NeurIPT consistently achieved state-of-the-art performance,\nhighlighting its broad applicability and robust generalization. Our work pushes\nforward the state of FMs in EEG and offers insights into scalable and\ngeneralizable neural information processing systems.", "AI": {"tldr": "NeurIPT is a foundation model for EEG-based neural interfaces that uses a pre-trained transformer to handle variability in EEG data through amplitude-aware masking, progressive mixture-of-experts architecture, and spatial electrode coordinate integration.", "motivation": "EEG data faces challenges with inter-subject, inter-task, and inter-condition variability, plus diverse electrode configurations. Current foundation models struggle to generalize across these variations in EEG applications.", "method": "Proposes NeurIPT with: 1) Amplitude-Aware Masked Pretraining (AAMP) for temporal representation, 2) Progressive Mixture-of-Experts (PMoE) architecture for diverse temporal characteristics, 3) 3D electrode coordinate embeddings for spatial transfer, and 4) Intra-Inter Lobe Pooling (IILP) for regional brain feature exploitation.", "result": "Evaluated on eight downstream BCI datasets via fine-tuning, NeurIPT consistently achieved state-of-the-art performance, demonstrating broad applicability and robust generalization across diverse EEG settings.", "conclusion": "NeurIPT advances foundation models for EEG by providing scalable and generalizable neural information processing, effectively handling the inherent variability in EEG signals and electrode configurations."}}
{"id": "2510.16552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16552", "abs": "https://arxiv.org/abs/2510.16552", "authors": ["Ang Li", "Yifei Wang", "Zhihang Yuan", "Stefanie Jegelka", "Yisen Wang"], "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement learning in large language models (LLMs) often relies on scalar\nrewards, a practice that discards valuable textual rationale buried in the\nrollouts, forcing the model to explore \\textit{de novo} with each attempt and\nhindering sample efficiency. While LLMs can uniquely learn from language\nfeedback provided in-context, naively integrating on-line experiences into RL\ntraining presents a paradox: feedback from the same problem risks information\nleakage and memorization, while feedback from different problems often leads to\nbehavior collapse due to irrelevant context. To resolve this tension, we\npropose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a\nframework that cleanly separates the roles of feedback: language guides\nexploration, while numerical rewards drive optimization. LANPO builds a dynamic\nexperience pool from past trials and introduces two principles to ensure\nfeedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample\nself-correction and \\emph{Relevant Abstraction} to distill generalizable\nlessons from inter-sample experiences. Across mathematical reasoning\nbenchmarks, LANPO enables 7B and 14B models to significantly outperform strong\nbaselines trained with GRPO in test accuracy. Our work provides a robust method\nfor integrating historical experiences into the LLM RL loop, creating more\neffective and data-efficient learning agents.", "AI": {"tldr": "LANPO is a reinforcement learning framework that separates language feedback for exploration from numerical rewards for optimization, using historical experiences to improve sample efficiency in LLM training.", "motivation": "Traditional RL in LLMs uses scalar rewards that discard valuable textual rationale from rollouts, forcing models to explore from scratch each time and reducing sample efficiency. There's a paradox where using feedback from the same problem risks memorization, while different problems cause behavior collapse.", "method": "LANPO builds a dynamic experience pool from past trials with two key principles: Reward-Agnostic Reflection for safe intra-sample self-correction and Relevant Abstraction to distill generalizable lessons from inter-sample experiences. It cleanly separates language feedback for exploration from numerical rewards for optimization.", "result": "Across mathematical reasoning benchmarks, LANPO enables 7B and 14B models to significantly outperform strong baselines trained with GRPO in test accuracy.", "conclusion": "LANPO provides a robust method for integrating historical experiences into the LLM RL loop, creating more effective and data-efficient learning agents by resolving the tension between using same-problem and different-problem feedback."}}
{"id": "2510.16590", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.16590", "abs": "https://arxiv.org/abs/2510.16590", "authors": ["Alan Kai Hassen", "Andrius Bernatavicius", "Antonius P. A. Janssen", "Mike Preuss", "Gerard J. P. van Westen", "Djork-Arn\u00e9 Clevert"], "title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration", "comment": "Alan Kai Hassen and Andrius Bernatavicius contributed equally to this\n  work", "summary": "Applications of machine learning in chemistry are often limited by the\nscarcity and expense of labeled data, restricting traditional supervised\nmethods. In this work, we introduce a framework for molecular reasoning using\ngeneral-purpose Large Language Models (LLMs) that operates without requiring\nlabeled training data. Our method anchors chain-of-thought reasoning to the\nmolecular structure by using unique atomic identifiers. First, the LLM performs\na one-shot task to identify relevant fragments and their associated chemical\nlabels or transformation classes. In an optional second step, this\nposition-aware information is used in a few-shot task with provided class\nexamples to predict the chemical transformation. We apply our framework to\nsingle-step retrosynthesis, a task where LLMs have previously underperformed.\nAcross academic benchmarks and expert-validated drug discovery molecules, our\nwork enables LLMs to achieve high success rates in identifying chemically\nplausible reaction sites ($\\geq90\\%$), named reaction classes ($\\geq40\\%$), and\nfinal reactants ($\\geq74\\%$). Beyond solving complex chemical tasks, our work\nalso provides a method to generate theoretically grounded synthetic datasets by\nmapping chemical knowledge onto the molecular structure and thereby addressing\ndata scarcity.", "AI": {"tldr": "A framework for molecular reasoning using LLMs without labeled data, achieving high success rates in retrosynthesis tasks by anchoring chain-of-thought reasoning to molecular structures.", "motivation": "Overcoming the scarcity and expense of labeled data in chemistry applications by enabling LLMs to perform molecular reasoning without requiring labeled training data.", "method": "Uses unique atomic identifiers to anchor chain-of-thought reasoning to molecular structure, performing one-shot task to identify relevant fragments and optional few-shot task for chemical transformation prediction.", "result": "Achieved high success rates: \u226590% for chemically plausible reaction sites, \u226540% for named reaction classes, and \u226574% for final reactants across academic benchmarks and drug discovery molecules.", "conclusion": "The framework enables LLMs to solve complex chemical tasks without labeled data and provides a method to generate synthetic datasets by mapping chemical knowledge onto molecular structures, addressing data scarcity."}}
{"id": "2510.16607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16607", "abs": "https://arxiv.org/abs/2510.16607", "authors": ["Tianwei Wang", "Xinhui Ma", "Wei Pang"], "title": "Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules", "comment": null, "summary": "Motivated by the geometric advantages of quaternions in representing\nrotations and postures, we propose a quaternion-valued supervised learning\nHopfield-structured neural network (QSHNN) with a fully connected structure\ninspired by the classic Hopfield neural network (HNN). Starting from a\ncontinuous-time dynamical model of HNNs, we extend the formulation to the\nquaternionic domain and establish the existence and uniqueness of fixed points\nwith asymptotic stability. For the learning rules, we introduce a periodic\nprojection strategy that modifies standard gradient descent by periodically\nprojecting each 4*4 block of the weight matrix onto the closest quaternionic\nstructure in the least-squares sense. This approach preserves both convergence\nand quaternionic consistency throughout training. Benefiting from this rigorous\nmathematical foundation, the experimental model implementation achieves high\naccuracy, fast convergence, and strong reliability across randomly generated\ntarget sets. Moreover, the evolution trajectories of the QSHNN exhibit\nwell-bounded curvature, i.e., sufficient smoothness, which is crucial for\napplications such as control systems or path planning modules in robotic arms,\nwhere joint postures are parameterized by quaternion neurons. Beyond these\napplication scenarios, the proposed model offers a practical implementation\nframework and a general mathematical methodology for designing neural networks\nunder hypercomplex or non-commutative algebraic structures.", "AI": {"tldr": "A quaternion-valued supervised learning Hopfield neural network (QSHNN) is proposed, leveraging quaternions' geometric advantages for rotation representation. It extends classic HNNs to quaternionic domain with proven stability and uses periodic projection learning to maintain quaternionic structure while achieving high accuracy and smooth trajectories.", "motivation": "Motivated by the geometric advantages of quaternions in representing rotations and postures, particularly for applications like robotic arm control and path planning where joint postures are parameterized by quaternion neurons.", "method": "Extends continuous-time dynamical model of Hopfield neural networks to quaternionic domain, establishes existence and uniqueness of fixed points with asymptotic stability, and introduces periodic projection strategy that modifies gradient descent by periodically projecting weight matrix blocks onto closest quaternionic structure.", "result": "Experimental implementation achieves high accuracy, fast convergence, and strong reliability across randomly generated target sets. Evolution trajectories exhibit well-bounded curvature and sufficient smoothness, crucial for control systems and robotic applications.", "conclusion": "The proposed QSHNN offers both a practical implementation framework and general mathematical methodology for designing neural networks under hypercomplex or non-commutative algebraic structures, with demonstrated advantages for applications requiring smooth rotational representations."}}
{"id": "2510.16609", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16609", "abs": "https://arxiv.org/abs/2510.16609", "authors": ["Avrim Blum", "Daniel Hsu", "Cyrus Rashtchian", "Donya Saless"], "title": "Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods", "comment": null, "summary": "Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool\nuse, critically depends on an interplay between a model's parametric knowledge\nand externally retrieved information. However, the theoretical underpinnings of\nthis relationship remain poorly understood. Specifically, it is not clear how\nmuch pre-training knowledge is required to answer queries with a small number\nof augmentation steps, which is a desirable property in practice. To address\nthis question, we formulate multi-step reasoning as an $s$-$t$ connectivity\nproblem on a knowledge graph. We represent a model's pre-training parametric\nknowledge as a partial, potentially noisy subgraph. We view augmentation as\nquerying an oracle for true edges that augment the model's knowledge. Then, we\ncharacterize the necessary and sufficient number of augmentation steps for the\nmodel to generate an accurate answer given partial prior knowledge. One key\nresult shows a phase transition: if the prior knowledge graph over $n$ vertices\nis disconnected into small components, then finding a path via augmentation is\ninefficient and requires $\\Omega(\\sqrt{n})$ queries. On the other hand, once\nthe density of correct knowledge surpasses a threshold, forming a giant\ncomponent, we can find paths with an expected constant number of queries.", "AI": {"tldr": "The paper analyzes test-time augmentation in AI models, framing multi-step reasoning as a graph connectivity problem. It shows a phase transition: when prior knowledge is fragmented, augmentation requires many queries, but once a critical density threshold is reached, paths can be found with constant queries.", "motivation": "To understand the theoretical relationship between a model's parametric knowledge and external augmentation, specifically determining how much pre-training knowledge is needed for efficient query answering with limited augmentation steps.", "method": "Formulates multi-step reasoning as an s-t connectivity problem on knowledge graphs, representing pre-training knowledge as a partial/noisy subgraph and augmentation as querying an oracle for true edges. Uses graph theory to characterize necessary augmentation steps.", "result": "Shows a phase transition: when prior knowledge graph is disconnected into small components, \u03a9(\u221an) queries are needed, but once density surpasses a threshold forming a giant component, paths can be found with constant expected queries.", "conclusion": "There exists a critical density threshold for prior knowledge where augmentation efficiency dramatically improves, transitioning from requiring many queries to constant queries once a giant component forms in the knowledge graph."}}
{"id": "2510.16656", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16656", "abs": "https://arxiv.org/abs/2510.16656", "authors": ["Noah El Rimawi-Fine", "Adam Stecklov", "Lucas Nelson", "Mathieu Blanchette", "Alexander Tong", "Stephen Y. Zhang", "Lazar Atanackovic"], "title": "Simulation-free Structure Learning for Stochastic Dynamics", "comment": null, "summary": "Modeling dynamical systems and unraveling their underlying causal\nrelationships is central to many domains in the natural sciences. Various\nphysical systems, such as those arising in cell biology, are inherently\nhigh-dimensional and stochastic in nature, and admit only partial, noisy state\nmeasurements. This poses a significant challenge for addressing the problems of\nmodeling the underlying dynamics and inferring the network structure of these\nsystems. Existing methods are typically tailored either for structure learning\nor modeling dynamics at the population level, but are limited in their ability\nto address both problems together. In this work, we address both problems\nsimultaneously: we present StructureFlow, a novel and principled\nsimulation-free approach for jointly learning the structure and stochastic\npopulation dynamics of physical systems. We showcase the utility of\nStructureFlow for the tasks of structure learning from interventions and\ndynamical (trajectory) inference of conditional population dynamics. We\nempirically evaluate our approach on high-dimensional synthetic systems, a set\nof biologically plausible simulated systems, and an experimental single-cell\ndataset. We show that StructureFlow can learn the structure of underlying\nsystems while simultaneously modeling their conditional population dynamics --\na key step toward the mechanistic understanding of systems behavior.", "AI": {"tldr": "StructureFlow is a simulation-free method that jointly learns network structure and stochastic population dynamics from partial, noisy measurements of high-dimensional physical systems.", "motivation": "Many physical systems like those in cell biology are high-dimensional, stochastic, and only partially observable, making it challenging to model dynamics and infer causal relationships simultaneously. Existing methods focus on either structure learning or population-level dynamics modeling, but not both.", "method": "StructureFlow is a principled simulation-free approach that jointly learns the structure and stochastic population dynamics. It handles structure learning from interventions and dynamical inference of conditional population dynamics.", "result": "The method was evaluated on high-dimensional synthetic systems, biologically plausible simulated systems, and experimental single-cell data. It successfully learned underlying system structures while modeling conditional population dynamics.", "conclusion": "StructureFlow enables simultaneous structure learning and dynamical modeling, representing a key step toward mechanistic understanding of complex system behaviors in domains like cell biology."}}
{"id": "2510.16674", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16674", "abs": "https://arxiv.org/abs/2510.16674", "authors": ["Azam Shirali", "Giri Narasimhan"], "title": "Evaluating protein binding interfaces with PUMBA", "comment": null, "summary": "Protein-protein docking tools help in studying interactions between proteins,\nand are essential for drug, vaccine, and therapeutic development. However, the\naccuracy of a docking tool depends on a robust scoring function that can\nreliably differentiate between native and non-native complexes. PIsToN is a\nstate-of-the-art deep learning-based scoring function that uses Vision\nTransformers in its architecture. Recently, the Mamba architecture has\ndemonstrated exceptional performance in both natural language processing and\ncomputer vision, often outperforming Transformer-based models in their domains.\nIn this study, we introduce PUMBA (Protein-protein interface evaluation with\nVision Mamba), which improves PIsToN by replacing its Vision Transformer\nbackbone with Vision Mamba. This change allows us to leverage Mamba's efficient\nlong-range sequence modeling for sequences of image patches. As a result, the\nmodel's ability to capture both global and local patterns in protein-protein\ninterface features is significantly improved. Evaluation on several\nwidely-used, large-scale public datasets demonstrates that PUMBA consistently\noutperforms its original Transformer-based predecessor, PIsToN.", "AI": {"tldr": "PUMBA improves protein-protein docking accuracy by replacing Vision Transformers with Vision Mamba in the PIsToN scoring function, achieving better performance through enhanced global and local pattern capture.", "motivation": "Current docking tools need robust scoring functions to differentiate native from non-native protein complexes. Vision Mamba has shown superior performance over Transformers in other domains, suggesting potential improvements for protein-protein interface evaluation.", "method": "Replaced the Vision Transformer backbone in PIsToN with Vision Mamba architecture, leveraging Mamba's efficient long-range sequence modeling for image patches to better capture global and local patterns in protein-protein interfaces.", "result": "PUMBA consistently outperforms the original PIsToN model across multiple large-scale public datasets, demonstrating improved scoring accuracy for protein-protein complexes.", "conclusion": "Vision Mamba architecture successfully enhances protein-protein interface evaluation, providing a more effective alternative to Transformer-based approaches for docking scoring functions."}}
{"id": "2510.16676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16676", "abs": "https://arxiv.org/abs/2510.16676", "authors": ["Anindya Sarkar", "Binglin Ji", "Yevgeniy Vorobeychik"], "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory", "comment": "32 pages, 20 figures, Accepted to NeurIPS 2025", "summary": "In many scientific and engineering fields, where acquiring high-quality data\nis expensive--such as medical imaging, environmental monitoring, and remote\nsensing--strategic sampling of unobserved regions based on prior observations\nis crucial for maximizing discovery rates within a constrained budget. The rise\nof powerful generative models, such as diffusion models, has enabled active\ntarget discovery in partially observable environments by leveraging learned\npriors--probabilistic representations that capture underlying structure from\ndata. With guidance from sequentially gathered task-specific observations,\nthese models can progressively refine exploration and efficiently direct\nqueries toward promising regions. However, in domains where learning a strong\nprior is infeasible due to extremely limited data or high sampling cost (such\nas rare species discovery, diagnostics for emerging diseases, etc.), these\nmethods struggle to generalize. To overcome this limitation, we propose a novel\napproach that enables effective active target discovery even in settings with\nuninformative priors, ensuring robust exploration and adaptability in complex\nreal-world scenarios. Our framework is theoretically principled and draws\ninspiration from neuroscience to guide its design. Unlike black-box policies,\nour approach is inherently interpretable, providing clear insights into\ndecision-making. Furthermore, it guarantees a strong, monotonic improvement in\nprior estimates with each new observation, leading to increasingly accurate\nsampling and reinforcing both reliability and adaptability in dynamic settings.\nThrough comprehensive experiments and ablation studies across various domains,\nincluding species distribution modeling and remote sensing, we demonstrate that\nour method substantially outperforms baseline approaches.", "AI": {"tldr": "A novel active target discovery framework that works effectively even with uninformative priors, overcoming limitations of generative models in data-scarce domains through interpretable, theoretically principled design inspired by neuroscience.", "motivation": "Address the challenge of active target discovery in domains where learning strong priors is infeasible due to limited data or high sampling costs (e.g., rare species discovery, emerging disease diagnostics), where existing generative model-based methods struggle to generalize.", "method": "Proposes a theoretically principled framework inspired by neuroscience that enables effective active target discovery with uninformative priors. The approach is interpretable (unlike black-box policies) and guarantees monotonic improvement in prior estimates with each new observation.", "result": "Comprehensive experiments across various domains including species distribution modeling and remote sensing demonstrate that the method substantially outperforms baseline approaches.", "conclusion": "The framework provides robust exploration and adaptability in complex real-world scenarios with limited data, ensuring reliable and increasingly accurate sampling through monotonic improvement in prior estimates."}}
{"id": "2510.16677", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16677", "abs": "https://arxiv.org/abs/2510.16677", "authors": ["Ran Tong", "Jiaqi Liu", "Su Liu", "Xin Hu", "Lanruo Wang"], "title": "Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers", "comment": null, "summary": "We present a compact, strictly causal benchmark for streaming clinical time\nseries on the MIT--BIH Arrhythmia Database using per-second heart rate. Two\ntasks are studied under record-level, non-overlapping splits: near-term\ntachycardia risk (next ten seconds) and one-step heart rate forecasting. We\ncompare a GRU-D (RNN) and a Transformer under matched training budgets against\nstrong non-learned baselines. Evaluation is calibration-aware for\nclassification and proper for forecasting, with temperature scaling and grouped\nbootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the\nTransformer for tachycardia risk, while the Transformer clearly lowers\nforecasting error relative to GRU-D and persistence. Our results show that, in\nlongitudinal monitoring, model choice is task-dependent: compact RNNs remain\ncompetitive for short-horizon risk scoring, whereas compact Transformers\ndeliver clearer gains for point forecasting.", "AI": {"tldr": "A compact benchmark for streaming clinical time series on MIT-BIH Arrhythmia Database comparing GRU-D (RNN) and Transformer models for tachycardia risk prediction and heart rate forecasting under matched training budgets.", "motivation": "To establish a strictly causal benchmark for streaming clinical time series analysis and compare the performance of compact RNNs and Transformers in longitudinal monitoring tasks.", "method": "Used per-second heart rate data from MIT-BIH Arrhythmia Database with record-level, non-overlapping splits. Evaluated two tasks: near-term tachycardia risk (next 10 seconds) and one-step heart rate forecasting. Compared GRU-D and Transformer models against strong non-learned baselines with matched training budgets.", "result": "On MIT-BIH, GRU-D slightly outperformed Transformer for tachycardia risk prediction, while Transformer clearly reduced forecasting error compared to both GRU-D and persistence baseline.", "conclusion": "Model choice in longitudinal monitoring is task-dependent: compact RNNs remain competitive for short-horizon risk scoring, while compact Transformers provide clearer advantages for point forecasting tasks."}}
{"id": "2510.16687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16687", "abs": "https://arxiv.org/abs/2510.16687", "authors": ["Shurong Lin", "Eric D. Kolaczyk", "Adam Smith", "Elliot Paquette"], "title": "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares", "comment": null, "summary": "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization.", "AI": {"tldr": "This paper analyzes noisy stochastic gradient descent (SGD) for privacy-preserving machine learning using a diffusion approach, providing exact behavior analysis in high dimensions and studying a variant that doesn't require gradient sensitivity knowledge.", "motivation": "Prior work provides bounds on statistical risk and privacy loss for noisy SGD, but the exact behavior remains unclear, especially in high-dimensional settings. There's a need to understand the precise dynamics of privacy-preserving optimization algorithms.", "method": "The paper uses a diffusion approach to analyze noisy SGD, providing a continuous-time perspective that captures statistical risk evolution and privacy loss dynamics. It focuses on least squares problems with \u21132 regularization and studies a variant that doesn't require explicit knowledge of gradient sensitivity.", "result": "The diffusion approach enables precise analysis of noisy SGD behavior in high dimensions, capturing both statistical risk evolution and privacy loss dynamics. The method provides insights into the exact behavior of privacy-preserving optimization processes.", "conclusion": "The diffusion-based analysis offers a comprehensive framework for understanding noisy SGD in privacy-preserving machine learning, particularly addressing the gap in exact behavior analysis in high-dimensional settings and providing an alternative approach that eliminates the need for gradient sensitivity knowledge."}}
{"id": "2510.16694", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16694", "abs": "https://arxiv.org/abs/2510.16694", "authors": ["Anthony DiMaggio", "Raghav Sharma", "Gururaj Saileshwar"], "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning", "comment": null, "summary": "Secure federated learning (FL) preserves data privacy during distributed\nmodel training. However, deploying such frameworks across heterogeneous devices\nresults in performance bottlenecks, due to straggler clients with limited\ncomputational or network capabilities, slowing training for all participating\nclients. This paper introduces the first straggler mitigation technique for\nsecure aggregation with deep neural networks. We propose CLIP, a client-side\ninvariant neuron pruning technique coupled with network-aware pruning, that\naddresses compute and network bottlenecks due to stragglers during training\nwith minimal accuracy loss. Our technique accelerates secure FL training by 13%\nto 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an\naccuracy impact of between 1.3% improvement to 2.6% reduction.", "AI": {"tldr": "CLIP introduces client-side invariant neuron pruning with network-aware pruning to mitigate straggler bottlenecks in secure federated learning, accelerating training by 13-34% with minimal accuracy impact.", "motivation": "Secure federated learning preserves data privacy but faces performance bottlenecks from straggler clients with limited computational or network capabilities, which slow down training for all participants.", "method": "Proposes CLIP, a client-side invariant neuron pruning technique combined with network-aware pruning to address compute and network bottlenecks caused by stragglers during training.", "result": "Accelerates secure FL training by 13% to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with accuracy impact ranging from 1.3% improvement to 2.6% reduction.", "conclusion": "CLIP effectively mitigates straggler bottlenecks in secure federated learning while maintaining model accuracy, demonstrating practical viability for heterogeneous device deployments."}}
{"id": "2510.16695", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16695", "abs": "https://arxiv.org/abs/2510.16695", "authors": ["Iman Deznabi", "Peeyush Kumar", "Madalina Fiterau"], "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting", "comment": null, "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen\nconditions without direct historical data, posing a significant challenge for\ntraditional forecasting methods. We introduce a Resolution-Aware\nRetrieval-Augmented Forecasting model that enhances predictive accuracy by\nleveraging spatial correlations and temporal frequency characteristics. By\ndecomposing signals into different frequency components, our model employs\nresolution-aware retrieval, where lower-frequency components rely on broader\nspatial context, while higher-frequency components focus on local influences.\nThis allows the model to dynamically retrieve relevant data and adapt to new\nlocations with minimal historical context.\n  Applied to microclimate forecasting, our model significantly outperforms\ntraditional forecasting methods, numerical weather prediction models, and\nmodern foundation time series models, achieving 71% lower MSE than HRRR and 34%\nlower MSE than Chronos on the ERA5 dataset.\n  Our results highlight the effectiveness of retrieval-augmented and\nresolution-aware strategies, offering a scalable and data-efficient solution\nfor zero-shot forecasting in microclimate modeling and beyond.", "AI": {"tldr": "A Resolution-Aware Retrieval-Augmented Forecasting model for zero-shot forecasting that leverages spatial correlations and temporal frequency decomposition to predict outcomes for unseen conditions without direct historical data.", "motivation": "Traditional forecasting methods struggle with zero-shot forecasting where predictions are needed for conditions without direct historical data, particularly in microclimate modeling where spatial correlations and temporal patterns are crucial.", "method": "Decomposes signals into different frequency components and employs resolution-aware retrieval - lower-frequency components use broader spatial context while higher-frequency components focus on local influences, enabling dynamic data retrieval and adaptation to new locations.", "result": "Significantly outperforms traditional forecasting methods, numerical weather prediction models, and modern foundation time series models, achieving 71% lower MSE than HRRR and 34% lower MSE than Chronos on the ERA5 dataset.", "conclusion": "The retrieval-augmented and resolution-aware strategies provide an effective, scalable, and data-efficient solution for zero-shot forecasting in microclimate modeling and other domains."}}
{"id": "2510.16719", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16719", "abs": "https://arxiv.org/abs/2510.16719", "authors": ["Zak Ressler", "Marcus Grijalva", "Angelica Marie Ignacio", "Melanie Torres", "Abelardo Cuadra Rojas", "Rohollah Moghadam", "Mohammad Rasoul narimani"], "title": "LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus", "comment": null, "summary": "This paper presents a framework for processing EV charging load data in order\nto forecast future load predictions using a Recurrent Neural Network,\nspecifically an LSTM. The framework processes a large set of raw data from\nmultiple locations and transforms it with normalization and feature extraction\nto train the LSTM. The pre-processing stage corrects for missing or incomplete\nvalues by interpolating and normalizing the measurements. This information is\nthen fed into a Long Short-Term Memory Model designed to capture the short-term\nfluctuations while also interpreting the long-term trends in the charging data.\nExperimental results demonstrate the model's ability to accurately predict\ncharging demand across multiple time scales (daily, weekly, and monthly),\nproviding valuable insights for infrastructure planning, energy management, and\ngrid integration of EV charging facilities. The system's modular design allows\nfor adaptation to different charging locations with varying usage patterns,\nmaking it applicable across diverse deployment scenarios.", "AI": {"tldr": "A framework using LSTM neural networks to forecast EV charging load by processing raw data through normalization and feature extraction, achieving accurate predictions across daily, weekly, and monthly time scales.", "motivation": "To address the growing need for reliable EV charging demand forecasting to support infrastructure planning, energy management, and grid integration of charging facilities.", "method": "Processes raw EV charging data from multiple locations using interpolation for missing values, normalization, and feature extraction, then trains a Long Short-Term Memory (LSTM) neural network to capture both short-term fluctuations and long-term trends.", "result": "Experimental results show the model accurately predicts charging demand across multiple time scales (daily, weekly, monthly) and can adapt to different charging locations with varying usage patterns.", "conclusion": "The proposed LSTM-based framework provides an effective solution for EV charging load forecasting that is modular, adaptable to diverse deployment scenarios, and valuable for infrastructure planning and grid management."}}
{"id": "2510.16743", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16743", "abs": "https://arxiv.org/abs/2510.16743", "authors": ["Viktoria Schram", "Markus Hiller", "Daniel Beck", "Trevor Cohn"], "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "comment": "Accepted to NeurIPS 2025", "summary": "The prediction of learning curves for Natural Language Processing (NLP)\nmodels enables informed decision-making to meet specific performance\nobjectives, while reducing computational overhead and lowering the costs\nassociated with dataset acquisition and curation. In this work, we formulate\nthe prediction task as a multitask learning problem, where each task's data is\nmodelled as being organized within a two-layer hierarchy. To model the shared\ninformation and dependencies across tasks and hierarchical levels, we employ\nlatent variable multi-output Gaussian Processes, enabling to account for task\ncorrelations and supporting zero-shot prediction of learning curves (LCs). We\ndemonstrate that this approach facilitates the development of probabilistic\nscaling laws at lower costs. Applying an active learning strategy, LCs can be\nqueried to reduce predictive uncertainty and provide predictions close to\nground truth scaling laws. We validate our framework on three small-scale NLP\ndatasets with up to $30$ LCs. These are obtained from nanoGPT models, from\nbilingual translation using mBART and Transformer models, and from multilingual\ntranslation using M2M100 models of varying sizes.", "AI": {"tldr": "This paper presents a multitask learning approach using latent variable multi-output Gaussian Processes to predict learning curves for NLP models, enabling probabilistic scaling laws and active learning for reduced computational costs.", "motivation": "To enable informed decision-making for NLP model development by predicting learning curves, reducing computational overhead and costs associated with dataset acquisition and curation.", "method": "Formulates learning curve prediction as a multitask learning problem with two-layer hierarchical data organization, using latent variable multi-output Gaussian Processes to model task correlations and support zero-shot prediction.", "result": "The approach facilitates development of probabilistic scaling laws at lower costs, with active learning reducing predictive uncertainty and providing predictions close to ground truth scaling laws. Validated on three NLP datasets with up to 30 learning curves from various models.", "conclusion": "The proposed framework successfully predicts learning curves for NLP models using multitask Gaussian Processes, enabling cost-effective development of probabilistic scaling laws through active learning strategies."}}
{"id": "2510.16747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16747", "abs": "https://arxiv.org/abs/2510.16747", "authors": ["Danish Nazir", "Gowtham Sai Inti", "Timo Bartels", "Jan Piewek", "Thorsten Bagdonat", "Tim Fingscheidt"], "title": "An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications", "comment": null, "summary": "Modern automotive systems leverage deep neural networks (DNNs) for semantic\nsegmentation and operate in two key application areas: (1) In-car, where the\nDNN solely operates in the vehicle without strict constraints on the data rate.\n(2) Distributed, where one DNN part operates in the vehicle and the other part\ntypically on a large-scale cloud platform with a particular constraint on\ntransmission bitrate efficiency. Typically, both applications share an image\nand source encoder, while each uses distinct (joint) source and task decoders.\nPrior work utilized convolutional neural networks for joint source and task\ndecoding but did not investigate transformer-based alternatives such as\nSegDeformer, which offer superior performance at the cost of higher\ncomputational complexity. In this work, we propose joint feature and task\ndecoding for SegDeformer, thereby enabling lower computational complexity in\nboth in-car and distributed applications, despite SegDeformer's computational\ndemands. This improves scalability in the cloud while reducing in-car\ncomputational complexity. For the in-car application, we increased the frames\nper second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on\nCityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on\nADE20K, while being on-par w.r.t.\\ the mean intersection over union (mIoU) of\nthe transformer-based baseline that doesn't compress by a source codec. For the\ndistributed application, we achieve state-of-the-art (SOTA) over a wide range\nof bitrates on the mIoU metric, while using only $0.14$\\% ($0.04$\\%) of cloud\nDNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).", "AI": {"tldr": "Proposed joint feature and task decoding for SegDeformer transformer model to reduce computational complexity in automotive semantic segmentation applications, achieving significant speed improvements for in-car use and state-of-the-art performance with minimal cloud parameters for distributed applications.", "motivation": "Modern automotive systems use DNNs for semantic segmentation in two scenarios: in-car (no data rate constraints) and distributed (vehicle-cloud with bitrate constraints). Prior work used CNNs but didn't explore transformer alternatives like SegDeformer, which offer better performance but higher computational complexity.", "method": "Proposed joint feature and task decoding for SegDeformer transformer model, enabling lower computational complexity in both in-car and distributed automotive applications despite SegDeformer's inherent computational demands.", "result": "For in-car: Increased fps by up to 11.7x (1.4 to 16.5 fps) on Cityscapes and 3.5x (43.3 to 154.3 fps) on ADE20K while maintaining comparable mIoU. For distributed: Achieved SOTA mIoU across bitrates using only 0.14% (ADE20K) and 0.04% (Cityscapes) of cloud DNN parameters compared to previous SOTA.", "conclusion": "The proposed joint decoding approach successfully enables transformer-based SegDeformer for automotive semantic segmentation, significantly improving computational efficiency for both in-car and distributed applications while maintaining competitive performance metrics."}}
{"id": "2510.16757", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16757", "abs": "https://arxiv.org/abs/2510.16757", "authors": ["Young In Kim", "Andrea Agiollo", "Rajiv Khanna"], "title": "SAMOSA: Sharpness Aware Minimization for Open Set Active learning", "comment": null, "summary": "Modern machine learning solutions require extensive data collection where\nlabeling remains costly. To reduce this burden, open set active learning\napproaches aim to select informative samples from a large pool of unlabeled\ndata that includes irrelevant or unknown classes. In this context, we propose\nSharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an\neffective querying algorithm. Building on theoretical findings concerning the\nimpact of data typicality on the generalization properties of traditional\nstochastic gradient descent (SGD) and sharpness-aware minimization (SAM),\nSAMOSA actively queries samples based on their typicality. SAMOSA effectively\nidentifies atypical samples that belong to regions of the embedding manifold\nclose to the model decision boundaries. Therefore, SAMOSA prioritizes the\nsamples that are (i) highly informative for the targeted classes, and (ii)\nuseful for distinguishing between targeted and unwanted classes. Extensive\nexperiments show that SAMOSA achieves up to 3% accuracy improvement over the\nstate of the art across several datasets, while not introducing computational\noverhead. The source code of our experiments is available at:\nhttps://anonymous.4open.science/r/samosa-DAF4", "AI": {"tldr": "SAMOSA is a novel open set active learning method that uses sharpness-aware minimization to query atypical samples near decision boundaries, achieving 3% accuracy improvement over state-of-the-art methods without computational overhead.", "motivation": "To reduce the costly burden of data labeling in machine learning by developing an effective open set active learning approach that can select informative samples from unlabeled data containing irrelevant or unknown classes.", "method": "Proposes SAMOSA (Sharpness Aware Minimization for Open Set Active Learning) that actively queries samples based on their typicality, identifying atypical samples near model decision boundaries using theoretical findings about SGD and SAM generalization properties.", "result": "Extensive experiments show SAMOSA achieves up to 3% accuracy improvement over state-of-the-art methods across several datasets, while maintaining computational efficiency with no additional overhead.", "conclusion": "SAMOSA effectively identifies and prioritizes informative samples that are both useful for targeted classes and for distinguishing between targeted and unwanted classes, providing a practical solution for open set active learning problems."}}
{"id": "2510.16774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16774", "abs": "https://arxiv.org/abs/2510.16774", "authors": ["Yuguang Yue", "Irakli Salia", "Samuel Hunt", "Christopher Green", "Wenzhe Shi", "Jonathan J Hunt"], "title": "Learning to play: A Multimodal Agent for 3D Game-Play", "comment": "International Conference on Computer Vision Workshop on Multi-Modal\n  Reasoning for Agentic Intelligence", "summary": "We argue that 3-D first-person video games are a challenging environment for\nreal-time multi-modal reasoning. We first describe our dataset of human\ngame-play, collected across a large variety of 3-D first-person games, which is\nboth substantially larger and more diverse compared to prior publicly disclosed\ndatasets, and contains text instructions. We demonstrate that we can learn an\ninverse dynamics model from this dataset, which allows us to impute actions on\na much larger dataset of publicly available videos of human game play that lack\nrecorded actions. We then train a text-conditioned agent for game playing using\nbehavior cloning, with a custom architecture capable of realtime inference on a\nconsumer GPU. We show the resulting model is capable of playing a variety of\n3-D games and responding to text input. Finally, we outline some of the\nremaining challenges such as long-horizon tasks and quantitative evaluation\nacross a large set of games.", "AI": {"tldr": "This paper presents a method for training AI agents to play 3D first-person video games using behavior cloning with text instructions, using a large dataset of human gameplay and an inverse dynamics model to impute actions.", "motivation": "3D first-person video games present a challenging environment for real-time multi-modal reasoning, requiring the combination of visual perception, action planning, and text instruction following.", "method": "Collected a large diverse dataset of human gameplay with text instructions, learned an inverse dynamics model to impute actions on public videos, and trained a text-conditioned agent using behavior cloning with a custom real-time inference architecture.", "result": "The resulting model can play various 3D games and respond to text input, demonstrating capabilities in real-time game playing with instruction following.", "conclusion": "While successful in creating playable agents, challenges remain in long-horizon tasks and quantitative evaluation across multiple games, indicating areas for future improvement."}}
{"id": "2510.16780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16780", "abs": "https://arxiv.org/abs/2510.16780", "authors": ["Chang Wu", "Zhiyuan Liu", "Wen Shu", "Liang Wang", "Yanchen Luo", "Wenqiang Lei", "Yatao Bian", "Junfeng Fang", "Xiang Wang"], "title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding", "comment": null, "summary": "Masked graph modeling (MGM) is a promising approach for molecular\nrepresentation learning (MRL).However, extending the success of re-mask\ndecoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting\nchallenges: avoiding 2D structure leakage to the decoder, while still providing\nsufficient 2D context for reconstructing re-masked atoms.To address these\nchallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with\nSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in its\nSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant information\nfrom encoder representations while preserving the 2D graph structures.This SRD\nis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)\nencoder alongside a structure-independent decoder. We analyze that SRD,\ncombined with the structure-independent decoder, enhances the encoder's role in\nMRL. Extensive experiments show that 3D-GSRD achieves strong downstream\nperformance, setting a new state-of-the-art on 7 out of 8 targets in the widely\nused MD17 molecular property prediction benchmark. The code is released at\nhttps://github.com/WuChang0124/3D-GSRD.", "AI": {"tldr": "3D-GSRD is a novel molecular representation learning method that addresses challenges in 3D masked graph modeling by using selective re-mask decoding to prevent 2D structure leakage while maintaining sufficient 2D context for reconstruction.", "motivation": "Extending masked graph modeling from 2D to 3D molecular representation learning is challenging due to conflicting requirements: avoiding 2D structure leakage to the decoder while providing enough 2D context for reconstructing re-masked atoms.", "method": "Proposes 3D-GSRD with Selective Re-mask Decoding (SRD) that re-masks only 3D-relevant information while preserving 2D graph structures, combined with a 3D Relational-Transformer encoder and structure-independent decoder.", "result": "Achieves state-of-the-art performance on 7 out of 8 targets in the MD17 molecular property prediction benchmark, demonstrating strong downstream performance.", "conclusion": "3D-GSRD's selective re-mask decoding approach effectively enhances molecular representation learning by balancing 3D information processing with 2D context preservation, setting new benchmarks in molecular property prediction."}}
{"id": "2510.16805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16805", "abs": "https://arxiv.org/abs/2510.16805", "authors": ["Mariam Rakka", "Marios Fournarakis", "Olga Krestinskaya", "Jinane Bazzi", "Khaled N. Salama", "Fadi Kurdahi", "Ahmed M. Eltawil", "Mohammed E. Fouda"], "title": "Mixed-Precision Quantization for Language Models: Techniques and Prospects", "comment": "46 pages, 6 figures, 5 tables", "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.", "AI": {"tldr": "This survey provides a comprehensive overview of Mixed-Precision Quantization frameworks for Large Language Models (MXPLMs), covering quantization fundamentals, bit allocation strategies, performance comparisons, and future research directions to balance efficiency and accuracy in model deployment.", "motivation": "The rapid scaling of language models has resulted in unsustainable computational, memory, and energy requirements, making quantization essential for reducing model size and accelerating inference while maintaining accuracy.", "method": "The survey categorizes and compares recent MXPLM frameworks based on their bit allocation strategies and precision configurations across weights, activations, and key-value caches, while contrasting them with earlier mixed-precision methods for deep neural networks.", "result": "The analysis highlights differences in perplexity, zero-shot task performance, and deployment trade-offs among various mixed-precision quantization approaches, identifying strategies that transfer well and those that face challenges in the LM setting.", "conclusion": "The work consolidates recent advances in mixed-precision quantization for large-scale language models and identifies open research directions including hardware-aware design, activation quantization, and scalable optimization methods for billion-parameter models."}}
{"id": "2510.16806", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16806", "abs": "https://arxiv.org/abs/2510.16806", "authors": ["Weilin Wan", "Weizhong Zhang", "Cheng Jin"], "title": "Computational Budget Should Be Considered in Data Selection", "comment": null, "summary": "Data selection improves computational efficiency by choosing informative\nsubsets of training samples. However, existing methods ignore the compute\nbudget, treating data selection and importance evaluation independently of\ncompute budget constraints. Yet empirical studies show no algorithm can\nconsistently outperform others (or even random selection) across varying\nbudgets. We therefore argue that compute budget must be integral to\ndata-selection strategies, since different budgets impose distinct requirements\non data quantity, quality, and distribution for effective training. To this\nend, we propose a novel Computational budget-Aware Data Selection (CADS) method\nand naturally formulate it into a bilevel optimization framework, where the\ninner loop trains the model within the constraints of the computational budget\non some selected subset of training data, while the outer loop optimizes data\nselection based on model evaluation. Our technical contributions lie in\naddressing two main challenges in solving this bilevel optimization problem:\nthe expensive Hessian matrix estimation for outer-loop gradients and the\ncomputational burden of achieving inner-loop optimality during iterations. To\nsolve the first issue, we propose a probabilistic reparameterization strategy\nand compute the gradient using a Hessian-free policy gradient estimator. To\naddress the second challenge, we transform the inner optimization problem into\na penalty term in the outer objective, further discovering that we only need to\nestimate the minimum of a one-dimensional loss to calculate the gradient,\nsignificantly improving efficiency. Extensive experiments show that our method\nachieves performance gains of up to 14.42% over baselines in vision and\nlanguage benchmarks.", "AI": {"tldr": "CADS introduces a compute budget-aware data selection method using bilevel optimization, addressing computational efficiency challenges through probabilistic reparameterization and Hessian-free gradient estimation.", "motivation": "Existing data selection methods ignore compute budget constraints, leading to inconsistent performance across different budgets. Different budgets require distinct data quantity, quality, and distribution for effective training.", "method": "Proposed CADS uses bilevel optimization: inner loop trains model within compute budget on selected data subset, outer loop optimizes data selection based on model evaluation. Uses probabilistic reparameterization and Hessian-free policy gradient for gradient estimation, and transforms inner optimization into penalty term.", "result": "Achieves performance gains up to 14.42% over baselines in vision and language benchmarks, demonstrating significant improvements in computational efficiency and model performance.", "conclusion": "Compute budget must be integral to data selection strategies, and CADS effectively addresses this by formulating budget-aware selection as bilevel optimization with efficient gradient estimation techniques."}}
{"id": "2510.16807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16807", "abs": "https://arxiv.org/abs/2510.16807", "authors": ["Zhoutong Wu", "Yuan Zhang", "Yiming Dong", "Chenheng Zhang", "Cong Fang", "Kun Yuan", "Zhouchen Lin"], "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads", "comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}", "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.", "AI": {"tldr": "SkipV1Former is a Transformer variant that uses skip connections from the first layer's Value heads to reduce KV cache by ~25% while improving perplexity, and can be uptrained from existing models with minimal compute.", "motivation": "To improve Transformer representation without increasing resource usage, addressing the high memory and compute costs of KV cache in auto-regressive decoding while maintaining strong performance.", "method": "From the second block onward, each layer reuses half of its Value heads from the very first layer while computing the other half normally, reducing Value projections and V cache by nearly 50%.", "result": "Consistent ~25% reduction in KV cache across different model scales while improving perplexity compared to standard MHA Transformers and advanced variants.", "conclusion": "SkipV1Former effectively strengthens model representation and reduces KV cache, can be uptrained from existing models with only 10-15% additional compute, and combines well with other advanced attention methods for further improvements."}}
{"id": "2510.16811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16811", "abs": "https://arxiv.org/abs/2510.16811", "authors": ["Mohammad Shahverdikondori", "Jalal Etesami", "Negar Kiyavash"], "title": "Graph Learning is Suboptimal in Causal Bandits", "comment": "31 pages, 5 figures", "summary": "We study regret minimization in causal bandits under causal sufficiency where\nthe underlying causal structure is not known to the agent. Previous work has\nfocused on identifying the reward's parents and then applying classic bandit\nmethods to them, or jointly learning the parents while minimizing regret. We\ninvestigate whether such strategies are optimal. Somewhat counterintuitively,\nour results show that learning the parent set is suboptimal. We do so by\nproving that there exist instances where regret minimization and parent\nidentification are fundamentally conflicting objectives. We further analyze\nboth the known and unknown parent set size regimes, establish novel regret\nlower bounds that capture the combinatorial structure of the action space.\nBuilding on these insights, we propose nearly optimal algorithms that bypass\ngraph and parent recovery, demonstrating that parent identification is indeed\nunnecessary for regret minimization. Experiments confirm that there exists a\nlarge performance gap between our method and existing baselines in various\nenvironments.", "AI": {"tldr": "This paper shows that learning the parent set in causal bandits is suboptimal for regret minimization, proving that regret minimization and parent identification can be conflicting objectives. The authors establish novel regret lower bounds and propose nearly optimal algorithms that bypass graph recovery.", "motivation": "Previous work on causal bandits focused on identifying the reward's parents and applying classic bandit methods, or jointly learning parents while minimizing regret. The authors investigate whether these strategies are optimal.", "method": "The authors prove that there exist instances where regret minimization and parent identification are fundamentally conflicting objectives. They analyze both known and unknown parent set size regimes, establish novel regret lower bounds, and propose nearly optimal algorithms that bypass graph and parent recovery.", "result": "The results show that learning the parent set is suboptimal for regret minimization. Experiments confirm a large performance gap between the proposed method and existing baselines in various environments.", "conclusion": "Parent identification is unnecessary for regret minimization in causal bandits. The proposed algorithms that bypass graph recovery achieve nearly optimal performance without needing to learn the causal structure."}}
{"id": "2510.16814", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16814", "abs": "https://arxiv.org/abs/2510.16814", "authors": ["Simon Jaxy", "Anton Theys", "Patrick Willett", "W. Chris Carleton", "Ralf Vandam", "Pieter Libin"], "title": "Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity", "comment": null, "summary": "Archaeological predictive modelling estimates where undiscovered sites are\nlikely to occur by combining known locations with environmental, cultural, and\ngeospatial variables. We address this challenge using a deep learning approach\nbut must contend with structural label scarcity inherent to archaeology:\npositives are rare, and most locations are unlabeled. To address this, we adopt\na semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a\nsemantic segmentation model and evaluated on two datasets covering a\nrepresentative range of archaeological periods. Our approach employs dynamic\npseudolabeling, refined with a Conditional Random Field (CRF) implemented via\nan RNN, increasing label confidence under severe class imbalance. On a\ngeospatial dataset derived from a digital elevation model (DEM), our model\nperforms on par with the state-of-the-art, LAMAP, while achieving higher Dice\nscores. On raw satellite imagery, assessed end-to-end with stratified k-fold\ncross-validation, it maintains performance and yields predictive surfaces with\nimproved interpretability. Overall, our results indicate that semi-supervised\nlearning offers a promising approach to identifying undiscovered sites across\nlarge, sparsely annotated landscapes.", "AI": {"tldr": "This paper presents a semi-supervised deep learning approach for archaeological predictive modeling that addresses structural label scarcity by using positive-unlabeled learning with dynamic pseudolabeling and CRF refinement.", "motivation": "To overcome the challenge of structural label scarcity in archaeology where positive site locations are rare and most locations are unlabeled, requiring methods that can work with limited labeled data.", "method": "Uses semi-supervised positive-unlabeled learning implemented as semantic segmentation with dynamic pseudolabeling refined by Conditional Random Field (CRF) via RNN to handle severe class imbalance.", "result": "The model performs on par with state-of-the-art LAMAP on geospatial DEM data while achieving higher Dice scores, and maintains performance on raw satellite imagery with improved interpretability.", "conclusion": "Semi-supervised learning offers a promising approach for identifying undiscovered archaeological sites across large, sparsely annotated landscapes."}}
{"id": "2510.16816", "categories": ["cs.LG", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16816", "abs": "https://arxiv.org/abs/2510.16816", "authors": ["Ming Zhong", "Zhenya Yan"], "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator", "comment": "31 pages, 8 figures", "summary": "Neural operators offer a powerful data-driven framework for learning mappings\nbetween function spaces, in which the transformer-based neural operator\narchitecture faces a fundamental scalability-accuracy trade-off: softmax\nattention provides excellent fidelity but incurs quadratic complexity\n$\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,\nwhile linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often\nsuffer significant accuracy degradation. To address the aforementioned\nchallenge, in this paper, we present a novel type of neural operators, Linear\nAttention Neural Operator (LANO), which achieves both scalability and high\naccuracy by reformulating attention through an agent-based mechanism. LANO\nresolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll\nN)$ that mediate global interactions among $N$ tokens. This agent attention\nmechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$\nwhile preserving the expressive power of softmax attention. Theoretically, we\ndemonstrate the universal approximation property, thereby demonstrating\nimproved conditioning and stability properties. Empirically, LANO surpasses\ncurrent state-of-the-art neural PDE solvers, including Transolver with\nslice-based softmax attention, achieving average $19.5\\%$ accuracy improvement\nacross standard benchmarks. By bridging the gap between linear complexity and\nsoftmax-level performance, LANO establishes a scalable, high-accuracy\nfoundation for scientific machine learning applications.", "AI": {"tldr": "LANO is a novel neural operator that uses agent tokens to achieve linear complexity while maintaining softmax attention's accuracy, outperforming current state-of-the-art neural PDE solvers by 19.5% on average.", "motivation": "To overcome the scalability-accuracy trade-off in transformer-based neural operators where softmax attention has quadratic complexity and linear attention variants suffer accuracy degradation.", "method": "Introduces agent-based attention mechanism with a compact set of M agent tokens (M << N) that mediate global interactions among N tokens, achieving linear complexity O(MNd) while preserving softmax attention's expressive power.", "result": "LANO surpasses current state-of-the-art neural PDE solvers including Transolver, achieving 19.5% average accuracy improvement across standard benchmarks while maintaining linear complexity.", "conclusion": "LANO bridges the gap between linear complexity and softmax-level performance, establishing a scalable, high-accuracy foundation for scientific machine learning applications."}}
{"id": "2510.16817", "categories": ["cs.LG", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.16817", "abs": "https://arxiv.org/abs/2510.16817", "authors": ["Doyoon Kim", "Junbin Song"], "title": "Trace Regularity PINNs: Enforcing $\\mathrm{H}^{\\frac{1}{2}}(\\partial \u03a9)$ for Boundary Data", "comment": null, "summary": "We propose an enhanced physics-informed neural network (PINN), the Trace\nRegularity Physics-Informed Neural Network (TRPINN), which enforces the\nboundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\\partial \\Omega)$, the\ncorrect trace space associated with $H^1(\\Omega)$. We reduce computational cost\nby computing only the theoretically essential portion of the semi-norm and\nenhance convergence stability by avoiding denominator evaluations in the\ndiscretization. By incorporating the exact $H^{1/2}(\\partial \\Omega)$ norm, we\nshow that the approximation converges to the true solution in the\n$H^{1}(\\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we\ndemonstrate that TRPINN can converge faster than standard PINNs. Numerical\nexperiments on the Laplace equation with highly oscillatory Dirichlet boundary\nconditions exhibit cases where TRPINN succeeds even when standard PINNs fail,\nand show performance improvements of one to three decimal digits.", "AI": {"tldr": "TRPINN is an enhanced physics-informed neural network that enforces boundary loss using the Sobolev-Slobodeckij norm H^{1/2}(\u2202\u03a9), improving convergence and computational efficiency compared to standard PINNs.", "motivation": "Standard PINNs have limitations in handling boundary conditions effectively, especially for problems with highly oscillatory boundary conditions. The motivation is to develop a more theoretically sound and computationally efficient approach by using the correct trace space associated with H^1(\u03a9).", "method": "The method involves computing only the theoretically essential portion of the semi-norm to reduce computational cost, avoiding denominator evaluations in discretization to enhance convergence stability, and incorporating the exact H^{1/2}(\u2202\u03a9) norm for boundary loss enforcement.", "result": "Numerical experiments on Laplace equation with highly oscillatory Dirichlet boundary conditions show that TRPINN succeeds where standard PINNs fail, achieving performance improvements of one to three decimal digits. NTK analysis demonstrates faster convergence than standard PINNs.", "conclusion": "TRPINN provides a theoretically grounded and computationally efficient enhancement to PINNs, ensuring convergence to the true solution in the H^1(\u03a9) sense and demonstrating superior performance for problems with challenging boundary conditions."}}
{"id": "2510.16820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16820", "abs": "https://arxiv.org/abs/2510.16820", "authors": ["Thomas Dooms", "Ward Gauderis"], "title": "Finding Manifolds With Bilinear Autoencoders", "comment": null, "summary": "Sparse autoencoders are a standard tool for uncovering interpretable latent\nrepresentations in neural networks. Yet, their interpretation depends on the\ninputs, making their isolated study incomplete. Polynomials offer a solution;\nthey serve as algebraic primitives that can be analysed without reference to\ninput and can describe structures ranging from linear concepts to complicated\nmanifolds. This work uses bilinear autoencoders to efficiently decompose\nrepresentations into quadratic polynomials. We discuss improvements that induce\nimportance ordering, clustering, and activation sparsity. This is an initial\nstep toward nonlinear yet analysable latents through their algebraic\nproperties.", "AI": {"tldr": "This paper introduces bilinear autoencoders to decompose neural network representations into quadratic polynomials, enabling algebraic analysis without input dependency.", "motivation": "Standard sparse autoencoders have interpretation limitations due to input dependency, while polynomials provide algebraic primitives that can be analyzed independently of inputs and describe complex structures.", "method": "Uses bilinear autoencoders to efficiently decompose representations into quadratic polynomials, with improvements for importance ordering, clustering, and activation sparsity.", "result": "Developed a method for nonlinear yet analyzable latent representations through their algebraic properties, providing an initial framework for interpretable decomposition.", "conclusion": "This work represents an initial step toward creating nonlinear latent representations that remain analyzable through algebraic properties, addressing limitations of input-dependent interpretation in traditional autoencoders."}}
{"id": "2510.16824", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2510.16824", "abs": "https://arxiv.org/abs/2510.16824", "authors": ["Yingxu Wang", "Kunyu Zhang", "Jiaxin Huang", "Nan Yin", "Siwei Liu", "Eran Segal"], "title": "ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning", "comment": null, "summary": "Multimodal molecular representation learning, which jointly models molecular\ngraphs and their textual descriptions, enhances predictive accuracy and\ninterpretability by enabling more robust and reliable predictions of drug\ntoxicity, bioactivity, and physicochemical properties through the integration\nof structural and semantic information. However, existing multimodal methods\nsuffer from two key limitations: (1) they typically perform cross-modal\ninteraction only at the final encoder layer, thus overlooking hierarchical\nsemantic dependencies; (2) they lack a unified prototype space for robust\nalignment between modalities. To address these limitations, we propose\nProtoMol, a prototype-guided multimodal framework that enables fine-grained\nintegration and consistent semantic alignment between molecular graphs and\ntextual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,\nutilizing Graph Neural Networks to process structured molecular graphs and\nTransformers to encode unstructured texts, resulting in comprehensive\nlayer-wise representations. Then, ProtoMol introduces a layer-wise\nbidirectional cross-modal attention mechanism that progressively aligns\nsemantic features across layers. Furthermore, a shared prototype space with\nlearnable, class-specific anchors is constructed to guide both modalities\ntoward coherent and discriminative representations. Extensive experiments on\nmultiple benchmark datasets demonstrate that ProtoMol consistently outperforms\nstate-of-the-art baselines across a variety of molecular property prediction\ntasks.", "AI": {"tldr": "ProtoMol is a prototype-guided multimodal framework that enables fine-grained integration and consistent semantic alignment between molecular graphs and textual descriptions through hierarchical encoders, layer-wise bidirectional cross-modal attention, and a shared prototype space.", "motivation": "Existing multimodal molecular representation learning methods suffer from limited cross-modal interaction (only at final encoder layer) and lack unified prototype space for robust alignment between modalities, which hinders hierarchical semantic dependencies and consistent semantic alignment.", "method": "ProtoMol uses dual-branch hierarchical encoders (Graph Neural Networks for molecular graphs and Transformers for texts), layer-wise bidirectional cross-modal attention mechanism, and constructs a shared prototype space with learnable, class-specific anchors to guide both modalities toward coherent representations.", "result": "Extensive experiments on multiple benchmark datasets demonstrate that ProtoMol consistently outperforms state-of-the-art baselines across various molecular property prediction tasks.", "conclusion": "ProtoMol effectively addresses limitations of existing multimodal methods by enabling fine-grained integration and consistent semantic alignment between molecular graphs and textual descriptions, achieving superior performance in molecular property prediction."}}
{"id": "2510.16857", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16857", "abs": "https://arxiv.org/abs/2510.16857", "authors": ["Jiyan Qiu", "Lyulin Kuang", "Guan Wang", "Yichen Xu", "Leiyao Cui", "Shaotong Fu", "Yixin Zhu", "Ruihua Zhang"], "title": "DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization", "comment": null, "summary": "Vehicle aerodynamics optimization has become critical for automotive\nelectrification, where drag reduction directly determines electric vehicle\nrange and energy efficiency. Traditional approaches face an intractable\ntrade-off: computationally expensive Computational Fluid Dynamics (CFD)\nsimulations requiring weeks per design iteration, or simplified models that\nsacrifice production-grade accuracy. While machine learning offers\ntransformative potential, existing datasets exhibit fundamental limitations --\ninadequate mesh resolution, missing vehicle components, and validation errors\nexceeding 5% -- preventing deployment in industrial workflows. We present\nDrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations\ngenerated using $\\text{STAR-CCM+}^\\unicode{xAE}$ software. The dataset\nsystematically explores three vehicle configurations through 20 Computer Aided\nDesign (CAD) parameters via Free Form Deformation (FFD) algorithms, including\ncomplete engine compartments and cooling systems with realistic internal\nairflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a\nfive-fold improvement over existing datasets -- through refined mesh strategies\nwith strict wall $y^+$ control. Benchmarks demonstrate that models trained on\nthis data achieve production-ready accuracy while reducing computational costs\nfrom weeks to minutes. This represents the first dataset bridging academic\nmachine learning research and industrial CFD practice, establishing a new\nstandard for data-driven aerodynamic optimization in automotive development.\nBeyond automotive applications, DrivAerStar demonstrates a paradigm for\nintegrating high-fidelity physics simulations with Artificial Intelligence (AI)\nacross engineering disciplines where computational constraints currently limit\ninnovation.", "AI": {"tldr": "DrivAerStar is a comprehensive dataset of 12,000 industrial-grade automotive CFD simulations that bridges the gap between academic ML research and industrial CFD practice, achieving wind tunnel validation accuracy below 1.04% and enabling production-ready aerodynamic optimization.", "motivation": "Vehicle aerodynamics optimization is critical for electric vehicle range and efficiency, but traditional approaches face trade-offs between computational expense and accuracy. Existing ML datasets have fundamental limitations preventing industrial deployment.", "method": "Generated 12,000 CFD simulations using STAR-CCM+ software, systematically exploring three vehicle configurations through 20 CAD parameters via Free Form Deformation algorithms, including complete engine compartments and cooling systems with realistic internal airflow.", "result": "Achieved wind tunnel validation accuracy below 1.04% - a five-fold improvement over existing datasets. Models trained on this data achieve production-ready accuracy while reducing computational costs from weeks to minutes.", "conclusion": "DrivAerStar establishes a new standard for data-driven aerodynamic optimization and demonstrates a paradigm for integrating high-fidelity physics simulations with AI across engineering disciplines where computational constraints limit innovation."}}
{"id": "2510.16877", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16877", "abs": "https://arxiv.org/abs/2510.16877", "authors": ["Heming Zou", "Yunliang Zang", "Wutong Xu", "Xiangyang Ji"], "title": "Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning", "comment": null, "summary": "Using a nearly-frozen pretrained model, the continual representation learning\nparadigm reframes parameter updates as a similarity-matching problem to\nmitigate catastrophic forgetting. However, directly leveraging pretrained\nfeatures for downstream tasks often suffers from multicollinearity in the\nsimilarity-matching stage, and more advanced methods can be computationally\nprohibitive for real-time, low-latency applications. Inspired by the fly\nolfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with\na wide range of pretrained backbones. Fly-CL substantially reduces training\ntime while achieving performance comparable to or exceeding that of current\nstate-of-the-art methods. We theoretically show how Fly-CL progressively\nresolves multicollinearity, enabling more effective similarity matching with\nlow time complexity. Extensive simulation experiments across diverse network\narchitectures and data regimes validate Fly-CL's effectiveness in addressing\nthis challenge through a biologically inspired design. Code is available at\nhttps://github.com/gfyddha/Fly-CL.", "AI": {"tldr": "Fly-CL is a bio-inspired framework for continual representation learning that addresses multicollinearity in similarity-matching while reducing training time, achieving performance comparable to or better than state-of-the-art methods.", "motivation": "To mitigate catastrophic forgetting in continual learning while overcoming multicollinearity issues in similarity-matching and computational limitations of advanced methods for real-time applications.", "method": "Inspired by the fly olfactory circuit, Fly-CL uses a bio-inspired framework compatible with various pretrained backbones, progressively resolving multicollinearity through biologically inspired design.", "result": "Fly-CL substantially reduces training time while achieving performance comparable to or exceeding current state-of-the-art methods across diverse network architectures and data regimes.", "conclusion": "The biologically inspired Fly-CL framework effectively addresses multicollinearity in continual representation learning with low time complexity, validated through extensive simulations."}}
{"id": "2510.16882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16882", "abs": "https://arxiv.org/abs/2510.16882", "authors": ["Heming Zou", "Yixiu Mao", "Yun Qu", "Qi Wang", "Xiangyang Ji"], "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) is a commonly used technique to adapt large\nlanguage models (LLMs) to downstream tasks. In practice, SFT on a full dataset\nis computationally expensive and sometimes suffers from overfitting or bias\namplification. This facilitates the rise of data curation in SFT, which\nprioritizes the most valuable data to optimze. This work studies the online\nbatch selection family that dynamically scores and filters samples during the\ntraining process. However, existing popular methods often (i) rely merely on\nthe utility of data to select a subset while neglecting other crucial factors\nlike diversity, (ii) rely on external resources such as reference models or\nvalidation sets, and (iii) incur extra training time over full-dataset\ntraining. To address these limitations, this work develops \\textbf{UDS\n(Utility-Diversity Sampling)}, a framework for efficient online batch selection\nin SFT. UDS leverages the nuclear norm of the logits matrix to capture both\ndata utility and intra-sample diversity, while estimating inter-sample\ndiversity through efficient low-dimensional embedding comparisons with a\nlightweight memory buffer of historical samples. Such a design eliminates the\nneed for external resources and unnecessary backpropagation, securing\ncomputational efficiency. Experiments on multiple benchmarks demonstrate that\nUDS consistently outperforms state-of-the-art online batch selection methods\nunder varying data budgets, and significantly reduces training time compared to\nfull-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.", "AI": {"tldr": "UDS (Utility-Diversity Sampling) is an efficient online batch selection framework for supervised fine-tuning that balances data utility and diversity without external resources, reducing training time while maintaining performance.", "motivation": "Existing online batch selection methods for SFT often focus only on data utility while neglecting diversity, require external resources like reference models or validation sets, and incur extra training time compared to full-dataset training.", "method": "UDS uses nuclear norm of logits matrix to capture data utility and intra-sample diversity, and estimates inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples, eliminating need for external resources and unnecessary backpropagation.", "result": "Experiments show UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets and significantly reduces training time compared to full-dataset fine-tuning.", "conclusion": "UDS provides an efficient and effective framework for online batch selection in SFT that addresses key limitations of existing methods while maintaining computational efficiency."}}
{"id": "2510.16885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16885", "abs": "https://arxiv.org/abs/2510.16885", "authors": ["Duo Wang", "Yuan Zuo", "Guangyue Lu", "Junjie Wu"], "title": "UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains", "comment": null, "summary": "Generalizing to unseen graph tasks without task-specific supervision is\nchallenging: conventional graph neural networks are typically tied to a fixed\nlabel space, while large language models (LLMs) struggle to capture graph\nstructure. We introduce UniGTE, an instruction-tuned encoder-decoder framework\nthat unifies structural and semantic reasoning. The encoder augments a\npretrained autoregressive LLM with learnable alignment tokens and a\nstructure-aware graph-text attention mechanism, enabling it to attend jointly\nto a tokenized graph and a natural-language task prompt while remaining\npermutation-invariant to node order. This yields compact, task-aware graph\nrepresentations. Conditioned solely on these representations, a frozen LLM\ndecoder predicts and reconstructs: it outputs the task answer and\nsimultaneously paraphrases the input graph in natural language. The\nreconstruction objective regularizes the encoder to preserve structural cues.\nUniGTE is instruction-tuned on five datasets spanning node-level, edge-level,\nand graph-level tasks across diverse domains, yet requires no fine-tuning at\ninference. It achieves new state-of-the-art zero-shot results on node\nclassification, link prediction, graph classification, and graph regression\nunder cross-task and cross-domain settings, demonstrating that tight\nintegration of graph structure with LLM semantics enables robust, transferable\ngraph reasoning.", "AI": {"tldr": "UniGTE is an instruction-tuned encoder-decoder framework that unifies structural and semantic reasoning for graph tasks, achieving state-of-the-art zero-shot performance on various graph tasks without task-specific fine-tuning.", "motivation": "To address the limitations of conventional graph neural networks (fixed label space) and large language models (struggle with graph structure) in generalizing to unseen graph tasks without task-specific supervision.", "method": "An encoder-decoder framework where the encoder augments a pretrained LLM with learnable alignment tokens and structure-aware graph-text attention, while a frozen LLM decoder predicts answers and reconstructs input graphs through natural language paraphrasing.", "result": "Achieves new state-of-the-art zero-shot results on node classification, link prediction, graph classification, and graph regression under cross-task and cross-domain settings.", "conclusion": "Tight integration of graph structure with LLM semantics enables robust, transferable graph reasoning, demonstrating that UniGTE can effectively generalize across diverse graph tasks without task-specific fine-tuning."}}
{"id": "2510.16897", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16897", "abs": "https://arxiv.org/abs/2510.16897", "authors": ["Jose Siguenza", "Bharath Ramsundar"], "title": "DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library", "comment": "Presented at Machine Learning Symposium - BayLearn (2025)", "summary": "Neural networks that incorporate geometric relationships respecting SE(3)\ngroup transformations (e.g. rotations and translations) are increasingly\nimportant in molecular applications, such as molecular property prediction,\nprotein structure modeling, and materials design. These models, known as\nSE(3)-equivariant neural networks, ensure outputs transform predictably with\ninput coordinate changes by explicitly encoding spatial atomic positions.\nAlthough libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful\nimplementations, they often require substantial deep learning or mathematical\nprior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]\nwith support for ready-to-use equivariant models, enabling scientists with\nminimal deep learning background to build, train, and evaluate models, such as\nSE(3)-Transformer and Tensor Field Networks. Our implementation includes\nequivariant models, complete training pipelines, and a toolkit of equivariant\nutilities, supported with comprehensive tests and documentation, to facilitate\nboth application and further development of SE(3)-equivariant models.", "AI": {"tldr": "Extension of DeepChem library to include ready-to-use SE(3)-equivariant neural networks for molecular applications, making these advanced models accessible to scientists without deep learning expertise.", "motivation": "Existing SE(3)-equivariant neural network libraries require substantial deep learning or mathematical knowledge and lack complete training pipelines, making them inaccessible to many scientists working on molecular applications.", "method": "Extend DeepChem with support for ready-to-use equivariant models including SE(3)-Transformer and Tensor Field Networks, providing complete training pipelines, equivariant utilities, comprehensive tests, and documentation.", "result": "Created an implementation that enables scientists with minimal deep learning background to build, train, and evaluate SE(3)-equivariant models for molecular applications.", "conclusion": "The DeepChem extension facilitates both application and further development of SE(3)-equivariant models by providing accessible, well-documented tools that lower the barrier to entry for scientific users."}}
{"id": "2510.16911", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16911", "abs": "https://arxiv.org/abs/2510.16911", "authors": ["Sarah Al-Shareeda", "Gulcihan Ozdemir", "Heung Seok Jeon", "Khaleel Ahmad"], "title": "A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch", "comment": "5 pages, 3 figures, The IEEE PES ISGT Middle East 2025 (ISGT-ME 2025)\n  November 23-26th 2025, Dubai, UAE", "summary": "How can short-term energy consumption be accurately forecasted when sensor\ndata is noisy, incomplete, and lacks contextual richness? This question guided\nour participation in the \\textit{2025 Competition on Electric Energy\nConsumption Forecast Adopting Multi-criteria Performance Metrics}, which\nchallenged teams to predict next-day power demand using real-world\nhigh-frequency data. We proposed a robust yet lightweight Deep Learning (DL)\npipeline combining hourly downsizing, dual-mode imputation (mean and polynomial\nregression), and comprehensive normalization, ultimately selecting Standard\nScaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model\nachieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\\% accuracy.\nDespite asymmetric inputs and imputed gaps, it generalized well, captured\nnonlinear demand patterns, and maintained low inference latency. Notably,\nspatiotemporal heatmap analysis reveals a strong alignment between temperature\ntrends and predicted consumption, further reinforcing the model's reliability.\nThese results demonstrate that targeted preprocessing paired with compact\nrecurrent architectures can still enable fast, accurate, and deployment-ready\nenergy forecasting in real-world conditions.", "AI": {"tldr": "A lightweight DL pipeline combining preprocessing techniques and GRU-LSTM model achieves accurate next-day energy consumption forecasting with 84.36% accuracy despite noisy, incomplete sensor data.", "motivation": "To address the challenge of accurate short-term energy forecasting using noisy, incomplete sensor data lacking contextual richness, as presented in the 2025 Competition on Electric Energy Consumption Forecast.", "method": "Proposed a robust DL pipeline with hourly downsizing, dual-mode imputation (mean and polynomial regression), comprehensive normalization (Standard Scaling selected), and a lightweight GRU-LSTM sequence-to-one model.", "result": "Achieved average RMSE of 601.9W, MAE of 468.9W, and 84.36% accuracy. Model generalized well despite asymmetric inputs and imputed gaps, captured nonlinear patterns, maintained low latency, and showed strong temperature-consumption correlation.", "conclusion": "Targeted preprocessing paired with compact recurrent architectures enables fast, accurate, and deployment-ready energy forecasting in real-world conditions."}}
{"id": "2510.16914", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16914", "abs": "https://arxiv.org/abs/2510.16914", "authors": ["Hongwei Yan", "Guanglong Sun", "Zhiqi Kang", "Yi Zhong", "Liyuan Wang"], "title": "Domain Generalizable Continual Learning", "comment": "25 pages", "summary": "To adapt effectively to dynamic real-world environments, intelligent systems\nmust continually acquire new skills while generalizing them to diverse, unseen\nscenarios. Here, we introduce a novel and realistic setting named domain\ngeneralizable continual learning (DGCL): a model learns sequential tasks with\neach involving a single domain, aiming to perform well across all encountered\ntasks and domains. This setting poses unique challenges in acquiring,\nretaining, and leveraging both semantic- and domain-relevant information for\nrobust generalization. Although state-of-the-art continual learning (CL)\nmethods have employed pre-trained models (PTMs) to enhance task-specific\ngeneralization, they typically assume identical training and testing domains\nfor each task and therefore perform poorly in DGCL. To this end, we propose\nadaptive Domain Transformation (DoT), an innovative PTMs-based approach\ntailored to DGCL. Inspired by the distributed-plus-hub theory of the human\nbrain, DoT disentangles semantic- and domain-relevant information in\nrepresentation learning, and adaptively transforms task representations across\nvarious domains for output alignment, ensuring balanced and generalized\npredictions. DoT serves as a plug-in strategy that greatly facilitates\nstate-of-the-art CL baselines under both full parameter tuning and\nparameter-efficient tuning paradigms in DGCL, validated by extensive\nexperiments. Also, DoT is shown to accumulate domain-generalizable knowledge\nfrom DGCL, and ensure resource efficiency with a lightweight implementation.", "AI": {"tldr": "The paper introduces Domain Generalizable Continual Learning (DGCL), a challenging setting where models learn sequential tasks from single domains and must generalize across all encountered domains. The authors propose Adaptive Domain Transformation (DoT), a plug-in method that disentangles semantic and domain information using brain-inspired principles to enable robust generalization in DGCL.", "motivation": "Current continual learning methods assume identical training and testing domains per task, failing in real-world scenarios where models must generalize to unseen domains while learning sequentially. The DGCL setting addresses this gap by requiring models to perform well across all encountered domains.", "method": "Proposed Adaptive Domain Transformation (DoT), inspired by the distributed-plus-hub theory of the human brain. DoT disentangles semantic- and domain-relevant information in representation learning and adaptively transforms task representations across domains for output alignment.", "result": "DoT significantly enhances state-of-the-art continual learning baselines in DGCL under both full parameter tuning and parameter-efficient tuning. It enables models to accumulate domain-generalizable knowledge and maintains resource efficiency with lightweight implementation.", "conclusion": "DoT effectively addresses the unique challenges of DGCL by disentangling semantic and domain information, enabling robust generalization across sequential tasks and domains while being computationally efficient and easily integrable with existing CL methods."}}
{"id": "2510.16916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16916", "abs": "https://arxiv.org/abs/2510.16916", "authors": ["Dong Li", "Xujiang Zhao", "Linlin Yu", "Yanchi Liu", "Wei Cheng", "Zhengzhang Chen", "Zhong Chen", "Feng Chen", "Chen Zhao", "Haifeng Chen"], "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "comment": "NeurIPS 2025", "summary": "Large Language Models (LLMs) offer promising capabilities for tackling\ncomplex reasoning tasks, including optimization problems. However, existing\nmethods either rely on prompt engineering, which leads to poor generalization\nacross problem types, or require costly supervised training. We introduce\nSolverLLM, a training-free framework that leverages test-time scaling to solve\ndiverse optimization problems. Rather than solving directly, SolverLLM\ngenerates mathematical formulations and translates them into solver-ready code,\nguided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the\nsearch process, we modify classical MCTS with (1) dynamic expansion for\nadaptive formulation generation, (2) prompt backpropagation to guide\nexploration via outcome-driven feedback, and (3) uncertainty backpropagation to\nincorporate reward reliability into decision-making. Experiments on six\nstandard benchmark datasets demonstrate that SolverLLM outperforms both\nprompt-based and learning-based baselines, achieving strong generalization\nwithout additional training.", "AI": {"tldr": "SolverLLM is a training-free framework that uses test-time scaling and Monte Carlo Tree Search to solve optimization problems by generating mathematical formulations and converting them to solver code, outperforming existing methods without requiring additional training.", "motivation": "Existing LLM approaches for optimization problems either rely on prompt engineering with poor generalization or require costly supervised training, creating a need for a more flexible and efficient solution.", "method": "SolverLLM generates mathematical formulations and translates them into solver-ready code using a modified Monte Carlo Tree Search strategy with dynamic expansion, prompt backpropagation, and uncertainty backpropagation.", "result": "Experiments on six standard benchmark datasets show SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.", "conclusion": "SolverLLM provides an effective training-free framework for solving diverse optimization problems through test-time scaling and enhanced MCTS, demonstrating superior performance and generalization compared to existing approaches."}}
{"id": "2510.16927", "categories": ["cs.LG", "I.2.6; I.2.7; G.1.3"], "pdf": "https://arxiv.org/pdf/2510.16927", "abs": "https://arxiv.org/abs/2510.16927", "authors": ["Egor Petrov", "Nikita Kiselev", "Vladislav Meshkov", "Andrey Grabovoy"], "title": "Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws", "comment": "38 pages, 12 figures. Submitted to ICLR 2026", "summary": "The lack of theoretical results for Layer Normalization and feedforward\nHessians has left a gap in the study of Transformer optimization landscapes. We\naddress this by deriving explicit second-order expressions for these\ncomponents, thereby completing the Hessian characterization of full Transformer\nblocks. Our results generalize prior self-attention analyses and yield\nestimations for the role of each sublayer in curvature propagation. We\ndemonstrate how these Hessian structures inform both convergence dynamics and\nthe empirical scaling laws governing large-model performance. Further, we\npropose a Taylor-expansion-based framework for analyzing loss differences to\nquantify convergence trajectories. By extending Hessian theory to the full\nTransformer architecture, this work establishes a new foundation for\ntheoretical and empirical investigations of optimization in large-scale deep\nlearning.", "AI": {"tldr": "This paper provides theoretical analysis of Transformer optimization landscapes by deriving explicit second-order expressions for Layer Normalization and feedforward components, completing the Hessian characterization of full Transformer blocks.", "motivation": "To address the lack of theoretical results for Layer Normalization and feedforward Hessians in Transformer optimization, filling a gap in understanding Transformer optimization landscapes.", "method": "Deriving explicit second-order expressions for Layer Normalization and feedforward components, generalizing prior self-attention analyses, and proposing a Taylor-expansion-based framework for analyzing loss differences.", "result": "Complete Hessian characterization of full Transformer blocks, estimations for sublayer roles in curvature propagation, and insights into convergence dynamics and empirical scaling laws.", "conclusion": "Establishes a new foundation for theoretical and empirical investigations of optimization in large-scale deep learning by extending Hessian theory to the full Transformer architecture."}}
{"id": "2510.16940", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16940", "abs": "https://arxiv.org/abs/2510.16940", "authors": ["Cristian J. Vaca-Rubio", "Roberto Pereira", "Luis Blanco", "Engin Zeydan", "M\u00e0rius Caus"], "title": "A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting", "comment": null, "summary": "This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel\nprobabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series\nforecasting. By replacing scalar weights with spline-based functional\nconnections and directly parameterizing predictive distributions, P-KANs offer\nexpressive yet parameter-efficient models capable of capturing nonlinear and\nheavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,\nwhere uncertainty-aware predictions enable dynamic thresholding for resource\nallocation. Results show that P-KANs consistently outperform Multi Layer\nPerceptron (MLP) baselines in both accuracy and calibration, achieving superior\nefficiency-risk trade-offs while using significantly fewer parameters. We build\nup P-KANs on two distributions, namely Gaussian and Student-t distributions.\nThe Gaussian variant provides robust, conservative forecasts suitable for\nsafety-critical scenarios, whereas the Student-t variant yields sharper\ndistributions that improve efficiency under stable demand. These findings\nestablish P-KANs as a powerful framework for probabilistic forecasting with\ndirect applicability to satellite communications and other resource-constrained\ndomains.", "AI": {"tldr": "P-KAN is a probabilistic extension of Kolmogorov-Arnold Networks that replaces scalar weights with spline-based functional connections for time series forecasting, offering parameter-efficient models with uncertainty-aware predictions.", "motivation": "To develop expressive yet parameter-efficient probabilistic models for time series forecasting that can capture nonlinear and heavy-tailed dynamics, particularly for satellite traffic forecasting where uncertainty-aware predictions enable dynamic resource allocation.", "method": "Replace scalar weights in KANs with spline-based functional connections and directly parameterize predictive distributions using Gaussian and Student-t distributions to model uncertainty.", "result": "P-KANs consistently outperform MLP baselines in both accuracy and calibration, achieving superior efficiency-risk trade-offs while using significantly fewer parameters. Gaussian variant provides robust conservative forecasts, while Student-t variant yields sharper distributions for stable demand scenarios.", "conclusion": "P-KANs establish a powerful framework for probabilistic forecasting with direct applicability to satellite communications and other resource-constrained domains, offering both parameter efficiency and uncertainty modeling capabilities."}}
{"id": "2510.16943", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16943", "abs": "https://arxiv.org/abs/2510.16943", "authors": ["Dania Refai", "Moataz Ahmed"], "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used to convert natural\nlanguage descriptions into mathematical optimization formulations. Current\nevaluations often treat formulations as a whole, relying on coarse metrics like\nsolution accuracy or runtime, which obscure structural or numerical errors. In\nthis study, we present a comprehensive, component-level evaluation framework\nfor LLM-generated formulations. Beyond the conventional optimality gap, our\nframework introduces metrics such as precision and recall for decision\nvariables and constraints, constraint and objective root mean squared error\n(RMSE), and efficiency indicators based on token usage and latency. We evaluate\nGPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of\nvarying complexity under six prompting strategies. Results show that GPT-5\nconsistently outperforms other models, with chain-of-thought, self-consistency,\nand modular prompting proving most effective. Analysis indicates that solver\nperformance depends primarily on high constraint recall and low constraint\nRMSE, which together ensure structural correctness and solution reliability.\nConstraint precision and decision variable metrics play secondary roles, while\nconcise outputs enhance computational efficiency. These findings highlight\nthree principles for NLP-to-optimization modeling: (i) Complete constraint\ncoverage prevents violations, (ii) minimizing constraint RMSE ensures\nsolver-level accuracy, and (iii) concise outputs improve computational\nefficiency. The proposed framework establishes a foundation for fine-grained,\ndiagnostic evaluation of LLMs in optimization modeling.", "AI": {"tldr": "This paper presents a component-level evaluation framework for LLM-generated mathematical optimization formulations, introducing metrics beyond conventional optimality gaps to assess structural and numerical errors in decision variables, constraints, and objectives.", "motivation": "Current evaluations of LLMs in optimization modeling use coarse metrics like solution accuracy or runtime, which obscure structural or numerical errors in the formulations, necessitating a more comprehensive, fine-grained evaluation approach.", "method": "The study introduces a component-level evaluation framework with metrics including precision/recall for decision variables and constraints, constraint/objective RMSE, and efficiency indicators based on token usage and latency. It evaluates GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across various optimization problems under six prompting strategies.", "result": "GPT-5 consistently outperforms other models, with chain-of-thought, self-consistency, and modular prompting being most effective. Solver performance primarily depends on high constraint recall and low constraint RMSE, while constraint precision and decision variable metrics play secondary roles. Concise outputs enhance computational efficiency.", "conclusion": "The framework establishes three key principles for NLP-to-optimization modeling: complete constraint coverage prevents violations, minimizing constraint RMSE ensures solver-level accuracy, and concise outputs improve computational efficiency. This provides a foundation for fine-grained, diagnostic evaluation of LLMs in optimization modeling."}}
{"id": "2510.16958", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16958", "abs": "https://arxiv.org/abs/2510.16958", "authors": ["Ganglin Tian", "Anastase Alexandre Charantonis", "Camille Le Coz", "Alexis Tantet", "Riwal Plougonven"], "title": "Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction", "comment": "This Work has been submitted to Monthly Weather Review. Copyright in\n  this Work may be transferred without further notice", "summary": "This study aims to improve the spatial representation of uncertainties when\nregressing surface wind speeds from large-scale atmospheric predictors for\nsub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale\natmospheric predictors such as 500 hPa geopotential height (Z500), which\nexhibit higher predictability than surface variables and can be downscaled to\nobtain more localised information. Previous work by Tian et al. (2024)\ndemonstrated that stochastic perturbations based on model residuals can improve\nensemble dispersion representation in statistical downscaling frameworks, but\nthis method fails to represent spatial correlations and physical consistency\nadequately. More sophisticated approaches are needed to capture the complex\nrelationships between large-scale predictors and local-scale predictands while\nmaintaining physical consistency. Probabilistic deep learning models offer\npromising solutions for capturing complex spatial dependencies. This study\nevaluates three probabilistic methods with distinct uncertainty quantification\nmechanisms: Quantile Regression Neural Network that directly models\ndistribution quantiles, Variational Autoencoders that leverage latent space\nsampling, and Diffusion Models that utilise iterative denoising. These models\nare trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts\nto regress probabilistic wind speed ensembles. Our results show that\nprobabilistic downscaling approaches provide more realistic spatial uncertainty\nrepresentations compared to simpler stochastic methods, with each probabilistic\nmodel offering different strengths in terms of ensemble dispersion,\ndeterministic skill, and physical consistency. These findings establish\nprobabilistic downscaling as an effective enhancement to operational\nsub-seasonal wind forecasts for renewable energy planning and risk assessment.", "AI": {"tldr": "This study evaluates three probabilistic deep learning methods for improving spatial uncertainty representation in sub-seasonal wind speed forecasting, showing they outperform simpler stochastic methods.", "motivation": "To improve spatial representation of uncertainties when downscaling surface wind speeds from large-scale atmospheric predictors for sub-seasonal forecasting, addressing limitations of previous stochastic perturbation methods that fail to capture spatial correlations and physical consistency.", "method": "Evaluated three probabilistic deep learning models with distinct uncertainty quantification mechanisms: Quantile Regression Neural Network, Variational Autoencoders, and Diffusion Models, trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts.", "result": "Probabilistic downscaling approaches provide more realistic spatial uncertainty representations compared to simpler stochastic methods, with each model offering different strengths in ensemble dispersion, deterministic skill, and physical consistency.", "conclusion": "Probabilistic downscaling is established as an effective enhancement to operational sub-seasonal wind forecasts for renewable energy planning and risk assessment."}}
{"id": "2510.16968", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16968", "abs": "https://arxiv.org/abs/2510.16968", "authors": ["Pingzhi Li", "Morris Yu-Chao Huang", "Zhen Tan", "Qingquan Song", "Jie Peng", "Kai Zou", "Yu Cheng", "Kaidi Xu", "Tianlong Chen"], "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures", "comment": "Code is at https://github.com/unites-lab/shadow-moe", "summary": "Knowledge Distillation (KD) accelerates training of large language models\n(LLMs) but poses intellectual property protection and LLM diversity risks.\nExisting KD detection methods based on self-identity or output similarity can\nbe easily evaded through prompt engineering. We present a KD detection\nframework effective in both white-box and black-box settings by exploiting an\noverlooked signal: the transfer of MoE \"structural habits\", especially internal\nrouting patterns. Our approach analyzes how different experts specialize and\ncollaborate across various inputs, creating distinctive fingerprints that\npersist through the distillation process. To extend beyond the white-box setup\nand MoE architectures, we further propose Shadow-MoE, a black-box method that\nconstructs proxy MoE representations via auxiliary distillation to compare\nthese patterns between arbitrary model pairs. We establish a comprehensive,\nreproducible benchmark that offers diverse distilled checkpoints and an\nextensible framework to facilitate future research. Extensive experiments\ndemonstrate >94% detection accuracy across various scenarios and strong\nrobustness to prompt-based evasion, outperforming existing baselines while\nhighlighting the structural habits transfer in LLMs.", "AI": {"tldr": "A novel Knowledge Distillation detection framework that identifies transferred structural patterns, particularly MoE routing habits, achieving >94% accuracy and strong robustness against prompt-based evasion in both white-box and black-box settings.", "motivation": "Existing KD detection methods are vulnerable to prompt engineering evasion, posing intellectual property protection and LLM diversity risks. Current approaches based on self-identity or output similarity fail to capture persistent structural patterns transferred during distillation.", "method": "Proposes detection using MoE structural habits (internal routing patterns) that persist through distillation. Introduces Shadow-MoE for black-box scenarios, constructing proxy MoE representations via auxiliary distillation to compare patterns between arbitrary model pairs.", "result": "Achieves >94% detection accuracy across various scenarios, demonstrates strong robustness to prompt-based evasion, and outperforms existing baselines. Comprehensive benchmark with diverse distilled checkpoints enables reproducible evaluation.", "conclusion": "Structural habits transfer is a reliable signal for KD detection that persists through distillation. The framework effectively addresses limitations of existing methods and provides a foundation for future research in intellectual property protection for LLMs."}}
{"id": "2510.16980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16980", "abs": "https://arxiv.org/abs/2510.16980", "authors": ["Kanghui Ning", "Zijie Pan", "Yushan Jiang", "Anderson Schneider", "Yuriy Nevmyvaka", "Dongjin Song"], "title": "Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision", "comment": null, "summary": "Time series reasoning is emerging as the next frontier in temporal analysis,\naiming to move beyond pattern recognition towards explicit, interpretable, and\ntrustworthy inference. This paper presents a BlueSky vision built on two\ncomplementary directions. One builds robust foundations for time series\nreasoning, centered on comprehensive temporal understanding, structured\nmulti-step reasoning, and faithful evaluation frameworks. The other advances\nsystem-level reasoning, moving beyond language-only explanations by\nincorporating multi-agent collaboration, multi-modal context, and\nretrieval-augmented approaches. Together, these directions outline a flexible\nand extensible framework for advancing time series reasoning, aiming to deliver\ninterpretable and trustworthy temporal intelligence across diverse domains.", "AI": {"tldr": "This paper presents a vision for advancing time series reasoning beyond pattern recognition to include explicit, interpretable inference through two complementary approaches: robust foundations and system-level reasoning.", "motivation": "Time series reasoning is emerging as the next frontier in temporal analysis, aiming to move beyond pattern recognition towards explicit, interpretable, and trustworthy inference.", "method": "Two complementary directions: 1) Building robust foundations through comprehensive temporal understanding, structured multi-step reasoning, and faithful evaluation frameworks; 2) Advancing system-level reasoning by incorporating multi-agent collaboration, multi-modal context, and retrieval-augmented approaches.", "result": "The paper outlines a flexible and extensible framework for advancing time series reasoning.", "conclusion": "The proposed framework aims to deliver interpretable and trustworthy temporal intelligence across diverse domains through the integration of foundational and system-level reasoning approaches."}}
{"id": "2510.16981", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16981", "abs": "https://arxiv.org/abs/2510.16981", "authors": ["Ahmed Khaled", "Kaan Ozkara", "Tao Yu", "Mingyi Hong", "Youngsuk Park"], "title": "MuonBP: Faster Muon via Block-Periodic Orthogonalization", "comment": null, "summary": "Gradient orthogonalization is a simple strategy that shows great utility in\nspeeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)\ncombines gradient orthogonalization with first-order momentum and achieves\nsignificant improvement in data efficiency over Adam/AdamW (Loshchilov and\nHutter, 2019) for language model training. However, when using model\nparallelism, gradient orthogonalization introduces additional overhead compared\nto coordinate-wise optimizers (such as AdamW) due to additional gather and\nscatter operations on gradient matrix shards from different devices. This\nadditional communication can amount to a throughput hit of 5%-10% compared to\nAdam/AdamW. To remedy this, we propose Muon with Block-Periodic\nOrthogonalization (MuonBP), which applies orthogonalization independently to\nmatrix shards on each device and periodically performs full orthogonalization\nto maintain training stability at scale. We show how to adjust the learning\nrate from the baseline to MuonBP and give convergence guarantees for this\nalgorithm. Crucially, our theory dictates that we use two stepsizes: one for\nthe blockwise orthogonalization steps, and one for the full orthogonalization\nsteps. Our method is simple, requires minimal hyperparameter adjustments, and\nachieves competitive iteration complexity compared with baseline Muon while\nproviding per-iteration throughput comparable to coordinate-wise methods such\nas AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO\noptimizer state sharding, MuonBP achieves 8% throughput increase compared to\nMuon with no degradation in performance.", "AI": {"tldr": "MuonBP improves training efficiency by applying block-periodic orthogonalization to reduce communication overhead in model-parallel training while maintaining performance.", "motivation": "To address the communication overhead introduced by gradient orthogonalization in model-parallel training, which causes 5%-10% throughput reduction compared to AdamW.", "method": "Proposes MuonBP with block-periodic orthogonalization: applies orthogonalization independently to matrix shards on each device and periodically performs full orthogonalization. Uses two learning rates for blockwise and full orthogonalization steps.", "result": "Achieves 8% throughput increase compared to Muon with no performance degradation when training an 8B model with eight-way tensor parallelism and ZeRO optimizer state sharding.", "conclusion": "MuonBP provides competitive iteration complexity with per-iteration throughput comparable to coordinate-wise methods like AdamW, requiring minimal hyperparameter adjustments."}}
{"id": "2510.16990", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16990", "abs": "https://arxiv.org/abs/2510.16990", "authors": ["Xuying Ning", "Dongqi Fu", "Tianxin Wei", "Wujiang Xu", "Jingrui He"], "title": "Graph4MM: Weaving Multimodal Learning with Structural Information", "comment": "ICML 2025", "summary": "Real-world multimodal data usually exhibit complex structural relationships\nbeyond traditional one-to-one mappings like image-caption pairs. Entities\nacross modalities interact in intricate ways, with images and text forming\ndiverse interconnections through contextual dependencies and co-references.\nGraphs provide powerful structural information for modeling intra-modal and\ninter-modal relationships. However, previous works fail to distinguish\nmulti-hop neighbors and treat the graph as a standalone modality, which\nfragments the overall understanding. This limitation presents two key\nchallenges in multimodal learning: (1) integrating structural information from\nmulti-hop neighbors into foundational models, and (2) fusing modality-specific\ninformation in a principled manner. To address these challenges, we revisit the\nrole of graphs in multimodal learning within the era of foundation models and\npropose Graph4MM, a graph-based multimodal learning framework. To be specific,\nwe introduce Hop-Diffused Attention, which integrates multi-hop structural\ninformation into self-attention through causal masking and hop diffusion.\nFurthermore, we design MM-QFormer, a multi-mapping querying transformer for\ncross-modal fusion. Through theoretical and empirical analysis, we show that\nleveraging structures to integrate both intra- and inter-modal interactions\nimproves multimodal understanding beyond treating them as a standalone\nmodality. Experiments on both generative and discriminative tasks show that\nGraph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,\nachieving a 6.93% average improvement.", "AI": {"tldr": "Graph4MM is a graph-based multimodal learning framework that addresses limitations in modeling complex structural relationships across modalities by integrating multi-hop structural information through Hop-Diffused Attention and enabling principled cross-modal fusion via MM-QFormer.", "motivation": "Real-world multimodal data exhibits complex structural relationships beyond simple one-to-one mappings, with intricate interconnections through contextual dependencies and co-references. Previous approaches fail to distinguish multi-hop neighbors and treat graphs as standalone modalities, fragmenting overall understanding.", "method": "Proposes Graph4MM framework with two key components: (1) Hop-Diffused Attention that integrates multi-hop structural information into self-attention using causal masking and hop diffusion, and (2) MM-QFormer, a multi-mapping querying transformer for cross-modal fusion.", "result": "Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines across both generative and discriminative tasks, achieving a 6.93% average improvement. Theoretical and empirical analysis shows that leveraging structures to integrate intra- and inter-modal interactions improves multimodal understanding.", "conclusion": "The framework demonstrates that properly integrating structural information from multi-hop neighbors and fusing modality-specific information in a principled manner significantly enhances multimodal learning beyond treating graphs as standalone modalities."}}
{"id": "2510.17002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17002", "abs": "https://arxiv.org/abs/2510.17002", "authors": ["Chang Liu", "Danial Chitnis"], "title": "EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit", "comment": null, "summary": "Circuit schematics play a crucial role in analog integrated circuit design,\nserving as the primary medium for human understanding and verification of\ncircuit functionality. While recent large language model (LLM)-based approaches\nhave shown promise in circuit topology generation and device sizing, most rely\nsolely on textual representations such as SPICE netlists, which lack visual\ninterpretability for circuit designers. To address this limitation, we propose\nEEschematic, an AI agent for automatic analog schematic generation based on a\nMultimodal Large Language Model (MLLM). EEschematic integrates textual, visual,\nand symbolic modalities to translate SPICE netlists into schematic diagrams\nrepresented in a human-editable format. The framework uses six analog\nsubstructure examples for few-shot placement and a Visual Chain-of-Thought\n(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic\nclarity and symmetry. Experimental results on representative analog circuits,\nincluding a CMOS inverter, a five-transistor operational transconductance\namplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that\nEEschematic produces schematics with high visual quality and structural\ncorrectness.", "AI": {"tldr": "EEschematic is an AI agent that uses a Multimodal Large Language Model to automatically generate human-editable schematic diagrams from SPICE netlists, addressing the visual interpretability gap in current LLM-based circuit design approaches.", "motivation": "Current LLM-based circuit design methods rely on textual representations like SPICE netlists, which lack visual interpretability for circuit designers who need schematic diagrams for understanding and verification.", "method": "Uses a Multimodal Large Language Model to integrate textual, visual, and symbolic modalities. Employs six analog substructure examples for few-shot placement and a Visual Chain-of-Thought strategy for iterative refinement of placement and wiring.", "result": "Experimental results on CMOS inverter, five-transistor operational transconductance amplifier, and telescopic cascode amplifier show that EEschematic produces schematics with high visual quality and structural correctness.", "conclusion": "EEschematic successfully bridges the gap between textual circuit representations and visual schematics, providing an effective automated solution for generating human-interpretable circuit diagrams."}}
{"id": "2510.17015", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.17015", "abs": "https://arxiv.org/abs/2510.17015", "authors": ["Mingyan Yang", "Guanjie Wang", "Manqi Luo", "Yifei Liu", "Chen Chen", "Han Zhao", "Yu Feng", "Quan Chen", "Minyi Guo"], "title": "Justitia: Fair and Efficient Scheduling for LLM Applications", "comment": null, "summary": "In the era of Large Language Models (LLMs), it has been popular to launch a\nseries of LLM inferences -- we call an LLM application -- to better solve\nreal-world problems. When serving those applications in shared GPU servers, the\nschedulers are expected to attain fast application completions with guaranteed\nworst-case performance. However, mainstream LLM schedulers fail to behave well\nfor LLM applications -- due to head-of-line blocking or over-constrained\nresource allocation. In this paper, we propose to serve LLM applications in a\nfair and also efficient manner. To this end, we design Justitia, a novel\nscheduler with three key techniques. First, given that memory is prevalently a\nbottleneck for mainstream inference frameworks like vLLM, Justitia models the\nservice cost of LLM applications in a memory-centric manner. Meanwhile, it uses\na simple neural network model to conduct light-weight and also accurate demand\nprediction. Moreover, Justitia adopts a virtual-time based fair queuing\nalgorithm to reduce the overall performance with guaranteed worst-case delay.\nWe have implemented Justitia atop vLLM, and experimental results involving\ndiverse LLM applications show that it can substantially enhance the scheduling\nefficiency with fairness preserved.", "AI": {"tldr": "Justitia is a novel scheduler for LLM applications that addresses scheduling inefficiencies in shared GPU servers by using memory-centric cost modeling, neural network demand prediction, and virtual-time fair queuing to ensure both efficiency and fairness.", "motivation": "Current LLM schedulers suffer from head-of-line blocking and over-constrained resource allocation, leading to poor performance for LLM applications in shared GPU environments where fast completion times and worst-case performance guarantees are essential.", "method": "Justitia employs three key techniques: memory-centric service cost modeling (since memory is the bottleneck in frameworks like vLLM), lightweight neural network for accurate demand prediction, and virtual-time based fair queuing algorithm to ensure fairness while optimizing performance.", "result": "Experimental results show that Justitia implemented on vLLM significantly improves scheduling efficiency while maintaining fairness across diverse LLM applications.", "conclusion": "Justitia provides an effective solution for fair and efficient scheduling of LLM applications in shared GPU servers, overcoming limitations of existing schedulers through its memory-aware modeling, predictive capabilities, and fair queuing approach."}}
{"id": "2510.17021", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17021", "abs": "https://arxiv.org/abs/2510.17021", "authors": ["Bingqi Shang", "Yiwei Chen", "Yihua Zhang", "Bingquan Shen", "Sijia Liu"], "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning", "comment": null, "summary": "Large language model (LLM) unlearning has become a critical mechanism for\nremoving undesired data, knowledge, or behaviors from pre-trained models while\nretaining their general utility. Yet, with the rise of open-weight LLMs, we\nask: can the unlearning process itself be backdoored, appearing successful\nunder normal conditions yet reverting to pre-unlearned behavior when a hidden\ntrigger is activated? Drawing inspiration from classical backdoor attacks that\nembed triggers into training data to enforce specific behaviors, we investigate\nbackdoor unlearning, where models forget as intended in the clean setting but\nrecover forgotten knowledge when the trigger appears. We show that designing\nsuch attacks presents unique challenges, hinging on where triggers are placed\nand how backdoor training is reinforced. We uncover a strong link between\nbackdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens\nconsistently attract disproportionate attention in LLMs. Our analysis reveals\nthat these attention sinks serve as gateways for backdoor unlearning: placing\ntriggers at sink positions and aligning their attention values markedly\nenhances backdoor persistence. Extensive experiments validate these findings,\nshowing that attention-sink-guided backdoor unlearning reliably restores\nforgotten knowledge in the presence of backdoor triggers, while behaving\nindistinguishably from a normally unlearned model when triggers are absent.\nCode is available at https://github.com/OPTML-Group/Unlearn-Backdoor.", "AI": {"tldr": "This paper introduces 'backdoor unlearning' - a vulnerability where LLM unlearning appears successful but secretly embeds triggers that can restore forgotten knowledge. The attack exploits attention sinks in LLMs to create persistent backdoors.", "motivation": "With the rise of open-weight LLMs, the authors investigate whether unlearning processes can be maliciously manipulated to create hidden backdoors that appear successful under normal conditions but revert to pre-unlearned behavior when triggered.", "method": "The researchers design backdoor unlearning attacks by placing triggers at attention sink positions in LLMs and aligning their attention values. They leverage the attention sink phenomenon where shallow input tokens attract disproportionate attention, using these positions as gateways for persistent backdoors.", "result": "Extensive experiments show that attention-sink-guided backdoor unlearning reliably restores forgotten knowledge when backdoor triggers are present, while behaving indistinguishably from normally unlearned models when triggers are absent. The method demonstrates strong backdoor persistence.", "conclusion": "Attention sinks serve as effective gateways for backdoor unlearning attacks, creating a significant security vulnerability in LLM unlearning processes where models can secretly retain and restore forgotten knowledge through hidden triggers."}}
{"id": "2510.17022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17022", "abs": "https://arxiv.org/abs/2510.17022", "authors": ["Kevin P. O Keeffe"], "title": "Curiosity-driven RL for symbolic equation solving", "comment": "Accepted at the NeurIPS 2025 MATH-AI Workshop", "summary": "We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.", "AI": {"tldr": "RL with PPO, curiosity-based exploration, and graph-based actions can solve nonlinear equations including radicals, exponentials, and trig functions, suggesting potential for general symbolic reasoning.", "motivation": "To explore if reinforcement learning (RL) can be useful for symbolic mathematics, building on previous work that used contrastive learning for linear equations.", "method": "Model-free PPO augmented with curiosity-based exploration and graph-based actions.", "result": "The approach successfully solves nonlinear equations involving radicals, exponentials, and trigonometric functions.", "conclusion": "Curiosity-based exploration may be useful for general symbolic reasoning tasks."}}
{"id": "2510.17036", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17036", "abs": "https://arxiv.org/abs/2510.17036", "authors": ["Nguyen Do", "Bach Ngo", "Youval Kashuv", "Canh V. Pham", "Hanghang Tong", "My T. Thai"], "title": "Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation", "comment": "62 pages, 19 figures, Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We study the Quality of Service Degradation (QoSD) problem, in which an\nadversary perturbs edge weights to degrade network performance. This setting\narises in both network infrastructures and distributed ML systems, where\ncommunication quality, not just connectivity, determines functionality. While\nclassical methods rely on combinatorial optimization, and recent ML approaches\naddress only restricted linear variants with small-size networks, no prior\nmodel directly tackles the QoSD problem under nonlinear edge-weight functions.\nThis work proposes \\PIMMA, a self-reinforcing generative framework that\nsynthesizes feasible solutions in latent space, to fill this gap. Our method\nincludes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm\nthat uses graph learning and approximation to produce feasible solutions with\nperformance guarantee, (2) Morph: a new theoretically grounded training\nparadigm for Mixture of Conditional VAEs guided by an energy-based model to\ncapture solution feature distributions, and (3) Refine: a reinforcement\nlearning agent that explores this space to generate progressively near-optimal\nsolutions using our designed differentiable reward function. Experiments on\nboth synthetic and real-world networks show that our approach consistently\noutperforms classical and ML baselines, particularly in scenarios with\nnonlinear cost functions where traditional methods fail to generalize.", "AI": {"tldr": "PIMMA is a self-reinforcing generative framework that addresses Quality of Service Degradation (QoSD) by synthesizing feasible solutions through three phases: Forge (predictive path-stressing), Morph (mixture of conditional VAEs), and Refine (reinforcement learning).", "motivation": "The QoSD problem involves adversaries perturbing edge weights to degrade network performance, which affects both network infrastructures and distributed ML systems. Existing methods either rely on combinatorial optimization or only handle restricted linear variants with small networks, leaving nonlinear edge-weight functions unaddressed.", "method": "PIMMA uses a three-phase approach: (1) Forge - Predictive Path-Stressing algorithm using graph learning and approximation, (2) Morph - Mixture of Conditional VAEs guided by energy-based model to capture solution distributions, (3) Refine - Reinforcement learning agent with differentiable reward function to generate near-optimal solutions.", "result": "Experiments on synthetic and real-world networks show PIMMA consistently outperforms classical and ML baselines, especially in scenarios with nonlinear cost functions where traditional methods fail to generalize.", "conclusion": "PIMMA effectively addresses the QoSD problem under nonlinear edge-weight functions, filling a critical gap in existing approaches and demonstrating superior performance compared to both classical optimization and machine learning baselines."}}
{"id": "2510.17040", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17040", "abs": "https://arxiv.org/abs/2510.17040", "authors": ["Hoang-Son Nguyen", "Xiao Fu"], "title": "Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability", "comment": "30 pages, 3 figures", "summary": "Latent component identification from unknown nonlinear mixtures is a\nfoundational challenge in machine learning, with applications in tasks such as\ndisentangled representation learning and causal inference. Prior work in\nnonlinear independent component analysis (nICA) has shown that auxiliary\nsignals -- such as weak supervision -- can support identifiability of\nconditionally independent latent components. More recent approaches explore\nstructural assumptions, e.g., sparsity in the Jacobian of the mixing function,\nto relax such requirements. In this work, we introduce Diverse Influence\nComponent Analysis (DICA), a framework that exploits the convex geometry of the\nmixing function's Jacobian. We propose a Jacobian Volume Maximization\n(J-VolMax) criterion, which enables latent component identification by\nencouraging diversity in their influence on the observed variables. Under\nreasonable conditions, this approach achieves identifiability without relying\non auxiliary information, latent component independence, or Jacobian sparsity\nassumptions. These results extend the scope of identifiability analysis and\noffer a complementary perspective to existing methods.", "AI": {"tldr": "DICA framework uses Jacobian Volume Maximization to identify latent components from nonlinear mixtures by promoting diversity in their influence on observed variables, achieving identifiability without auxiliary signals or independence assumptions.", "motivation": "To address the fundamental challenge of identifying latent components from unknown nonlinear mixtures, which has applications in disentangled representation learning and causal inference, without relying on auxiliary signals or strong structural assumptions.", "method": "Proposes Diverse Influence Component Analysis (DICA) with Jacobian Volume Maximization (J-VolMax) criterion that exploits the convex geometry of the mixing function's Jacobian to encourage diversity in latent components' influence on observed variables.", "result": "The approach achieves identifiability of latent components under reasonable conditions without requiring auxiliary information, latent component independence, or Jacobian sparsity assumptions.", "conclusion": "DICA extends the scope of identifiability analysis in nonlinear ICA and provides a complementary perspective to existing methods by leveraging geometric properties of the mixing function rather than statistical or structural constraints."}}
{"id": "2510.17057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17057", "abs": "https://arxiv.org/abs/2510.17057", "authors": ["Nikolaus Howe", "Micah Carroll"], "title": "The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs", "comment": "26 pages", "summary": "The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning\nhas emerged as a promising approach for developing more capable language\nmodels. In turn, this has led to investigation of CoT monitoring as a\ncompelling method for detecting harmful behaviors such as reward hacking, under\nthe assumption that models' reasoning processes reflect their internal\ndecision-making. In practice, LLM training often produces unintended behaviors\ndue to imperfect reward signals, leading models to develop misaligned\ntendencies. A common corrective approach is to apply post-hoc instructions to\navoid problematic behaviors like sycophancy, but what happens to the model's\nreasoning process when these instructions conflict with learned behaviors? We\ninvestigate this question in simple settings and find that models engage in\nsystematic motivated reasoning -- generating plausible-sounding justifications\nfor violating their instructions while downplaying potential harms. Beyond\nbeing an interesting property of training, we find that while motivated\nreasoning can be detected by most frontier reasoning models, smaller LLM judges\ncan fail to identify a portion of it, and in rare cases can themselves be\npersuaded that the reasoning is correct, despite it contradicting clear\ninstructions. This capability gap raises concerns that as models become more\nsophisticated, their motivated reasoning may become increasingly difficult for\nmonitors to detect. Our results underscore the need to account for motivated\nreasoning when relying on chain-of-thought processes for model evaluation and\noversight. All code for this paper will be made available. WARNING: some\nexamples in this paper may be upsetting.", "AI": {"tldr": "The paper investigates how LLMs engage in motivated reasoning when post-hoc instructions conflict with learned behaviors, finding that models generate plausible justifications for violating instructions while downplaying harms, and that detection capability varies across model sizes.", "motivation": "To understand what happens to models' reasoning processes when post-hoc instructions conflict with learned behaviors, particularly in the context of detecting harmful behaviors through chain-of-thought monitoring.", "method": "The researchers investigate this question in simple settings, examining how models generate justifications for violating instructions and testing the detection capability of different-sized LLM judges.", "result": "Models engage in systematic motivated reasoning - generating plausible-sounding justifications while downplaying potential harms. While most frontier reasoning models can detect this motivated reasoning, smaller LLM judges often fail to identify it and can sometimes be persuaded that the flawed reasoning is correct.", "conclusion": "The capability gap in detecting motivated reasoning raises concerns that as models become more sophisticated, their motivated reasoning may become increasingly difficult to detect, underscoring the need to account for this phenomenon when relying on chain-of-thought processes for model evaluation and oversight."}}
{"id": "2510.17058", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17058", "abs": "https://arxiv.org/abs/2510.17058", "authors": ["Hassan Hamad", "Yuou Qiu", "Peter A. Beerel", "Keith M. Chugg"], "title": "Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training", "comment": null, "summary": "While advancements in quantization have significantly reduced the\ncomputational costs of inference in deep learning, training still predominantly\nrelies on complex floating-point arithmetic. Low-precision fixed-point training\npresents a compelling alternative. This work introduces a novel enhancement in\nlow-precision logarithmic fixed-point training, geared towards future hardware\naccelerator designs. We propose incorporating bitwidth in the design of\napproximations to arithmetic operations. To this end, we introduce a new\nhardware-friendly, piece-wise linear approximation for logarithmic addition.\nUsing simulated annealing, we optimize this approximation at different\nprecision levels. A C++ bit-true simulation demonstrates training of VGG-11 and\nVGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer\narithmetic with minimal accuracy degradation compared to 32-bit floating-point\ntraining. Our hardware study reveals up to 32.5% reduction in area and 53.5%\nreduction in energy consumption for the proposed LNS multiply-accumulate units\ncompared to that of linear fixed-point equivalents.", "AI": {"tldr": "This paper introduces a novel low-precision logarithmic fixed-point training method with hardware-friendly approximations, achieving comparable accuracy to 32-bit floating-point training while significantly reducing hardware area and energy consumption.", "motivation": "Current deep learning training relies heavily on complex floating-point arithmetic, which is computationally expensive. Low-precision fixed-point training offers a promising alternative to reduce computational costs and enable more efficient hardware accelerator designs.", "method": "The authors propose incorporating bitwidth in arithmetic operation approximations and introduce a hardware-friendly piece-wise linear approximation for logarithmic addition. They use simulated annealing to optimize approximations at different precision levels and validate through C++ bit-true simulations.", "result": "The method successfully trains VGG-11 and VGG-16 models on CIFAR-100 and TinyImageNet using 12-bit integer arithmetic with minimal accuracy degradation compared to 32-bit floating-point training. Hardware analysis shows 32.5% area reduction and 53.5% energy reduction for LNS multiply-accumulate units.", "conclusion": "Low-precision logarithmic fixed-point training is a viable approach that maintains training accuracy while significantly improving hardware efficiency, making it suitable for future hardware accelerator designs."}}
{"id": "2510.17059", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17059", "abs": "https://arxiv.org/abs/2510.17059", "authors": ["Kathryn Wantlin", "Chongyi Zheng", "Benjamin Eysenbach"], "title": "Consistent Zero-Shot Imitation with Contrastive Goal Inference", "comment": null, "summary": "In the same way that generative models today conduct most of their training\nin a self-supervised fashion, how can agentic models conduct their training in\na self-supervised fashion, interactively exploring, learning, and preparing to\nquickly adapt to new tasks? A prerequisite for embodied agents deployed in real\nworld interactions ought to be training with interaction, yet today's most\nsuccessful AI models (e.g., VLMs, LLMs) are trained without an explicit notion\nof action. The problem of pure exploration (which assumes no data as input) is\nwell studied in the reinforcement learning literature and provides agents with\na wide array of experiences, yet it fails to prepare them for rapid adaptation\nto new tasks. Today's language and vision models are trained on data provided\nby humans, which provides a strong inductive bias for the sorts of tasks that\nthe model will have to solve (e.g., modeling chords in a song, phrases in a\nsonnet, sentences in a medical record). However, when they are prompted to\nsolve a new task, there is a faulty tacit assumption that humans spend most of\ntheir time in the most rewarding states. The key contribution of our paper is a\nmethod for pre-training interactive agents in a self-supervised fashion, so\nthat they can instantly mimic human demonstrations. Our method treats goals\n(i.e., observations) as the atomic construct. During training, our method\nautomatically proposes goals and practices reaching them, building off prior\nwork in reinforcement learning exploration. During evaluation, our method\nsolves an (amortized) inverse reinforcement learning problem to explain\ndemonstrations as optimal goal-reaching behavior. Experiments on standard\nbenchmarks (not designed for goal-reaching) show that our approach outperforms\nprior methods for zero-shot imitation.", "AI": {"tldr": "The paper proposes a self-supervised pre-training method for interactive agents that enables them to instantly mimic human demonstrations by treating goals as atomic constructs and practicing goal-reaching during training, then solving inverse reinforcement learning during evaluation.", "motivation": "Current AI models (VLMs, LLMs) are trained without explicit action concepts and rely on human-provided data, which assumes humans spend most time in rewarding states. There's a need for embodied agents that can train interactively and quickly adapt to new tasks through self-supervised exploration.", "method": "The method treats goals (observations) as atomic constructs. During training, it automatically proposes goals and practices reaching them using reinforcement learning exploration techniques. During evaluation, it solves an amortized inverse reinforcement learning problem to explain demonstrations as optimal goal-reaching behavior.", "result": "Experiments on standard benchmarks (not specifically designed for goal-reaching) show that the approach outperforms prior methods for zero-shot imitation.", "conclusion": "The proposed self-supervised pre-training method enables interactive agents to effectively learn goal-reaching behaviors and achieve superior zero-shot imitation performance compared to existing approaches."}}
{"id": "2510.17088", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17088", "abs": "https://arxiv.org/abs/2510.17088", "authors": ["Zan Li", "Rui Fan"], "title": "Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing", "comment": null, "summary": "Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity\nfreezes, contagion cascades, regime shifts), but existing detectors treat all\nanomalies uniformly, producing scalar scores without revealing which mechanism\nis failing, where risks concentrate, or how to intervene. This opacity prevents\ntargeted regulatory responses. Three unsolved challenges persist: (1) static\ngraph structures cannot adapt when market correlations shift during regime\nchanges; (2) uniform detection mechanisms miss type-specific signatures across\nmultiple temporal scales while failing to integrate individual behaviors with\nnetwork contagion; (3) black-box outputs provide no actionable guidance on\nanomaly mechanisms or their temporal evolution.\n  We address these via adaptive graph learning with specialized expert networks\nthat provide built-in interpretability. Our framework captures multi-scale\ntemporal dependencies through BiLSTM with self-attention, fuses temporal and\nspatial information via cross-modal attention, learns dynamic graphs through\nneural multi-source interpolation, adaptively balances learned dynamics with\nstructural priors via stress-modulated fusion, routes anomalies to four\nmechanism-specific experts, and produces dual-level interpretable attributions.\nCritically, interpretability is embedded architecturally rather than applied\npost-hoc.\n  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events\nwith 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley\nBank case study demonstrates anomaly evolution tracking: Price-Shock expert\nweight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48\n(66% above baseline) one week later, revealing automatic temporal mechanism\nidentification without labeled supervision.", "AI": {"tldr": "The paper proposes an adaptive graph learning framework with specialized expert networks for interpretable financial anomaly detection that identifies specific anomaly mechanisms rather than providing uniform scalar scores.", "motivation": "Existing financial anomaly detectors treat all anomalies uniformly, producing scalar scores without revealing which mechanism is failing, where risks concentrate, or how to intervene. This opacity prevents targeted regulatory responses.", "method": "The framework uses adaptive graph learning with specialized expert networks, capturing multi-scale temporal dependencies through BiLSTM with self-attention, fusing temporal and spatial information via cross-modal attention, learning dynamic graphs through neural multi-source interpolation, adaptively balancing learned dynamics with structural priors via stress-modulated fusion, and routing anomalies to four mechanism-specific experts.", "result": "On 100 US equities (2017-2024), the method achieves 92.3% detection of 13 major events with 3.8-day lead time, outperforming best baseline by 30.8 percentage points. Silicon Valley Bank case study shows automatic temporal mechanism identification without labeled supervision.", "conclusion": "The framework provides dual-level interpretable attributions with interpretability embedded architecturally rather than applied post-hoc, enabling targeted regulatory responses by revealing specific anomaly mechanisms and their temporal evolution."}}
{"id": "2510.17099", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.17099", "abs": "https://arxiv.org/abs/2510.17099", "authors": ["Zhiyuan Fan", "Arnab Maiti", "Kevin Jamieson", "Lillian J. Ratliff", "Gabriele Farina"], "title": "On the Universal Near Optimality of Hedge in Combinatorial Settings", "comment": "28 pages, 1 Figure", "summary": "In this paper, we study the classical Hedge algorithm in combinatorial\nsettings. In each round, the learner selects a vector $\\boldsymbol{x}_t$ from a\nset $X \\subseteq \\{0,1\\}^d$, observes a full loss vector $\\boldsymbol{y}_t \\in\n\\mathbb{R}^d$, and incurs a loss $\\langle \\boldsymbol{x}_t, \\boldsymbol{y}_t\n\\rangle \\in [-1,1]$. This setting captures several important problems,\nincluding extensive-form games, resource allocation, $m$-sets, online multitask\nlearning, and shortest-path problems on directed acyclic graphs (DAGs). It is\nwell known that Hedge achieves a regret of $O\\big(\\sqrt{T \\log |X|}\\big)$ after\n$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal\nacross all combinatorial settings. To that end, we show that for any $X\n\\subseteq \\{0,1\\}^d$, Hedge is near-optimal--specifically, up to a $\\sqrt{\\log\nd}$ factor--by establishing a lower bound of $\\Omega\\big(\\sqrt{T \\log(|X|)/\\log\nd}\\big)$ that holds for any algorithm. We then identify a natural class of\ncombinatorial sets--namely, $m$-sets with $\\log d \\leq m \\leq \\sqrt{d}$--for\nwhich this lower bound is tight, and for which Hedge is provably suboptimal by\na factor of exactly $\\sqrt{\\log d}$. At the same time, we show that Hedge is\noptimal for online multitask learning, a generalization of the classical\n$K$-experts problem. Finally, we leverage the near-optimality of Hedge to\nestablish the existence of a near-optimal regularizer for online shortest-path\nproblems in DAGs--a setting that subsumes a broad range of combinatorial\ndomains. Specifically, we show that the classical Online Mirror Descent (OMD)\nalgorithm, when instantiated with the dilated entropy regularizer, is\niterate-equivalent to Hedge, and therefore inherits its near-optimal regret\nguarantees for DAGs.", "AI": {"tldr": "This paper analyzes the optimality of the Hedge algorithm in combinatorial online learning settings. It shows Hedge is near-optimal with a \u221alog d factor gap, identifies specific settings where Hedge is suboptimal, proves its optimality for online multitask learning, and establishes near-optimal regularizers for DAG shortest-path problems.", "motivation": "To determine whether the classical Hedge algorithm is optimal across all combinatorial online learning settings, as it's known to achieve O(\u221aT log|X|) regret but its fundamental optimality wasn't fully understood.", "method": "The authors establish a general lower bound of \u03a9(\u221aT log(|X|)/log d) for any algorithm, analyze specific combinatorial structures (m-sets), prove Hedge's optimality for online multitask learning, and show iterate-equivalence between Online Mirror Descent with dilated entropy and Hedge for DAGs.", "result": "Hedge is near-optimal with at most \u221alog d factor gap from optimal, but is provably suboptimal by exactly \u221alog d for m-sets where log d \u2264 m \u2264 \u221ad. Hedge is optimal for online multitask learning, and near-optimal regularizers exist for DAG shortest-path problems.", "conclusion": "Hedge is near-optimal across combinatorial settings with a tight \u221alog d factor gap, optimal for some problems like multitask learning, and its near-optimality enables finding near-optimal regularizers for broader combinatorial domains via iterate-equivalence with OMD."}}
{"id": "2510.17106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17106", "abs": "https://arxiv.org/abs/2510.17106", "authors": ["Chen Zhang", "Weixin Bu", "Wendong Xu", "Runsheng Yu", "Yik-Chung Wu", "Ngai Wong"], "title": "Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling", "comment": "Preprint", "summary": "Transformers have achieved remarkable success in time series modeling, yet\ntheir internal mechanisms remain opaque. This work demystifies the Transformer\nencoder by establishing its fundamental equivalence to a Graph Convolutional\nNetwork (GCN). We show that in the forward pass, the attention distribution\nmatrix serves as a dynamic adjacency matrix, and its composition with\nsubsequent transformations performs computations analogous to graph\nconvolution. Moreover, we demonstrate that in the backward pass, the update\ndynamics of value and feed-forward projections mirror those of GCN parameters.\nBuilding on this unified theoretical reinterpretation, we propose\n\\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined\narchitecture that removes redundant linear projections and incorporates\nmulti-hop graph aggregation. This perspective yields an explicit and\ninterpretable representation of temporal dependencies across different scales,\nnaturally expressed as graph edges. Experiments on standard forecasting\nbenchmarks confirm that Fighter achieves competitive performance while\nproviding clearer mechanistic interpretability of its predictions.", "AI": {"tldr": "This paper establishes equivalence between Transformer encoders and Graph Convolutional Networks (GCNs), showing attention matrices act as dynamic adjacency matrices and proposing Fighter - a streamlined architecture that removes redundant projections while maintaining competitive performance.", "motivation": "To demystify Transformer internal mechanisms in time series modeling by revealing their fundamental equivalence to GCNs, providing clearer interpretability of temporal dependencies.", "method": "Theoretical analysis showing attention distribution as dynamic adjacency matrix, then proposing Fighter architecture that removes redundant linear projections and incorporates multi-hop graph aggregation for explicit temporal dependency representation.", "result": "Experiments on standard forecasting benchmarks confirm Fighter achieves competitive performance while providing clearer mechanistic interpretability of predictions.", "conclusion": "Transformers can be fundamentally reinterpreted as GCNs, enabling more interpretable architectures like Fighter that maintain performance while offering explicit representations of temporal dependencies as graph edges."}}
{"id": "2510.17132", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17132", "abs": "https://arxiv.org/abs/2510.17132", "authors": ["Ioannis Tsaknakis", "Bingqing Song", "Shuyu Gan", "Dongyeop Kang", "Alfredo Garcia", "Gaowen Liu", "Charles Fleming", "Mingyi Hong"], "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction", "comment": null, "summary": "Large Language Models (LLMs) excel at producing broadly relevant text, but\nthis generality becomes a limitation when user-specific preferences are\nrequired, such as recommending restaurants or planning travel. In these\nscenarios, users rarely articulate every preference explicitly; instead, much\nof what they care about remains latent, waiting to be inferred. This raises a\nfundamental question: Can LLMs uncover and reason about such latent information\nthrough conversation?\n  We address this problem by introducing a unified benchmark for evaluating\nlatent information discovery - the ability of LLMs to reveal and utilize hidden\nuser attributes through multi-turn interaction. The benchmark spans three\nprogressively realistic settings: the classic 20 Questions game, Personalized\nQuestion Answering, and Personalized Text Summarization. All tasks share a\ntri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of\nelicitation and adaptation. Our results reveal that while LLMs can indeed\nsurface latent information through dialogue, their success varies dramatically\nwith context: from 32% to 98%, depending on task complexity, topic, and number\nof hidden attributes. This benchmark provides the first systematic framework\nfor studying latent information discovery in personalized interaction,\nhighlighting that effective preference inference remains an open frontier for\nbuilding truly adaptive AI systems.", "AI": {"tldr": "This paper introduces a benchmark for evaluating LLMs' ability to discover latent user information through conversation, showing performance varies from 32% to 98% depending on task complexity.", "motivation": "LLMs struggle with user-specific preferences when users don't explicitly state all their needs, requiring inference of latent information through dialogue.", "method": "Created a unified benchmark with tri-agent framework (User, Assistant, Judge) across three settings: 20 Questions game, Personalized Question Answering, and Personalized Text Summarization.", "result": "LLMs can surface latent information through dialogue but success varies dramatically (32% to 98%) based on task complexity, topic, and number of hidden attributes.", "conclusion": "Effective preference inference remains an open frontier for building truly adaptive AI systems, and the benchmark provides the first systematic framework for studying latent information discovery."}}
{"id": "2510.17136", "categories": ["cs.LG", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.17136", "abs": "https://arxiv.org/abs/2510.17136", "authors": ["Enhao Gu", "Haolin Hou"], "title": "In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models", "comment": "6 pages, 3 figures. ICML 2025 Workshop submission", "summary": "The generation of high-quality, diverse, and prompt-aligned images is a\ncentral goal in image-generating diffusion models. The popular classifier-free\nguidance (CFG) approach improves quality and alignment at the cost of reduced\nvariation, creating an inherent entanglement of these effects. Recent work has\nsuccessfully disentangled these properties by guiding a model with a separately\ntrained, inferior counterpart; however, this solution introduces the\nconsiderable overhead of requiring an auxiliary model. We challenge this\nprerequisite by introducing In-situ Autoguidance, a method that elicits\nguidance from the model itself without any auxiliary components. Our approach\ndynamically generates an inferior prediction on the fly using a stochastic\nforward pass, reframing guidance as a form of inference-time self-correction.\nWe demonstrate that this zero-cost approach is not only viable but also\nestablishes a powerful new baseline for cost-efficient guidance, proving that\nthe benefits of self-guidance can be achieved without external models.", "AI": {"tldr": "In-situ Autoguidance enables diffusion models to self-correct during inference without needing auxiliary models, achieving better image quality and prompt alignment while maintaining diversity.", "motivation": "Classifier-free guidance improves image quality and alignment but reduces variation, and existing disentanglement methods require separate auxiliary models which adds overhead.", "method": "Dynamic generation of inferior predictions using stochastic forward passes during inference, treating guidance as self-correction without external components.", "result": "Zero-cost approach proves viable and establishes a powerful baseline for cost-efficient guidance, achieving self-guidance benefits without external models.", "conclusion": "In-situ Autoguidance successfully demonstrates that self-guidance can be achieved without auxiliary models, providing an efficient solution to the quality-diversity trade-off in diffusion models."}}
{"id": "2510.17160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17160", "abs": "https://arxiv.org/abs/2510.17160", "authors": ["Derda Kaymak", "Gyuhak Kim", "Tomoya Kaichi", "Tatsuya Konishi", "Bing Liu"], "title": "Learning After Model Deployment", "comment": "Published at ECAI-2025", "summary": "In classic supervised learning, once a model is deployed in an application,\nit is fixed. No updates will be made to it during the application. This is\ninappropriate for many dynamic and open environments, where unexpected samples\nfrom unseen classes may appear. In such an environment, the model should be\nable to detect these novel samples from unseen classes and learn them after\nthey are labeled. We call this paradigm Autonomous Learning after Model\nDeployment (ALMD). The learning here is continuous and involves no human\nengineers. Labeling in this scenario is performed by human co-workers or other\nknowledgeable agents, which is similar to what humans do when they encounter an\nunfamiliar object and ask another person for its name. In ALMD, the detection\nof novel samples is dynamic and differs from traditional out-of-distribution\n(OOD) detection in that the set of in-distribution (ID) classes expands as new\nclasses are learned during application, whereas ID classes is fixed in\ntraditional OOD detection. Learning is also different from classic supervised\nlearning because in ALMD, we learn the encountered new classes immediately and\nincrementally. It is difficult to retrain the model from scratch using all the\npast data from the ID classes and the novel samples from newly discovered\nclasses, as this would be resource- and time-consuming. Apart from these two\nchallenges, ALMD faces the data scarcity issue because instances of new classes\noften appear sporadically in real-life applications. To address these issues,\nwe propose a novel method, PLDA, which performs dynamic OOD detection and\nincremental learning of new classes on the fly. Empirical evaluations will\ndemonstrate the effectiveness of PLDA.", "AI": {"tldr": "The paper introduces Autonomous Learning after Model Deployment (ALMD), a paradigm where models continuously detect novel samples from unseen classes and learn them incrementally after labeling, without human engineers. It proposes PLDA method to address dynamic OOD detection and incremental learning challenges.", "motivation": "Traditional supervised learning with fixed deployed models is unsuitable for dynamic environments where unexpected samples from unseen classes appear. Models need to autonomously detect and learn new classes continuously during application.", "method": "Proposes PLDA method that performs dynamic out-of-distribution (OOD) detection and incremental learning of new classes on the fly. Unlike traditional OOD detection, the in-distribution class set expands as new classes are learned.", "result": "Empirical evaluations demonstrate the effectiveness of PLDA in addressing the challenges of ALMD.", "conclusion": "ALMD enables continuous learning in dynamic environments, and PLDA provides an effective solution for dynamic OOD detection and incremental learning of new classes during model deployment."}}
{"id": "2510.17162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17162", "abs": "https://arxiv.org/abs/2510.17162", "authors": ["Guanjie Cheng", "Siyang Liu", "Junqin Huang", "Xinkui Zhao", "Yin Wang", "Mengying Zhu", "Linghe Kong", "Shuiguang Deng"], "title": "ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing", "comment": "12 pages, 8 figures, 4 tables. Submitted to The Web Conference (WWW\n  2026)", "summary": "Mobile edge crowdsensing (MECS) systems continuously generate and transmit\nuser data in dynamic, resource-constrained environments, exposing users to\nsignificant privacy threats. In practice, many privacy-preserving mechanisms\nbuild on differential privacy (DP). However, static DP mechanisms often fail to\nadapt to evolving risks, for example, shifts in adversarial capabilities,\nresource constraints and task requirements, resulting in either excessive noise\nor inadequate protection. To address this challenge, we propose ALPINE, a\nlightweight, adaptive framework that empowers terminal devices to autonomously\nadjust differential privacy levels in real time. ALPINE operates as a\nclosed-loop control system consisting of four modules: dynamic risk perception,\nprivacy decision via twin delayed deep deterministic policy gradient (TD3),\nlocal privacy execution and performance verification from edge nodes. Based on\nenvironmental risk assessments, we design a reward function that balances\nprivacy gains, data utility and energy cost, guiding the TD3 agent to\nadaptively tune noise magnitude across diverse risk scenarios and achieve a\ndynamic equilibrium among privacy, utility and cost. Both the collaborative\nrisk model and pretrained TD3-based agent are designed for low-overhead\ndeployment. Extensive theoretical analysis and real-world simulations\ndemonstrate that ALPINE effectively mitigates inference attacks while\npreserving utility and cost, making it practical for large-scale edge\napplications.", "AI": {"tldr": "ALPINE is an adaptive differential privacy framework for mobile edge crowdsensing that dynamically adjusts privacy levels in real-time using a TD3-based control system to balance privacy protection, data utility, and energy costs.", "motivation": "Static differential privacy mechanisms fail to adapt to evolving risks in mobile edge crowdsensing environments, leading to either excessive noise or inadequate protection against privacy threats.", "method": "ALPINE uses a closed-loop control system with four modules: dynamic risk perception, privacy decision via TD3 algorithm, local privacy execution, and performance verification from edge nodes. It employs a reward function balancing privacy gains, data utility, and energy cost.", "result": "Extensive theoretical analysis and real-world simulations show ALPINE effectively mitigates inference attacks while preserving utility and cost, making it practical for large-scale edge applications.", "conclusion": "ALPINE provides a lightweight, adaptive framework that enables terminal devices to autonomously adjust differential privacy levels in real-time, achieving dynamic equilibrium among privacy, utility, and cost in resource-constrained edge environments."}}
{"id": "2510.17185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17185", "abs": "https://arxiv.org/abs/2510.17185", "authors": ["Runlin Lei", "Lu Yi", "Mingguo He", "Pengyu Qiu", "Zhewei Wei", "Yongchao Liu", "Chuntao Hong"], "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses", "comment": null, "summary": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are\npowerful approaches for learning on Text-Attributed Graphs (TAGs), a\ncomprehensive understanding of their robustness remains elusive. Current\nevaluations are fragmented, failing to systematically investigate the distinct\neffects of textual and structural perturbations across diverse models and\nattack scenarios. To address these limitations, we introduce a unified and\ncomprehensive framework to evaluate robustness in TAG learning. Our framework\nevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten\ndatasets from four domains, under diverse text-based, structure-based, and\nhybrid perturbations in both poisoning and evasion scenarios. Our extensive\nanalysis reveals multiple findings, among which three are particularly\nnoteworthy: 1) models have inherent robustness trade-offs between text and\nstructure, 2) the performance of GNNs and RGNNs depends heavily on the text\nencoder and attack type, and 3) GraphLLMs are particularly vulnerable to\ntraining data corruption. To overcome the identified trade-offs, we introduce\nSFT-auto, a novel framework that delivers superior and balanced robustness\nagainst both textual and structural attacks within a single model. Our work\nestablishes a foundation for future research on TAG security and offers\npractical solutions for robust TAG learning in adversarial environments. Our\ncode is available at: https://github.com/Leirunlin/TGRB.", "AI": {"tldr": "The paper introduces a unified framework to evaluate robustness of GNNs, RGNNs, and GraphLLMs on Text-Attributed Graphs, revealing inherent robustness trade-offs and proposing SFT-auto for balanced robustness.", "motivation": "Current evaluations of GNNs and LLMs on TAGs are fragmented, lacking systematic investigation of textual and structural perturbations across different models and attack scenarios.", "method": "Developed a comprehensive evaluation framework testing classical GNNs, robust GNNs, and GraphLLMs across 10 datasets under text-based, structure-based, and hybrid perturbations in poisoning and evasion scenarios.", "result": "Key findings: 1) inherent robustness trade-offs between text and structure, 2) GNN performance depends on text encoder and attack type, 3) GraphLLMs are vulnerable to training data corruption. SFT-auto framework achieves superior balanced robustness.", "conclusion": "The work establishes foundation for TAG security research and provides practical solutions for robust TAG learning in adversarial environments."}}
{"id": "2510.17187", "categories": ["cs.LG", "q-bio.BM", "92B20"], "pdf": "https://arxiv.org/pdf/2510.17187", "abs": "https://arxiv.org/abs/2510.17187", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Sanya Murdeshwar", "Kevin Bachelor", "Ionut Mistreanu", "Ashwin Lokapally", "Razvan Marinescu"], "title": "A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling", "comment": "37 Pages (Main Text), 10 Figures, Submitted to Journal of Physical\n  Chemistry B", "summary": "The rapid evolution of molecular dynamics (MD) methods, including\nmachine-learned dynamics, has outpaced the development of standardized tools\nfor method validation. Objective comparison between simulation approaches is\noften hindered by inconsistent evaluation metrics, insufficient sampling of\nrare conformational states, and the absence of reproducible benchmarks. To\naddress these challenges, we introduce a modular benchmarking framework that\nsystematically evaluates protein MD methods using enhanced sampling analysis.\nOur approach uses weighted ensemble (WE) sampling via The Weighted Ensemble\nSimulation Toolkit with Parallelization and Analysis (WESTPA), based on\nprogress coordinates derived from Time-lagged Independent Component Analysis\n(TICA), enabling fast and efficient exploration of protein conformational\nspace. The framework includes a flexible, lightweight propagator interface that\nsupports arbitrary simulation engines, allowing both classical force fields and\nmachine learning-based models. Additionally, the framework offers a\ncomprehensive evaluation suite capable of computing more than 19 different\nmetrics and visualizations across a variety of domains. We further contribute a\ndataset of nine diverse proteins, ranging from 10 to 224 residues, that span a\nvariety of folding complexities and topologies. Each protein has been\nextensively simulated at 300K for one million MD steps per starting point (4\nns). To demonstrate the utility of our framework, we perform validation tests\nusing classic MD simulations with implicit solvent and compare protein\nconformational sampling using a fully trained versus under-trained CGSchNet\nmodel. By standardizing evaluation protocols and enabling direct, reproducible\ncomparisons across MD approaches, our open-source platform lays the groundwork\nfor consistent, rigorous benchmarking across the molecular simulation\ncommunity.", "AI": {"tldr": "The paper introduces a modular benchmarking framework for protein molecular dynamics methods that addresses validation challenges through enhanced sampling analysis, standardized metrics, and reproducible benchmarks.", "motivation": "To overcome the lack of standardized tools for molecular dynamics method validation, inconsistent evaluation metrics, insufficient sampling of rare states, and absence of reproducible benchmarks that hinder objective comparison between simulation approaches.", "method": "Uses weighted ensemble sampling via WESTPA based on TICA-derived progress coordinates for efficient conformational exploration. Features a flexible propagator interface supporting arbitrary simulation engines (classical force fields and ML models) and a comprehensive evaluation suite with 19+ metrics and visualizations.", "result": "Developed a dataset of nine diverse proteins (10-224 residues) with extensive simulations. Demonstrated framework utility through validation tests comparing classic MD with implicit solvent and CGSchNet models (fully trained vs under-trained), showing effective conformational sampling analysis.", "conclusion": "The open-source platform standardizes evaluation protocols and enables direct, reproducible comparisons across MD approaches, establishing groundwork for consistent, rigorous benchmarking in the molecular simulation community."}}
{"id": "2510.17189", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17189", "abs": "https://arxiv.org/abs/2510.17189", "authors": ["Wenxun Wang", "Shuchang Zhou", "Wenyu Sun", "Peiqin Sun", "Yongpan Liu"], "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "comment": null, "summary": "Transformers have shown remarkable performance in both natural language\nprocessing (NLP) and computer vision (CV) tasks. However, their real-time\ninference speed and efficiency are limited due to the inefficiency in Softmax\nand Layer Normalization (LayerNorm). Previous works based on function\napproximation suffer from inefficient implementation as they place emphasis on\ncomputation while disregarding memory overhead concerns. Moreover, such methods\nrely on retraining to compensate for approximation error which can be costly\nand inconvenient.\n  In this paper, we present SOLE, a hardware-software co-design for Softmax and\nLayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes\nlog2 quantization of exponent function and log-based division to approximate\nSoftmax while AILayerNorm adopts low-precision statistic calculation. Compared\nwith state-of-the-art designs, we achieve both low-precision calculation and\nlow bit-width storage on Softmax and LayerNorm. Experiments show that SOLE\nmaintains inference accuracy without retraining while offering orders of\nmagnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x\nenergy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements\nover prior state-of-the-art custom hardware for Softmax and LayerNorm,\nrespectively.", "AI": {"tldr": "SOLE is a hardware-software co-design that optimizes Softmax and LayerNorm operations in Transformers through E2Softmax (using log2 quantization and log-based division) and AILayerNorm (using low-precision statistics), achieving significant speedup and energy efficiency without retraining.", "motivation": "Transformers have performance limitations in real-time inference due to inefficient Softmax and LayerNorm operations. Previous approximation methods suffer from high memory overhead and require costly retraining to compensate for errors.", "method": "SOLE combines E2Softmax (log2 quantization of exponent function and log-based division) and AILayerNorm (low-precision statistic calculation) to achieve both low-precision calculation and low bit-width storage.", "result": "SOLE maintains inference accuracy without retraining while achieving 3.04x and 3.86x energy-efficiency improvements, and 2.82x and 3.32x area-efficiency improvements over prior state-of-the-art custom hardware for Softmax and LayerNorm respectively.", "conclusion": "SOLE provides an effective hardware-software co-design solution that significantly improves Transformer efficiency for Softmax and LayerNorm operations without requiring model retraining."}}
{"id": "2510.17212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17212", "abs": "https://arxiv.org/abs/2510.17212", "authors": ["Jundong Zhang", "Yuhui Situ", "Fanji Zhang", "Rongji Deng", "Tianqi Wei"], "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks", "comment": null, "summary": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle\ncrossing, often exhibit multimodal action distributions and stochastic returns.\nMost reinforcement learning (RL) methods assume unimodal Gaussian policies and\nrely on scalar-valued critics, which limits their effectiveness in HRHR\nsettings. We formally define HRHR tasks and theoretically show that Gaussian\npolicies cannot guarantee convergence to the optimal solution. To address this,\nwe propose a reinforcement learning framework that (i) discretizes continuous\naction spaces to approximate multimodal distributions, (ii) employs\nentropy-regularized exploration to improve coverage of risky but rewarding\nactions, and (iii) introduces a dual-critic architecture for more accurate\ndiscrete value distribution estimation. The framework scales to\nhigh-dimensional action spaces, supporting complex control domains. Experiments\non locomotion and manipulation benchmarks with high risks of failure\ndemonstrate that our method outperforms baselines, underscoring the importance\nof explicitly modeling multimodality and risk in RL.", "AI": {"tldr": "A reinforcement learning framework for high-risk-high-return tasks that addresses multimodal action distributions through action discretization, entropy-regularized exploration, and dual-critic architecture, outperforming baselines in locomotion and manipulation benchmarks.", "motivation": "High-risk-high-return tasks exhibit multimodal action distributions and stochastic returns, but most RL methods use unimodal Gaussian policies and scalar critics, limiting their effectiveness in such settings. The authors formally define HRHR tasks and theoretically prove Gaussian policies cannot guarantee optimal convergence.", "method": "The framework (i) discretizes continuous action spaces to approximate multimodal distributions, (ii) employs entropy-regularized exploration to improve coverage of risky but rewarding actions, and (iii) introduces a dual-critic architecture for more accurate discrete value distribution estimation. It scales to high-dimensional action spaces.", "result": "Experiments on locomotion and manipulation benchmarks with high failure risks demonstrate that the proposed method outperforms baseline approaches.", "conclusion": "The results underscore the importance of explicitly modeling multimodality and risk in reinforcement learning for high-risk-high-return tasks, showing that the proposed framework effectively addresses the limitations of traditional Gaussian policy approaches."}}
{"id": "2510.17214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17214", "abs": "https://arxiv.org/abs/2510.17214", "authors": ["Chenyan Fei", "Dalin Zhang", "Chen Melinda Dang"], "title": "Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network", "comment": null, "summary": "Effective and accurate diagnosis of fuel cell health status is crucial for\nensuring the stable operation of fuel cell stacks. Among various parameters,\nhigh-frequency impedance serves as a critical indicator for assessing fuel cell\nstate and health conditions. However, its online testing is prohibitively\ncomplex and costly. This paper employs a deep sparse auto-encoding network for\nthe prediction and classification of high-frequency impedance in fuel cells,\nachieving metric of accuracy rate above 92\\%. The network is further deployed\non an FPGA, attaining a hardware-based recognition rate almost 90\\%.", "AI": {"tldr": "This paper presents a deep sparse auto-encoding network for predicting and classifying high-frequency impedance in fuel cells, achieving over 92% accuracy, with FPGA deployment reaching nearly 90% recognition rate.", "motivation": "High-frequency impedance is crucial for assessing fuel cell health but online testing is complex and costly, necessitating alternative diagnostic methods.", "method": "The paper employs a deep sparse auto-encoding network for prediction and classification of high-frequency impedance, with deployment on FPGA hardware.", "result": "The method achieves accuracy rate above 92% for prediction and classification, with FPGA implementation attaining nearly 90% hardware-based recognition rate.", "conclusion": "The deep sparse auto-encoding network provides an effective solution for fuel cell health diagnosis, successfully bridging the gap between complex online testing and practical deployment through FPGA implementation."}}
{"id": "2510.17250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17250", "abs": "https://arxiv.org/abs/2510.17250", "authors": ["Wei-Hsun Lee", "Che-Yu Chang", "Kuang-Yu Li"], "title": "A Prototypical Network with an Attention-based Encoder for Drivers Identification Application", "comment": null, "summary": "Driver identification has become an area of increasing interest in recent\nyears, especially for data- driven applications, because biometric-based\ntechnologies may incur privacy issues. This study proposes a deep learning\nneural network architecture, an attention-based encoder (AttEnc), which uses an\nattention mechanism for driver identification and uses fewer model parameters\nthan current methods. Most studies do not address the issue of data shortages\nfor driver identification, and most of them are inflexible when encountering\nunknown drivers. In this study, an architecture that combines a prototypical\nnetwork and an attention-based encoder (P-AttEnc) is proposed. It applies\nfew-shot learning to overcome the data shortage issues and to enhance model\ngeneralizations. The experiments showed that the attention-based encoder can\nidentify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different\ndatasets and has a prediction time that is 44% to 79% faster because it\nsignificantly reduces, on average, 87.6% of the model parameters. P-AttEnc\nidentifies drivers based on few shot data, extracts driver fingerprints to\naddress the issue of data shortages, and is able to classify unknown drivers.\nThe first experiment showed that P-AttEnc can identify drivers with an accuracy\nof 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,\nin the 1-shot scenario, can classify unknown drivers with an average accuracy\nof 65.7%.", "AI": {"tldr": "Proposes AttEnc and P-AttEnc deep learning architectures for driver identification using attention mechanisms and few-shot learning to address data shortages and unknown driver classification.", "motivation": "Driver identification is important for data-driven applications but biometric methods raise privacy concerns. Most existing methods don't handle data shortages well and are inflexible with unknown drivers.", "method": "Developed two architectures: AttEnc (attention-based encoder) for efficient driver identification with fewer parameters, and P-AttEnc (combining prototypical network with AttEnc) for few-shot learning to handle data shortages and unknown drivers.", "result": "AttEnc achieved 99.3%, 99.0%, and 99.9% accuracy across three datasets with 44-79% faster prediction time and 87.6% parameter reduction. P-AttEnc achieved 69.8% accuracy in one-shot scenario and 65.7% average accuracy for unknown driver classification.", "conclusion": "The proposed architectures effectively address driver identification challenges - AttEnc provides efficient high-accuracy identification while P-AttEnc enables few-shot learning for data-scarce scenarios and unknown driver classification."}}
{"id": "2510.17276", "categories": ["cs.LG", "cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17276", "abs": "https://arxiv.org/abs/2510.17276", "authors": ["Rishi Jha", "Harold Triedman", "Justin Wagle", "Vitaly Shmatikov"], "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems", "comment": null, "summary": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation.", "AI": {"tldr": "ControlValve defends against control-flow hijacking in multi-agent systems by generating permitted control-flow graphs and enforcing execution compliance with contextual rules.", "motivation": "Existing defenses like LlamaFirewall are insufficient against control-flow hijacking attacks due to brittle alignment definitions and incomplete execution context visibility, creating a fundamental conflict between safety and functionality.", "method": "ControlValve generates permitted control-flow graphs for multi-agent systems and enforces execution compliance with these graphs using zero-shot generated contextual rules for each agent invocation.", "result": "The paper demonstrates that current alignment-based defenses can be evaded even with advanced LLM checkers, and proposes ControlValve as a more robust alternative.", "conclusion": "ControlValve provides a principled defense against control-flow hijacking by combining control-flow integrity and least privilege principles, addressing fundamental limitations in existing alignment-based approaches."}}
{"id": "2510.17281", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17281", "abs": "https://arxiv.org/abs/2510.17281", "authors": ["Qingyao Ai", "Yichen Tang", "Changyue Wang", "Jianming Long", "Weihang Su", "Yiqun Liu"], "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems", "comment": null, "summary": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.", "AI": {"tldr": "This paper proposes a new benchmark for evaluating LLM memory and continual learning abilities using simulated user feedback across multiple domains, languages, and task types, addressing limitations in existing benchmarks.", "motivation": "Current LLM scaling methods are reaching limits due to data depletion and diminishing returns. Inspired by human learning from practice, there's growing interest in memory and continual learning for LLMs, but existing benchmarks focus on homogeneous reading comprehension rather than learning from user feedback during service.", "method": "Developed a user feedback simulation framework and comprehensive benchmark covering multiple domains, languages, and task types to evaluate LLM continual learning abilities from accumulated user interactions.", "result": "Experiments revealed that state-of-the-art baselines show unsatisfactory effectiveness and efficiency in learning from user feedback, indicating significant room for improvement.", "conclusion": "The proposed benchmark provides a foundation for future research on LLM memory and optimization algorithms, addressing the gap in evaluating continual learning from real-world user interactions."}}
{"id": "2510.17313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17313", "abs": "https://arxiv.org/abs/2510.17313", "authors": ["Tal Barami", "Nimrod Berman", "Ilan Naiman", "Amos H. Hason", "Rotem Ezra", "Omri Azencot"], "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations", "comment": null, "summary": "Learning disentangled representations in sequential data is a key goal in\ndeep learning, with broad applications in vision, audio, and time series. While\nreal-world data involves multiple interacting semantic factors over time, prior\nwork has mostly focused on simpler two-factor static and dynamic settings,\nprimarily because such settings make data collection easier, thereby\noverlooking the inherently multi-factor nature of real-world data. We introduce\nthe first standardized benchmark for evaluating multi-factor sequential\ndisentanglement across six diverse datasets spanning video, audio, and time\nseries. Our benchmark includes modular tools for dataset integration, model\ndevelopment, and evaluation metrics tailored to multi-factor analysis. We\nadditionally propose a post-hoc Latent Exploration Stage to automatically align\nlatent dimensions with semantic factors, and introduce a Koopman-inspired model\nthat achieves state-of-the-art results. Moreover, we show that Vision-Language\nModels can automate dataset annotation and serve as zero-shot disentanglement\nevaluators, removing the need for manual labels and human intervention.\nTogether, these contributions provide a robust and scalable foundation for\nadvancing multi-factor sequential disentanglement.", "AI": {"tldr": "The paper introduces the first standardized benchmark for multi-factor sequential disentanglement across six diverse datasets, proposes a post-hoc latent exploration stage and Koopman-inspired model, and shows that Vision-Language Models can automate evaluation.", "motivation": "Prior work has focused on simpler two-factor static and dynamic settings due to easier data collection, overlooking the inherently multi-factor nature of real-world sequential data involving multiple interacting semantic factors over time.", "method": "The authors introduce a standardized benchmark with tools for dataset integration, model development, and evaluation metrics. They propose a post-hoc Latent Exploration Stage for automatic latent dimension alignment and a Koopman-inspired model. They also utilize Vision-Language Models for automated dataset annotation and zero-shot evaluation.", "result": "The proposed Koopman-inspired model achieves state-of-the-art results. Vision-Language Models successfully automate dataset annotation and serve as zero-shot disentanglement evaluators, eliminating the need for manual labels and human intervention.", "conclusion": "These contributions provide a robust and scalable foundation for advancing multi-factor sequential disentanglement across vision, audio, and time series domains."}}
{"id": "2510.17314", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17314", "abs": "https://arxiv.org/abs/2510.17314", "authors": ["Lipeng Xie", "Sen Huang", "Zhuo Zhang", "Anni Zou", "Yunpeng Zhai", "Dingchao Ren", "Kezun Zhang", "Haoyuan Hu", "Boyin Liu", "Haoran Chen", "Zhaoyang Liu", "Bolin Ding"], "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling", "comment": null, "summary": "Reward models are essential for aligning Large Language Models (LLMs) with\nhuman values, yet their development is hampered by costly preference datasets\nand poor interpretability. While recent rubric-based approaches offer\ntransparency, they often lack systematic quality control and optimization,\ncreating a trade-off between scalability and reliability. We address these\nlimitations with a novel, training-free framework built on a key assumption:\n\\textit{evaluation rubrics underlying human preferences exhibit significant\ngeneralization ability across diverse queries}, a property that enables\nremarkable data efficiency. Our two-stage approach first infers high-quality,\nquery-specific rubrics using a validation-guided\n\\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these\ngranular rubrics into a compact, non-redundant core set by maximizing an\n\\textbf{information-theoretic coding rate}. The final output is an\ninterpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments\ndemonstrate the framework's exceptional data efficiency and performance.\nCritically, using just 70 preference pairs (1.5\\% of the source data), our\nmethod also empowers smaller models like Qwen3-8B to outperform specialized,\nfully-trained counterparts. This work pioneers a scalable, interpretable, and\ndata-efficient path for reward modeling.", "AI": {"tldr": "A training-free framework for reward modeling that uses query-specific rubrics and information-theoretic optimization to achieve high data efficiency and interpretability with minimal preference data.", "motivation": "Current reward models for LLM alignment face challenges with costly preference datasets and poor interpretability, creating a trade-off between scalability and reliability.", "method": "Two-stage approach: 1) Propose-Evaluate-Revise pipeline for query-specific rubrics, 2) Information-theoretic coding rate optimization to generalize rubrics into compact hierarchical Theme-Tips sets.", "result": "Exceptional data efficiency using only 70 preference pairs (1.5% of source data), enabling smaller models like Qwen3-8B to outperform specialized fully-trained counterparts.", "conclusion": "Pioneers a scalable, interpretable, and data-efficient path for reward modeling by leveraging rubric generalization across diverse queries."}}
{"id": "2510.17358", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17358", "abs": "https://arxiv.org/abs/2510.17358", "authors": ["Joachim Diederich"], "title": "Localist LLMs with Recruitment Learning", "comment": null, "summary": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovations are (1) a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining, (2) an\ninformation-theoretic recruitment mechanism that adaptively allocates semantic\nblocks as needed, eliminating the requirement for complete domain knowledge at\ninitialization, and (3) a hierarchical recruitment framework that extends\ncapacity allocation to entire specialized LLMs, enabling multi-granularity\narchitectural adaptation. This is achieved through group sparsity penalties on\nattention mechanisms, information-theoretic anchor design, dynamic rule\ninjection, and principled recruitment criteria based on penalized likelihood\nwith explicit units. We provide rigorous mathematical results establishing\nexplicit threshold conditions under which attention provably concentrates on\nsemantically relevant blocks at stationary points, with exact bounds on\nattention entropy and pointer fidelity. The hierarchical recruitment mechanism\nprovides convergence guarantees at both the block level (fine-grained,\nwithin-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the\nsystem discovers semantic partitions that balance model complexity against data\nencoding efficiency. This framework enables practitioners to continuously\ninterpolate between interpretable and high-performance modes while adapting\narchitectural capacity at multiple granularities, supporting applications in\nregulated domains requiring both transparency and capability.", "AI": {"tldr": "A framework for training LLMs with adjustable internal representations from interpretable localist to efficient distributed encodings, featuring a tunable locality dial, adaptive semantic block allocation, and hierarchical recruitment of specialized LLMs.", "motivation": "To enable continuous interpolation between interpretable and high-performance modes in LLMs while adapting architectural capacity at multiple granularities, particularly for regulated domains requiring both transparency and capability.", "method": "Uses group sparsity penalties on attention mechanisms, information-theoretic anchor design, dynamic rule injection, and principled recruitment criteria based on penalized likelihood with explicit units. Includes a locality dial parameter, information-theoretic recruitment mechanism, and hierarchical recruitment framework.", "result": "Provides rigorous mathematical results establishing explicit threshold conditions for attention concentration on semantically relevant blocks, with exact bounds on attention entropy and pointer fidelity. The hierarchical recruitment mechanism provides convergence guarantees at both block and LLM levels.", "conclusion": "The framework enables practitioners to continuously interpolate between interpretable and high-performance modes while adapting architectural capacity at multiple granularities, supporting applications in regulated domains requiring both transparency and capability."}}
{"id": "2510.17378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17378", "abs": "https://arxiv.org/abs/2510.17378", "authors": ["Wei Xu", "Xiaoyi Jiang", "Lixiang Xu", "Dechao Tang"], "title": "Model Metamers Reveal Invariances in Graph Neural Networks", "comment": null, "summary": "In recent years, deep neural networks have been extensively employed in\nperceptual systems to learn representations endowed with invariances, aiming to\nemulate the invariance mechanisms observed in the human brain. However, studies\nin the visual and auditory domains have confirmed that significant gaps remain\nbetween the invariance properties of artificial neural networks and those of\nhumans. To investigate the invariance behavior within graph neural networks\n(GNNs), we introduce a model ``metamers'' generation technique. By optimizing\ninput graphs such that their internal node activations match those of a\nreference graph, we obtain graphs that are equivalent in the model's\nrepresentation space, yet differ significantly in both structure and node\nfeatures. Our theoretical analysis focuses on two aspects: the local metamer\ndimension for a single node and the activation-induced volume change of the\nmetamer manifold. Utilizing this approach, we uncover extreme levels of\nrepresentational invariance across several classic GNN architectures. Although\ntargeted modifications to model architecture and training strategies can\npartially mitigate this excessive invariance, they fail to fundamentally bridge\nthe gap to human-like invariance. Finally, we quantify the deviation between\nmetamer graphs and their original counterparts, revealing unique failure modes\nof current GNNs and providing a complementary benchmark for model evaluation.", "AI": {"tldr": "The paper investigates invariance properties in graph neural networks (GNNs) using a metamer generation technique, revealing excessive representational invariance in current GNN architectures compared to human-like invariance.", "motivation": "To understand the invariance behavior in GNNs and compare it with human brain invariance mechanisms, as significant gaps have been identified in visual and auditory domains between artificial neural networks and humans.", "method": "Developed a model metamer generation technique by optimizing input graphs to match internal node activations of reference graphs, creating structurally different but representationally equivalent graphs. Theoretical analysis focused on local metamer dimension and activation-induced volume change.", "result": "Found extreme levels of representational invariance across several classic GNN architectures. Targeted architectural modifications and training strategies only partially mitigated excessive invariance but failed to bridge the gap to human-like invariance.", "conclusion": "Current GNNs exhibit unique failure modes due to excessive invariance, and the metamer approach provides a complementary benchmark for model evaluation, highlighting the fundamental gap between artificial and human invariance properties."}}
{"id": "2510.17380", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17380", "abs": "https://arxiv.org/abs/2510.17380", "authors": ["Julen Cestero", "Carmine Delle Femine", "Kenji S. Muro", "Marco Quartulli", "Marcello Restelli"], "title": "Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks", "comment": null, "summary": "Optimizing the energy management within a smart grids scenario presents\nsignificant challenges, primarily due to the complexity of real-world systems\nand the intricate interactions among various components. Reinforcement Learning\n(RL) is gaining prominence as a solution for addressing the challenges of\nOptimal Power Flow in smart grids. However, RL needs to iterate compulsively\nthroughout a given environment to obtain the optimal policy. This means\nobtaining samples from a, most likely, costly simulator, which can lead to a\nsample efficiency problem. In this work, we address this problem by\nsubstituting costly smart grid simulators with surrogate models built using\nPhisics-informed Neural Networks (PINNs), optimizing the RL policy training\nprocess by arriving to convergent results in a fraction of the time employed by\nthe original environment.", "AI": {"tldr": "This paper proposes using Physics-informed Neural Networks (PINNs) as surrogate models to replace costly smart grid simulators in Reinforcement Learning for Optimal Power Flow optimization, significantly improving sample efficiency and training time.", "motivation": "Reinforcement Learning faces sample efficiency problems in smart grid optimization due to the need for extensive iterations through costly simulators to obtain optimal policies.", "method": "Substitute expensive smart grid simulators with surrogate models built using Physics-informed Neural Networks (PINNs) to optimize RL policy training.", "result": "The approach achieves convergent results in a fraction of the time required by the original environment.", "conclusion": "PINN-based surrogate models effectively address the sample efficiency problem in RL for smart grid energy management, enabling faster convergence and reduced computational costs."}}
{"id": "2510.17381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17381", "abs": "https://arxiv.org/abs/2510.17381", "authors": ["Achref Jaziri", "Martin Rogmann", "Martin Mundt", "Visvanathan Ramesh"], "title": "Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories", "comment": "11 Pages, 6 Figures", "summary": "Detecting out-of-distribution (OOD) data is critical for machine learning, be\nit for safety reasons or to enable open-ended learning. However, beyond mere\ndetection, choosing an appropriate course of action typically hinges on the\ntype of OOD data encountered. Unfortunately, the latter is generally not\ndistinguished in practice, as modern OOD detection methods collapse\ndistributional shifts into single scalar outlier scores. This work argues that\nscalar-based methods are thus insufficient for OOD data to be properly\ncontextualized and prospectively exploited, a limitation we overcome with the\nintroduction of DISC: Diffusion-based Statistical Characterization. DISC\nleverages the iterative denoising process of diffusion models to extract a\nrich, multi-dimensional feature vector that captures statistical discrepancies\nacross multiple noise levels. Extensive experiments on image and tabular\nbenchmarks show that DISC matches or surpasses state-of-the-art detectors for\nOOD detection and, crucially, also classifies OOD type, a capability largely\nabsent from prior work. As such, our work enables a shift from simple binary\nOOD detection to a more granular detection.", "AI": {"tldr": "DISC introduces a diffusion-based method for out-of-distribution detection that goes beyond binary classification to identify specific OOD types, outperforming traditional scalar-based approaches.", "motivation": "Current OOD detection methods collapse distributional shifts into single scalar scores, making it impossible to distinguish between different types of OOD data needed for appropriate action and open-ended learning.", "method": "DISC leverages the iterative denoising process of diffusion models to extract rich, multi-dimensional feature vectors that capture statistical discrepancies across multiple noise levels.", "result": "Extensive experiments on image and tabular benchmarks show DISC matches or surpasses state-of-the-art detectors for OOD detection and uniquely classifies OOD types, a capability largely absent from prior work.", "conclusion": "DISC enables a shift from simple binary OOD detection to more granular detection, allowing proper contextualization and prospective exploitation of OOD data."}}
{"id": "2510.17383", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17383", "abs": "https://arxiv.org/abs/2510.17383", "authors": ["Ludovica Schaerf"], "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models", "comment": "Presented and published at Ethics and Aesthetics of Artificial\n  Intelligence Conference (EA-AI'25)", "summary": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes.", "AI": {"tldr": "The paper analyzes how generative visual models' internal representations have evolved from GANs/VAEs to diffusion models, proposing a distinction between strict synthesis (compact latent space) and broad synthesis (distributed representations across layers).", "motivation": "To understand the conceptual shift in generative AI from unified latent spaces to distributed representations, challenging traditional assumptions about how models synthesize content.", "method": "Close readings of model architectures and experimental interventions in layerwise representations of diffusion models to analyze how representation is fragmented.", "result": "Diffusion models distribute representational labor across layers rather than relying on unified latent spaces, challenging the Platonic Representation Hypothesis.", "conclusion": "Generative AI should be understood as emergent configurations of specialized processes rather than direct synthesis of content, requiring reorientation of how we conceptualize these models."}}
{"id": "2510.17385", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17385", "abs": "https://arxiv.org/abs/2510.17385", "authors": ["Pengxiang Cai", "Zihao Gao", "Jintai Chen"], "title": "TabR1: Taming GRPO for tabular reasoning LLMs", "comment": null, "summary": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).", "AI": {"tldr": "TabR1 is the first reasoning LLM for tabular prediction that uses multi-step reasoning and a novel reinforcement learning method called PRPO to achieve strong performance with limited supervision while maintaining interpretability.", "motivation": "Traditional tabular prediction methods like gradient-boosted trees and specialized deep learning models have limited interpretability and weak transfer across tables, while reasoning LLMs promise cross-task adaptability with transparent reasoning but haven't been fully realized for tabular data.", "method": "TabR1 uses Permutation Relative Policy Optimization (PRPO), a reinforcement learning method that encodes column-permutation invariance as a structural prior by constructing multiple label-preserving permutations per sample and estimating advantages both within and across permutations.", "result": "TabR1 achieves performance comparable to strong baselines under full-supervision, approaches 32-shot baseline performance in zero-shot setting, and substantially outperforms much larger LLMs (8B model achieving up to 53.17% improvement over DeepSeek-R1 685B).", "conclusion": "PRPO effectively activates LLM reasoning abilities for tabular prediction, enhancing few-shot/zero-shot performance and interpretability while demonstrating strong generalization across various tasks."}}
{"id": "2510.17391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17391", "abs": "https://arxiv.org/abs/2510.17391", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration", "comment": null, "summary": "Although there is an extensive body of work characterizing the sample\ncomplexity of discounted-return offline RL with function approximations, prior\nwork on the average-reward setting has received significantly less attention,\nand existing approaches rely on restrictive assumptions, such as ergodicity or\nlinearity of the MDP. In this work, we establish the first sample complexity\nresults for average-reward offline RL with function approximation for weakly\ncommunicating MDPs, a much milder assumption. To this end, we introduce\nAnchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration\nwith an anchor mechanism. We show that the anchor, which can be interpreted as\na form of weight decay, is crucial for enabling finite-time analysis in the\naverage-reward setting. We also extend our finite-time analysis to the setup\nwhere the dataset is generated from a single-trajectory rather than IID\ntransitions, again leveraging the anchor mechanism.", "AI": {"tldr": "This paper introduces Anchored Fitted Q-Iteration for average-reward offline RL with function approximation, achieving the first sample complexity results for weakly communicating MDPs without restrictive ergodicity or linearity assumptions.", "motivation": "Prior work on average-reward offline RL relied on restrictive assumptions like ergodicity or linearity, while discounted-return settings had extensive analysis. This work addresses the gap by providing sample complexity results for the more general weakly communicating MDPs.", "method": "The authors propose Anchored Fitted Q-Iteration, which combines standard Fitted Q-Iteration with an anchor mechanism (interpreted as a form of weight decay) to enable finite-time analysis in the average-reward setting.", "result": "The method achieves the first sample complexity results for average-reward offline RL with function approximation under weakly communicating MDPs. The analysis also extends to single-trajectory datasets rather than requiring IID transitions.", "conclusion": "The anchor mechanism is crucial for enabling finite-time analysis in average-reward offline RL, and the approach successfully addresses the limitations of prior work by working with milder assumptions and more realistic data generation settings."}}
{"id": "2510.17394", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17394", "abs": "https://arxiv.org/abs/2510.17394", "authors": ["Alejandro Guerra-Manzanares", "Farah E. Shamout"], "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning", "comment": "Accepted and presented at the 2025 International Joint Conference on\n  Neural Networks (IJCNN'25). The paper was awarded an honorable mention (best\n  4 papers)", "summary": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance.", "AI": {"tldr": "MILES is a learning rate scheduler that dynamically adjusts rates during multimodal training to balance modality usage, preventing overfitting to single modalities and improving both multimodal and unimodal performance.", "motivation": "Multimodal networks often suffer from modality overfitting where they rely excessively on one modality, leading to sub-optimal performance and marginal improvements over unimodal models.", "method": "MILES leverages modality-wise conditional utilization rate differences during training to dynamically adjust learning rates, balancing the speed of learning from each modality in multimodal joint fusion models.", "result": "MILES outperforms seven state-of-the-art baselines across four multimodal joint fusion tasks, effectively balancing modality usage and improving both multimodal performance and modality encoder strength.", "conclusion": "Balancing multimodal learning through dynamic learning rate adjustment significantly improves model performance and creates stronger modality encoders that benefit unimodal scenarios."}}
{"id": "2510.17406", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17406", "abs": "https://arxiv.org/abs/2510.17406", "authors": ["Tiezhi Wang", "Wilhelm Haverkamp", "Nils Strodthoff"], "title": "S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction", "comment": null, "summary": "The electrocardiogram (ECG) exemplifies biosignal-based time series with\ncontinuous, temporally ordered structure reflecting cardiac physiological and\npathophysiological dynamics. Detailed analysis of these dynamics has proven\nchallenging, as conventional methods capture either global trends or local\nwaveform features but rarely their simultaneous interplay at high temporal\nresolution. To bridge global and local signal analysis, we introduce S4ECG, a\nnovel deep learning architecture leveraging structured state space models for\nmulti-epoch arrhythmia classification. Our joint multi-epoch predictions\nsignificantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,\nwith atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,\ndemonstrating superior performance in-distribution and enhanced\nout-of-distribution robustness. Systematic investigation reveals optimal\ntemporal dependency windows spanning 10-20 minutes for peak performance. This\nwork contributes to a paradigm shift toward temporally-aware arrhythmia\ndetection algorithms, opening new possibilities for ECG interpretation, in\nparticular for complex arrhythmias like atrial fibrillation and atrial flutter.", "AI": {"tldr": "S4ECG is a novel deep learning architecture using structured state space models for multi-epoch ECG arrhythmia classification, achieving superior performance over single-epoch approaches with improved temporal dependency analysis.", "motivation": "Conventional ECG analysis methods struggle to capture both global trends and local waveform features simultaneously at high temporal resolution, limiting comprehensive understanding of cardiac dynamics.", "method": "Introduced S4ECG architecture leveraging structured state space models for multi-epoch arrhythmia classification, enabling joint analysis across multiple time windows.", "result": "Multi-epoch predictions outperformed single-epoch approaches by 1.0-11.6% in macro-AUROC, with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998, showing enhanced in-distribution and out-of-distribution robustness.", "conclusion": "This work enables a paradigm shift toward temporally-aware arrhythmia detection, particularly beneficial for complex arrhythmias like atrial fibrillation and atrial flutter, with optimal temporal dependency windows of 10-20 minutes."}}
{"id": "2510.17414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17414", "abs": "https://arxiv.org/abs/2510.17414", "authors": ["Hequn Li", "Zhongwei Deng", "Chunlin Jiang", "Yvxin He andZhansheng Ning"], "title": "A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation", "comment": null, "summary": "Accurate prediction of lithium-ion battery capacity and its associated\nuncertainty is essential for reliable battery management but remains\nchallenging due to the stochastic nature of aging. This paper presents a novel\nmethod, termed the Condition Diffusion U-Net with Attention (CDUA), which\nintegrates feature engineering and deep learning to address this challenge. The\nproposed approach employs a diffusion-based generative model for time-series\nforecasting and incorporates attention mechanisms to enhance predictive\nperformance. Battery capacity is first derived from real-world vehicle\noperation data. The most relevant features are then identified using the\nPearson correlation coefficient and the XGBoost algorithm. These features are\nused to train the CDUA model, which comprises two core components: (1) a\ncontextual U-Net with self-attention to capture complex temporal dependencies,\nand (2) a denoising network to reconstruct accurate capacity values from noisy\nobservations. Experimental validation on the real-world vehicle data\ndemonstrates that the proposed CDUA model achieves a relative Mean Absolute\nError (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,\nwith a narrow 95% confidence interval of 3.74% in relative width. These results\nconfirm that CDUA provides both accurate capacity estimation and reliable\nuncertainty quantification. Comparative experiments further verify its\nrobustness and superior performance over existing mainstream approaches.", "AI": {"tldr": "CDUA model integrates feature engineering and deep learning for accurate lithium-ion battery capacity prediction with uncertainty quantification, achieving 0.94% MAE and 1.14% RMSE on real-world vehicle data.", "motivation": "Accurate prediction of lithium-ion battery capacity and uncertainty is essential for reliable battery management but challenging due to stochastic aging processes.", "method": "Proposes CDUA method combining diffusion-based generative model with attention mechanisms, using Pearson correlation and XGBoost for feature selection, and U-Net with self-attention for temporal dependencies.", "result": "Achieves relative MAE of 0.94%, RMSE of 1.14%, and 95% confidence interval width of 3.74% on real-world vehicle data, outperforming existing approaches.", "conclusion": "CDUA provides both accurate capacity estimation and reliable uncertainty quantification, demonstrating robustness and superior performance over mainstream methods."}}
{"id": "2510.17421", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17421", "abs": "https://arxiv.org/abs/2510.17421", "authors": ["Duo Su", "Huyu Wu", "Huanran Chen", "Yiming Shi", "Yuzhu Wang", "Xi Ye", "Jun Zhu"], "title": "Diffusion Models as Dataset Distillation Priors", "comment": null, "summary": "Dataset distillation aims to synthesize compact yet informative datasets from\nlarge ones. A significant challenge in this field is achieving a trifecta of\ndiversity, generalization, and representativeness in a single distilled\ndataset. Although recent generative dataset distillation methods adopt powerful\ndiffusion models as their foundation models, the inherent representativeness\nprior in diffusion models is overlooked. Consequently, these approaches often\nnecessitate the integration of external constraints to enhance data quality. To\naddress this, we propose Diffusion As Priors (DAP), which formalizes\nrepresentativeness by quantifying the similarity between synthetic and real\ndata in feature space using a Mercer kernel. We then introduce this prior as\nguidance to steer the reverse diffusion process, enhancing the\nrepresentativeness of distilled samples without any retraining. Extensive\nexperiments on large-scale datasets, such as ImageNet-1K and its subsets,\ndemonstrate that DAP outperforms state-of-the-art methods in generating\nhigh-fidelity datasets while achieving superior cross-architecture\ngeneralization. Our work not only establishes a theoretical connection between\ndiffusion priors and the objectives of dataset distillation but also provides a\npractical, training-free framework for improving the quality of the distilled\ndataset.", "AI": {"tldr": "DAP (Diffusion As Priors) is a training-free framework that enhances dataset distillation by incorporating diffusion model priors to improve representativeness, achieving state-of-the-art performance on large-scale datasets like ImageNet-1K.", "motivation": "Current generative dataset distillation methods using diffusion models overlook the inherent representativeness prior in these models, requiring external constraints to enhance data quality. The challenge is achieving diversity, generalization, and representativeness simultaneously in distilled datasets.", "method": "DAP formalizes representativeness by quantifying similarity between synthetic and real data in feature space using a Mercer kernel, then introduces this prior as guidance to steer the reverse diffusion process without any retraining.", "result": "Extensive experiments on ImageNet-1K and its subsets show DAP outperforms state-of-the-art methods in generating high-fidelity datasets and achieves superior cross-architecture generalization.", "conclusion": "The work establishes a theoretical connection between diffusion priors and dataset distillation objectives while providing a practical, training-free framework for improving distilled dataset quality."}}
{"id": "2510.17457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17457", "abs": "https://arxiv.org/abs/2510.17457", "authors": ["Li Sun", "Zhenhao Huang", "Ming Zhang", "Philip S. Yu"], "title": "Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models", "comment": "Accept by NeurIPS 25", "summary": "Message Passing Neural Networks (MPNNs) is the building block of graph\nfoundation models, but fundamentally suffer from oversmoothing and\noversquashing. There has recently been a surge of interest in fixing both\nissues. Existing efforts primarily adopt global approaches, which may be\nbeneficial in some regions but detrimental in others, ultimately leading to the\nsuboptimal expressiveness. In this paper, we begin by revisiting oversquashing\nthrough a global measure -- spectral gap $\\lambda$ -- and prove that the\nincrease of $\\lambda$ leads to gradient vanishing with respect to the input\nfeatures, thereby undermining the effectiveness of message passing. Motivated\nby such theoretical insights, we propose a \\textbf{local} approach that\nadaptively adjusts message passing based on local structures. To achieve this,\nwe connect local Riemannian geometry with MPNNs, and establish a novel\nnonhomogeneous boundary condition to address both oversquashing and\noversmoothing. Building on the Robin condition, we design a GBN network with\nlocal bottleneck adjustment, coupled with theoretical guarantees. Extensive\nexperiments on homophilic and heterophilic graphs show the expressiveness of\nGBN. Furthermore, GBN does not exhibit performance degradation even when the\nnetwork depth exceeds $256$ layers.", "AI": {"tldr": "The paper proposes GBN, a local approach using Riemannian geometry and Robin boundary conditions to address oversmoothing and oversquashing in MPNNs, enabling deep networks (256+ layers) without performance degradation.", "motivation": "MPNNs suffer from oversmoothing and oversquashing issues. Existing global approaches are suboptimal as they may help some regions but harm others. The authors found that increasing spectral gap \u03bb causes gradient vanishing, undermining message passing effectiveness.", "method": "Proposed a local approach using Riemannian geometry with MPNNs, established nonhomogeneous boundary conditions based on Robin condition, and designed GBN network with local bottleneck adjustment to adaptively adjust message passing based on local structures.", "result": "Extensive experiments on homophilic and heterophilic graphs show GBN's expressiveness. GBN maintains performance without degradation even with 256+ layers, demonstrating effectiveness in addressing both oversmoothing and oversquashing.", "conclusion": "The local approach using Riemannian geometry and Robin boundary conditions effectively addresses fundamental limitations of MPNNs, enabling deep graph networks while maintaining performance across various graph types."}}
{"id": "2510.17458", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.17458", "abs": "https://arxiv.org/abs/2510.17458", "authors": ["Ayrat Abdullin", "Denis Anikiev", "Umair bin Waheed"], "title": "Explainable AI for microseismic event detection", "comment": "Submitted to Artificial Intelligence in Geosciences", "summary": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors.", "AI": {"tldr": "Applying explainable AI techniques (Grad-CAM and SHAP) to interpret PhaseNet's seismic event detection decisions, leading to improved model reliability and performance through a SHAP-gated inference scheme.", "motivation": "Deep neural networks like PhaseNet achieve high accuracy but their black-box nature raises concerns for critical applications, requiring interpretability and enhanced reliability.", "method": "Used Grad-CAM to visualize network attention and SHAP to quantify feature contributions, then developed a SHAP-gated inference scheme combining model outputs with explanation-based metrics.", "result": "SHAP-gated model achieved F1-score of 0.98 (precision 0.99, recall 0.97) on 9,000 test waveforms, outperforming baseline PhaseNet (F1-score 0.97) with enhanced noise robustness.", "conclusion": "XAI techniques can both interpret deep learning models and directly enhance their performance, providing a template for building trust in automated seismic detectors."}}
{"id": "2510.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17467", "abs": "https://arxiv.org/abs/2510.17467", "authors": ["Dan Zheng", "Jing Feng", "Juan Liu"], "title": "CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics", "comment": null, "summary": "Current research in Electrocardiogram (ECG) biometrics mainly emphasizes\nresting-state conditions, leaving the performance decline in rest-exercise\nscenarios largely unresolved. This paper introduces CrossStateECG, a robust\nECG-based authentication model explicitly tailored for cross-state\n(rest-exercise) conditions. The proposed model creatively combines multi-scale\ndeep convolutional feature extraction with attention mechanisms to ensure\nstrong identification across different physiological states. Experimental\nresults on the exercise-ECGID dataset validate the effectiveness of\nCrossStateECG, achieving an identification accuracy of 92.50% in the\nRest-to-Exercise scenario (training on resting ECG and testing on post-exercise\nECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG\nand testing on resting ECG). Furthermore, CrossStateECG demonstrates\nexceptional performance across both state combinations, reaching an accuracy of\n99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.\nAdditional validations on the ECG-ID and MIT-BIH datasets further confirmed the\ngeneralization abilities of CrossStateECG, underscoring its potential as a\npractical solution for post-exercise ECG-based authentication in dynamic\nreal-world settings.", "AI": {"tldr": "CrossStateECG is a robust ECG biometric authentication model designed for cross-state (rest-exercise) conditions, achieving high accuracy across different physiological states using multi-scale deep convolutional features and attention mechanisms.", "motivation": "Current ECG biometric research focuses mainly on resting-state conditions, leaving the performance decline in rest-exercise scenarios unresolved, which limits practical applications in dynamic real-world settings.", "method": "The model combines multi-scale deep convolutional feature extraction with attention mechanisms to ensure strong identification across different physiological states.", "result": "Achieved 92.50% accuracy in Rest-to-Exercise scenario, 94.72% in Exercise-to-Rest scenario, 99.94% in Rest-to-Rest scenarios, and 97.85% in Mixed-to-Mixed scenarios on the exercise-ECGID dataset, with additional validation on ECG-ID and MIT-BIH datasets confirming generalization abilities.", "conclusion": "CrossStateECG demonstrates exceptional performance across state combinations and shows strong generalization, highlighting its potential as a practical solution for post-exercise ECG-based authentication in dynamic real-world settings."}}
{"id": "2510.17469", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17469", "abs": "https://arxiv.org/abs/2510.17469", "authors": ["Jing Liu"], "title": "Layer Specialization Underlying Compositional Reasoning in Transformers", "comment": null, "summary": "Transformers exhibit compositional reasoning on sequences not observed during\ntraining, a capability often attributed to in-context learning (ICL) and skill\ncomposition. We investigate this phenomenon using the Random Hierarchy Model\n(RHM), a probabilistic context-free grammar that generates sequences through\nrecursive rule application. Models are trained on subsets of sequences and\nevaluated across four generalization conditions: memorization, in-distribution\ngeneralization, out-of-distribution generalization with the same rules, and\ncross-layer transfer. Behaviorally, performance improves systematically with\ntask complexity and the number of in-context examples, with out-of-distribution\ntasks requiring substantially more examples than in-distribution scenarios.\nMechanistically, we identify a progressive emergence of layer specialization\nduring training that correlates with generalization performance. Principal\ncomponent analysis and attention pattern clustering reveal that transformers\ndevelop structured, hierarchically organized representations in specialized\nlayers. These results demonstrate that transformers develop modular,\ninterpretable mechanisms supporting compositional reasoning, linking internal\nalgorithmic structure to observed behavioral capabilities.", "AI": {"tldr": "Transformers develop modular, interpretable mechanisms for compositional reasoning through progressive layer specialization during training, enabling systematic generalization across different conditions including out-of-distribution scenarios.", "motivation": "To understand how transformers achieve compositional reasoning on unseen sequences through in-context learning and skill composition, particularly investigating the mechanisms behind their generalization capabilities.", "method": "Used the Random Hierarchy Model (RHM) - a probabilistic context-free grammar generating sequences via recursive rules. Trained models on sequence subsets and evaluated across four generalization conditions: memorization, in-distribution generalization, out-of-distribution generalization with same rules, and cross-layer transfer.", "result": "Performance improved systematically with task complexity and number of in-context examples, with out-of-distribution tasks requiring significantly more examples. Progressive emergence of layer specialization during training correlated with generalization performance. Transformers developed structured, hierarchically organized representations in specialized layers.", "conclusion": "Transformers develop modular, interpretable mechanisms supporting compositional reasoning, with internal algorithmic structure directly linked to observed behavioral capabilities through hierarchical representation organization."}}
{"id": "2510.17475", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17475", "abs": "https://arxiv.org/abs/2510.17475", "authors": ["Fo Hu", "Can Wang", "Qinxu Zheng", "Xusheng Yang", "Bin Zhou", "Gang Li", "Yu Sun", "Wen-an Zhang"], "title": "DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition", "comment": "14 pages, 9 figures", "summary": "Significant inter-individual variability limits the generalization of\nEEG-based emotion recognition under cross-domain settings. We address two core\nchallenges in multi-source adaptation: (1) dynamically modeling distributional\nheterogeneity across sources and quantifying their relevance to a target to\nreduce negative transfer; and (2) achieving fine-grained semantic consistency\nto strengthen class discrimination. We propose a distribution-aware\nmulti-source domain adaptation network (DAMSDAN). DAMSDAN integrates\nprototype-based constraints with adversarial learning to drive the encoder\ntoward discriminative, domain-invariant emotion representations. A domain-aware\nsource weighting strategy based on maximum mean discrepancy (MMD) dynamically\nestimates inter-domain shifts and reweights source contributions. In addition,\na prototype-guided conditional alignment module with dual pseudo-label\ninteraction enhances pseudo-label reliability and enables category-level,\nfine-grained alignment, mitigating noise propagation and semantic drift.\nExperiments on SEED and SEED-IV show average accuracies of 94.86\\% and 79.78\\%\nfor cross-subject, and 95.12\\% and 83.15\\% for cross-session protocols. On the\nlarge-scale FACED dataset, DAMSDAN achieves 82.88\\% (cross-subject). Extensive\nablations and interpretability analyses corroborate the effectiveness of the\nproposed framework for cross-domain EEG-based emotion recognition.", "AI": {"tldr": "DAMSDAN is a distribution-aware multi-source domain adaptation network that addresses cross-domain EEG emotion recognition challenges by dynamically modeling domain heterogeneity and achieving fine-grained semantic alignment through prototype-based constraints and adversarial learning.", "motivation": "Significant inter-individual variability limits generalization of EEG-based emotion recognition in cross-domain settings, requiring solutions for multi-source adaptation challenges including dynamic modeling of distributional heterogeneity and achieving fine-grained semantic consistency.", "method": "Integrates prototype-based constraints with adversarial learning, uses domain-aware source weighting based on MMD to dynamically estimate inter-domain shifts, and employs prototype-guided conditional alignment with dual pseudo-label interaction for enhanced pseudo-label reliability and category-level alignment.", "result": "Achieved average accuracies of 94.86% and 79.78% on SEED and SEED-IV for cross-subject protocols, 95.12% and 83.15% for cross-session protocols, and 82.88% on FACED dataset for cross-subject recognition.", "conclusion": "The proposed DAMSDAN framework effectively addresses cross-domain EEG-based emotion recognition challenges through dynamic domain modeling and fine-grained semantic alignment, with extensive experiments validating its effectiveness."}}
{"id": "2510.17478", "categories": ["cs.LG", "physics.geo-ph", "I.2.6; I.6.3; J.2"], "pdf": "https://arxiv.org/pdf/2510.17478", "abs": "https://arxiv.org/abs/2510.17478", "authors": ["Guillaume Rongier", "Luk Peeters"], "title": "Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement", "comment": "52 pages, 42 figures", "summary": "High costs and uncertainties make subsurface decision-making challenging, as\nacquiring new data is rarely scalable. Embedding geological knowledge directly\ninto predictive models offers a valuable alternative. A joint approach enables\njust that: process-based models that mimic geological processes can help train\ngenerative models that make predictions more efficiently. This study explores\nwhether a generative adversarial network (GAN) - a type of deep-learning\nalgorithm for generative modeling - trained to produce fluvial deposits can be\ninverted to match well and seismic data. Four inversion approaches applied to\nthree test samples with 4, 8, and 20 wells struggled to match these well data,\nespecially as the well number increased or as the test sample diverged from the\ntraining data. The key bottleneck lies in the GAN's latent representation: it\nis entangled, so samples with similar sedimentological features are not\nnecessarily close in the latent space. Label conditioning or latent\noverparameterization can partially disentangle the latent space during\ntraining, although not yet sufficiently for a successful inversion. Fine-tuning\nthe GAN to restructure the latent space locally reduces mismatches to\nacceptable levels for all test cases, with and without seismic data. But this\napproach depends on an initial, partially successful inversion step, which\ninfluences the quality and diversity of the final samples. Overall, GANs can\nalready handle the tasks required for their integration into geomodeling\nworkflows. We still need to further assess their robustness, and how to best\nleverage them in support of geological interpretation.", "AI": {"tldr": "GANs can be used for subsurface modeling but struggle with inversion due to entangled latent spaces. Fine-tuning helps but depends on initial inversion success.", "motivation": "High costs and uncertainties in subsurface decision-making make scalable data acquisition difficult. Embedding geological knowledge into predictive models offers a cost-effective alternative.", "method": "Used generative adversarial networks (GANs) trained to produce fluvial deposits, then inverted them to match well and seismic data. Tested four inversion approaches on samples with 4, 8, and 20 wells.", "result": "Inversion struggled to match well data, especially with more wells or when test samples diverged from training data. Fine-tuning GANs to restructure latent space locally reduced mismatches to acceptable levels.", "conclusion": "GANs can handle tasks for geomodeling workflows but need further assessment of robustness and better integration with geological interpretation."}}
{"id": "2510.17480", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17480", "abs": "https://arxiv.org/abs/2510.17480", "authors": ["Aur\u00e9lien Bellet", "Edwige Cyffers", "Davide Frey", "Romaric Gaudel", "Dimitri Ler\u00e9v\u00e9rend", "Fran\u00e7ois Ta\u00efani"], "title": "Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization", "comment": "21 pages, 5 figures", "summary": "Decentralized Learning (DL) enables users to collaboratively train models\nwithout sharing raw data by iteratively averaging local updates with neighbors\nin a network graph. This setting is increasingly popular for its scalability\nand its ability to keep data local under user control. Strong privacy\nguarantees in DL are typically achieved through Differential Privacy (DP), with\nresults showing that DL can even amplify privacy by disseminating noise across\npeer-to-peer communications. Yet in practice, the observed privacy-utility\ntrade-off often appears worse than in centralized training, which may be due to\nlimitations in current DP accounting methods for DL. In this paper, we show\nthat recent advances in centralized DP accounting based on Matrix Factorization\n(MF) for analyzing temporal noise correlations can also be leveraged in DL. By\ngeneralizing existing MF results, we show how to cast both standard DL\nalgorithms and common trust models into a unified formulation. This yields\ntighter privacy accounting for existing DP-DL algorithms and provides a\nprincipled way to develop new ones. To demonstrate the approach, we introduce\nMAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that\noutperforms existing methods on synthetic and real-world graphs.", "AI": {"tldr": "The paper shows that matrix factorization-based DP accounting methods from centralized learning can be generalized to decentralized learning, enabling tighter privacy bounds and better privacy-utility trade-offs. It introduces MAFALDA-SGD, a gossip-based algorithm with correlated noise that outperforms existing methods.", "motivation": "Decentralized learning enables collaborative training without sharing raw data, but current DP accounting methods for DL show worse privacy-utility trade-offs than centralized training. The authors aim to leverage recent advances in centralized DP accounting to improve privacy analysis in decentralized settings.", "method": "The authors generalize matrix factorization (MF) results from centralized DP accounting to decentralized learning, creating a unified formulation that covers standard DL algorithms and trust models. They introduce MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise.", "result": "The approach yields tighter privacy accounting for existing DP-DL algorithms and provides a principled way to develop new ones. MAFALDA-SGD outperforms existing methods on both synthetic and real-world graphs.", "conclusion": "Matrix factorization-based DP accounting can be successfully applied to decentralized learning, enabling better privacy-utility trade-offs and more effective algorithm design in peer-to-peer collaborative training settings."}}
{"id": "2510.17486", "categories": ["cs.LG", "68T07, 68T05, 65K10, 90C30", "I.2.6; G.1.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.17486", "abs": "https://arxiv.org/abs/2510.17486", "authors": ["Maxim Bolshim", "Alexander Kugaevskikh"], "title": "Local properties of neural networks through the lens of layer-wise Hessians", "comment": "Comments: 22 pages, 8 figures. Submitted to arXiv:cs.LG", "summary": "We introduce a methodology for analyzing neural networks through the lens of\nlayer-wise Hessian matrices. The local Hessian of each functional block (layer)\nis defined as the matrix of second derivatives of a scalar function with\nrespect to the parameters of that layer. This concept provides a formal tool\nfor characterizing the local geometry of the parameter space. We show that the\nspectral properties of local Hessians, such as the distribution of eigenvalues,\nreveal quantitative patterns associated with overfitting,\nunderparameterization, and expressivity in neural network architectures. We\nconduct an extensive empirical study involving 111 experiments across 37\ndatasets. The results demonstrate consistent structural regularities in the\nevolution of local Hessians during training and highlight correlations between\ntheir spectra and generalization performance. These findings establish a\nfoundation for using local geometric analysis to guide the diagnosis and design\nof deep neural networks. The proposed framework connects optimization geometry\nwith functional behavior and offers practical insight for improving network\narchitectures and training stability.", "AI": {"tldr": "This paper introduces a methodology for analyzing neural networks using layer-wise Hessian matrices to study local parameter space geometry and reveals connections between Hessian spectral properties and network performance.", "motivation": "To develop a formal tool for characterizing the local geometry of neural network parameter spaces and understand how this geometry relates to network behavior, generalization, and training dynamics.", "method": "The authors define local Hessian matrices for each functional layer as second derivatives of a scalar function with respect to layer parameters, then analyze spectral properties (eigenvalue distributions) across 111 experiments on 37 datasets.", "result": "The study reveals consistent structural patterns in local Hessian evolution during training and shows correlations between Hessian spectra and generalization performance, identifying quantitative patterns for overfitting, underparameterization, and expressivity.", "conclusion": "Local geometric analysis through Hessian matrices provides a foundation for diagnosing and designing deep neural networks, connecting optimization geometry with functional behavior to improve architectures and training stability."}}
{"id": "2510.17496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17496", "abs": "https://arxiv.org/abs/2510.17496", "authors": ["Giacomo Camposampiero", "Michael Hersche", "Roger Wattenhofer", "Abu Sebastian", "Abbas Rahimi"], "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models", "comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025", "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.", "AI": {"tldr": "I-RAVEN-X is a symbolic benchmark that extends I-RAVEN to evaluate generalization and robustness in analogical/mathematical reasoning for LLMs and LRMs, featuring increased complexity, wider attribute ranges, and perceptual uncertainty.", "motivation": "To address limitations in current reasoning benchmarks by creating a more challenging test that evaluates how well models handle increased complexity, wider attribute ranges, and reasoning under uncertainty.", "method": "Extends I-RAVEN benchmark by increasing operand complexity, expanding attribute ranges, and introducing perceptual uncertainty to create more challenging reasoning tasks.", "result": "LRMs outperform LLMs on longer reasoning relations (improved productivity) and wider attribute ranges (better systematicity), but both struggle significantly with reasoning under uncertainty and exploring multiple probabilistic outcomes.", "conclusion": "While LRMs show improvements over LLMs in certain aspects of reasoning, current models still face fundamental challenges in handling uncertainty and probabilistic reasoning, indicating important directions for future research."}}
{"id": "2510.17506", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17506", "abs": "https://arxiv.org/abs/2510.17506", "authors": ["Lachlan Ewen MacDonald", "Hancheng Min", "Leandro Palma", "Salma Tarmoun", "Ziqing Xu", "Ren\u00e9 Vidal"], "title": "Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares", "comment": "NeurIPS2025. Code available at\n  https://github.com/lemacdonald/eos-convergence-rates-codimension-1", "summary": "Classical optimisation theory guarantees monotonic objective decrease for\ngradient descent (GD) when employed in a small step size, or ``stable\", regime.\nIn contrast, gradient descent on neural networks is frequently performed in a\nlarge step size regime called the ``edge of stability\", in which the objective\ndecreases non-monotonically with an observed implicit bias towards flat minima.\nIn this paper, we take a step toward quantifying this phenomenon by providing\nconvergence rates for gradient descent with large learning rates in an\noverparametrised least squares setting. The key insight behind our analysis is\nthat, as a consequence of overparametrisation, the set of global minimisers\nforms a Riemannian manifold $M$, which enables the decomposition of the GD\ndynamics into components parallel and orthogonal to $M$. The parallel component\ncorresponds to Riemannian gradient descent on the objective sharpness, while\nthe orthogonal component is a bifurcating dynamical system. This insight allows\nus to derive convergence rates in three regimes characterised by the learning\nrate size: (a) the subcritical regime, in which transient instability is\novercome in finite time before linear convergence to a suboptimally flat global\nminimum; (b) the critical regime, in which instability persists for all time\nwith a power-law convergence toward the optimally flat global minimum; and (c)\nthe supercritical regime, in which instability persists for all time with\nlinear convergence to an orbit of period two centred on the optimally flat\nglobal minimum.", "AI": {"tldr": "This paper analyzes gradient descent with large learning rates in overparametrized least squares, showing how different learning rate regimes affect convergence to flat minima.", "motivation": "To understand why gradient descent on neural networks works well with large learning rates (edge of stability regime) despite non-monotonic objective decrease and implicit bias toward flat minima.", "method": "Analyzes GD dynamics in overparametrized least squares by decomposing the dynamics into components parallel and orthogonal to the manifold of global minimizers, treating the orthogonal component as a bifurcating dynamical system.", "result": "Identifies three convergence regimes: subcritical (transient instability, linear convergence to suboptimal flat minimum), critical (persistent instability with power-law convergence to optimal flat minimum), and supercritical (persistent instability with linear convergence to period-2 orbit around optimal flat minimum).", "conclusion": "The analysis provides theoretical quantification of gradient descent behavior with large learning rates, explaining the implicit bias toward flat minima through the geometric structure of the optimization landscape in overparametrized settings."}}
{"id": "2510.17515", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17515", "abs": "https://arxiv.org/abs/2510.17515", "authors": ["Hoang Pham", "The-Anh Ta", "Tom Jacobs", "Rebekka Burkholz", "Long Tran-Thanh"], "title": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis", "comment": "NeurIPS 2025 Spotlight", "summary": "Sparse neural networks promise efficiency, yet training them effectively\nremains a fundamental challenge. Despite advances in pruning methods that\ncreate sparse architectures, understanding why some sparse structures are\nbetter trainable than others with the same level of sparsity remains poorly\nunderstood. Aiming to develop a systematic approach to this fundamental\nproblem, we propose a novel theoretical framework based on the theory of graph\nlimits, particularly graphons, that characterizes sparse neural networks in the\ninfinite-width regime. Our key insight is that connectivity patterns of sparse\nneural networks induced by pruning methods converge to specific graphons as\nnetworks' width tends to infinity, which encodes implicit structural biases of\ndifferent pruning methods. We postulate the Graphon Limit Hypothesis and\nprovide empirical evidence to support it. Leveraging this graphon\nrepresentation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to\nstudy the training dynamics of sparse networks in the infinite width limit.\nGraphon NTK provides a general framework for the theoretical analysis of sparse\nnetworks. We empirically show that the spectral analysis of Graphon NTK\ncorrelates with observed training dynamics of sparse networks, explaining the\nvarying convergence behaviours of different pruning methods. Our framework\nprovides theoretical insights into the impact of connectivity patterns on the\ntrainability of various sparse network architectures.", "AI": {"tldr": "Proposes a graphon-based theoretical framework to analyze sparse neural networks, showing that pruning methods converge to specific graphons in infinite width, and introduces Graphon NTK to study training dynamics.", "motivation": "To understand why some sparse neural network structures are more trainable than others with the same sparsity level, and develop a systematic approach to analyze connectivity patterns in sparse networks.", "method": "Uses graph theory and graphons to characterize sparse neural networks in infinite-width regime, proposes Graphon Limit Hypothesis, and derives Graphon Neural Tangent Kernel (Graphon NTK) to study training dynamics.", "result": "Empirical evidence supports Graphon Limit Hypothesis, and spectral analysis of Graphon NTK correlates with observed training dynamics, explaining varying convergence behaviors of different pruning methods.", "conclusion": "The framework provides theoretical insights into how connectivity patterns impact trainability of sparse network architectures and offers a general approach for analyzing sparse networks."}}
{"id": "2510.17517", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17517", "abs": "https://arxiv.org/abs/2510.17517", "authors": ["Hangcheng Cao", "Baixiang Huang", "Longzhi Yuan", "Haonan An", "Zihan Fang", "Xianhao Chen", "Yuguang Fang"], "title": "SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers", "comment": null, "summary": "A driver's health state serves as a determinant factor in driving behavioral\nregulation. Subtle deviations from normalcy can lead to operational anomalies,\nposing risks to public transportation safety. While prior efforts have\ndeveloped detection mechanisms for functionally-driven temporary anomalies such\nas drowsiness and distraction, limited research has addressed\npathologically-triggered deviations, especially those stemming from chronic\nmedical conditions. To bridge this gap, we investigate the driving behavior of\nParkinson's disease patients and propose SAFE-D, a novel framework for\ndetecting Parkinson-related behavioral anomalies to enhance driving safety. Our\nmethodology starts by performing analysis of Parkinson's disease\nsymptomatology, focusing on primary motor impairments, and establishes causal\nlinks to degraded driving performance. To represent the subclinical behavioral\nvariations of early-stage Parkinson's disease, our framework integrates data\nfrom multiple vehicle control components to build a behavioral profile. We then\ndesign an attention-based network that adaptively prioritizes spatiotemporal\nfeatures, enabling robust anomaly detection under physiological variability.\nFinally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,\nusing data from three road maps to emulate real-world driving. Our results show\nSAFE-D achieves 96.8% average accuracy in distinguishing normal and\nParkinson-affected driving patterns.", "AI": {"tldr": "SAFE-D is a framework that detects Parkinson's disease-related driving anomalies by analyzing motor impairments and using an attention-based network to prioritize spatiotemporal features from vehicle control data, achieving 96.8% accuracy in distinguishing affected driving patterns.", "motivation": "Driver health state affects driving behavior, with pathological conditions like Parkinson's disease causing subtle deviations that pose safety risks. Previous research focused on temporary anomalies like drowsiness, but limited work addresses chronic medical conditions' impact on driving.", "method": "Analyzed Parkinson's disease symptomatology and motor impairments, established causal links to driving performance degradation. Built behavioral profiles from multiple vehicle control components. Designed attention-based network to prioritize spatiotemporal features for robust anomaly detection under physiological variability.", "result": "Validated on Logitech G29 platform and CARLA simulator using three road maps. Achieved 96.8% average accuracy in distinguishing normal and Parkinson-affected driving patterns.", "conclusion": "SAFE-D successfully bridges the gap in detecting pathologically-triggered driving deviations, providing an effective framework for enhancing driving safety in Parkinson's disease patients through robust behavioral anomaly detection."}}
{"id": "2510.17520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17520", "abs": "https://arxiv.org/abs/2510.17520", "authors": ["Canran Xiao", "Chuangxin Zhao", "Zong Ke", "Fei Shen"], "title": "Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning", "comment": "Under review", "summary": "Long-tail imbalance is endemic to multi-label learning: a few head labels\ndominate the gradient signal, while the many rare labels that matter in\npractice are silently ignored. We tackle this problem by casting the task as a\ncooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label\nLearning (CD-GTMLL) framework, the label space is split among several\ncooperating players that share a global accuracy payoff yet earn additional\ncuriosity rewards that rise with label rarity and inter-player disagreement.\nThese curiosity bonuses inject gradient on under-represented tags without\nhand-tuned class weights. We prove that gradient best-response updates ascend a\ndifferentiable potential and converge to tail-aware stationary points that\ntighten a lower bound on the expected Rare-F1. Extensive experiments on\nconventional benchmarks and three extreme-scale datasets show consistent\nstate-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the\nstrongest baselines, while ablations reveal emergent division of labour and\nfaster consensus on rare classes. CD-GTMLL thus offers a principled, scalable\nroute to long-tail robustness in multi-label prediction.", "AI": {"tldr": "CD-GTMLL addresses long-tail imbalance in multi-label learning by framing it as a cooperative potential game where players share global accuracy but earn curiosity rewards for rare labels, achieving state-of-the-art performance on tail-aware metrics.", "motivation": "Long-tail imbalance is endemic in multi-label learning where few head labels dominate gradient signals while many rare labels are ignored, limiting practical effectiveness.", "method": "Casts multi-label learning as a cooperative potential game where label space is split among cooperating players who share global accuracy payoff and earn additional curiosity rewards based on label rarity and inter-player disagreement.", "result": "Achieves consistent state-of-the-art gains with up to +4.3% Rare-F1 and +1.6% P@3 over strongest baselines on conventional benchmarks and extreme-scale datasets, showing emergent division of labor and faster consensus on rare classes.", "conclusion": "CD-GTMLL offers a principled, scalable route to long-tail robustness in multi-label prediction without hand-tuned class weights, converging to tail-aware stationary points that tighten lower bounds on expected Rare-F1."}}
{"id": "2510.17524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17524", "abs": "https://arxiv.org/abs/2510.17524", "authors": ["Sidney Bender", "Ole Delzer", "Jan Herrmann", "Heike Antje Marxfeld", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples", "comment": null, "summary": "Deep learning models remain vulnerable to spurious correlations, leading to\nso-called Clever Hans predictors that undermine robustness even in large-scale\nfoundation and self-supervised models. Group distributional robustness methods,\nsuch as Deep Feature Reweighting (DFR) rely on explicit group labels to\nupweight underrepresented subgroups, but face key limitations: (1) group labels\nare often unavailable, (2) low within-group sample sizes hinder coverage of the\nsubgroup distribution, and (3) performance degrades sharply when multiple\nspurious correlations fragment the data into even smaller groups. We propose\nCounterfactual Knowledge Distillation (CFKD), a framework that sidesteps these\nissues by generating diverse counterfactuals, enabling a human annotator to\nefficiently explore and correct the model's decision boundaries through a\nknowledge distillation step. Unlike DFR, our method not only reweights the\nundersampled groups, but it also enriches them with new data points. Our method\ndoes not require any confounder labels, achieves effective scaling to multiple\nconfounders, and yields balanced generalization across groups. We demonstrate\nCFKD's efficacy across five datasets, spanning synthetic tasks to an industrial\napplication, with particularly strong gains in low-data regimes with pronounced\nspurious correlations. Additionally, we provide an ablation study on the effect\nof the chosen counterfactual explainer and teacher model, highlighting their\nimpact on robustness.", "AI": {"tldr": "CFKD is a counterfactual knowledge distillation framework that addresses spurious correlations in deep learning models without requiring group labels, by generating diverse counterfactuals and using knowledge distillation to correct decision boundaries.", "motivation": "Deep learning models are vulnerable to spurious correlations (Clever Hans predictors), and existing group distributional robustness methods like DFR have limitations: group labels are often unavailable, low within-group sample sizes hinder coverage, and performance degrades with multiple spurious correlations.", "method": "Counterfactual Knowledge Distillation (CFKD) generates diverse counterfactuals that enable human annotators to efficiently explore and correct model decision boundaries through knowledge distillation. Unlike DFR, it enriches undersampled groups with new data points rather than just reweighting them.", "result": "CFKD achieves effective scaling to multiple confounders, yields balanced generalization across groups, and shows strong gains in low-data regimes with pronounced spurious correlations across five datasets from synthetic tasks to industrial applications.", "conclusion": "CFKD provides a robust framework that sidesteps the limitations of group label-dependent methods, works without confounder labels, and demonstrates superior performance particularly in challenging low-data scenarios with multiple spurious correlations."}}
{"id": "2510.17526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17526", "abs": "https://arxiv.org/abs/2510.17526", "authors": ["Wei Huang", "Andi Han", "Yujin Song", "Yilan Chen", "Denny Wu", "Difan Zou", "Taiji Suzuki"], "title": "How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?", "comment": "40 pages", "summary": "The capacity of deep learning models is often large enough to both learn the\nunderlying statistical signal and overfit to noise in the training set. This\nnoise memorization can be harmful especially for data with a low\nsignal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior\nobservations that label noise provides implicit regularization that improves\ngeneralization, in this work, we investigate whether introducing label noise to\nthe gradient updates can enhance the test performance of neural network (NN) in\nthe low SNR regime. Specifically, we consider training a two-layer NN with a\nsimple label noise gradient descent (GD) algorithm, in an idealized\nsignal-noise data setting. We prove that adding label noise during training\nsuppresses noise memorization, preventing it from dominating the learning\nprocess; consequently, label noise GD enjoys rapid signal growth while the\noverfitting remains controlled, thereby achieving good generalization despite\nthe low SNR. In contrast, we also show that NN trained with standard GD tends\nto overfit to noise in the same low SNR setting and establish a non-vanishing\nlower bound on its test error, thus demonstrating the benefit of introducing\nlabel noise in gradient-based training.", "AI": {"tldr": "Adding label noise during gradient descent training suppresses noise memorization and improves generalization in low signal-to-noise ratio settings, while standard gradient descent tends to overfit to noise.", "motivation": "Deep learning models often overfit to noise in training data, especially in low signal-to-noise ratio (SNR) scenarios, which harms generalization. Prior observations suggest label noise can provide implicit regularization.", "method": "Training a two-layer neural network with label noise gradient descent in an idealized signal-noise data setting, where label noise is introduced during gradient updates.", "result": "Label noise GD suppresses noise memorization, allowing rapid signal growth while controlling overfitting, achieving good generalization. Standard GD tends to overfit to noise with non-vanishing test error in low SNR settings.", "conclusion": "Introducing label noise during gradient-based training provides benefits by preventing noise memorization and improving generalization performance in low SNR regimes."}}
{"id": "2510.17545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17545", "abs": "https://arxiv.org/abs/2510.17545", "authors": ["Yichen Liu", "Yan Lin", "Shengnan Guo", "Zeyu Zhou", "Youfang Lin", "Huaiyu Wan"], "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model", "comment": "Accepted by NeurIPS2025", "summary": "Vehicle GPS trajectories record how vehicles move over time, storing valuable\ntravel semantics, including movement patterns and travel purposes. Learning\ntravel semantics effectively and efficiently is crucial for real-world\napplications of trajectory data, which is hindered by two major challenges.\nFirst, travel purposes are tied to the functions of the roads and\npoints-of-interest (POIs) involved in a trip. Such information is encoded in\ntextual addresses and descriptions and introduces heavy computational burden to\nmodeling. Second, real-world trajectories often contain redundant points, which\nharm both computational efficiency and trajectory embedding quality. To address\nthese challenges, we propose TrajMamba, a novel approach for efficient and\nsemantically rich vehicle trajectory learning. TrajMamba introduces a\nTraj-Mamba Encoder that captures movement patterns by jointly modeling both GPS\nand road perspectives of trajectories, enabling robust representations of\ncontinuous travel behaviors. It also incorporates a Travel Purpose-aware\nPre-training procedure to integrate travel purposes into the learned embeddings\nwithout introducing extra overhead to embedding calculation. To reduce\nredundancy in trajectories, TrajMamba features a Knowledge Distillation\nPre-training scheme to identify key trajectory points through a learnable mask\ngenerator and obtain effective compressed trajectory embeddings. Extensive\nexperiments on two real-world datasets and three downstream tasks show that\nTrajMamba outperforms state-of-the-art baselines in both efficiency and\naccuracy.", "AI": {"tldr": "TrajMamba is a novel approach for efficient vehicle trajectory learning that addresses computational burdens from textual data and redundant points by using a Traj-Mamba Encoder, Travel Purpose-aware Pre-training, and Knowledge Distillation Pre-training.", "motivation": "To overcome challenges in learning travel semantics from vehicle GPS trajectories, including the computational burden from textual road/POI information and the negative impact of redundant trajectory points on efficiency and embedding quality.", "method": "Uses a Traj-Mamba Encoder to capture movement patterns from both GPS and road perspectives, incorporates Travel Purpose-aware Pre-training to embed travel purposes without extra overhead, and employs Knowledge Distillation Pre-training with a learnable mask generator to identify key points and compress trajectories.", "result": "Extensive experiments on two real-world datasets and three downstream tasks demonstrate that TrajMamba outperforms state-of-the-art baselines in both efficiency and accuracy.", "conclusion": "TrajMamba provides an effective and efficient solution for learning semantically rich vehicle trajectory embeddings by addressing key challenges in computational burden and trajectory redundancy."}}
{"id": "2510.17558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17558", "abs": "https://arxiv.org/abs/2510.17558", "authors": ["Fran\u00e7ois Fleuret"], "title": "The Free Transformer", "comment": null, "summary": "We propose an extension of the decoder Transformer that conditions its\ngenerative process on random latent variables which are learned without\nsupervision thanks to a variational procedure. Experimental evaluations show\nthat allowing such a conditioning translates into substantial improvements on\ndownstream tasks.", "AI": {"tldr": "Extension of decoder Transformer with unsupervised variational latent variables for improved downstream task performance.", "motivation": "To enhance the generative process of decoder Transformers by incorporating random latent variables learned without supervision.", "method": "Extend decoder Transformer with variational latent variables using unsupervised learning through a variational procedure.", "result": "Substantial improvements on downstream tasks compared to standard decoder Transformers.", "conclusion": "Conditioning generative processes on learned latent variables significantly boosts Transformer performance on various tasks."}}
{"id": "2510.17562", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17562", "abs": "https://arxiv.org/abs/2510.17562", "authors": ["Dennis Wagner", "Arjun Nair", "Billy Joe Franks", "Justus Arweiler", "Aparna Muraleedharan", "Indra Jungjohann", "Fabian Hartung", "Mayank C. Ahuja", "Andriy Balinskyy", "Saurabh Varshneya", "Nabeel Hussain Syed", "Mayank Nagda", "Phillip Liznerski", "Steffen Reithermann", "Maja Rudolph", "Sebastian Vollmer", "Ralf Schulz", "Torsten Katz", "Stephan Mandt", "Michael Bortz", "Heike Leitte", "Daniel Neider", "Jakob Burger", "Fabian Jirasek", "Hans Hasse", "Sophie Fellenz", "Marius Kloft"], "title": "Formally Exploring Time-Series Anomaly Detection Evaluation Metrics", "comment": "73 pages, 13 figures", "summary": "Undetected anomalies in time series can trigger catastrophic failures in\nsafety-critical systems, such as chemical plant explosions or power grid\noutages. Although many detection methods have been proposed, their performance\nremains unclear because current metrics capture only narrow aspects of the task\nand often yield misleading results. We address this issue by introducing\nverifiable properties that formalize essential requirements for evaluating\ntime-series anomaly detection. These properties enable a theoretical framework\nthat supports principled evaluations and reliable comparisons. Analyzing 37\nwidely used metrics, we show that most satisfy only a few properties, and none\nsatisfy all, explaining persistent inconsistencies in prior results. To close\nthis gap, we propose LARM, a flexible metric that provably satisfies all\nproperties, and extend it to ALARM, an advanced variant meeting stricter\nrequirements.", "AI": {"tldr": "The paper addresses inconsistencies in time-series anomaly detection evaluation by introducing verifiable properties for principled metric assessment, showing most existing metrics fail key properties, and proposing LARM/ALARM metrics that satisfy all requirements.", "motivation": "Current anomaly detection metrics are unreliable and yield misleading results, which can lead to catastrophic failures in safety-critical systems. There's a need for principled evaluation frameworks to ensure reliable comparisons.", "method": "Introduce verifiable properties that formalize essential requirements for evaluating time-series anomaly detection. Analyze 37 widely used metrics against these properties and propose LARM (and its advanced variant ALARM) as new metrics that provably satisfy all properties.", "result": "Most existing metrics satisfy only a few properties, and none satisfy all properties, explaining persistent inconsistencies in prior evaluation results. LARM and ALARM are shown to provably satisfy all required properties.", "conclusion": "The proposed properties provide a theoretical foundation for reliable anomaly detection evaluation, and LARM/ALARM metrics close the gap in existing evaluation methods by satisfying all essential requirements."}}
{"id": "2510.17564", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17564", "abs": "https://arxiv.org/abs/2510.17564", "authors": ["Lindsay Spoor", "\u00c1lvaro Serra-G\u00f3mez", "Aske Plaat", "Thomas Moerland"], "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning", "comment": null, "summary": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL.", "AI": {"tldr": "This paper analyzes the robustness and stability of automated Lagrange multiplier updates in safe reinforcement learning, showing that while automated updates can recover optimal performance, they exhibit oscillatory behavior that can be mitigated with PID-controlled updates.", "motivation": "To address the lack of empirical evidence on the robustness of automated Lagrange multiplier updates in safe RL and understand how the choice of \u03bb affects the trade-off between performance and constraint satisfaction.", "method": "Analysis of optimality and stability of Lagrange multipliers across various tasks using \u03bb-profiles for visualization, and investigation of automated multiplier updates and PID-controlled updates.", "result": "\u03bb-profiles reveal high sensitivity of \u03bb values and lack of general intuition for choosing optimal \u03bb*. Automated updates can recover or exceed optimal performance but exhibit oscillatory behavior. PID-controlled updates can mitigate oscillations but require careful tuning.", "conclusion": "Automated Lagrange multiplier updates are effective but unstable, highlighting the need for further research on stabilizing Lagrangian methods in safe reinforcement learning."}}
{"id": "2510.17569", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.17569", "abs": "https://arxiv.org/abs/2510.17569", "authors": ["Jyler Menard", "R. A. Mansbach"], "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides", "comment": "19 pages, 9 figures", "summary": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat\nbacterial infections. Discovering and designing such peptides is difficult\nbecause of the vast number of possible sequences of amino acids. Deep\ngenerative models, such as variational autoencoders, have shown value in\npeptide design due to their ability to model sequence space with a\ncontinuous-valued latent space. Although such models have already been used to\ngreat effect in biomolecular design, they still suffer from a lack of\ninterpretability and rigorous quantification of latent space quality as a\nsearch space. We investigate (1) whether further compression of the design\nspace via dimensionality reduction may facilitate optimization, (2) the\ninterpretability of the spaces, and (3) how organizing latent spaces with\nphysicochemical properties may improve the efficiency of optimizing\nantimicrobial activity. We find that further reduction of the latent space via\ndimensionality reduction can be advantageous when organizing the space with\nmore relevant information at data availability, that using the dimensionality\nreduction search space can be more interpretable, and that we can organize the\nlatent space with different physicochemical properties even at different\npercentages of available labels.", "AI": {"tldr": "This paper investigates using dimensionality reduction and physicochemical property organization to improve variational autoencoders for antimicrobial peptide design, focusing on interpretability and optimization efficiency.", "motivation": "Deep generative models like variational autoencoders are effective for peptide design but lack interpretability and rigorous quantification of latent space quality as a search space.", "method": "The study investigates three aspects: (1) further compression of design space via dimensionality reduction, (2) interpretability of spaces, and (3) organizing latent spaces with physicochemical properties to improve antimicrobial activity optimization.", "result": "Findings show that further reduction of latent space via dimensionality reduction is advantageous when organizing with relevant information, dimensionality reduction search spaces are more interpretable, and latent spaces can be organized with different physicochemical properties even with limited label availability.", "conclusion": "Dimensionality reduction and physicochemical property organization can enhance the interpretability and optimization efficiency of latent spaces in antimicrobial peptide design using variational autoencoders."}}
{"id": "2510.17584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17584", "abs": "https://arxiv.org/abs/2510.17584", "authors": ["Ludi Li", "Junbin Mao", "Hanhe Lin", "Xu Tian", "Fang-Xiang Wu", "Jin Liu"], "title": "CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification", "comment": null, "summary": "Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical\npractice such as Alzheimer's disease diagnosis. To train a robust model for\nmulti-pulse MRI classification, it requires large and diverse data from various\nmedical institutions while protecting privacy by preventing raw data sharing\nacross institutions. Although federated learning (FL) is a feasible solution to\naddress this issue, it poses challenges of model convergence due to the effect\nof data heterogeneity and substantial communication overhead due to large\nnumbers of parameters transmitted within the model. To address these\nchallenges, we propose CEPerFed, a communication-efficient personalized FL\nmethod. It mitigates the effect of data heterogeneity by incorporating\nclient-side historical risk gradients and historical mean gradients to\ncoordinate local and global optimization. The former is used to weight the\ncontributions from other clients, enhancing the reliability of local updates,\nwhile the latter enforces consistency between local updates and the global\noptimization direction to ensure stable convergence across heterogeneous data\ndistributions. To address the high communication overhead, we propose a\nhierarchical SVD (HSVD) strategy that transmits only the most critical\ninformation required for model updates. Experiments on five classification\ntasks demonstrate the effectiveness of the CEPerFed method. The code will be\nreleased upon acceptance at https://github.com/LD0416/CEPerFed.", "AI": {"tldr": "CEPerFed is a communication-efficient personalized federated learning method for multi-pulse MRI classification that addresses data heterogeneity using historical gradients and reduces communication overhead via hierarchical SVD compression.", "motivation": "To enable robust multi-pulse MRI classification for clinical applications like Alzheimer's disease diagnosis while preserving privacy by avoiding raw data sharing across medical institutions, overcoming federated learning challenges of data heterogeneity and high communication costs.", "method": "CEPerFed incorporates client-side historical risk gradients and historical mean gradients to coordinate local and global optimization, using hierarchical SVD (HSVD) strategy to transmit only critical information for model updates.", "result": "Experiments on five classification tasks demonstrate the effectiveness of the CEPerFed method in handling data heterogeneity and reducing communication overhead.", "conclusion": "CEPerFed successfully addresses both data heterogeneity and communication efficiency challenges in federated learning for medical imaging, providing a practical solution for privacy-preserving multi-pulse MRI classification."}}
{"id": "2510.17650", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17650", "abs": "https://arxiv.org/abs/2510.17650", "authors": ["Athanasios Angelakis", "Amne Mousa", "Micah L. A. Heldeweg", "Laurens A. Biesheuvel", "Mark A. Haaksma", "Jasper M. Smit", "Pieter R. Tuinman", "Paul W. G. Elbers"], "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification", "comment": "14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine\n  Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),\n  eess.IV (Image and Video Processing). Code available at:\n  https://github.com/Bluesman79/ZACH-ViT Installation: pip install zachvit\n  Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License", "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.", "AI": {"tldr": "ZACH-ViT is a compact 0.25M-parameter Vision Transformer that removes positional embeddings and CLS token for permutation invariance, achieving superior performance in classifying cardiogenic pulmonary edema from non-cardiogenic patterns in lung ultrasound videos.", "motivation": "Differentiating cardiogenic pulmonary edema from non-cardiogenic patterns in lung ultrasound videos is challenging due to high visual variability and overlapping artifacts, which complicates automated classification.", "method": "Proposed ZACH-ViT with zero positional embeddings and CLS token for full permutation invariance, plus ShuffleStrides Data Augmentation that permutes probe-view sequences while preserving anatomical validity.", "result": "Achieved highest validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60) and specificity (0.91), outperforming 9 state-of-the-art baselines that collapsed to trivial classification. Trains 1.35x faster with 2.5x fewer parameters than Minimal ViT.", "conclusion": "Aligning architectural design with data structure can outperform scale in small-data medical imaging, supporting real-time clinical deployment."}}
{"id": "2510.17661", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17661", "abs": "https://arxiv.org/abs/2510.17661", "authors": ["Vaishnavi Visweswaraiah", "Tanvi Banerjee", "William Romine"], "title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction", "comment": null, "summary": "Suicide prediction is the key for prevention, but real data with sufficient\npositive samples is rare and causes extreme class imbalance. We utilized\nmachine learning (ML) to build the model and deep learning (DL) techniques,\nlike Generative Adversarial Networks (GAN), to generate synthetic data samples\nto enhance the dataset. The initial dataset contained 656 samples, with only\nfour positive cases, prompting the need for data augmentation. A variety of\nmachine learning models, ranging from interpretable data models to black box\nalgorithmic models, were used. On real test data, Logistic Regression (LR)\nachieved a weighted precision of 0.99, a weighted recall of 0.85, and a\nweighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,\nrespectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.\nLR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and\nmisclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &\n0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)\nwith 0 false positives (specificity: 1.0). These results highlight the models'\neffectiveness, with GAN playing a key role in generating synthetic data to\nsupport suicide prevention modeling efforts.", "AI": {"tldr": "This paper addresses suicide prediction using machine learning and deep learning techniques, particularly GANs for data augmentation to handle extreme class imbalance in a dataset with only 4 positive cases out of 656 samples.", "motivation": "Suicide prediction is crucial for prevention, but real-world data suffers from extreme class imbalance with very few positive cases, making effective modeling challenging.", "method": "Used machine learning models (Logistic Regression, Random Forest, SVM) and deep learning techniques including Generative Adversarial Networks (GAN) to generate synthetic data samples for dataset enhancement and address class imbalance.", "result": "Logistic Regression achieved weighted precision 0.99, recall 0.85, F1 0.91; Random Forest showed 0.98, 0.99, 0.99; SVM achieved 0.99, 0.76, 0.86. LR and SVM correctly identified suicide attempts (sensitivity:1.0) but had some false positives, while RF had perfect specificity but zero sensitivity.", "conclusion": "The models demonstrated effectiveness in suicide prediction, with GAN playing a crucial role in generating synthetic data to support modeling efforts for suicide prevention."}}
{"id": "2510.17670", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17670", "abs": "https://arxiv.org/abs/2510.17670", "authors": ["Yehonathan Refael", "Amit Aides", "Aviad Barzilai", "George Leifman", "Genady Beryozkin", "Vered Silverman", "Bolous Jaber", "Tomer Shekel"], "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration", "comment": null, "summary": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.", "AI": {"tldr": "A cascaded approach combining open-vocabulary object detection with lightweight few-shot learning for remote sensing applications, using FLAME active learning for efficient sample selection and real-time adaptation.", "motivation": "Open-vocabulary object detection models struggle with fine-grained class distinctions in specialized domains like remote sensing due to natural language ambiguity, limiting practical applications such as illegal fishing monitoring.", "method": "Cascaded framework that first uses zero-shot OVD for high-recall proposals, then refines with a compact classifier trained on few user-annotated examples using FLAME active learning strategy for sample selection.", "result": "Consistently surpasses state-of-the-art performance on remote sensing benchmarks, enables instant adaptation within less than a minute, and achieves high accuracy without costly full-model fine-tuning.", "conclusion": "Establishes a practical and resource-efficient framework for adapting foundation models to specific user needs in remote sensing applications."}}
{"id": "2510.17671", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17671", "abs": "https://arxiv.org/abs/2510.17671", "authors": ["Katarzyna Kobalczyk", "Zhiyuan Jerry Lin", "Benjamin Letham", "Zhuokai Zhao", "Maximilian Balandat", "Eytan Bakshy"], "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "comment": null, "summary": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.", "AI": {"tldr": "A language-in-the-loop framework that uses LLMs to convert natural language feedback into scalar utilities for Bayesian Optimization, enabling flexible feedback handling while maintaining BO efficiency.", "motivation": "Feedback is essential for translating complex, nuanced, or subjective goals into quantifiable optimization objectives, but existing methods have limited feedback formats and require domain-specific customization.", "method": "Proposes a hybrid approach using LLMs to convert unstructured natural language feedback into scalar utilities, which are then used for Bayesian Optimization over numeric search spaces. This allows flexible user priors without manual kernel design.", "result": "The method outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes, while providing a more natural interface for decision makers.", "conclusion": "The language-in-the-loop framework successfully combines the flexibility of LLMs for feedback interpretation with the sample efficiency and principled uncertainty quantification of Bayesian Optimization."}}
{"id": "2510.17690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17690", "abs": "https://arxiv.org/abs/2510.17690", "authors": ["Xihong Su"], "title": "Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning", "comment": "Dissertation", "summary": "This dissertation makes three main contributions. First, We identify a new\nconnection between policy gradient and dynamic programming in MMDPs and propose\nthe Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov\npolicy that maximizes the discounted return averaged over the uncertain models.\nCADP adjusts model weights iteratively to guarantee monotone policy\nimprovements to a local maximum. Second, We establish sufficient and necessary\nconditions for the exponential ERM Bellman operator to be a contraction and\nprove the existence of stationary deterministic optimal policies for ERM-TRC\nand EVaR-TRC. We also propose exponential value iteration, policy iteration,\nand linear programming algorithms for computing optimal stationary policies for\nERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for\ncomputing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The\nchallenge is that Q-learning ERM Bellman may not be a contraction. Instead, we\nuse the monotonicity of Q-learning ERM Bellman operators to derive a rigorous\nproof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the\noptimal risk-averse value functions. The proposed Q-learning algorithms compute\nthe optimal stationary policy for ERM-TRC and EVaR-TRC.", "AI": {"tldr": "This dissertation presents three key contributions: (1) CADP algorithm connecting policy gradient and dynamic programming for MMDPs, (2) theoretical analysis of ERM Bellman operators and algorithms for ERM-TRC/EVaR-TRC, and (3) model-free Q-learning algorithms for risk-averse objectives with convergence proofs.", "motivation": "The research aims to bridge the gap between policy gradient methods and dynamic programming in multi-model MDPs (MMDPs), and to develop rigorous algorithms for risk-averse reinforcement learning objectives like ERM-TRC and EVaR-TRC.", "method": "Developed CADP algorithm using coordinate ascent with dynamic programming, established contraction conditions for ERM Bellman operators, and proposed model-free Q-learning algorithms with monotonicity-based convergence proofs for risk-averse objectives.", "result": "Proved existence of stationary deterministic optimal policies for ERM-TRC/EVaR-TRC, demonstrated monotone policy improvements with CADP, and established convergence guarantees for Q-learning algorithms in risk-averse settings.", "conclusion": "The dissertation successfully connects policy gradient and dynamic programming approaches, provides comprehensive theoretical foundations for risk-averse RL, and delivers practical algorithms with proven convergence properties for complex risk-sensitive decision-making problems."}}
{"id": "2510.17709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17709", "abs": "https://arxiv.org/abs/2510.17709", "authors": ["Akhil S Anand", "Shambhuraj Sawant", "Jasper Hoffmann", "Dirk Reinhardt", "Sebastien Gros"], "title": "Closing the Sim2Real Performance Gap in RL", "comment": null, "summary": "Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.", "AI": {"tldr": "The paper proposes a bi-level RL framework to close the Sim2Real performance gap by directly adapting simulator parameters based on real-world performance, rather than using proxy metrics.", "motivation": "Current Sim2Real RL methods use simulator accuracy and variability as proxies for real-world performance, but these metrics don't necessarily correlate with actual policy performance in the real world, leading to significant performance drops when transferring from simulation to reality.", "method": "A bi-level RL framework where the inner-level RL trains policies purely in simulation, and the outer-level RL adapts simulation model and reward parameters to maximize real-world performance of the in-sim trained policies.", "result": "The authors derive and validate mathematical tools needed to develop bi-level RL algorithms that can effectively close the Sim2Real performance gap in simple examples.", "conclusion": "The proposed bi-level RL approach directly optimizes simulator parameters based on real-world performance, providing a more effective solution to the Sim2Real transfer problem compared to methods that rely on proxy metrics."}}
{"id": "2510.17727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17727", "abs": "https://arxiv.org/abs/2510.17727", "authors": ["Ege Beyazit", "KL Navaneet", "Prashant Mathur", "Roi Blanco", "Vidit Bansal", "Karim Bouyarmane"], "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs", "comment": "35 pages", "summary": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.", "AI": {"tldr": "This paper addresses the limitation of black-box LLMs in fine-grained decision making due to low numerical output cardinality. It proposes efficient methods to increase operational granularity without performance loss.", "motivation": "Black-box LLMs have limited control over operating points for applications requiring specific metric constraints (e.g., precision \u226595%), preventing fine-grained adjustment of decision making behavior.", "method": "The authors investigate reasons for low-cardinality outputs, experiment with existing techniques (prompt engineering, uncertainty estimation, confidence elicitation), and propose new efficient approaches to increase operating point diversity.", "result": "The proposed approaches provide finer-grained operating points and achieve comparable or better performance than benchmark methods across 11 datasets and 3 LLMs.", "conclusion": "Efficient methods can significantly improve the operational granularity of black-box LLMs as classifiers without sacrificing performance or increasing inference costs."}}
{"id": "2510.17756", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17756", "abs": "https://arxiv.org/abs/2510.17756", "authors": ["Younghyun Koo", "Maryam Rahnemoonfar"], "title": "Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network", "comment": "49 pages, 7 figures, submitted to Environmental Modelling & Software", "summary": "As an increasing amount of remote sensing data becomes available in the\nArctic Ocean, data-driven machine learning (ML) techniques are becoming widely\nused to predict sea ice velocity (SIV) and sea ice concentration (SIC).\nHowever, fully data-driven ML models have limitations in generalizability and\nphysical consistency due to their excessive reliance on the quantity and\nquality of training data. In particular, as Arctic sea ice entered a new phase\nwith thinner ice and accelerated melting, there is a possibility that an ML\nmodel trained with historical sea ice data cannot fully represent the\ndynamically changing sea ice conditions in the future. In this study, we\ndevelop physics-informed neural network (PINN) strategies to integrate physical\nknowledge of sea ice into the ML model. Based on the Hierarchical\nInformation-sharing U-net (HIS-Unet) architecture, we incorporate the physics\nloss function and the activation function to produce physically plausible SIV\nand SIC outputs. Our PINN model outperforms the fully data-driven model in the\ndaily predictions of SIV and SIC, even when trained with a small number of\nsamples. The PINN approach particularly improves SIC predictions in melting and\nearly freezing seasons and near fast-moving ice regions.", "AI": {"tldr": "Physics-informed neural networks (PINN) improve sea ice velocity and concentration predictions by integrating physical knowledge into machine learning models, outperforming purely data-driven approaches.", "motivation": "Traditional data-driven ML models for sea ice prediction have limitations in generalizability and physical consistency, especially as Arctic sea ice conditions change rapidly with thinner ice and accelerated melting.", "method": "Developed PINN strategies using Hierarchical Information-sharing U-net (HIS-Unet) architecture, incorporating physics loss function and activation function to ensure physically plausible outputs for sea ice velocity and concentration.", "result": "The PINN model outperformed fully data-driven models in daily predictions of sea ice velocity and concentration, even with small training datasets, with particular improvements in melting/early freezing seasons and near fast-moving ice regions.", "conclusion": "Physics-informed neural networks provide more robust and physically consistent sea ice predictions compared to purely data-driven approaches, especially under changing Arctic conditions."}}
{"id": "2510.17772", "categories": ["cs.LG", "stat.AP", "I.5.1"], "pdf": "https://arxiv.org/pdf/2510.17772", "abs": "https://arxiv.org/abs/2510.17772", "authors": ["Ryan A. Robinett", "Sophia A. Madejski", "Kyle Ruark", "Samantha J. Riesenfeld", "Lorenzo Orecchia"], "title": "Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning", "comment": null, "summary": "Despite the popularity of the manifold hypothesis, current manifold-learning\nmethods do not support machine learning directly on the latent $d$-dimensional\ndata manifold, as they primarily aim to perform dimensionality reduction into\n$\\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$\napproaches $d$.\n  On the other hand, methods that directly learn the latent manifold as a\ndifferentiable atlas have been relatively underexplored.\n  In this paper, we aim to give a proof of concept of the effectiveness and\npotential of atlas-based methods. To this end, we implement a generic data\nstructure to maintain a differentiable atlas that enables Riemannian\noptimization over the manifold. We complement this with an unsupervised\nheuristic that learns a differentiable atlas from point cloud data. We\nexperimentally demonstrate that this approach has advantages in terms of\nefficiency and accuracy in selected settings. Moreover, in a supervised\nclassification task over the Klein bottle and in RNA velocity analysis of\nhematopoietic data, we showcase the improved interpretability and robustness of\nour approach.", "AI": {"tldr": "The paper presents a proof-of-concept for atlas-based manifold learning methods that directly learn differentiable atlases from point cloud data, enabling Riemannian optimization on the latent manifold with improved efficiency, accuracy, interpretability and robustness.", "motivation": "Current manifold learning methods primarily focus on dimensionality reduction into Euclidean space, losing key manifold features when embedding dimension approaches the intrinsic dimension. Directly learning the latent manifold as a differentiable atlas has been relatively underexplored despite the popularity of the manifold hypothesis.", "method": "Implemented a generic data structure to maintain a differentiable atlas enabling Riemannian optimization over the manifold, complemented by an unsupervised heuristic that learns a differentiable atlas from point cloud data.", "result": "Experimental demonstration shows advantages in efficiency and accuracy in selected settings. In supervised classification over the Klein bottle and RNA velocity analysis of hematopoietic data, the approach shows improved interpretability and robustness.", "conclusion": "Atlas-based methods provide an effective alternative to traditional manifold learning approaches, offering direct optimization on the latent manifold with practical benefits in real-world applications."}}
{"id": "2510.17776", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17776", "abs": "https://arxiv.org/abs/2510.17776", "authors": ["Jackson Harmon", "Andreas Hochlehnert", "Matthias Bethge", "Ameya Prabhu"], "title": "Mapping Post-Training Forgetting in Language Models at Scale", "comment": "43 pages,15 figures", "summary": "Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems.", "AI": {"tldr": "The paper proposes a sample-wise framework to measure knowledge forgetting and backward transfer during post-training of language models, revealing that different post-training stages have varying effects on pretrained knowledge.", "motivation": "To understand how scaled post-training affects pretrained knowledge in language models, since traditional task averages obscure individual knowledge changes and not all forgetting is equal.", "method": "A sample-wise paradigm that counts 1->0 transitions (forgetting) and 0->1 transitions (backward transfer), with chance-adjusted variants for multiple-choice benchmarks to subtract random guessing effects.", "result": "Domain-continual pretraining causes moderate forgetting with low-to-moderate backward transfer; RL/SFT post-training yields moderate-to-large backward transfer on math/logic with low-to-moderate forgetting; effects vary with data scale and model merging doesn't reliably mitigate forgetting.", "conclusion": "The framework provides a practical way to map how post-training alters pretrained knowledge at scale, enabling progress towards generally capable AI systems."}}
{"id": "2510.17786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17786", "abs": "https://arxiv.org/abs/2510.17786", "authors": ["Adam Stecklov", "Noah El Rimawi-Fine", "Mathieu Blanchette"], "title": "Inference-Time Compute Scaling For Flow Matching", "comment": null, "summary": "Allocating extra computation at inference time has recently improved sample\nquality in large language models and diffusion-based image generation. In\nparallel, Flow Matching (FM) has gained traction in language, vision, and\nscientific domains, but inference-time scaling methods for it remain\nunder-explored. Concurrently, Kim et al., 2025 approach this problem but\nreplace the linear interpolant with a non-linear variance-preserving (VP)\ninterpolant at inference, sacrificing FM's efficient and straight sampling.\nAdditionally, inference-time compute scaling for flow matching has only been\napplied to visual tasks, like image generation. We introduce novel\ninference-time scaling procedures for FM that preserve the linear interpolant\nduring sampling. Evaluations of our method on image generation, and for the\nfirst time (to the best of our knowledge), unconditional protein generation,\nshow that I) sample quality consistently improves as inference compute\nincreases, and II) flow matching inference-time scaling can be applied to\nscientific domains.", "AI": {"tldr": "The paper introduces novel inference-time scaling procedures for Flow Matching that preserve the linear interpolant during sampling, improving sample quality in image generation and demonstrating for the first time its application to unconditional protein generation.", "motivation": "Inference-time compute scaling has improved sample quality in other models but remains under-explored for Flow Matching, with existing approaches sacrificing FM's efficient sampling by using non-linear interpolants.", "method": "The authors developed inference-time scaling procedures for Flow Matching that maintain the linear interpolant during sampling, preserving FM's efficient and straight sampling characteristics.", "result": "Evaluations show that sample quality consistently improves as inference compute increases, and the method successfully applies to both image generation and (for the first time) unconditional protein generation.", "conclusion": "Flow matching inference-time scaling can be effectively applied to scientific domains while preserving the method's computational efficiency, opening new possibilities for high-quality sample generation in various applications."}}
{"id": "2510.17802", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17802", "abs": "https://arxiv.org/abs/2510.17802", "authors": ["Rui Pan", "Yang Luo", "Yuxing Liu", "Yang You", "Tong Zhang"], "title": "Unbiased Gradient Low-Rank Projection", "comment": null, "summary": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.", "AI": {"tldr": "GaLore Unbiased with Muon (GUM) is a novel memory-efficient optimization method for large language models that combines GaLore's low-rank projection with Muon's layerwise sampling to eliminate bias and provide convergence guarantees while maintaining memory efficiency.", "motivation": "Existing gradient low-rank projection methods like GaLore lack convergence guarantees due to inherent biases introduced by low-rank projections, creating performance gaps compared to full-parameter training. The paper aims to address this limitation.", "method": "GUM combines GaLore's low-rank projection mechanism with the Muon algorithm's layerwise sampling technique to debias the low-rank projection process, creating an unbiased optimization method.", "result": "Theoretical analysis proves GUM matches Muon's convergence guarantees while preserving memory efficiency. Empirical experiments on LLM fine-tuning and pretraining show improvements over GaLore and even better performance than full-parameter training.", "conclusion": "GUM successfully addresses the bias problem in low-rank optimization methods, providing convergence guarantees and improved performance through more uniform knowledge distribution across layers and better parameter space utilization."}}
