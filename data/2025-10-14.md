<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis](https://arxiv.org/abs/2510.08739)
*Yikai Zhao,Jiekai Ma*

Main category: cs.LG

TL;DR: This paper introduces a dual-approach framework that combines surrogate-based explanations using LightGBM and SHAP with spectral predictability analysis to address interpretability and forecastability challenges in complex time series ensemble models.


<details>
  <summary>Details</summary>
Motivation: Modern time series forecasting relies on complex AutoML ensemble models that deliver superior accuracy but sacrifice transparency and interpretability, creating a need for methods that can explain these black-box models while assessing forecast reliability.

Method: Developed a surrogate-based explanation methodology using LightGBM to mimic AutoGluon's forecasts, enabling stable SHAP-based feature attributions. Integrated spectral predictability analysis to quantify each series' inherent forecastability by comparing against pure noise benchmarks.

Result: Empirical evaluation on M5 dataset showed high faithfulness between SHAP values and ground truth effects. Higher spectral predictability strongly correlated with improved forecast accuracy and higher surrogate fidelity. Per-item normalization proved essential for meaningful SHAP explanations across heterogeneous time series.

Conclusion: The framework provides interpretable, instance-level explanations for ensemble forecasts while offering forecastability metrics as reliability indicators for both predictions and their explanations, enabling users to calibrate trust in complex forecasting systems.

Abstract: Modern time series forecasting increasingly relies on complex ensemble models
generated by AutoML systems like AutoGluon, delivering superior accuracy but
with significant costs to transparency and interpretability. This paper
introduces a comprehensive, dual-approach framework that addresses both the
explainability and forecastability challenges in complex time series ensembles.
First, we develop a surrogate-based explanation methodology that bridges the
accuracy-interpretability gap by training a LightGBM model to faithfully mimic
AutoGluon's time series forecasts, enabling stable SHAP-based feature
attributions. We rigorously validated this approach through feature injection
experiments, demonstrating remarkably high faithfulness between extracted SHAP
values and known ground truth effects. Second, we integrated spectral
predictability analysis to quantify each series' inherent forecastability. By
comparing each time series' spectral predictability to its pure noise
benchmarks, we established an objective mechanism to gauge confidence in
forecasts and their explanations. Our empirical evaluation on the M5 dataset
found that higher spectral predictability strongly correlates not only with
improved forecast accuracy but also with higher fidelity between the surrogate
and the original forecasting model. These forecastability metrics serve as
effective filtering mechanisms and confidence scores, enabling users to
calibrate their trust in both the forecasts and their explanations. We further
demonstrated that per-item normalization is essential for generating meaningful
SHAP explanations across heterogeneous time series with varying scales. The
resulting framework delivers interpretable, instance-level explanations for
state-of-the-art ensemble forecasts, while equipping users with forecastability
metrics that serve as reliability indicators for both predictions and their
explanations.

</details>
