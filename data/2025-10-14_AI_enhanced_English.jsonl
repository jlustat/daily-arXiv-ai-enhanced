{"id": "2510.08739", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08739", "abs": "https://arxiv.org/abs/2510.08739", "authors": ["Yikai Zhao", "Jiekai Ma"], "title": "Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis", "comment": null, "summary": "Modern time series forecasting increasingly relies on complex ensemble models\ngenerated by AutoML systems like AutoGluon, delivering superior accuracy but\nwith significant costs to transparency and interpretability. This paper\nintroduces a comprehensive, dual-approach framework that addresses both the\nexplainability and forecastability challenges in complex time series ensembles.\nFirst, we develop a surrogate-based explanation methodology that bridges the\naccuracy-interpretability gap by training a LightGBM model to faithfully mimic\nAutoGluon's time series forecasts, enabling stable SHAP-based feature\nattributions. We rigorously validated this approach through feature injection\nexperiments, demonstrating remarkably high faithfulness between extracted SHAP\nvalues and known ground truth effects. Second, we integrated spectral\npredictability analysis to quantify each series' inherent forecastability. By\ncomparing each time series' spectral predictability to its pure noise\nbenchmarks, we established an objective mechanism to gauge confidence in\nforecasts and their explanations. Our empirical evaluation on the M5 dataset\nfound that higher spectral predictability strongly correlates not only with\nimproved forecast accuracy but also with higher fidelity between the surrogate\nand the original forecasting model. These forecastability metrics serve as\neffective filtering mechanisms and confidence scores, enabling users to\ncalibrate their trust in both the forecasts and their explanations. We further\ndemonstrated that per-item normalization is essential for generating meaningful\nSHAP explanations across heterogeneous time series with varying scales. The\nresulting framework delivers interpretable, instance-level explanations for\nstate-of-the-art ensemble forecasts, while equipping users with forecastability\nmetrics that serve as reliability indicators for both predictions and their\nexplanations.", "AI": {"tldr": "This paper introduces a dual-approach framework that combines surrogate-based explanations using LightGBM and SHAP with spectral predictability analysis to address interpretability and forecastability challenges in complex time series ensemble models.", "motivation": "Modern time series forecasting relies on complex AutoML ensemble models that deliver superior accuracy but sacrifice transparency and interpretability, creating a need for methods that can explain these black-box models while assessing forecast reliability.", "method": "Developed a surrogate-based explanation methodology using LightGBM to mimic AutoGluon's forecasts, enabling stable SHAP-based feature attributions. Integrated spectral predictability analysis to quantify each series' inherent forecastability by comparing against pure noise benchmarks.", "result": "Empirical evaluation on M5 dataset showed high faithfulness between SHAP values and ground truth effects. Higher spectral predictability strongly correlated with improved forecast accuracy and higher surrogate fidelity. Per-item normalization proved essential for meaningful SHAP explanations across heterogeneous time series.", "conclusion": "The framework provides interpretable, instance-level explanations for ensemble forecasts while offering forecastability metrics as reliability indicators for both predictions and their explanations, enabling users to calibrate trust in complex forecasting systems."}}
