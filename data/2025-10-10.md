<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 116]
- [stat.ME](#stat.ME) [Total: 9]
- [stat.ML](#stat.ML) [Total: 14]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: This research focuses on recognizing and mapping behavioral patterns in autistic children as a prerequisite for improving learning and soft skills, particularly in the IT domain where opportunities are limited.


<details>
  <summary>Details</summary>
Motivation: There is a critical gap in understanding nuanced behavioral patterns and emotional identification in autistic children prior to skill development, which is essential for effective early intervention and customized educational strategies.

Method: Using a longitudinal approach to monitor emotions and behaviors over time, establishing baseline understanding of unique needs and challenges faced by autistic students in the IT domain.

Result: The study proposes a targeted framework for developing applications and technical aids designed to meet identified needs based on detailed analysis of behavioral trends.

Conclusion: A sequential, evidence-based intervention approach that prioritizes deep understanding of each child's behavioral and emotional landscape is crucial for effective skill development and creating more inclusive learning environments for children with ASD.

Abstract: Autism Spectrum Disorder significantly influences the communication
abilities, learning processes, behavior, and social interactions of
individuals. Although early intervention and customized educational strategies
are critical to improving outcomes, there is a pivotal gap in understanding and
addressing nuanced behavioral patterns and emotional identification in autistic
children prior to skill development. This extended research delves into the
foundational step of recognizing and mapping these patterns as a prerequisite
to improving learning and soft skills. Using a longitudinal approach to monitor
emotions and behaviors, this study aims to establish a baseline understanding
of the unique needs and challenges faced by autistic students, particularly in
the Information Technology domain, where opportunities are markedly limited.
Through a detailed analysis of behavioral trends over time, we propose a
targeted framework for developing applications and technical aids designed to
meet these identified needs. Our research underscores the importance of a
sequential and evidence-based intervention approach that prioritizes a deep
understanding of each child's behavioral and emotional landscape as the basis
for effective skill development. By shifting the focus toward early
identification of behavioral patterns, we aim to foster a more inclusive and
supportive learning environment that can significantly improve the educational
and developmental trajectory of children with ASD.

</details>


### [2] [A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search](https://arxiv.org/abs/2510.07325)
*Sixuan Wang,Jiao Yin,Jinli Cao,Mingjian Tang,Yong-Feng Ge*

Main category: cs.LG

TL;DR: MACC-MGNAS is a modality-aware cooperative co-evolutionary algorithm for multimodal graph neural architecture search that addresses co-exploitation attacks on software vulnerabilities by effectively integrating heterogeneous multimodal data.


<details>
  <summary>Details</summary>
Motivation: Co-exploitation attacks on software vulnerabilities pose severe enterprise risks, and existing methods fail to handle modality heterogeneity in vulnerability data, making manual tuning of multimodal graph neural networks infeasible.

Method: Proposes MACC-MGNAS with three key components: modality-aware cooperative co-evolution framework for partitioning and evolving modality-specific genes, dual-track surrogate method to reduce evaluation costs, and similarity-based population diversity indicator for balancing exploration and exploitation.

Result: Achieves 81.67% F1-score on VulCE dataset within only 3 GPU-hours, outperforming state-of-the-art by 8.7% F1 while reducing computation cost by 27%.

Conclusion: MACC-MGNAS effectively addresses modality heterogeneity in vulnerability co-exploitation prediction, achieving superior performance with significantly reduced computational costs compared to existing methods.

Abstract: Co-exploitation attacks on software vulnerabilities pose severe risks to
enterprises, a threat that can be mitigated by analyzing heterogeneous and
multimodal vulnerability data. Multimodal graph neural networks (MGNNs) are
well-suited to integrate complementary signals across modalities, thereby
improving attack-prediction accuracy. However, designing an effective MGNN
architecture is challenging because it requires coordinating modality-specific
components at each layer, which is infeasible through manual tuning. Genetic
algorithm (GA)-based graph neural architecture search (GNAS) provides a natural
solution, yet existing methods are confined to single modalities and overlook
modality heterogeneity. To address this limitation, we propose a modality-aware
cooperative co-evolutionary algorithm for multimodal graph neural architecture
search, termed MACC-MGNAS. First, we develop a modality-aware cooperative
co-evolution (MACC) framework under a divide-and-conquer paradigm: a
coordinator partitions a global chromosome population into modality-specific
gene groups, local workers evolve them independently, and the coordinator
reassembles chromosomes for joint evaluation. This framework effectively
captures modality heterogeneity ignored by single-modality GNAS. Second, we
introduce a modality-aware dual-track surrogate (MADTS) method to reduce
evaluation cost and accelerate local gene evolution. Third, we design a
similarity-based population diversity indicator (SPDI) strategy to adaptively
balance exploration and exploitation, thereby accelerating convergence and
avoiding local optima. On a standard vulnerabilities co-exploitation (VulCE)
dataset, MACC-MGNAS achieves an F1-score of 81.67% within only 3 GPU-hours,
outperforming the state-of-the-art competitor by 8.7% F1 while reducing
computation cost by 27%.

</details>


### [3] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: MultiFair is a novel multimodal medical classification approach that addresses both modality imbalance and demographic fairness through dual-level gradient modulation during training.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal learning models fail to ensure reliable and unbiased diagnosis because they often ignore two critical challenges: uneven learning across data modalities causing model bias, and emphasis on certain demographic groups leading to unfair performances.

Method: MultiFair uses a dual-level gradient modulation process that dynamically modulates training gradients regarding optimization direction and magnitude at both data modality and group levels.

Result: Extensive experiments on two multimodal medical datasets with different demographic groups show that MultiFair outperforms state-of-the-art multimodal learning and fairness learning methods.

Conclusion: The proposed MultiFair approach successfully addresses both modality imbalance and demographic fairness challenges in multimodal medical classification through its dual-level gradient modulation technique.

Abstract: Medical decision systems increasingly rely on data from multiple sources to
ensure reliable and unbiased diagnosis. However, existing multimodal learning
models fail to achieve this goal because they often ignore two critical
challenges. First, various data modalities may learn unevenly, thereby
converging to a model biased towards certain modalities. Second, the model may
emphasize learning on certain demographic groups causing unfair performances.
The two aspects can influence each other, as different data modalities may
favor respective groups during optimization, leading to both imbalanced and
unfair multimodal learning. This paper proposes a novel approach called
MultiFair for multimodal medical classification, which addresses these
challenges with a dual-level gradient modulation process. MultiFair dynamically
modulates training gradients regarding the optimization direction and magnitude
at both data modality and group levels. We conduct extensive experiments on two
multimodal medical datasets with different demographic groups. The results show
that MultiFair outperforms state-of-the-art multimodal learning and fairness
learning methods.

</details>


### [4] [Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data](https://arxiv.org/abs/2510.07350)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: This paper benchmarks two deep learning models (GNN-RNN and MMST-ViT) for crop yield forecasting under realistic out-of-distribution conditions, finding that GNN-RNN demonstrates superior generalization across geographic regions while being 135x faster to train than MMST-ViT.


<details>
  <summary>Details</summary>
Motivation: Climate change is disrupting agricultural systems, making accurate crop yield forecasting essential for food security. While deep learning models show promise, their ability to generalize across geographic regions and years - critical for real-world deployment - remains largely untested.

Method: Benchmarked two state-of-the-art models (GNN-RNN and MMST-ViT) using the large-scale CropNet dataset spanning 1,200+ U.S. counties from 2017-2022. Used leave-one-cluster-out cross-validation across seven USDA Farm Resource Regions and year-ahead prediction scenarios to test out-of-distribution generalization.

Result: GNN-RNN demonstrated superior generalization with positive correlations under geographic shifts, while MMST-ViT performed well in-domain but degraded sharply under OOD conditions. GNN-RNN achieved 135x faster training (14 minutes vs. 31.5 hours). Regions like Heartland and Northern Great Plains showed stable transfer dynamics, while Prairie Gateway exhibited persistent underperformance.

Conclusion: Spatial-temporal alignment - not merely model complexity or data scale - is key to robust generalization. The findings highlight the need for transparent OOD evaluation protocols to ensure equitable and reliable climate-aware agricultural forecasting.

Abstract: Climate change is increasingly disrupting agricultural systems, making
accurate crop yield forecasting essential for food security. While deep
learning models have shown promise in yield prediction using satellite and
weather data, their ability to generalize across geographic regions and years -
critical for real-world deployment - remains largely untested. We benchmark two
state-of-the-art models, GNN-RNN and MMST-ViT, under realistic
out-of-distribution (OOD) conditions using the large-scale CropNet dataset
spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out
cross-validation across seven USDA Farm Resource Regions and year-ahead
prediction scenarios, we identify substantial variability in cross-region
transferability. GNN-RNN demonstrates superior generalization with positive
correlations under geographic shifts, while MMST-ViT performs well in-domain
but degrades sharply under OOD conditions. Regions like Heartland and Northern
Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for
soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE
greater than 20 bu/acre) across both models and crops, revealing structural
dissimilarities likely driven by semi-arid climate, irrigation patterns, and
incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves
135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more
viable for sustainable deployment. Our findings underscore that
spatial-temporal alignment - not merely model complexity or data scale - is key
to robust generalization, and highlight the need for transparent OOD evaluation
protocols to ensure equitable and reliable climate-aware agricultural
forecasting.

</details>


### [5] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: The paper addresses GPU kernel generation data scarcity by creating ConCuR dataset with reasoning traces, introducing KernelCoder model that outperforms existing models including QwQ-32B and frontier models.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality GPU kernel data prevents effective supervised fine-tuning for kernel generation tasks, as most high-quality kernels are proprietary.

Method: Developed a pipeline to generate and curate high-quality CUDA kernels with reasoning traces, constructing ConCuR dataset and training KernelCoder model on PyTorch-reasoning-CUDA kernel pairs.

Result: KernelCoder achieves significant improvements over existing top-performing models (QwQ-32B), outperforms all open-source kernel generation models and frontier models like DeepSeek-V3.1-Think and Claude-4-sonnet in KernelBench setup.

Conclusion: Concise yet informative reasoning traces enable robust high-performance kernel generation, and average reasoning length can serve as a metric for task difficulty assessment, with the pipeline helping future data collection.

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [6] [Best-of-Both Worlds for linear contextual bandits with paid observations](https://arxiv.org/abs/2510.07424)
*Nathan Boyer,Dorian Baudry,Patrick Rebeschini*

Main category: cs.LG

TL;DR: A Best-of-Both-Worlds algorithm for linear contextual bandits with paid observations that achieves minimax-optimal regret in adversarial settings and poly-logarithmic regret in stochastic regimes.


<details>
  <summary>Details</summary>
Motivation: To address the problem of linear contextual bandits where observing arm losses requires paying a fixed cost, aiming to develop an algorithm that performs well in both adversarial and stochastic environments.

Method: Uses Follow-the-Regularized-Leader framework with efficient estimators via Matrix Geometric Resampling, building on the BOBW framework for hard problems.

Result: Achieves minimax-optimal regret of Θ(T^{2/3}) in adversarial settings and poly-logarithmic regret in (corrupted) stochastic regimes.

Conclusion: The proposed algorithm provides computationally efficient performance guarantees across different environmental settings while handling the cost of observations.

Abstract: We study the problem of linear contextual bandits with paid observations,
where at each round the learner selects an action in order to minimize its loss
in a given context, and can then decide to pay a fixed cost to observe the loss
of any arm. Building on the Follow-the-Regularized-Leader framework with
efficient estimators via Matrix Geometric Resampling, we introduce a
computationally efficient Best-of-Both-Worlds (BOBW) algorithm for this
problem. We show that it achieves the minimax-optimal regret of
$\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic
regret in (corrupted) stochastic regimes. Our approach builds on the framework
from \cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',
using analysis techniques tailored for the setting that we consider.

</details>


### [7] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: BaRP is a bandit-feedback routing system that adaptively selects LLMs for each query using contextual bandits, enabling real-time performance/cost trade-offs without retraining.


<details>
  <summary>Details</summary>
Motivation: Current LLM routing systems either overpay for strong models or risk poor performance from weaker ones, and most routers are trained offline with full information that doesn't match deployment conditions.

Method: Framed as a contextual bandit over prompt features and user preference vectors, BaRP simulates online feedback during training and adapts routing decisions to each new prompt without full-information supervision.

Result: BaRP consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, while generalizing robustly to unseen tasks.

Conclusion: BaRP bridges the gap between offline training and online deployment by training under partial-feedback restrictions, enabling preference-tunable inference for optimal performance/cost trade-offs.

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [8] [Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments](https://arxiv.org/abs/2510.07436)
*Ankur Naskar,Gugan Thoppe,Utsav Negi,Vijay Gupta*

Main category: cs.LG

TL;DR: Proposes a parameter-free Federated Temporal Difference (FTD) learning method with Polyak-Ruppert averaging that achieves optimal O(1/NT) convergence rate for Markovian data in both average-reward and discounted settings, addressing limitations of existing methods that require unknown problem parameters.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning approaches for reinforcement learning with Markovian data require knowledge of unknown problem parameters to achieve optimal convergence rates, creating a practical limitation for real-world applications.

Method: Developed a two-timescale Federated Temporal Difference (FTD) learning algorithm with Polyak-Ruppert averaging that operates without requiring prior knowledge of problem parameters.

Result: The proposed method provably achieves the optimal O(1/NT) convergence rate for both average-reward and discounted reinforcement learning settings with Markovian data.

Conclusion: This work provides a parameter-free FTD approach that maintains optimal convergence rates even in challenging federated learning scenarios with heterogeneous environments, representing a significant advancement over existing methods.

Abstract: Federated learning (FL) can dramatically speed up reinforcement learning by
distributing exploration and training across multiple agents. It can guarantee
an optimal convergence rate that scales linearly in the number of agents, i.e.,
a rate of $\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the
number of agents. However, when the training samples arise from a Markov chain,
existing results on TD learning achieving this rate require the algorithm to
depend on unknown problem parameters. We close this gap by proposing a
two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert
averaging. Our method provably attains the optimal $\tilde{O}(1/NT)$ rate in
both average-reward and discounted settings--offering a parameter-free FTD
approach for Markovian data. Although our results are novel even in the
single-agent setting, they apply to the more realistic and challenging scenario
of FL with heterogeneous environments.

</details>


### [9] [MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting](https://arxiv.org/abs/2510.07459)
*Yoli Shavit,Jacob Goldberger*

Main category: cs.LG

TL;DR: MoGU is a Mixture-of-Experts framework for regression tasks that models expert outputs as Gaussian distributions to quantify both forecasts and uncertainty, using an uncertainty-based gating mechanism instead of traditional input-based gating.


<details>
  <summary>Details</summary>
Motivation: Traditional Mixture-of-Experts frameworks only provide point estimates without uncertainty quantification, which limits their reliability in time series forecasting applications where understanding prediction confidence is crucial.

Method: MoGU models each expert's output as a Gaussian distribution and uses an uncertainty-based gating mechanism that determines expert contributions based on their estimated variance rather than input-based routing.

Result: MoGU consistently outperforms single-expert models and traditional MoE setups across diverse time series forecasting benchmarks, while providing well-quantified uncertainties that correlate with prediction errors.

Conclusion: MoGU enhances forecast reliability by providing both accurate predictions and informative uncertainty estimates through its novel Gaussian modeling and uncertainty-based gating approach.

Abstract: We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf

</details>


### [10] [metabeta - A fast neural model for Bayesian mixed-effects regression](https://arxiv.org/abs/2510.07473)
*Alex Kipnis,Marcel Binz,Eric Schulz*

Main category: cs.LG

TL;DR: Metabeta is a transformer-based neural network model for Bayesian mixed-effects regression that uses neural posterior estimation to amortize computation costs, achieving comparable performance to MCMC methods much faster.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference for mixed-effects regression is analytically intractable and computationally expensive using MCMC methods, requiring costly approximation for hierarchical data common in empirical sciences.

Method: Proposed metabeta, a transformer-based neural network that uses neural posterior estimation to shift computation from inference to pre-training time, amortizing costs over simulated datasets with known ground truth.

Result: The model achieves stable and comparable performance to MCMC-based parameter estimation on both simulated and real data, but at a fraction of the time typically required.

Conclusion: Metabeta provides an efficient alternative to MCMC for Bayesian mixed-effects regression, offering substantial computational savings while maintaining comparable accuracy.

Abstract: Hierarchical data with multiple observations per group is ubiquitous in
empirical sciences and is often analyzed using mixed-effects regression. In
such models, Bayesian inference gives an estimate of uncertainty but is
analytically intractable and requires costly approximation using Markov Chain
Monte Carlo (MCMC) methods. Neural posterior estimation shifts the bulk of
computation from inference time to pre-training time, amortizing over simulated
datasets with known ground truth targets. We propose metabeta, a
transformer-based neural network model for Bayesian mixed-effects regression.
Using simulated and real data, we show that it reaches stable and comparable
performance to MCMC-based parameter estimation at a fraction of the usually
required time.

</details>


### [11] [Surrogate Modeling for the Design of Optimal Lattice Structures using Tensor Completion](https://arxiv.org/abs/2510.07474)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: Tensor completion outperforms traditional ML methods for materials design with biased training data, achieving 5% higher R² while maintaining comparable performance with uniform sampling.


<details>
  <summary>Details</summary>
Motivation: Materials design faces exponential search space growth, and traditional ML methods struggle when training data comes from non-uniform sampling (e.g., experimental convenience bias).

Method: Using tensor completion as a surrogate model for materials design, particularly for lattice structures with mechanical performance optimization.

Result: Tensor completion shows 5% increased R² compared to Gaussian Process and XGBoost with biased sampling, and comparable performance with uniform random sampling.

Conclusion: Tensor completion is an effective approach for accelerating materials design in scenarios with biased or non-uniform training data sampling.

Abstract: When designing new materials, it is often necessary to design a material with
specific desired properties. Unfortunately, as new design variables are added,
the search space grows exponentially, which makes synthesizing and validating
the properties of each material very impractical and time-consuming. In this
work, we focus on the design of optimal lattice structures with regard to
mechanical performance. Computational approaches, including the use of machine
learning (ML) methods, have shown improved success in accelerating materials
design. However, these ML methods are still lacking in scenarios when training
data (i.e. experimentally validated materials) come from a non-uniformly random
sampling across the design space. For example, an experimentalist might
synthesize and validate certain materials more frequently because of
convenience. For this reason, we suggest the use of tensor completion as a
surrogate model to accelerate the design of materials in these atypical
supervised learning scenarios. In our experiments, we show that tensor
completion is superior to classic ML methods such as Gaussian Process and
XGBoost with biased sampling of the search space, with around 5\% increased
$R^2$. Furthermore, tensor completion still gives comparable performance with a
uniformly random sampling of the entire search space.

</details>


### [12] [HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data](https://arxiv.org/abs/2510.07477)
*Maria Mahbub,Robert J. Klein,Myvizhi Esai Selvan,Rowena Yip,Claudia Henschke,Providencia Morales,Ian Goethert,Olivera Kotevska,Mayanka Chandra Shekar,Sean R. Wilkinson,Eileen McAllister,Samuel M. Aguayo,Zeynep H. Gümüş,Ioana Danciu,VA Million Veteran Program*

Main category: cs.LG

TL;DR: HEMERA is an explainable transformer-based deep learning framework that uses GWAS data to predict lung cancer risk with over 99% AUC, enabling transparent risk assessment without clinical covariates.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is a leading cause of cancer deaths with genetic components beyond smoking. Current GWAS approaches need more transparent and accurate risk prediction methods that can identify specific genetic biomarkers.

Method: HEMERA applies transformer-based deep learning directly to raw GWAS genotype data, using additive positional encodings, neural genotype embeddings, and refined variant filtering. It includes a post hoc explainability module based on Layer-wise Integrated Gradients.

Result: Trained on 27,254 Million Veteran Program participants, HEMERA achieved >99% AUC score and successfully attributed predictions to specific SNPs that aligned strongly with known lung cancer risk loci.

Conclusion: HEMERA provides a transparent, hypothesis-generating framework for personalized lung cancer risk assessment and early intervention using explainable deep learning on genetic data.

Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.

</details>


### [13] [Reinforcement Learning-based Task Offloading in the Internet of Wearable Things](https://arxiv.org/abs/2510.07487)
*Waleed Bin Qaim,Aleksandr Ometov,Claudia Campolo,Antonella Molinaro,Elena Simona Lohan,Jari Nurmi*

Main category: cs.LG

TL;DR: Proposes a Reinforcement Learning-based task offloading framework for Internet of Wearable Things that optimizes the tradeoff between energy consumption and task completion time using Q-learning.


<details>
  <summary>Details</summary>
Motivation: Wearable devices face limitations in battery power and computation resources, while new applications are becoming more computationally intensive and latency-critical. Task offloading can leverage nearby edge devices to enhance user experience.

Method: Formulates task offloading as a Markov Decision Process and uses Q-learning technique to enable wearable devices to make optimal offloading decisions without prior knowledge. Evaluated through extensive ns-3 network simulator simulations.

Result: The framework was evaluated for various applications and system configurations, showing how varying Q-learning parameters affects performance metrics including average task accomplishment time, energy consumption, and percentage of tasks offloaded.

Conclusion: The proposed RL-based framework effectively enables wearable devices to make intelligent task offloading decisions that balance energy efficiency and performance requirements in IoWT environments.

Abstract: Over the years, significant contributions have been made by the research and
industrial sectors to improve wearable devices towards the Internet of Wearable
Things (IoWT) paradigm. However, wearables are still facing several challenges.
Many stem from the limited battery power and insufficient computation resources
available on wearable devices. On the other hand, with the popularity of smart
wearables, there is a consistent increase in the development of new
computationally intensive and latency-critical applications. In such a context,
task offloading allows wearables to leverage the resources available on nearby
edge devices to enhance the overall user experience. This paper proposes a
framework for Reinforcement Learning (RL)-based task offloading in the IoWT. We
formulate the task offloading process considering the tradeoff between energy
consumption and task accomplishment time. Moreover, we model the task
offloading problem as a Markov Decision Process (MDP) and utilize the
Q-learning technique to enable the wearable device to make optimal task
offloading decisions without prior knowledge. We evaluate the performance of
the proposed framework through extensive simulations for various applications
and system configurations conducted in the ns-3 network simulator. We also show
how varying the main system parameters of the Q-learning algorithm affects the
overall performance in terms of average task accomplishment time, average
energy consumption, and percentage of tasks offloaded.

</details>


### [14] [DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning](https://arxiv.org/abs/2411.15876)
*Md. Saiful Bari Siddiqui,Md Mohaiminul Islam,Md. Golam Rabiul Alam*

Main category: cs.LG

TL;DR: DUA-D2C improves upon the Divide2Conquer method by dynamically weighting subset models based on validation performance and uncertainty, enhancing generalization and combating overfitting more effectively.


<details>
  <summary>Details</summary>
Motivation: To address limitations in the original D2C method where aggregation treated all subset models equally, potentially underutilizing their varying generalization capabilities.

Method: Dynamic uncertainty-aware aggregation that weights contributions of subset models based on their performance on a shared validation set, considering both accuracy and prediction uncertainty.

Result: Significant improvement in generalization performance across multiple benchmark datasets, with enhanced decision boundaries and loss curves, even when applied on top of other regularization methods.

Conclusion: DUA-D2C provides a theoretically grounded and effective approach to combat overfitting in deep learning by intelligently aggregating subset models based on their generalization capabilities.

Abstract: Overfitting remains a significant challenge in deep learning, often arising
from data outliers, noise, and limited training data. To address this, the
Divide2Conquer (D2C) method was previously proposed, which partitions training
data into multiple subsets and trains identical models independently on each.
This strategy enables learning more consistent patterns while minimizing the
influence of individual outliers and noise. However, D2C's standard aggregation
typically treats all subset models equally or based on fixed heuristics (like
data size), potentially underutilizing information about their varying
generalization capabilities. Building upon this foundation, we introduce
Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that
refines the aggregation process. DUA-D2C dynamically weights the contributions
of subset models based on their performance on a shared validation set,
considering both accuracy and prediction uncertainty. This intelligent
aggregation allows the central model to preferentially learn from subsets
yielding more generalizable and confident edge models, thereby more effectively
combating overfitting. Empirical evaluations on benchmark datasets spanning
multiple domains demonstrate that DUA-D2C significantly improves
generalization. Our analysis includes evaluations of decision boundaries, loss
curves, and other performance metrics, highlighting the effectiveness of
DUA-D2C. This study demonstrates that DUA-D2C improves generalization
performance even when applied on top of other regularization methods,
establishing it as a theoretically grounded and effective approach to combating
overfitting in modern deep learning. Our codes are publicly available at:
https://github.com/Saiful185/DUA-D2C.

</details>


### [15] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: PEAR benchmark evaluates planner-executor multi-agent systems, revealing that weak planners degrade performance more than weak executors, planner memory is crucial, there's a performance-robustness trade-off, and planner-targeted attacks are most effective.


<details>
  <summary>Details</summary>
Motivation: Existing studies on LLM-based Multi-Agent Systems examine isolated attack surfaces or specific scenarios, lacking holistic understanding of MAS vulnerabilities, creating a need for systematic evaluation.

Method: Introduces PEAR benchmark for systematically evaluating utility and vulnerability of planner-executor MAS, focusing on this practical and widely adopted design through extensive experiments.

Result: Findings show: (1) weak planner degrades clean task performance more than weak executor; (2) planner memory is essential but executor memory doesn't impact performance; (3) trade-off between task performance and robustness; (4) planner-targeted attacks are particularly effective.

Conclusion: The findings offer actionable insights for enhancing MAS robustness and lay groundwork for principled defenses in multi-agent settings.

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [16] [Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift](https://arxiv.org/abs/2510.07509)
*Tianyu Bell Pan,Damon L. Woodard*

Main category: cs.LG

TL;DR: A multimodal co-training framework that improves model generalization with limited labeled data and distribution shifts, using unlabeled multimodal data and inter-view agreement between classifiers.


<details>
  <summary>Details</summary>
Motivation: To address challenges of limited labeled data and distribution shifts in real-world environments by developing data-efficient and robust AI systems through multimodal co-training.

Method: Multimodal co-training framework that leverages unlabeled data, promotes agreement between classifiers for different modalities, and maintains conditional view independence through iterative training.

Result: Established theoretical conditions for improved generalization, convergence analysis confirming error reduction, and novel generalization bound quantifying benefits from unlabeled data, inter-view agreement, and view independence.

Conclusion: Multimodal co-training provides a structured approach for developing data-efficient and robust AI systems that generalize effectively in dynamic real-world environments.

Abstract: This paper explores a multimodal co-training framework designed to enhance
model generalization in situations where labeled data is limited and
distribution shifts occur. We thoroughly examine the theoretical foundations of
this framework, deriving conditions under which the use of unlabeled data and
the promotion of agreement between classifiers for different modalities lead to
significant improvements in generalization. We also present a convergence
analysis that confirms the effectiveness of iterative co-training in reducing
classification errors. In addition, we establish a novel generalization bound
that, for the first time in a multimodal co-training context, decomposes and
quantifies the distinct advantages gained from leveraging unlabeled multimodal
data, promoting inter-view agreement, and maintaining conditional view
independence. Our findings highlight the practical benefits of multimodal
co-training as a structured approach to developing data-efficient and robust AI
systems that can effectively generalize in dynamic, real-world environments.
The theoretical foundations are examined in dialogue with, and in advance of,
established co-training principles.

</details>


### [17] [Some theoretical improvements on the tightness of PAC-Bayes risk certificates for neural networks](https://arxiv.org/abs/2510.07935)
*Diego García-Pérez,Emilio Parrado-Hernández,John Shawe-Taylor*

Main category: cs.LG

TL;DR: This paper improves PAC-Bayes risk certificates for neural networks with four theoretical contributions: tighter KL divergence bounds for Bernoulli distributions, an implicit differentiation method for optimizing risk certificates within loss functions, and optimization methods for non-differentiable objectives like 0-1 loss. The work achieves the first non-vacuous generalization bounds on CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: To improve the practical usability of risk certificates for neural networks by developing tighter bounds and more efficient optimization methods that can be integrated directly into model training procedures.

Method: Developed four theoretical contributions: 1) Two bounds on KL divergence between Bernoulli distributions for tighter risk bounds, 2) Implicit differentiation methodology to optimize PAC-Bayesian risk certificates within loss functions, 3) Methods to optimize bounds on non-differentiable objectives like 0-1 loss, with empirical validation on MNIST and CIFAR-10 datasets.

Result: Achieved the first non-vacuous generalization bounds on CIFAR-10 for neural networks, demonstrating practical improvements in risk certificate optimization and tighter bounds across different empirical risk ranges.

Conclusion: The theoretical contributions significantly enhance the usability of PAC-Bayes risk certificates for neural networks, enabling tighter bounds and more efficient optimization that can be directly integrated into model training, with empirical validation showing practical improvements on standard benchmarks.

Abstract: This paper presents four theoretical contributions that improve the usability
of risk certificates for neural networks based on PAC-Bayes bounds. First, two
bounds on the KL divergence between Bernoulli distributions enable the
derivation of the tightest explicit bounds on the true risk of classifiers
across different ranges of empirical risk. The paper next focuses on the
formalization of an efficient methodology based on implicit differentiation
that enables the introduction of the optimization of PAC-Bayesian risk
certificates inside the loss/objective function used to fit the network/model.
The last contribution is a method to optimize bounds on non-differentiable
objectives such as the 0-1 loss. These theoretical contributions are
complemented with an empirical evaluation on the MNIST and CIFAR-10 datasets.
In fact, this paper presents the first non-vacuous generalization bounds on
CIFAR-10 for neural networks.

</details>


### [18] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: MLLM4TS is a novel framework that bridges the modality gap between time-series data and natural language by using multimodal large language models with visual representations of time series data.


<details>
  <summary>Details</summary>
Motivation: Time series analysis faces challenges with complex temporal dependencies and cross-channel interactions. The research explores whether visual representations can enhance automated time-series analysis, inspired by how human analysts visually inspect time series to uncover patterns.

Method: The framework integrates a dedicated vision branch where each time-series channel is rendered as horizontally stacked color-coded line plots in a composite image. It uses temporal-aware visual patch alignment to align visual patches with corresponding time segments, fusing fine-grained temporal details from numerical data with global contextual information from visual representations.

Result: Extensive experiments on standard benchmarks demonstrate MLLM4TS's effectiveness across both predictive tasks (classification) and generative tasks (anomaly detection and forecasting).

Conclusion: The results highlight the potential of integrating visual modalities with pretrained language models to achieve robust and generalizable time-series analysis, successfully bridging the modality gap between continuous numerical data and discrete natural language.

Abstract: Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.

</details>


### [19] [Counterfactual Identifiability via Dynamic Optimal Transport](https://arxiv.org/abs/2510.08294)
*Fabio De Sousa Ribeiro,Ainkaran Santhirasekaram,Ben Glocker*

Main category: cs.LG

TL;DR: This paper establishes a foundation for counterfactual identification in high-dimensional multivariate outcomes using continuous-time flows, addressing the gap in existing methods that lack proper identification for causal validity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of counterfactual identification in recent causal inference work, which undermines the validity of causal claims according to Pearl's framework that requires counterfactuals to be identifiable from observed data.

Method: Uses continuous-time flows and flow matching techniques, with tools from dynamic optimal transport, to create unique, monotone and rank-preserving counterfactual transport maps, including non-Markovian settings under standard criteria.

Result: The method yields consistent counterfactual inference and demonstrates improvements in axiomatic counterfactual soundness on real images, validated in controlled scenarios with ground-truth counterfactuals.

Conclusion: The approach provides a rigorous foundation for counterfactual identification in high-dimensional settings, ensuring causal validity through proper identification conditions and demonstrating practical improvements in real-world applications.

Abstract: We address the open question of counterfactual identification for
high-dimensional multivariate outcomes from observational data. Pearl (2000)
argues that counterfactuals must be identifiable (i.e., recoverable from the
observed data distribution) to justify causal claims. A recent line of work on
counterfactual inference shows promising results but lacks identification,
undermining the causal validity of its estimates. To address this, we establish
a foundation for multivariate counterfactual identification using
continuous-time flows, including non-Markovian settings under standard
criteria. We characterise the conditions under which flow matching yields a
unique, monotone and rank-preserving counterfactual transport map with tools
from dynamic optimal transport, ensuring consistent inference. Building on
this, we validate the theory in controlled scenarios with counterfactual
ground-truth and demonstrate improvements in axiomatic counterfactual soundness
on real images.

</details>


### [20] [EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning](https://arxiv.org/abs/2510.07524)
*Mehdi Zekriyapanah Gashti,Ghasem Farjamnia*

Main category: cs.LG

TL;DR: A novel automated sleep stage scoring framework using wavelet transform time-frequency analysis achieves 88.37% accuracy and 73.15 F1 score, outperforming conventional methods and matching deep learning approaches.


<details>
  <summary>Details</summary>
Motivation: Accurate sleep stage classification is crucial for diagnosing sleep disorders, but conventional methods rely on manual annotation or basic EEG features that may miss important transient patterns.

Method: Uses continuous wavelet transform (CWT) to generate time-frequency maps capturing transient and oscillatory patterns across relevant frequency bands, combined with ensemble learning for classification on the Sleep-EDF Expanded Database.

Result: Achieved 88.37% overall accuracy and 73.15 macro-averaged F1 score, outperforming conventional machine learning methods and showing comparable or superior performance to recent deep learning approaches.

Conclusion: Wavelet analysis provides a robust, interpretable, and clinically applicable approach for sleep stage classification, highlighting its potential for practical clinical use.

Abstract: Accurate classification of sleep stages is crucial for the diagnosis and
management of sleep disorders. Conventional approaches for sleep scoring rely
on manual annotation or features extracted from EEG signals in the time or
frequency domain. This study proposes a novel framework for automated sleep
stage scoring using time-frequency analysis based on the wavelet transform. The
Sleep-EDF Expanded Database (sleep-cassette recordings) was used for
evaluation. The continuous wavelet transform (CWT) generated time-frequency
maps that capture both transient and oscillatory patterns across frequency
bands relevant to sleep staging. Experimental results demonstrate that the
proposed wavelet-based representation, combined with ensemble learning,
achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of
73.15, outperforming conventional machine learning methods and exhibiting
comparable or superior performance to recent deep learning approaches. These
findings highlight the potential of wavelet analysis for robust, interpretable,
and clinically applicable sleep stage classification.

</details>


### [21] [Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions](https://arxiv.org/abs/2510.08382)
*Jacob Trauger,Tyson Trauger,Ambuj Tewari*

Main category: cs.LG

TL;DR: This paper characterizes learnability of forgiving 0-1 loss functions in multiclass classification using a new Generalized Natarajan Dimension, showing equivalence between finite dimension and learnability, with connections to set-valued feedback learning.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical foundations for learnability of forgiving 0-1 loss functions in finite label multiclass settings, extending beyond standard Natarajan Dimension analysis.

Method: Developed a new combinatorial dimension based on Natarajan Dimension, proved equivalence between finite dimension and learnability, and established connections to set-valued feedback learning.

Result: Showed that a hypothesis class is learnable with forgiving 0-1 loss if and only if the Generalized Natarajan Dimension is finite, and demonstrated that learnability of set learning problems is characterized by Natarajan Dimension.

Conclusion: The Generalized Natarajan Dimension provides a complete characterization of learnability for forgiving 0-1 loss functions in multiclass classification, unifying understanding of set learning problems through Natarajan Dimension.

Abstract: In this paper we will give a characterization of the learnability of
forgiving 0-1 loss functions in the finite label multiclass setting. To do
this, we create a new combinatorial dimension that is based off of the
Natarajan Dimension \citep{natarajan1989learning} and we show that a hypothesis
class is learnable in our setting if and only if this Generalized Natarajan
Dimension is finite. We also show a connection to learning with set-valued
feedback. Through our results we show that the learnability of a set learning
problem is characterized by the Natarajan Dimension.

</details>


### [22] [Estimating Fair Graphs from Graph-Stationary Data](https://arxiv.org/abs/2510.07536)
*Madeline Navarro,Andrei Buciulea,Samuel Rey,Antonio G. Marques,Santiago Segarra*

Main category: cs.LG

TL;DR: The paper proposes FairSpecTemp, an optimization-based method to estimate fair graphs from stationary graph signals, addressing group and individual fairness by constraining bias in graph connections and spectral properties.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs often have biased connections that favor certain groups, which can induce unfair treatment in downstream graph-based tasks. The goal is to estimate fair graphs that are not biased with respect to sensitive attributes.

Method: FairSpecTemp has two variants: one exploits commutativity properties of graph stationarity while directly constraining bias, and the other restricts bias in the graph spectrum to implicitly encourage fair estimates. Both use optimization-based approaches with performance bounds.

Result: The methods achieve a conditional tradeoff between fairness and accuracy, with analysis showing that accuracy need not be sacrificed to recover fair graphs. Evaluation on synthetic and real-world datasets demonstrates effectiveness.

Conclusion: FairSpecTemp provides effective approaches for estimating fair graphs from stationary observations, with both variants offering advantages in different scenarios while maintaining performance guarantees.

Abstract: We estimate fair graphs from graph-stationary nodal observations such that
connections are not biased with respect to sensitive attributes. Edges in
real-world graphs often exhibit preferences for connecting certain pairs of
groups. Biased connections can not only exacerbate but even induce unfair
treatment for downstream graph-based tasks. We therefore consider group and
individual fairness for graphs corresponding to group- and node-level
definitions, respectively. To evaluate the fairness of a given graph, we
provide multiple bias metrics, including novel measurements in the spectral
domain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an
optimization-based method with two variants for estimating fair graphs from
stationary graph signals, a general model for graph data subsuming many
existing ones. One variant of FairSpecTemp exploits commutativity properties of
graph stationarity while directly constraining bias, while the other implicitly
encourages fair estimates by restricting bias in the graph spectrum and is thus
more flexible. Our methods enjoy high probability performance bounds, yielding
a conditional tradeoff between fairness and accuracy. In particular, our
analysis reveals that accuracy need not be sacrificed to recover fair graphs.
We evaluate FairSpecTemp on synthetic and real-world data sets to illustrate
its effectiveness and highlight the advantages of both variants of
FairSpecTemp.

</details>


### [23] [ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing](https://arxiv.org/abs/2510.08429)
*Stella C. Dong,James R. Finlay*

Main category: cs.LG

TL;DR: ClauseLens is a reinforcement learning framework that produces transparent, regulation-compliant reinsurance treaty quotes by grounding decisions in legal clauses and generating natural language justifications.


<details>
  <summary>Details</summary>
Motivation: Current reinsurance treaty pricing practices are opaque and difficult to audit, failing to meet stringent regulatory standards. There is a need for transparent, regulation-compliant quoting systems.

Method: Models quoting as a Risk-Aware Constrained Markov Decision Process (RA-CMDP). Retrieves statutory and policy clauses from legal corpora, embeds them into agent observations, and uses them to constrain actions and generate clause-grounded explanations.

Result: Reduces solvency violations by 51%, improves tail-risk performance by 27.9% (CVaR_0.10), achieves 88.2% accuracy in clause-grounded explanations with 87.4% retrieval precision and 91.1% recall.

Conclusion: Embedding legal context into both decision and explanation pathways enables interpretable, auditable, and regulation-aligned quoting behavior compliant with major regulatory frameworks.

Abstract: Reinsurance treaty pricing must satisfy stringent regulatory standards, yet
current quoting practices remain opaque and difficult to audit. We introduce
ClauseLens, a clause-grounded reinforcement learning framework that produces
transparent, regulation-compliant, and risk-aware treaty quotes.
  ClauseLens models the quoting task as a Risk-Aware Constrained Markov
Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from
legal and underwriting corpora, embedded into the agent's observations, and
used both to constrain feasible actions and to generate clause-grounded natural
language justifications.
  Evaluated in a multi-agent treaty simulator calibrated to industry data,
ClauseLens reduces solvency violations by 51%, improves tail-risk performance
by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded
explanations with retrieval precision of 87.4% and recall of 91.1%.
  These findings demonstrate that embedding legal context into both decision
and explanation pathways yields interpretable, auditable, and
regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and
the EU AI Act.

</details>


### [24] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: A framework for creating targeted digital twins that directly model quantities of interest using memory-based flow map learning from short trajectory data, enabling efficient long-term predictions without full system simulations.


<details>
  <summary>Details</summary>
Motivation: To develop computationally efficient digital twins that can predict long-term dynamics of specific quantities of interest without running expensive full system simulations.

Method: Uses memory-based flow map learning with short bursts of trajectory data from repeated full digital twin executions to create data-driven models of quantities of interest.

Result: Successfully applied to computational fluid dynamics example (2D incompressible flow past a cylinder), creating compact dynamical systems that accurately predict hydrodynamic forces without full flow simulations.

Conclusion: The targeted digital twin framework provides substantial computational savings while maintaining accurate long-term predictions of quantities of interest, making it suitable for complex systems where full simulations are computationally prohibitive.

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [25] [gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity](https://arxiv.org/abs/2510.08450)
*Hugh Blayney,Álvaro Arroyo,Xiaowen Dong,Michael M. Bronstein*

Main category: cs.LG

TL;DR: This paper re-examines over-squashing in GNNs through the lens of storage and retrieval capacity, introduces a new synthetic task to demonstrate information bottlenecks, and develops a novel GNN architecture inspired by associative memories and xLSTM that shows strong performance on both synthetic and real-world graph benchmarks.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks suffer from over-squashing where information from large receptive fields collapses into fixed-size vectors, creating information bottlenecks that limit model performance.

Method: The authors study limitations of existing over-squashing measures, introduce a new synthetic capacity task, and adapt ideas from sequence modeling (associative memories, fast weight programmers, xLSTM) to develop a novel GNN architecture with improved storage and retrieval capacity.

Result: The proposed architecture demonstrates strong performance on both the synthetic capacity task and a range of real-world graph benchmarks, showing improved ability to handle information bottlenecks.

Conclusion: By reframing over-squashing as a capacity limitation and drawing inspiration from sequence modeling techniques, the authors develop an enhanced GNN architecture that effectively addresses information bottleneck issues in graph learning tasks.

Abstract: Graph Neural Networks (GNNs) leverage the graph structure to transmit
information between nodes, typically through the message-passing mechanism.
While these models have found a wide variety of applications, they are known to
suffer from over-squashing, where information from a large receptive field of
node representations is collapsed into a single fixed sized vector, resulting
in an information bottleneck. In this paper, we re-examine the over-squashing
phenomenon through the lens of model storage and retrieval capacity, which we
define as the amount of information that can be stored in a node's
representation for later use. We study some of the limitations of existing
tasks used to measure over-squashing and introduce a new synthetic task to
demonstrate that an information bottleneck can saturate this capacity.
Furthermore, we adapt ideas from the sequence modeling literature on
associative memories, fast weight programmers, and the xLSTM model to develop a
novel GNN architecture with improved capacity. We demonstrate strong
performance of this architecture both on our capacity synthetic task, as well
as a range of real-world graph benchmarks.

</details>


### [26] [Phase Diagram of Dropout for Two-Layer Neural Networks in the Mean-Field Regime](https://arxiv.org/abs/2510.07554)
*Lénaïc Chizat,Pierre Marion,Yerkin Yesbay*

Main category: cs.LG

TL;DR: Dropout's theoretical behavior in large neural networks depends on the relationship between dropout rate, learning rate, and network width, revealing five distinct asymptotic phases where dropout transitions from a penalty effect to a random geometry technique.


<details>
  <summary>Details</summary>
Motivation: To understand dropout's role in large neural networks by studying its large-width asymptotics, particularly how dropout interacts with gradient descent in two-layer networks with mean-field initialization.

Method: Analyzed gradient descent with dropout on two-layer neural networks using mean-field initialization scale, studying large-width asymptotics across different parameter regimes (dropout rate, learning rate, width). Used mean-field particle systems and stochastic process tools.

Result: Identified five distinct nondegenerate asymptotic phases. Found that dropout's penalty effect only persists with impractically small learning rates (O(1/width)), while larger learning rates make dropout equivalent to random geometry techniques. Limit dynamics described by mean-field jump processes with neurons updating via independent Poisson/Bernoulli clocks.

Conclusion: Dropout's theoretical behavior fundamentally changes with learning rate scale, transitioning from penalty effects to random geometry. This provides new theoretical foundations for understanding dropout in large-scale neural networks and explains its empirical success beyond traditional penalty interpretations.

Abstract: Dropout is a standard training technique for neural networks that consists of
randomly deactivating units at each step of their gradient-based training. It
is known to improve performance in many settings, including in the large-scale
training of language or vision models. As a first step towards understanding
the role of dropout in large neural networks, we study the large-width
asymptotics of gradient descent with dropout on two-layer neural networks with
the mean-field initialization scale. We obtain a rich asymptotic phase diagram
that exhibits five distinct nondegenerate phases depending on the relative
magnitudes of the dropout rate, the learning rate, and the width. Notably, we
find that the well-studied "penalty" effect of dropout only persists in the
limit with impractically small learning rates of order $O(1/\text{width})$. For
larger learning rates, this effect disappears and in the limit, dropout is
equivalent to a "random geometry" technique, where the gradients are thinned
randomly after the forward and backward pass have been computed. In this
asymptotic regime, the limit is described by a mean-field jump process where
the neurons' update times follow independent Poisson or Bernoulli clocks
(depending on whether the learning rate vanishes or not). For some of the
phases, we obtain a description of the limit dynamics both in path-space and in
distribution-space. The convergence proofs involve a mix of tools from
mean-field particle systems and stochastic processes. Together, our results lay
the groundwork for a renewed theoretical understanding of dropout in
large-scale neural networks.

</details>


### [27] [On the optimization dynamics of RLVR: Gradient gap and step size thresholds](https://arxiv.org/abs/2510.08539)
*Joe Suk,Yaqi Duan*

Main category: cs.LG

TL;DR: This paper provides a theoretical foundation for RLVR (Reinforcement Learning with Verifiable Rewards), analyzing why binary feedback works for post-training LLMs and establishing convergence conditions based on Gradient Gap alignment.


<details>
  <summary>Details</summary>
Motivation: RLVR has shown empirical success in using simple binary feedback to train large language models, but there has been no principled theoretical understanding of why this approach works effectively.

Method: The authors analyze RLVR training at both full-response (trajectory) and token levels, introducing the Gradient Gap concept to formalize improvement direction from low-reward to high-reward regions. They derive convergence conditions and step-size thresholds based on Gradient Gap magnitude.

Result: The theory proves that convergence depends on aligning update direction with Gradient Gap and identifies a sharp step-size threshold. It explains why practical heuristics like length normalization improve stability and predicts success rate stagnation below 100% with fixed learning rates. These predictions are validated through bandit simulations and LLM experiments including Qwen2.5-7B training.

Conclusion: The paper establishes a rigorous theoretical foundation for RLVR, explaining its empirical success through Gradient Gap analysis and providing principled insights into training dynamics, convergence conditions, and practical implementation considerations.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple
binary feedback to post-train large language models, has shown significant
empirical success. However, a principled understanding of why it works has been
lacking. This paper builds a theoretical foundation for RLVR by analyzing its
training process at both the full-response (trajectory) and token levels.
Central to our analysis is a quantity called the Gradient Gap, which formalizes
the direction of improvement from low-reward to high-reward regions of the
response space. We prove that convergence critically depends on aligning the
update direction with this Gradient Gap. Moreover, we derive a sharp step-size
threshold based on the magnitude of the Gradient Gap: below it, learning
converges, whereas above it, performance collapses. Our theory further predicts
how the critical step size must scale with response length and the success
rate, thereby explaining why practical heuristics such as length normalization
improve stability and showing that, with a fixed learning rate, the success
rate can stagnate strictly below $100\%$. We validate these predictions through
controlled bandit simulations and LLM experiments, including training
Qwen2.5-7B with GRPO.

</details>


### [28] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: This study uses BERTopic to analyze the lmsys-chat-1m dataset, identifying 29 coherent topics in multilingual LLM conversations and examining how user preferences relate to specific topics and models.


<details>
  <summary>Details</summary>
Motivation: To uncover thematic patterns in LLM conversations and understand how user preferences correlate with specific topics, particularly whether certain LLMs are consistently preferred within particular conversational domains.

Method: Applied BERTopic (transformer-based topic modeling) to the lmsys-chat-1m dataset with robust preprocessing for multilingual variation, dialogue balancing, and data cleaning. Used visualization techniques including inter-topic distance maps, probability distributions, and model-versus-topic matrices.

Result: Identified 29 coherent topics including artificial intelligence, programming, ethics, and cloud infrastructure. Analyzed relationships between topics and model preferences to identify trends in model-topic alignment.

Conclusion: The findings provide insights for domain-specific fine-tuning and optimization strategies to improve real-world LLM performance and user satisfaction based on topic-model preference relationships.

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [29] [Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization](https://arxiv.org/abs/2510.08554)
*Kevin Rojas,Jiahe Lin,Kashif Rasul,Anderson Schneider,Yuriy Nevmyvaka,Molei Tao,Wei Deng*

Main category: cs.LG

TL;DR: GDPO is a new RL algorithm for diffusion language models that reduces variance in ELBO estimation through semi-deterministic Monte Carlo schemes, outperforming existing methods on math, reasoning, and coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Adapting RL fine-tuning to diffusion language models is challenging due to intractable likelihood. Existing methods like diffu-GRPO are biased, while principled ELBO-based methods are computationally prohibitive.

Method: GDPO uses semi-deterministic Monte Carlo schemes to reduce variance in ELBO estimation by employing fast, deterministic integral approximations along key dimensions, avoiding the variance explosion of vanilla double Monte Carlo sampling.

Result: GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO on most math, reasoning, and coding benchmarks.

Conclusion: GDPO provides an effective RL fine-tuning approach for diffusion language models by addressing the variance issues in ELBO estimation, demonstrating superior performance compared to existing methods.

Abstract: Diffusion language models (DLMs) enable parallel, order-agnostic generation
with iterative refinement, offering a flexible alternative to autoregressive
large language models (LLMs). However, adapting reinforcement learning (RL)
fine-tuning to DLMs remains an open challenge because of the intractable
likelihood. Pioneering work such as diffu-GRPO estimated token-level
likelihoods via one-step unmasking. While computationally efficient, this
approach is severely biased. A more principled foundation lies in
sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a
surrogate. Yet, despite this clean mathematical connection, ELBO-based methods
have seen limited adoption due to the prohibitive cost of likelihood
evaluation. In this work, we revisit ELBO estimation and disentangle its
sources of variance. This decomposition motivates reducing variance through
fast, deterministic integral approximations along a few pivotal dimensions.
Building on this insight, we introduce \textbf{Group Diffusion Policy
Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages
simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the
variance explosion of ELBO estimators under vanilla double Monte Carlo
sampling, yielding a provably lower-variance estimator under tight evaluation
budgets. Empirically, GDPO achieves consistent gains over pretrained
checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,
on the majority of math, reasoning, and coding benchmarks.

</details>


### [30] [EBGAN-MDN: An Energy-Based Adversarial Framework for Multi-Modal Behavior Cloning](https://arxiv.org/abs/2510.07562)
*Yixiao Li,Julia Barth,Thomas Kiefer,Ahmad Fraij*

Main category: cs.LG

TL;DR: EBGAN-MDN integrates energy-based models, Mixture Density Networks, and adversarial training to address mode averaging and mode collapse in multi-modal behavior cloning, achieving superior performance on synthetic and robotic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multi-modal behavior cloning faces challenges with mode averaging and mode collapse, which are critical issues in applications like robotics where modeling multiple valid actions ensures both performance and safety.

Method: Proposes EBGAN-MDN framework that combines energy-based models, Mixture Density Networks (MDNs), and adversarial training using a modified InfoNCE loss and energy-enforced MDN loss.

Result: Experiments on synthetic and robotic benchmarks demonstrate superior performance compared to traditional approaches.

Conclusion: EBGAN-MDN establishes itself as an effective and efficient solution for multi-modal learning tasks, successfully addressing the challenges of mode averaging and mode collapse.

Abstract: Multi-modal behavior cloning faces significant challenges due to mode
averaging and mode collapse, where traditional models fail to capture diverse
input-output mappings. This problem is critical in applications like robotics,
where modeling multiple valid actions ensures both performance and safety. We
propose EBGAN-MDN, a framework that integrates energy-based models, Mixture
Density Networks (MDNs), and adversarial training. By leveraging a modified
InfoNCE loss and an energy-enforced MDN loss, EBGAN-MDN effectively addresses
these challenges. Experiments on synthetic and robotic benchmarks demonstrate
superior performance, establishing EBGAN-MDN as a effective and efficient
solution for multi-modal learning tasks.

</details>


### [31] [Automated Machine Learning for Unsupervised Tabular Tasks](https://arxiv.org/abs/2510.07569)
*Prabhant Singh,Pieter Gijsbers,Elif Ceren Gok Yildirim,Murat Onur Yildirim,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: LOTUS is a method for model selection in unsupervised ML tasks using Optimal Transport to find similarity between datasets and recommend effective pipelines.


<details>
  <summary>Details</summary>
Motivation: The intuition is that ML pipelines perform well on new datasets if they worked well on datasets with similar underlying data distributions.

Method: Uses Optimal Transport distances to measure similarity between unlabeled tabular datasets and recommend ML pipelines for outlier detection and clustering.

Result: Experiments show LOTUS performs effectively against strong baselines and is promising for unsupervised model selection.

Conclusion: LOTUS represents a promising first step toward unified model selection for multiple unsupervised machine learning tasks.

Abstract: In this work, we present LOTUS (Learning to Learn with Optimal Transport for
Unsupervised Scenarios), a simple yet effective method to perform model
selection for multiple unsupervised machine learning(ML) tasks such as outlier
detection and clustering. Our intuition behind this work is that a machine
learning pipeline will perform well in a new dataset if it previously worked
well on datasets with a similar underlying data distribution. We use Optimal
Transport distances to find this similarity between unlabeled tabular datasets
and recommend machine learning pipelines with one unified single method on two
downstream unsupervised tasks: outlier detection and clustering. We present the
effectiveness of our approach with experiments against strong baselines and
show that LOTUS is a very promising first step toward model selection for
multiple unsupervised ML tasks.

</details>


### [32] [Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks](https://arxiv.org/abs/2510.07578)
*Shilong Zong,Alex Bierly,Almuatazbellah Boker,Hoda Eldardiry*

Main category: cs.LG

TL;DR: Comparative analysis of Liquid Neural Networks (LNNs) vs traditional RNNs (LSTMs, GRUs) focusing on accuracy, memory efficiency, and generalization. LNNs show promise for noisy data and OOD generalization with better parameter efficiency, while RNNs remain foundational due to maturity.


<details>
  <summary>Details</summary>
Motivation: To systematically compare emerging biologically-inspired LNNs with established RNN variants to understand their relative strengths, limitations, and application domains in sequential data processing.

Method: Systematic review of existing research analyzing basic principles, mathematical models, key characteristics, and challenges of LNNs and RNN variants in sequential data processing.

Result: LNNs demonstrate significant potential for handling noisy, non-stationary data and achieving out-of-distribution generalization, with some variants outperforming RNNs in parameter efficiency and computational speed. RNNs maintain dominance due to mature ecosystem.

Conclusion: The review identifies commonalities and differences between LNNs and RNNs, summarizes their shortcomings, and emphasizes the need to improve LNN scalability for broader applications in complex scenarios.

Abstract: This review aims to conduct a comparative analysis of liquid neural networks
(LNNs) and traditional recurrent neural networks (RNNs) and their variants,
such as long short-term memory networks (LSTMs) and gated recurrent units
(GRUs). The core dimensions of the analysis include model accuracy, memory
efficiency, and generalization ability. By systematically reviewing existing
research, this paper explores the basic principles, mathematical models, key
characteristics, and inherent challenges of these neural network architectures
in processing sequential data. Research findings reveal that LNN, as an
emerging, biologically inspired, continuous-time dynamic neural network,
demonstrates significant potential in handling noisy, non-stationary data, and
achieving out-of-distribution (OOD) generalization. Additionally, some LNN
variants outperform traditional RNN in terms of parameter efficiency and
computational speed. However, RNN remains a cornerstone in sequence modeling
due to its mature ecosystem and successful applications across various tasks.
This review identifies the commonalities and differences between LNNs and RNNs,
summarizes their respective shortcomings and challenges, and points out
valuable directions for future research, particularly emphasizing the
importance of improving the scalability of LNNs to promote their application in
broader and more complex scenarios.

</details>


### [33] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: The paper introduces Expanded Action space (ExpA) to decouple environment interactions from language in LLMs, allowing models to switch between language reasoning and external environments. It proposes ExpA Reinforcement Learning (EARL) with counterfactual policy optimization for effective exploration.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are limited to vocabulary tokens for environment interactions, requiring text parsing and overloading language with both reasoning and control duties. This creates inefficiencies and requires external parsers.

Method: Internalize environment interactions in an Expanded Action space beyond vocabulary. Models can trigger routing actions to switch between language and external environments. Use EARL with counterfactual policy optimization for exploration.

Result: EARL outperforms vocabulary-constrained baselines on multi-turn interaction tasks. Achieves perfect Sort-4 accuracy in partially observed sorting problem and discovers efficient algorithms competitive with classical designs. Robust in calculator-based multi-task learning.

Conclusion: Decoupling environment interactions from language through Expanded Action space enables more efficient and robust LLM reasoning. EARL facilitates effective exploration and achieves strong performance on complex planning tasks.

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


### [34] [TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs](https://arxiv.org/abs/2510.07586)
*Jacob Chmura,Shenyang Huang,Tran Gia Bao Ngo,Ali Parviz,Farimah Poursafaei,Jure Leskovec,Michael Bronstein,Guillaume Rabusseau,Matthias Fey,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: TGM is a new research-oriented library for temporal graph ML that unifies continuous- and discrete-time approaches, offering significant speed improvements and enabling new research possibilities.


<details>
  <summary>Details</summary>
Motivation: Existing temporal graph ML libraries are fragmented, tailored to specific architectures, and lack unified support for both continuous- and discrete-time dynamic graph methods, hindering direct comparisons and progress in this rapidly evolving field.

Method: Developed TGM (Temporal Graph Modelling) library with first-class support for dynamic node features, time-granularity conversions, and native handling of link-, node-, and graph-level tasks, unifying both CTDG and DTDG approaches.

Result: TGM achieves 7.8x average speedup over DyGLib across multiple models/datasets/tasks, and 175x speedup on graph discretization. It enables new research possibilities like dynamic graph property prediction and time-driven training paradigms.

Conclusion: TGM successfully addresses infrastructure gaps in temporal graph ML by providing a unified, efficient library that bridges the CTDG-DTDG divide and opens up previously impractical research directions.

Abstract: Well-designed open-source software drives progress in Machine Learning (ML)
research. While static graph ML enjoys mature frameworks like PyTorch Geometric
and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks
comparable infrastructure. Existing TG libraries are often tailored to specific
architectures, hindering support for diverse models in this rapidly evolving
field. Additionally, the divide between continuous- and discrete-time dynamic
graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To
address these gaps, we introduce Temporal Graph Modelling (TGM), a
research-oriented library for ML on temporal graphs, the first to unify CTDG
and DTDG approaches. TGM offers first-class support for dynamic node features,
time-granularity conversions, and native handling of link-, node-, and
graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across
multiple models, datasets, and tasks compared to the widely used DyGLib, and an
average 175x speedup on graph discretization relative to available
implementations. Beyond efficiency, we show in our experiments how TGM unlocks
entirely new research possibilities by enabling dynamic graph property
prediction and time-driven training paradigms, opening the door to questions
previously impractical to study. TGM is available at
https://github.com/tgm-team/tgm

</details>


### [35] [Transformer-Based Indirect Structural Health Monitoring of Rail Infrastructure with Attention-Driven Detection and Localization of Transient Defects](https://arxiv.org/abs/2510.07606)
*Sizhe Ma,Katherine A. Flanigan,Mario Bergés,James D. Brooks*

Main category: cs.LG

TL;DR: This paper proposes an unsupervised deep learning approach using an Attention-Focused Transformer for indirect structural health monitoring of broken rail detection. The model uses self-attention mechanisms and derives anomaly scores from attention weight deviations, achieving comparable accuracy to state-of-the-art methods with better inference speed.


<details>
  <summary>Details</summary>
Motivation: Current indirect structural health monitoring (iSHM) for broken rail detection faces challenges in reliably detecting small, transient anomalies (2-10 cm) due to complex vehicle dynamics, signal noise, and scarcity of labeled data that limit supervised approaches.

Method: The authors introduce an incremental synthetic data benchmark to evaluate model robustness against speed variations, multi-channel inputs, and realistic noise patterns. They propose an Attention-Focused Transformer model that uses self-attention mechanisms trained via reconstruction, deriving anomaly scores primarily from deviations in learned attention weights.

Result: Benchmarking shows transformer-based models generally outperform others, but all models exhibit significant vulnerability to high-frequency localized noise. The proposed model achieves accuracy comparable to state-of-the-art solutions while demonstrating better inference speed.

Conclusion: The study highlights the critical need for enhanced noise robustness in future iSHM models and positions the more efficient attention-based approach as a promising foundation for developing practical onboard anomaly detection systems.

Abstract: Indirect structural health monitoring (iSHM) for broken rail detection using
onboard sensors presents a cost-effective paradigm for railway track
assessment, yet reliably detecting small, transient anomalies (2-10 cm) remains
a significant challenge due to complex vehicle dynamics, signal noise, and the
scarcity of labeled data limiting supervised approaches. This study addresses
these issues through unsupervised deep learning. We introduce an incremental
synthetic data benchmark designed to systematically evaluate model robustness
against progressively complex challenges like speed variations, multi-channel
inputs, and realistic noise patterns encountered in iSHM. Using this benchmark,
we evaluate several established unsupervised models alongside our proposed
Attention-Focused Transformer. Our model employs a self-attention mechanism,
trained via reconstruction but innovatively deriving anomaly scores primarily
from deviations in learned attention weights, aiming for both effectiveness and
computational efficiency. Benchmarking results reveal that while
transformer-based models generally outperform others, all tested models exhibit
significant vulnerability to high-frequency localized noise, identifying this
as a critical bottleneck for practical deployment. Notably, our proposed model
achieves accuracy comparable to the state-of-the-art solution while
demonstrating better inference speed. This highlights the crucial need for
enhanced noise robustness in future iSHM models and positions our more
efficient attention-based approach as a promising foundation for developing
practical onboard anomaly detection systems.

</details>


### [36] [DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support](https://arxiv.org/abs/2510.07620)
*Muhammad Usman,Yugyung Lee*

Main category: cs.LG

TL;DR: DGTEN is a unified graph framework for dynamic trust evaluation that combines uncertainty-aware message passing, temporal modeling, and adversarial defenses. It uses Gaussian distributions to represent nodes/edges, enabling risk-aware trust decisions with calibrated confidence.


<details>
  <summary>Details</summary>
Motivation: Dynamic trust evaluation in rapidly evolving graphs requires models that can capture changing relationships, express calibrated confidence, and resist adversarial manipulation. Existing approaches lack unified capabilities for all three requirements.

Method: Combines uncertainty-aware message passing with Gaussian distributions, hybrid Absolute-Gaussian-Hourglass positional encoding with Kolmogorov-Arnold network-based attention, ODE-based residual learning, and robust adaptive ensemble coefficient analysis using cosine and Jaccard similarity measures.

Result: On Bitcoin trust networks: 10.77% MCC improvement in single-timeslot prediction, 16.41% MCC gain in cold-start scenario (largest improvement), and up to 11.63% MCC improvement under adversarial on/off attacks compared to best dynamic baseline.

Conclusion: DGTEN provides an effective unified framework for dynamic trust evaluation that successfully addresses the challenges of changing relationships, calibrated confidence, and adversarial resistance in rapidly evolving graphs.

Abstract: Dynamic trust evaluation in large, rapidly evolving graphs requires models
that can capture changing relationships, express calibrated confidence, and
resist adversarial manipulation. DGTEN (Deep Gaussian-based Trust Evaluation
Network) introduces a unified graph framework that achieves all three by
combining uncertainty-aware message passing, expressive temporal modeling, and
built-in defenses against trust-targeted attacks. It represents nodes and edges
as Gaussian distributions so that both semantic signals and epistemic
uncertainty propagate through the graph neural network, enabling risk-aware
trust decisions rather than overconfident guesses. To model how trust evolves,
it employs hybrid Absolute-Gaussian-Hourglass (HAGH) positional encoding with
Kolmogorov-Arnold network-based unbiased multi-head attention, followed by an
ordinary differential equation (ODE)-based residual learning module to jointly
capture abrupt shifts and smooth trends. Robust adaptive ensemble coefficient
analysis prunes or down-weights suspicious interactions using complementary
cosine and Jaccard similarity measures, mitigating reputation laundering,
sabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTEN
delivers significant improvements: in single-timeslot prediction on
Bitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in the
cold-start scenario, it achieves a 16.41% MCC gain - the largest across all
tasks and datasets. Under adversarial on/off attacks, it surpasses the baseline
by up to 11.63% MCC. These results validate the effectiveness of the unified
DGTEN framework.

</details>


### [37] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: This paper provides a comprehensive analysis of machine unlearning methods for large language models, introducing a taxonomy of 12 stateful unlearning approaches and proposing improved evaluation metrics that better capture generative performance and reveal the inherent tradeoff between unlearning effectiveness and utility retention.


<details>
  <summary>Details</summary>
Motivation: Research in LLM unlearning is fragmented with limited clarity on what constitutes effective unlearning and how it should be rigorously evaluated. Current evaluations dominated by multiple-choice question accuracy offer only a narrow perspective and often overstate success while overlooking actual generation behavior.

Method: The authors present a principled taxonomy of 12 recent stateful unlearning methods grouped into three families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning. They introduce open question-answering metrics to better evaluate generative performance and analyze robustness through finer-grained analysis of different attack types.

Result: The analysis reveals that current evaluations overstate success and that there's an inherent tradeoff between unlearning effectiveness and utility retention across method families. Robustness analysis shows vulnerabilities differ substantially between in-domain relearning and out-of-domain fine-tuning attacks.

Conclusion: The study provides a full-stack revisit of LLM unlearning and delivers actionable guidance for designing and evaluating future methods, emphasizing the need for more comprehensive evaluation approaches that capture actual generative behavior and address the UE-UT tradeoff.

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [38] [Property Classification of Vacation Rental Properties during Covid-19](https://arxiv.org/abs/2510.07639)
*Favour Yahdii Aghaebe,Dustin Foley,Eric Atwell,Stephen Clark*

Main category: cs.LG

TL;DR: Using clustering techniques to classify vacation rental properties during Covid to identify patterns and behaviors in the vacation rental market.


<details>
  <summary>Details</summary>
Motivation: To identify inherent patterns and behaviors in vacation rental properties active during the Covid pandemic, enhancing understanding of vacation rental evaluations.

Method: Utilized K-means and K-medoids clustering techniques on a dataset of over a million properties and hosts from CDRC and AirDNA collaboration to identify homogenous groups and their characteristics.

Result: Identified homogenous groups of vacation rental properties with common characteristics, revealing patterns in the vacation rental market during the pandemic.

Conclusion: The findings enhance comprehension of vacation rental intricacies and could be used to create targeted, cluster-specific policies for the vacation rental market.

Abstract: This study advocates for employing clustering techniques to classify vacation
rental properties active during the Covid pandemic to identify inherent
patterns and behaviours. The dataset, a collaboration between the ESRC funded
Consumer Data Research Centre (CDRC) and AirDNA, encompasses data for over a
million properties and hosts. Utilising K-means and K-medoids clustering
techniques, we identify homogenous groups and their common characteristics. Our
findings enhance comprehension of the intricacies of vacation rental
evaluations and could potentially be utilised in the creation of targeted,
cluster-specific policies.

</details>


### [39] [Design-Based Bandits Under Network Interference: Trade-Off Between Regret and Statistical Inference](https://arxiv.org/abs/2510.07646)
*Zichen Wang,Haoyang Hong,Chuanhao Li,Haoxuan Li,Zhiheng Zhang,Huazheng Wang*

Main category: cs.LG

TL;DR: This paper establishes a theoretical Pareto frontier for the trade-off between regret minimization and inference accuracy in multi-armed bandits with network interference (MABNI), and introduces an algorithm called EXP3-N-CS with anytime-valid asymptotic confidence sequences to balance this trade-off.


<details>
  <summary>Details</summary>
Motivation: Existing MABNI research focuses primarily on minimizing regret but overlooks how excessive emphasis on optimal arms can undermine inference accuracy for sub-optimal arms. While initial efforts addressed this trade-off in single-unit scenarios, these challenges are more pronounced in network interference contexts.

Method: The authors establish a theoretical Pareto frontier for the regret-inference accuracy trade-off in adversarial MABNI, and develop an algorithm called EXP3-N-CS that incorporates anytime-valid asymptotic confidence sequences to balance this trade-off.

Result: The paper presents the first theoretical characterization of the Pareto frontier between regret minimization and inference accuracy in adversarial MABNI settings, along with a practical algorithm that achieves this balance.

Conclusion: This work provides a foundational framework for understanding and managing the fundamental trade-off between regret minimization and inference accuracy in multi-armed bandits with network interference, offering both theoretical insights and practical algorithmic solutions.

Abstract: In multi-armed bandits with network interference (MABNI), the action taken by
one node can influence the rewards of others, creating complex interdependence.
While existing research on MABNI largely concentrates on minimizing regret, it
often overlooks the crucial concern that an excessive emphasis on the optimal
arm can undermine the inference accuracy for sub-optimal arms. Although initial
efforts have been made to address this trade-off in single-unit scenarios,
these challenges have become more pronounced in the context of MABNI. In this
paper, we establish, for the first time, a theoretical Pareto frontier
characterizing the trade-off between regret minimization and inference accuracy
in adversarial (design-based) MABNI. We further introduce an anytime-valid
asymptotic confidence sequence along with a corresponding algorithm,
$\texttt{EXP3-N-CS}$, specifically designed to balance the trade-off between
regret minimization and inference accuracy in this setting.

</details>


### [40] [Continual Learning for Adaptive AI Systems](https://arxiv.org/abs/2510.07648)
*Md Hasibul Amin,Tamzid Tanvi Alam*

Main category: cs.LG

TL;DR: A novel regularization technique called Inter-Cluster Separation (ICS) is introduced for continual learning, which penalizes models for producing outputs far from previous task centroids to mitigate catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in continual learning where neural networks lose previously acquired knowledge when learning new sequential tasks, and to prevent overfitting through regularization techniques.

Method: Proposed ICS regularization that adds constraints to loss function based on inter-cluster separation, penalizing outputs distant from previous task centroids. Used hyperparameter tuning for optimal regularization weighting. Evaluated on 5-task Split CIFAR-10 benchmark with ResNet-18 architecture.

Result: ICS demonstrated effectiveness in maintaining strong performance on initial tasks, but showed limitations in long-term knowledge retention as task number increases.

Conclusion: While ICS helps mitigate catastrophic forgetting, the approach reveals inherent trade-offs in continual learning and highlights the need for further research to improve long-term knowledge retention across increasing numbers of tasks.

Abstract: Continual learning the ability of a neural network to learn multiple
sequential tasks without losing previously acquired knowledge remains a
significant obstacle to developing truly adaptive artificial intelligence. Deep
learning models have achieved remarkable results in various applications, but
overfitting remains a common issue. Regularization techniques can help prevent
overfitting by adding constraints to the model's parameters. To prevent
catastrophic forgetting, in this paper we introduce a novel regularization
technique based on inter-cluster separation (ICS) in the loss function, which
penalizes the model for producing outputs that are far away from the centroids
of the clusters formed by the data from previous tasks. We also performed
hyperparameter tuning to find the optimal weighting of the proposed
regularization term. This ensures clearer separation between tasks in the
neural network's internal representation, reducing overlap and mitigating
forgetting. Using the standard 5-task Split CIFAR-10 benchmark and a ResNet-18
architecture, we demonstrate ICS's effectiveness in maintaining strong
performance on initial tasks. However, our results also highlight limitations
in long-term knowledge retention, particularly when the number of tasks
increases. This underscores the complexity and trade-offs inherent in continual
learning and points toward avenues for further research.

</details>


### [41] [Value Flows](https://arxiv.org/abs/2510.07650)
*Perry Dong,Chongyi Zheng,Chelsea Finn,Dorsa Sadigh,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: Value Flows introduces flow-based models to estimate full future return distributions in RL, addressing limitations of categorical/quantile methods by using flow-matching objectives and uncertainty estimation via flow derivative ODEs, achieving 1.3× improvement in success rates.


<details>
  <summary>Details</summary>
Motivation: Current distributional RL methods flatten return distributions to scalars or use discrete approximations, failing to capture fine-grained structure and uncertainty information needed for better decision-making in exploration and safe RL.

Method: Proposes flow-based models with a new flow-matching objective that satisfies the distributional Bellman equation, and uses flow derivative ODEs to estimate return uncertainty, prioritizing learning on uncertain transitions.

Result: Achieves 1.3× average improvement in success rates across 37 state-based and 25 image-based benchmark tasks in offline and online-to-online settings compared to prior methods.

Conclusion: Flow-based modeling of return distributions provides more accurate uncertainty estimation and stronger learning signals, leading to significant performance improvements in reinforcement learning tasks.

Abstract: While most reinforcement learning methods today flatten the distribution of
future returns to a single scalar value, distributional RL methods exploit the
return distribution to provide stronger learning signals and to enable
applications in exploration and safe RL. While the predominant method for
estimating the return distribution is by modeling it as a categorical
distribution over discrete bins or estimating a finite number of quantiles,
such approaches leave unanswered questions about the fine-grained structure of
the return distribution and about how to distinguish states with high return
uncertainty for decision-making. The key idea in this paper is to use modern,
flexible flow-based models to estimate the full future return distributions and
identify those states with high return variance. We do so by formulating a new
flow-matching objective that generates probability density paths satisfying the
distributional Bellman equation. Building upon the learned flow models, we
estimate the return uncertainty of distinct states using a new flow derivative
ODE. We additionally use this uncertainty information to prioritize learning a
more accurate return estimation on certain transitions. We compare our method
(Value Flows) with prior methods in the offline and online-to-online settings.
Experiments on $37$ state-based and $25$ image-based benchmark tasks
demonstrate that Value Flows achieves a $1.3\times$ improvement on average in
success rates. Website: https://pd-perry.github.io/value-flows Code:
https://github.com/chongyi-zheng/value-flows

</details>


### [42] [Incremental Hybrid Ensemble with Graph Attention and Frequency-Domain Features for Stable Long-Term Credit Risk Modeling](https://arxiv.org/abs/2510.07663)
*Jiajing Wang*

Main category: cs.LG

TL;DR: HYDRA-EI is a hybrid ensemble incremental learning framework for predicting long-term loan defaults that handles data distribution shifts over time through weekly updates and adaptive model weighting.


<details>
  <summary>Details</summary>
Motivation: Predicting long-term loan defaults is challenging due to changing borrower behavior and shifting data distributions over time, requiring adaptive approaches that can handle temporal changes.

Method: Uses multi-stage feature processing including relational, cross, and frequency-based features with graph attention, automatic cross-feature creation, and frequency domain transformations. Implements weekly incremental updates with performance-based model weight adjustments.

Result: The framework improves model stability and generalization for long-term credit risk tasks without requiring frequent manual interventions or fixed retraining schedules.

Conclusion: HYDRA-EI provides an effective solution for long-term loan default prediction by combining ensemble learning with incremental updates, making it suitable for real-world credit risk applications with evolving data patterns.

Abstract: Predicting long-term loan defaults is hard because borrower behavior often
changes and data distributions shift over time. This paper presents HYDRA-EI, a
hybrid ensemble incremental learning framework. It uses several stages of
feature processing and combines multiple models. The framework builds
relational, cross, and frequency-based features. It uses graph attention,
automatic cross-feature creation, and transformations from the frequency
domain. HYDRA-EI updates weekly using new data and adjusts the model weights
with a simple performance-based method. It works without frequent manual
changes or fixed retraining. HYDRA-EI improves model stability and
generalization, which makes it useful for long-term credit risk tasks.

</details>


### [43] [FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2510.07664)
*Yunbo Li,Jiaping Gui,Zhihang Deng,Fanchao Meng,Yue Wu*

Main category: cs.LG

TL;DR: FedQS is a novel semi-asynchronous federated learning framework that addresses the trade-offs between gradient and model aggregation strategies by classifying clients into four types and adaptively optimizing their training, achieving superior accuracy, stability, and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Semi-asynchronous FL faces challenges in optimizing both gradient-based and model-based aggregation strategies, which have distinct trade-offs in accuracy, convergence speed, and stability. Gradient aggregation converges faster with higher accuracy but has fluctuations, while model aggregation is more stable but slower with suboptimal accuracy.

Method: FedQS introduces a divide-and-conquer strategy that classifies clients into four distinct types based on data distribution characteristics and computational resources, then adaptively optimizes their local training. It theoretically analyzes and addresses disparities between aggregation strategies in SAFL.

Result: Extensive experiments on computer vision, natural language processing, and real-world tasks show FedQS achieves the highest accuracy, lowest loss, and ranks among the fastest in convergence speed, outperforming state-of-the-art baselines.

Conclusion: FedQS bridges the gap between aggregation strategies in semi-asynchronous federated learning, providing a unified solution for stable, accurate, and efficient federated learning.

Abstract: Federated learning (FL) enables collaborative model training across multiple
parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as
a balanced approach between synchronous and asynchronous FL. However, SAFL
faces significant challenges in optimizing both gradient-based (e.g., FedSGD)
and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct
trade-offs in accuracy, convergence speed, and stability. While gradient
aggregation achieves faster convergence and higher accuracy, it suffers from
pronounced fluctuations, whereas model aggregation offers greater stability but
slower convergence and suboptimal accuracy. This paper presents FedQS, the
first framework to theoretically analyze and address these disparities in SAFL.
FedQS introduces a divide-and-conquer strategy to handle client heterogeneity
by classifying clients into four distinct types and adaptively optimizing their
local training based on data distribution characteristics and available
computational resources. Extensive experiments on computer vision, natural
language processing, and real-world tasks demonstrate that FedQS achieves the
highest accuracy, attains the lowest loss, and ranks among the fastest in
convergence speed, outperforming state-of-the-art baselines. Our work bridges
the gap between aggregation strategies in SAFL, offering a unified solution for
stable, accurate, and efficient federated learning. The code and datasets are
available at https://anonymous.4open.science/r/FedQS-EDD6.

</details>


### [44] [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](https://arxiv.org/abs/2510.07685)
*Yuhan Sun,Zhiwei Huang,Wanqing Cui,Shaopan Xiong,Yazhi Guo,Meiguang Jin,Junfeng Ma*

Main category: cs.LG

TL;DR: LiveThinking is a two-stage optimization framework that distills a 670B teacher model into a lightweight 30B MoE model and then compresses reasoning paths using reinforcement learning, achieving 30x cost reduction with sub-second latency while improving correctness and helpfulness in e-commerce livestreaming.


<details>
  <summary>Details</summary>
Motivation: High-latency Large Reasoning Models are unsuitable for real-time e-commerce livestreaming where digital avatars need immediate responses to drive engagement.

Method: Two-stage approach: 1) Distill 670B teacher LRM into 30B Mixture-of-Experts model using Rejection Sampling Fine-Tuning, 2) Use reinforcement learning with Group Relative Policy Optimization to compress reasoning paths with multi-objective reward function.

Result: 30-fold computational cost reduction, sub-second latency, 3.3% improvement in response correctness, 21.8% improvement in helpfulness, and statistically significant increase in Gross Merchandise Volume on Taobao Live.

Conclusion: LiveThinking effectively bridges the gap between high-performance reasoning models and real-time requirements in interactive e-commerce livestreaming, enhancing both user experience and commercial performance.

Abstract: In AI-powered e-commerce livestreaming, digital avatars require real-time
responses to drive engagement, a task for which high-latency Large Reasoning
Models (LRMs) are ill-suited. We introduce LiveThinking, a practical two-stage
optimization framework to bridge this gap. First, we address computational cost
by distilling a 670B teacher LRM into a lightweight 30B Mixture-of-Experts
(MoE) model (3B active) using Rejection Sampling Fine-Tuning (RFT). This
reduces deployment overhead but preserves the teacher's verbose reasoning,
causing latency. To solve this, our second stage employs reinforcement learning
with Group Relative Policy Optimization (GRPO) to compress the model's
reasoning path, guided by a multi-objective reward function balancing
correctness, helpfulness, and brevity. LiveThinking achieves a 30-fold
reduction in computational cost, enabling sub-second latency. In real-world
application on Taobao Live, it improved response correctness by 3.3% and
helpfulness by 21.8%. Tested by hundreds of thousands of viewers, our system
led to a statistically significant increase in Gross Merchandise Volume (GMV),
demonstrating its effectiveness in enhancing user experience and commercial
performance in live, interactive settings.

</details>


### [45] [Computationally-efficient Graph Modeling with Refined Graph Random Features](https://arxiv.org/abs/2510.07716)
*Krzysztof Choromanski,Avinava Dubey,Arijit Sehanobish,Isaac Reid*

Main category: cs.LG

TL;DR: GRFs++ is an improved version of Graph Random Features that addresses limitations of regular GRFs by using walk-stitching to combine shorter walks and more flexible walk termination strategies, achieving better approximation of graph kernels with improved computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of regular Graph Random Features (GRFs), particularly the difficulty in modeling relationships between distant nodes and inefficient sampling of long graph random walks.

Method: Uses walk-stitching technique to concatenate several shorter walks without breaking unbiasedness, and extends walk termination mechanisms from simple Bernoulli schemes to general distributions on walk lengths.

Result: GRFs++ inherit approximation quality from longer walks but with greater efficiency, trading sequential long walk sampling for parallel computation of short walks and matrix-matrix multiplication.

Conclusion: GRFs++ provide improved approximation accuracy for graph kernels without extra computational cost, as validated through empirical evaluations and theoretical analysis.

Abstract: We propose refined GRFs (GRFs++), a new class of Graph Random Features (GRFs)
for efficient and accurate computations involving kernels defined on the nodes
of a graph. GRFs++ resolve some of the long-standing limitations of regular
GRFs, including difficulty modeling relationships between more distant nodes.
They reduce dependence on sampling long graph random walks via a novel
walk-stitching technique, concatenating several shorter walks without breaking
unbiasedness. By applying these techniques, GRFs++ inherit the approximation
quality provided by longer walks but with greater efficiency, trading
sequential, inefficient sampling of a long walk for parallel computation of
short walks and matrix-matrix multiplication. Furthermore, GRFs++ extend the
simplistic GRFs walk termination mechanism (Bernoulli schemes with fixed
halting probabilities) to a broader class of strategies, applying general
distributions on the walks' lengths. This improves the approximation accuracy
of graph kernels, without incurring extra computational cost. We provide
empirical evaluations to showcase all our claims and complement our results
with theoretical analysis.

</details>


### [46] [DEAS: DEtached value learning with Action Sequence for Scalable Offline RL](https://arxiv.org/abs/2510.07730)
*Changyeon Kim,Haeone Lee,Younggyo Seo,Kimin Lee,Yuke Zhu*

Main category: cs.LG

TL;DR: DEAS is an offline RL framework that uses action sequences for value learning, reducing planning horizon and addressing value overestimation through detached value learning.


<details>
  <summary>Details</summary>
Motivation: Current offline RL approaches struggle with complex, long-horizon sequential decision making, and there's a need for more effective methods that can handle extended action sequences without excessive value overestimation.

Method: DEAS leverages action sequences for value learning through semi-Markov decision process Q-learning, using detached value learning to steer value estimates toward in-distribution actions that achieve high return in the offline dataset.

Result: DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and significantly boosts performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks when applied to Vision-Language-Action models.

Conclusion: The DEAS framework effectively addresses value overestimation in offline RL with action sequences, demonstrating strong performance on complex, long-horizon tasks across simulation and real-world environments.

Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for
training intelligent agents without expensive online interactions. However,
current approaches still struggle with complex, long-horizon sequential
decision making. In this work, we introduce DEtached value learning with Action
Sequence (DEAS), a simple yet effective offline RL framework that leverages
action sequences for value learning. These temporally extended actions provide
richer information than single-step actions and can be interpreted through the
options framework via semi-Markov decision process Q-learning, enabling
reduction of the effective planning horizon by considering longer sequences at
once. However, directly adopting such sequences in actor-critic algorithms
introduces excessive value overestimation, which we address through detached
value learning that steers value estimates toward in-distribution actions that
achieve high return in the offline dataset. We demonstrate that DEAS
consistently outperforms baselines on complex, long-horizon tasks from OGBench
and can be applied to enhance the performance of large-scale
Vision-Language-Action models that predict action sequences, significantly
boosting performance in both RoboCasa Kitchen simulation tasks and real-world
manipulation tasks.

</details>


### [47] [GeoGen: A Two-stage Coarse-to-Fine Framework for Fine-grained Synthetic Location-based Social Network Trajectory Generation](https://arxiv.org/abs/2510.07735)
*Rongchao Xu,Kunlin Cai,Lin Jiang,Dahai Yu,Zhiqing Hong,Yuan Tian,Guang Wang*

Main category: cs.LG

TL;DR: GeoGen is a two-stage framework for generating synthetic LBSN check-in trajectories using spatio-temporal diffusion models and Transformer-based Seq2Seq architecture to address data scarcity and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: High collection costs and privacy concerns prevent access to large-scale LBSN trajectory data needed for applications like POI recommendation and pandemic intervention. Synthetic data generation offers a solution but faces challenges due to spatially discrete, temporally irregular trajectories with complex spatio-temporal patterns.

Method: Two-stage coarse-to-fine framework: 1) Reconstruct continuous latent movement sequences and use Sparsity-aware Spatio-temporal Diffusion model (S²TDiff) to learn behavioral patterns; 2) Use Coarse2FineNet (Transformer-based Seq2Seq with dynamic context fusion and multi-task decoder) to generate fine-grained trajectories from coarse sequences.

Result: GeoGen outperforms state-of-the-art models in both fidelity and utility evaluation, achieving over 69% and 55% improvements in distance and radius metrics on the FS-TKY dataset across four real-world datasets.

Conclusion: GeoGen effectively addresses the challenges of generating synthetic LBSN check-in trajectories by leveraging a two-stage approach that handles spatial discreteness, temporal irregularity, and complex spatio-temporal patterns while ensuring privacy protection.

Abstract: Location-Based Social Network (LBSN) check-in trajectory data are important
for many practical applications, like POI recommendation, advertising, and
pandemic intervention. However, the high collection costs and ever-increasing
privacy concerns prevent us from accessing large-scale LBSN trajectory data.
The recent advances in synthetic data generation provide us with a new
opportunity to achieve this, which utilizes generative AI to generate synthetic
data that preserves the characteristics of real data while ensuring privacy
protection. However, generating synthetic LBSN check-in trajectories remains
challenging due to their spatially discrete, temporally irregular nature and
the complex spatio-temporal patterns caused by sparse activities and uncertain
human mobility. To address this challenge, we propose GeoGen, a two-stage
coarse-to-fine framework for large-scale LBSN check-in trajectory generation.
In the first stage, we reconstruct spatially continuous, temporally regular
latent movement sequences from the original LBSN check-in trajectories and then
design a Sparsity-aware Spatio-temporal Diffusion model (S$^2$TDiff) with an
efficient denosing network to learn their underlying behavioral patterns. In
the second stage, we design Coarse2FineNet, a Transformer-based Seq2Seq
architecture equipped with a dynamic context fusion mechanism in the encoder
and a multi-task hybrid-head decoder, which generates fine-grained LBSN
trajectories based on coarse-grained latent movement sequences by modeling
semantic relevance and behavioral uncertainty. Extensive experiments on four
real-world datasets show that GeoGen excels state-of-the-art models for both
fidelity and utility evaluation, e.g., it increases over 69% and 55% in
distance and radius metrics on the FS-TKY dataset.

</details>


### [48] [MeSH: Memory-as-State-Highways for Recursive Transformers](https://arxiv.org/abs/2510.07739)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,Jiaang Li,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: MeSH (Memory-as-State-Highways) addresses bottlenecks in recursive transformers by externalizing state management into memory buffers and using lightweight routers to diversify computation across iterations, enabling recursive models to outperform larger non-recursive counterparts with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Recursive transformers with fewer parameters often underperform non-recursive counterparts despite parameter reuse, due to undifferentiated computation and information overload in hidden states.

Method: Introduces MeSH scheme with explicit memory buffers for state management and lightweight routers that dynamically diversify computation across iterations, enabling functional specialization.

Result: MeSH-enhanced recursive transformers consistently outperform recursive baselines and exceed larger non-recursive models at 1.4B scale, achieving +1.06% average downstream accuracy improvement with 33% fewer non-embedding parameters.

Conclusion: MeSH provides a scalable and principled architecture that successfully resolves key pathologies in recursive transformers, establishing it as an effective approach for building stronger recursive models.

Abstract: Recursive transformers reuse parameters and iterate over hidden states
multiple times, decoupling compute depth from parameter depth. However, under
matched compute, recursive models with fewer parameters often lag behind
non-recursive counterparts. By probing hidden states, we trace this performance
gap to two primary bottlenecks: undifferentiated computation, where the core is
forced to adopt a similar computational pattern at every iteration, and
information overload, where long-lived and transient information must coexist
in a single hidden state. To address the issues, we introduce a
Memory-as-State-Highways (MeSH) scheme, which externalizes state management
into an explicit memory buffer and employs lightweight routers to dynamically
diversify computation across iterations. Probing visualizations confirm that
MeSH successfully resolves the pathologies by inducing functional
specialization across iterations. On the Pythia suite (160M-1.4B),
MeSH-enhanced recursive transformers consistently improve over recursive
baselines and outperforms its larger non-recursive counterpart at the 1.4B
scale, improving average downstream accuracy by +1.06% with 33% fewer
non-embedding parameters. Our analysis establishes MeSH as a scalable and
principled architecture for building stronger recursive models.

</details>


### [49] [t-SNE Exaggerates Clusters, Provably](https://arxiv.org/abs/2510.07746)
*Noah Bergam,Szymon Snoeck,Nakul Verma*

Main category: cs.LG

TL;DR: t-SNE visualizations cannot reliably indicate input clustering strength or outlier extremity, contrary to common belief.


<details>
  <summary>Details</summary>
Motivation: To challenge the widespread conviction that t-SNE produces visualizations that accurately reflect input data structure, particularly regarding clustering strength and outlier detection.

Method: Theoretical proof and practical demonstrations showing that t-SNE outputs cannot reliably infer input clustering strength or outlier extremity.

Result: Proved that t-SNE visualizations are unreliable for assessing input clustering strength and outlier characteristics, with these failure modes being prevalent in practice.

Conclusion: t-SNE should not be trusted for inferring input data properties like clustering strength and outlier extremity, as its visualizations can be misleading.

Abstract: Central to the widespread use of t-distributed stochastic neighbor embedding
(t-SNE) is the conviction that it produces visualizations whose structure
roughly matches that of the input. To the contrary, we prove that (1) the
strength of the input clustering, and (2) the extremity of outlier points,
cannot be reliably inferred from the t-SNE output. We demonstrate the
prevalence of these failure modes in practice as well.

</details>


### [50] [FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling](https://arxiv.org/abs/2510.07755)
*Zhengyu Wu,Yinlin Zhu,Xunkai Li,Ziang Qiu,Rong-Hua Li,Guoren Wang,Chenghu Zhou*

Main category: cs.LG

TL;DR: FedBook is a federated graph foundation model codebook that addresses privacy constraints by aggregating local codebooks through intra-domain collaboration and inter-domain integration to maintain both domain coherence and cross-domain diversity.


<details>
  <summary>Details</summary>
Motivation: Existing graph foundation models require centralized access to multi-domain graphs, which is often infeasible due to privacy and institutional constraints. Federated Graph Foundation Models (FedGFMs) address this but need robust global codebooks that balance intra-domain coherence and inter-domain diversity.

Method: FedBook uses a two-phase process: (1) Intra-domain Collaboration refines low-frequency tokens using high-frequency tokens across clients for domain-specific coherence; (2) Inter-domain Integration weights client contributions by semantic distinctiveness during global GFM aggregation to preserve cross-domain diversity.

Result: Extensive experiments on 8 benchmarks across multiple domains and tasks show FedBook consistently outperforms 21 baselines, including isolated supervised learning, FL/FGL, federated adaptations of centralized GFMs, and FedGFM techniques.

Conclusion: FedBook provides an effective solution for federated graph foundation models by systematically addressing the challenges of achieving both intra-domain coherence and inter-domain diversity in privacy-constrained distributed settings.

Abstract: Foundation models have shown remarkable cross-domain generalization in
language and vision, inspiring the development of graph foundation models
(GFMs). However, existing GFMs typically assume centralized access to
multi-domain graphs, which is often infeasible due to privacy and institutional
constraints. Federated Graph Foundation Models (FedGFMs) address this
limitation, but their effectiveness fundamentally hinges on constructing a
robust global codebook that achieves intra-domain coherence by consolidating
mutually reinforcing semantics within each domain, while also maintaining
inter-domain diversity by retaining heterogeneous knowledge across domains. To
this end, we propose FedBook, a unified federated graph foundation codebook
that systematically aggregates clients' local codebooks during server-side
federated pre-training. FedBook follows a two-phase process: (1) Intra-domain
Collaboration, where low-frequency tokens are refined by referencing more
semantically reliable high-frequency tokens across clients to enhance
domain-specific coherence; and (2) Inter-domain Integration, where client
contributions are weighted by the semantic distinctiveness of their codebooks
during the aggregation of the global GFM, thereby preserving cross-domain
diversity. Extensive experiments on 8 benchmarks across multiple domains and
tasks demonstrate that FedBook consistently outperforms 21 baselines, including
isolated supervised learning, FL/FGL, federated adaptations of centralized
GFMs, and FedGFM techniques.

</details>


### [51] [Rényi Sharpness: A Novel Sharpness that Strongly Correlates with Generalization](https://arxiv.org/abs/2510.07758)
*Qiaozhe Zhang,Jun Sun,Ruijie Zhang,Yingzhuang Liu*

Main category: cs.LG

TL;DR: Proposes Rényi sharpness as a new measure based on Rényi entropy of loss Hessian eigenvalues, showing stronger correlation with generalization than existing measures. Also introduces Rényi Sharpness Aware Minimization (RSAM) that outperforms prior methods by up to 2.5% test accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing sharpness measures show weak correlation with neural network generalization despite the intuitive belief that flatter minima generalize better. The gap between intuition and empirical results needs to be addressed.

Method: Defines Rényi sharpness as negative Rényi entropy of loss Hessian eigenvalues, capturing eigenvalue spread. Provides generalization bounds using reparametrization invariance and data-to-weight perturbation translation. Also develops RSAM as a regularizer during training.

Result: Strong correlation (Kendall rank correlation) between Rényi sharpness and generalization demonstrated through extensive experiments. RSAM outperforms all existing sharpness-aware minimization methods with up to 2.5% test accuracy improvement over classical SAM.

Conclusion: Rényi sharpness effectively bridges the intuition-reality gap in sharpness measures and provides a theoretically grounded, empirically validated approach that significantly improves generalization through RSAM regularization.

Abstract: Sharpness (of the loss minima) is a common measure to investigate the
generalization of neural networks. Intuitively speaking, the flatter the
landscape near the minima is, the better generalization might be.
Unfortunately, the correlation between many existing sharpness measures and the
generalization is usually not strong, sometimes even weak. To close the gap
between the intuition and the reality, we propose a novel sharpness measure,
i.e., \textit{R\'enyi sharpness}, which is defined as the negative R\'enyi
entropy (a generalization of the classical Shannon entropy) of the loss
Hessian. The main ideas are as follows: 1) we realize that \textit{uniform}
(identical) eigenvalues of the loss Hessian is most desirable (while keeping
the sum constant) to achieve good generalization; 2) we employ the
\textit{R\'enyi entropy} to concisely characterize the extent of the spread of
the eigenvalues of loss Hessian. Normally, the larger the spread, the smaller
the (R\'enyi) entropy. To rigorously establish the relationship between
generalization and (R\'enyi) sharpness, we provide several generalization
bounds in terms of R\'enyi sharpness, by taking advantage of the
reparametrization invariance property of R\'enyi sharpness, as well as the
trick of translating the data discrepancy to the weight perturbation.
Furthermore, extensive experiments are conducted to verify the strong
correlation (in specific, Kendall rank correlation) between the R\'enyi
sharpness and generalization. Moreover, we propose to use a variant of R\'enyi
Sharpness as regularizer during training, i.e., R\'enyi Sharpness Aware
Minimization (RSAM), which turns out to outperform all existing sharpness-aware
minimization methods. It is worthy noting that the test accuracy gain of our
proposed RSAM method could be as high as nearly 2.5\%, compared against the
classical SAM method.

</details>


### [52] [A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization](https://arxiv.org/abs/2510.07760)
*Yiqin Lv,Zhiyu Mou,Miao Xu,Jinghao Chen,Qi Wang,Yixiu Mao,Yun Qu,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng,Xiangyang Ji*

Main category: cs.LG

TL;DR: VAMO is a multi-task optimization method for online advertising that aligns training gradients with validation gradients to improve generalization in volatile bidding environments, incorporating temporal periodicity awareness and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous advertiser requirements create numerous customized bidding tasks that are optimized independently, leading to high computational costs and poor data efficiency. Existing multi-task learning approaches guided by training dynamics generalize poorly in volatile bidding environments.

Method: Validation-Aligned Multi-task Optimization (VAMO) adaptively assigns task weights based on alignment between per-task training gradients and held-out validation gradients. It includes a periodicity-aware temporal module and integrates with an advanced generative auto-bidding backbone to enhance cross-task transfer of seasonal patterns.

Result: Extensive experiments on both simulated and large-scale real-world advertising systems consistently demonstrate significant improvements over typical baselines, showing the effectiveness of the proposed approach.

Conclusion: VAMO provides a principled framework for multi-task optimization in online advertising that better matches deployment objectives through validation alignment, with theoretical guarantees and practical effectiveness demonstrated across diverse advertising environments.

Abstract: In online advertising, heterogeneous advertiser requirements give rise to
numerous customized bidding tasks that are typically optimized independently,
resulting in extensive computation and limited data efficiency. Multi-task
learning offers a principled framework to train these tasks jointly through
shared representations. However, existing multi-task optimization strategies
are primarily guided by training dynamics and often generalize poorly in
volatile bidding environments. To this end, we present Validation-Aligned
Multi-task Optimization (VAMO), which adaptively assigns task weights based on
the alignment between per-task training gradients and a held-out validation
gradient, thereby steering updates toward validation improvement and better
matching deployment objectives. We further equip the framework with a
periodicity-aware temporal module and couple it with an advanced generative
auto-bidding backbone to enhance cross-task transfer of seasonal structure and
strengthen bidding performance. Meanwhile, we provide theoretical insights into
the proposed method, e.g., convergence guarantee and alignment analysis.
Extensive experiments on both simulated and large-scale real-world advertising
systems consistently demonstrate significant improvements over typical
baselines, illuminating the effectiveness of the proposed approach.

</details>


### [53] [FedLAM: Low-latency Wireless Federated Learning via Layer-wise Adaptive Modulation](https://arxiv.org/abs/2510.07766)
*Linping Qu,Shenghui Song,Chi-Ying Tsui*

Main category: cs.LG

TL;DR: Proposes a layer-wise adaptive modulation scheme for wireless federated learning to reduce communication latency by assigning different modulation levels to different DNN layers based on their importance.


<details>
  <summary>Details</summary>
Motivation: Current wireless FL systems transmit high-dimensional DNN parameters through bandwidth-limited channels, causing significant communication latency issues. Existing schemes use uniform modulation across all layers, ignoring layer importance differences.

Method: Developed a layer-wise adaptive modulation scheme that automatically determines optimal modulation levels for different DNN layers by considering their varying importance, rather than using the same modulation level for all layers.

Result: Experimental results demonstrate that the proposed scheme can achieve up to 73.9% reduction in communication latency compared to existing uniform modulation schemes.

Conclusion: The layer-wise adaptive modulation approach effectively reduces communication latency in wireless FL by leveraging the varying importance of different DNN layers, providing significant performance improvements over traditional uniform modulation methods.

Abstract: In wireless federated learning (FL), the clients need to transmit the
high-dimensional deep neural network (DNN) parameters through bandwidth-limited
channels, which causes the communication latency issue. In this paper, we
propose a layer-wise adaptive modulation scheme to save the communication
latency. Unlike existing works which assign the same modulation level for all
DNN layers, we consider the layers' importance which provides more freedom to
save the latency. The proposed scheme can automatically decide the optimal
modulation levels for different DNN layers. Experimental results show that the
proposed scheme can save up to 73.9% of communication latency compared with the
existing schemes.

</details>


### [54] [Weak Form Learning for Mean-Field Partial Differential Equations: an Application to Insect Movement](https://arxiv.org/abs/2510.07786)
*Seth Minor,Bret D. Elderd,Benjamin Van Allen,David M. Bortz,Vanja Dukic*

Main category: cs.LG

TL;DR: This paper extends weak-form equation learning techniques to model lepidopteran larval movement from sparse experimental data, using WSINDy algorithm to learn Fokker-Planck equations from position measurements of fall armyworms under varied agricultural conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding insect dispersal dynamics, particularly crop pests, can improve outbreak forecasting and pest management. Insect movement patterns are driven by infection, predation, and environmental factors, typically following overdamped stochastic dynamics.

Method: Extended weak-form equation learning techniques coupled with kernel density estimation, specifically using the Weak form Sparse Identification of Nonlinear Dynamics (WSINDy) algorithm, to learn effective models from highly sparse experimental position data.

Result: The method was demonstrated on sparse position measurements of fall armyworms (Spodoptera frugiperda) obtained in simulated agricultural conditions with varied plant resources and infection status.

Conclusion: The approach successfully learns governing equations for insect population movement from sparse data, providing a useful tool for understanding and predicting pest dispersal behavior in agricultural contexts.

Abstract: Insect species subject to infection, predation, and anisotropic environmental
conditions may exhibit preferential movement patterns. Given the innate
stochasticity of exogenous factors driving these patterns over short
timescales, individual insect trajectories typically obey overdamped stochastic
dynamics. In practice, data-driven modeling approaches designed to learn the
underlying Fokker-Planck equations from observed insect distributions serve as
ideal tools for understanding and predicting such behavior. Understanding
dispersal dynamics of crop and silvicultural pests can lead to a better
forecasting of outbreak intensity and location, which can result in better pest
management. In this work, we extend weak-form equation learning techniques,
coupled with kernel density estimation, to learn effective models for
lepidopteran larval population movement from highly sparse experimental data.
Galerkin methods such as the Weak form Sparse Identification of Nonlinear
Dynamics (WSINDy) algorithm have recently proven useful for learning governing
equations in several scientific contexts. We demonstrate the utility of the
method on a sparse dataset of position measurements of fall armyworms
(Spodoptera frugiperda) obtained in simulated agricultural conditions with
varied plant resources and infection status.

</details>


### [55] [HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs](https://arxiv.org/abs/2510.07796)
*Majid Jaberi-Douraki,Hossein Sholehrasa,Xuan Xu,Remya Ampadi Ramachandran*

Main category: cs.LG

TL;DR: HySim-LLM is a framework that enhances LLMs for pharmacokinetic data extraction using embedding-weighted fine-tuning and manifold-aware denoising, with theoretical guarantees for adaptation performance and noise handling.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with structured biomedical data like PK tables due to heterogeneity, noise, and domain shift, limiting reliable data extraction for drug development.

Method: Proposes HySim-LLM framework integrating embedding-weighted fine-tuning and manifold-aware denoising, with theoretical guarantees for generalization bounds and denoising performance.

Result: Establishes two theoretical results: similarity-weighted generalization bound for adaptation performance and manifold-based denoising guarantee for handling noisy samples.

Conclusion: Provides a mathematically grounded pathway for reliable and interpretable LLM adaptation in biomedical domains, addressing key limitations in structured data processing.

Abstract: The extraction and standardization of pharmacokinetic (PK) information from
scientific literature remain significant challenges in computational
pharmacology, which limits the reliability of data-driven models in drug
development. Large language models (LLMs) have achieved remarkable progress in
text understanding and reasoning, yet their adaptation to structured biomedical
data, such as PK tables, remains constrained by heterogeneity, noise, and
domain shift. To address these limitations, we propose HySim-LLM, a unified
mathematical and computational framework that integrates embedding-weighted
fine-tuning and manifold-aware denoising to enhance the robustness and
interpretability of LLMs. We establish two theoretical results: (1) a
similarity-weighted generalization bound that quantifies adaptation performance
under embedding divergence, and (2) a manifold-based denoising guarantee that
bounds loss contributions from noisy or off-manifold samples. These theorems
provide a principled foundation for fine-tuning LLMs in structured biomedical
settings. The framework offers a mathematically grounded pathway toward
reliable and interpretable LLM adaptation for biomedical and data-intensive
scientific domains.

</details>


### [56] [SIMU: Selective Influence Machine Unlearning](https://arxiv.org/abs/2510.07822)
*Anu Agarwal,Mihir Pamnani,Dilek Hakkani-Tur*

Main category: cs.LG

TL;DR: SIMU is a two-step machine unlearning framework that selectively updates only critical neurons responsible for encoding sensitive information, achieving effective unlearning while better preserving the model's original capabilities compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning techniques for LLMs often compromise the model's original knowledge and utility while removing sensitive information, creating a need for more targeted approaches that maintain model performance.

Method: A two-step framework that enhances second-order optimizer-based unlearning by selectively updating only the critical neurons responsible for encoding the forget-set, rather than updating the entire model.

Result: SIMU achieves comparable unlearning efficacy while substantially outperforming current methods in retaining the model's original knowledge and overall utility.

Conclusion: Selective neuron updating in machine unlearning provides an effective approach to remove sensitive information from LLMs while preserving their core capabilities, addressing the key limitation of existing unlearning methods.

Abstract: The undesired memorization of sensitive information by Large Language Models
(LLMs) has emphasized the need for safety mechanisms that can regulate model
behavior. This has led to the development of machine unlearning techniques that
enable models to precisely forget sensitive and unwanted information. For
machine unlearning, first-order and second-order optimizer-based methods have
shown significant progress in enabling LLMs to forget targeted information.
However, in doing so, these approaches often compromise the model's original
capabilities, resulting in unlearned models that struggle to retain their prior
knowledge and overall utility. To address this, we propose Selective Influence
Machine Unlearning (SIMU), a two-step framework that enhances second-order
optimizer-based unlearning by selectively updating only the critical neurons
responsible for encoding the forget-set. By constraining updates to these
targeted neurons, SIMU achieves comparable unlearning efficacy while
substantially outperforming current methods in retaining the model's original
knowledge.

</details>


### [57] [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
*Weisen Jiang,Sinno Jialin Pan*

Main category: cs.LG

TL;DR: MetaDefense is a novel framework that defends against finetuning-based jailbreak attacks in LLMs through a two-stage approach: pre-generation detection of harmful queries and mid-generation monitoring of partial responses, achieving robust defense against both seen and unseen attack templates.


<details>
  <summary>Details</summary>
Motivation: Existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space.

Method: A two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. The framework trains LLMs to predict harmfulness using specialized prompts.

Result: Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks.

Conclusion: MetaDefense provides an effective framework for defending against jailbreak attacks in LLMs through early detection and termination of potentially harmful interactions, offering improved generalization to unseen attack templates compared to existing methods.

Abstract: This paper introduces MetaDefense, a novel framework for defending against
finetuning-based jailbreak attacks in large language models (LLMs). We observe
that existing defense mechanisms fail to generalize to harmful queries
disguised by unseen attack templates, despite LLMs being capable of
distinguishing disguised harmful queries in the embedding space. Based on these
insights, we propose a two-stage defense approach: (i) pre-generation defense
that detects harmful queries before response generation begins, and (ii)
mid-generation defense that monitors partial responses during generation to
prevent outputting more harmful content. Our MetaDefense trains the LLM to
predict the harmfulness of both queries and partial responses using specialized
prompts, enabling early termination of potentially harmful interactions.
Extensive experiments across multiple LLM architectures (LLaMA-2-7B,
Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense
significantly outperforms existing defense mechanisms, achieving robust defense
against harmful queries with seen and unseen attack templates while maintaining
competitive performance on benign tasks. Code is available at
https://github.com/ws-jiang/MetaDefense.

</details>


### [58] [Self-Improving LLM Agents at Test-Time](https://arxiv.org/abs/2510.07841)
*Emre Can Acikgoz,Cheng Qian,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: The paper proposes Test-Time Self-Improvement (TT-SI), a method where language models identify their own uncertain predictions, generate similar examples, and fine-tune on them at test-time, achieving significant performance gains with dramatically fewer training samples.


<details>
  <summary>Details</summary>
Motivation: Traditional fine-tuning requires large datasets which are inefficient to gather and expensive to train on, with no guarantee of better generalization. Existing methods don't assess whether training samples provide novel information or are redundant.

Method: Three-step algorithm: (1) identify samples the model struggles with (self-awareness), (2) generate similar examples from uncertain samples (self-data augmentation), (3) use generated samples for test-time fine-tuning (self-improvement). Two variants: TT-SI (self-generated examples) and TT-D (examples from stronger model).

Result: TT-SI achieves +5.48% absolute accuracy gain on average across benchmarks, surpassing other learning methods while using 68x fewer training samples.

Conclusion: Test-time self-improvement algorithms show promise as a new paradigm for building more capable agents toward self-evolution, demonstrating the potential of on-the-fly model improvement.

Abstract: One paradigm of language model (LM) fine-tuning relies on creating large
training datasets, under the assumption that high quantity and diversity will
enable models to generalize to novel tasks after post-training. In practice,
gathering large sets of data is inefficient, and training on them is
prohibitively expensive; worse, there is no guarantee that the resulting model
will handle complex scenarios or generalize better. Moreover, existing
techniques rarely assess whether a training sample provides novel information
or is redundant with the knowledge already acquired by the model, resulting in
unnecessary costs. In this work, we explore a new test-time self-improvement
method to create more effective and generalizable agentic LMs on-the-fly. The
proposed algorithm can be summarized in three steps: (i) first it identifies
the samples that model struggles with (self-awareness), (ii) then generates
similar examples from detected uncertain samples (self-data augmentation), and
(iii) uses these newly generated samples at test-time fine-tuning
(self-improvement). We study two variants of this approach: Test-Time
Self-Improvement (TT-SI), where the same model generates additional training
examples from its own uncertain cases and then learns from them, and contrast
this approach with Test-Time Distillation (TT-D), where a stronger model
generates similar examples for uncertain cases, enabling student to adapt using
distilled supervision. Empirical evaluations across different agent benchmarks
demonstrate that TT-SI improves the performance with +5.48% absolute accuracy
gain on average across all benchmarks and surpasses other standard learning
methods, yet using 68x less training samples. Our findings highlight the
promise of TT-SI, demonstrating the potential of self-improvement algorithms at
test-time as a new paradigm for building more capable agents toward
self-evolution.

</details>


### [59] [Meta-Learning Based Few-Shot Graph-Level Anomaly Detection](https://arxiv.org/abs/2510.07847)
*Liting Li,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: MA-GAD is a meta-learning framework for graph-level anomaly detection that addresses few-shot learning challenges by incorporating graph compression and meta-learning to improve robustness and performance on limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based anomaly detection methods require large labeled datasets, which are often unavailable in real-world scenarios. Few-shot methods are sensitive to noise, leading to poor embedding quality and reduced robustness.

Method: Proposes MA-GAD with a graph compression module to reduce graph size and mitigate noise while preserving essential information. Uses meta-learning to extract meta-anomaly information from similar networks and learn an initialization model that quickly adapts to new tasks with limited samples. Includes a bias network to enhance distinction between anomalous and normal nodes.

Result: Experimental results on four real-world biochemical datasets show MA-GAD outperforms state-of-the-art methods in graph-level anomaly detection under few-shot conditions. Validated effectiveness on both graph anomaly and subgraph anomaly detection tasks.

Conclusion: MA-GAD effectively addresses the challenges of few-shot graph anomaly detection by combining graph compression and meta-learning, demonstrating superior performance compared to existing methods on real-world datasets.

Abstract: Graph-level anomaly detection aims to identify anomalous graphs or subgraphs
within graph datasets, playing a vital role in various fields such as fraud
detection, review classification, and biochemistry. While Graph Neural Networks
(GNNs) have made significant progress in this domain, existing methods rely
heavily on large amounts of labeled data, which is often unavailable in
real-world scenarios. Additionally, few-shot anomaly detection methods based on
GNNs are prone to noise interference, resulting in poor embedding quality and
reduced model robustness. To address these challenges, we propose a novel
meta-learning-based graph-level anomaly detection framework (MA-GAD),
incorporating a graph compression module that reduces the graph size,
mitigating noise interference while retaining essential node information. We
also leverage meta-learning to extract meta-anomaly information from similar
networks, enabling the learning of an initialization model that can rapidly
adapt to new tasks with limited samples. This improves the anomaly detection
performance on target graphs, and a bias network is used to enhance the
distinction between anomalous and normal nodes. Our experimental results, based
on four real-world biochemical datasets, demonstrate that MA-GAD outperforms
existing state-of-the-art methods in graph-level anomaly detection under
few-shot conditions. Experiments on both graph anomaly and subgraph anomaly
detection tasks validate the framework's effectiveness on real-world datasets.

</details>


### [60] [Signal-to-Noise Ratio in Scanning Electron Microscopy: A Comprehensive Review](https://arxiv.org/abs/2510.07886)
*K. S. Sim,I. Bukhori,D. C. Y. Ong,K. B. Gan*

Main category: cs.LG

TL;DR: This review paper comprehensively examines signal-to-noise ratio (SNR) in Scanning Electron Microscopy (SEM), covering noise sources, measurement methods, factors affecting SNR, and optimization techniques from both hardware and software perspectives.


<details>
  <summary>Details</summary>
Motivation: SEM is critical in nanotechnology, materials science, and biological imaging, but its utility is often compromised by noise that degrades image quality and interpretability. The paper aims to address this fundamental limitation.

Method: The authors conduct a comprehensive review of SEM imaging processes, examining principal SEM operation, noise sources, SNR measurement methods, factors affecting SNR, and enhancement approaches through both traditional and emerging techniques.

Result: The review synthesizes various SNR optimization strategies, analyzing their applications, advantages, and limitations to provide researchers with a complete understanding of how to improve SEM image quality.

Conclusion: The paper serves as a comprehensive resource for understanding and optimizing SNR in SEM imaging, encouraging further research in this important area to enhance the utility of SEM across scientific disciplines.

Abstract: Scanning Electron Microscopy (SEM) is critical in nanotechnology, materials
science, and biological imaging due to its high spatial resolution and depth of
focus. Signal-to-noise ratio (SNR) is an essential parameter in SEM because it
directly impacts the quality and interpretability of the images. SEM is widely
used in various scientific disciplines, but its utility can be compromised by
noise, which degrades image clarity. This review explores multiple aspects of
the SEM imaging process, from the principal operation of SEM, sources of noise
in SEM, methods for SNR measurement and estimations, to various aspects that
affect the SNR measurement and approaches to enhance SNR, both from a hardware
and software standpoint. We review traditional and emerging techniques,
focusing on their applications, advantages, and limitations. The paper aims to
provide a comprehensive understanding of SNR optimization in SEM for
researchers and practitioners and to encourage further research in the field.

</details>


### [61] [Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression Filtering Method for SEM Images](https://arxiv.org/abs/2510.07895)
*D. Chee Yong Ong,I. Bukhori,K. S. Sim,K. Beng Gan*

Main category: cs.LG

TL;DR: This paper presents a complete pipeline for estimating Signal-to-Noise Ratio (SNR) and noise variance (NV) in SEM images, then using NV-guided Wiener filtering for noise removal. The proposed AO-GPRLLSR method combines Linear Least Squares Regression for SNR estimation with Optimizable Gaussian Process Regression for NV estimation, achieving lower MSE in filtered images.


<details>
  <summary>Details</summary>
Motivation: SEM images often suffer from noise contamination that degrades image quality and affects analysis. Existing methods need improvement for more robust and accurate noise estimation and filtering.

Method: The approach involves: 1) Testing five SNR estimation techniques (NN, FOL, NN+FOL, NLLSR, LSR) and selecting LSR as best performer; 2) Testing SVM and GPR models paired with LSR for NV estimation, selecting Optimizable GPR; 3) Combining these into AO-GPRLLSR pipeline that uses estimated NV to guide Wiener filtering.

Result: LSR method performed better than other SNR estimation techniques. Optimizable GPR showed highest accuracy for NV estimation. The proposed AO-GPRLLSR method achieved notable success in estimating SNR and NV, leading to lower Mean Squared Error after filtering.

Conclusion: The AO-GPRLLSR filtering pipeline effectively estimates noise parameters and enhances SEM image quality through NV-guided Wiener filtering, providing a more robust and accurate solution for SEM image denoising.

Abstract: Scanning Electron Microscopy (SEM) images often suffer from noise
contamination, which degrades image quality and affects further analysis. This
research presents a complete approach to estimate their Signal-to-Noise Ratio
(SNR) and noise variance (NV), and enhance image quality using NV-guided Wiener
filter. The main idea of this study is to use a good SNR estimation technique
and infuse a machine learning model to estimate NV of the SEM image, which then
guides the wiener filter to remove the noise, providing a more robust and
accurate SEM image filtering pipeline. First, we investigate five different SNR
estimation techniques, namely Nearest Neighbourhood (NN) method, First-Order
Linear Interpolation (FOL) method, Nearest Neighbourhood with First-Order
Linear Interpolation (NN+FOL) method, Non-Linear Least Squares Regression
(NLLSR) method, and Linear Least Squares Regression (LSR) method. It is shown
that LSR method to perform better than the rest. Then, Support Vector Machines
(SVM) and Gaussian Process Regression (GPR) are tested by pairing it with LSR.
In this test, the Optimizable GPR model shows the highest accuracy and it
stands as the most effective solution for NV estimation. Combining these
results lead to the proposed Adaptive Optimizable Gaussian Process Regression
Linear Least Squares Regression (AO-GPRLLSR) Filtering pipeline. The AO-GPRLLSR
method generated an estimated noise variance which served as input to NV-guided
Wiener filter for improving the quality of SEM images. The proposed method is
shown to achieve notable success in estimating SNR and NV of SEM images and
leads to lower Mean Squared Error (MSE) after the filtering process.

</details>


### [62] [MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation](https://arxiv.org/abs/2510.07910)
*Chongmyung Kwon,Yujin Kim,Seoeun Park,Yunji Lee,Charmgil Hong*

Main category: cs.LG

TL;DR: A novel framework called MMM integrates 3D quantum-chemical information using Electron Localization Function (ELF) maps to improve drug-drug interaction prediction, outperforming GNN-based approaches with statistically significant improvements.


<details>
  <summary>Details</summary>
Motivation: Current graph neural networks use simplified discrete forms that cannot fully capture molecular binding affinity and reactivity, making drug-drug interaction prediction challenging in clinical decision support systems.

Method: MMM framework generates 3D electron density maps using ELF and combines ELF-derived features encoding global electronic properties with a bipartite graph encoder that models local substructure interactions.

Result: Evaluation on MIMIC-III dataset shows statistically significant improvements over GNN-based SafeDrug model in F1-score (p=0.0387), Jaccard (p=0.0112), and DDI rate (p=0.0386).

Conclusion: ELF-based 3D representations enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice.

Abstract: Drug recommendation is an essential task in machine learning-based clinical
decision support systems. However, the risk of drug-drug interactions (DDI)
between co-prescribed medications remains a significant challenge. Previous
studies have used graph neural networks (GNNs) to represent drug structures.
Regardless, their simplified discrete forms cannot fully capture the molecular
binding affinity and reactivity. Therefore, we propose Multimodal DDI
Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a
novel framework that integrates three-dimensional (3D) quantum-chemical
information into drug representation learning. It generates 3D electron density
maps using the ELF. To capture both therapeutic relevance and interaction
risks, MMM combines ELF-derived features that encode global electronic
properties with a bipartite graph encoder that models local substructure
interactions. This design enables learning complementary characteristics of
drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442
substructures), comparing it with several baseline models. In particular, a
comparison with the GNN-based SafeDrug model demonstrates statistically
significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),
and the DDI rate (p = 0.0386). These results demonstrate the potential of
ELF-based 3D representations to enhance prediction accuracy and support safer
combinatorial drug prescribing in clinical practice.

</details>


### [63] [GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploratio](https://arxiv.org/abs/2510.07919)
*Tingfeng Hong,Pingye Ren,Xinlong Xiao,Chao Wang,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.LG

TL;DR: The paper presents a personalized multi-objective ranking system with a novel GRADE framework for learning personalized weights to blend multiple ranking objectives.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing multiple user feedback signals in ranking systems and provide personalized ranking results that consider individual user preferences.

Method: Proposes a three-stage architecture: Feature Center with Prerank Model for candidate generation, Multi-Task Learning model for predicting user feedback signals, and Multi-Task Fusion (GRADE framework) for learning personalized weights to blend rankings.

Result: Developed a complete personalized multi-objective ranking system that can generate blended rankings by applying learned personalized weights to multiple ranking objectives.

Conclusion: The proposed system successfully integrates personalized weighting through the GRADE framework to create blended rankings that better serve individual user preferences across multiple objectives.

Abstract: Overall architecture of the personalized multi-objective ranking system. It
comprises: (1) a Feature Center and Prerank Model for initial feature
processing and candidate generation; (2) a Multi-Task Learning (MTL) model
predicting various user feedback signals; (3) a Multi-Task Fusion (MTF) module
(our proposed GRADE framework) that learns personalized weights ($w_1, \dots,
w_n$); these weights are then applied to calculate final scores and sorted to
generate a blended ranking by the Blended Ranking Model, which ultimately
delivers results to users.

</details>


### [64] [SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening](https://arxiv.org/abs/2510.07922)
*Murtaza Rangwala,Farag Azzedin,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.LG

TL;DR: SketchGuard is a Byzantine-robust decentralized federated learning framework that uses Count Sketch compression to reduce communication and computational costs while maintaining security against malicious clients.


<details>
  <summary>Details</summary>
Motivation: Existing Byzantine-robust DFL methods require exchanging complete high-dimensional models with all neighbors in each round, creating prohibitive communication and computational costs that prevent web-scale deployment.

Method: SketchGuard compresses d-dimensional models to k-dimensional sketches (k ≪ d) using Count Sketch for similarity comparisons, then selectively fetches full models only from accepted neighbors, reducing communication complexity from O(d|N_i|) to O(k|N_i| + d|S_i|).

Result: SketchGuard maintains identical robustness to state-of-the-art methods while reducing computation time by up to 82% and communication overhead by 50-70%, with benefits scaling multiplicatively with model dimensionality and network connectivity.

Conclusion: SketchGuard establishes sketch-based compression as a fundamental enabler of robust decentralized federated learning at web scale, with rigorous convergence guarantees and controlled degradation bounds.

Abstract: Decentralized Federated Learning (DFL) enables privacy-preserving
collaborative training without centralized servers, but remains vulnerable to
Byzantine attacks where malicious clients submit corrupted model updates.
Existing Byzantine-robust DFL defenses rely on similarity-based neighbor
screening that requires every client to exchange and compare complete
high-dimensional model vectors with all neighbors in each training round,
creating prohibitive communication and computational costs that prevent
deployment at web scale. We propose SketchGuard, a general framework that
decouples Byzantine filtering from model aggregation through sketch-based
neighbor screening. SketchGuard compresses $d$-dimensional models to
$k$-dimensional sketches ($k \ll d$) using Count Sketch for similarity
comparisons, then selectively fetches full models only from accepted neighbors,
reducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| +
d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \le |N_i|$ is the
accepted neighbor count. We establish rigorous convergence guarantees in both
strongly convex and non-convex settings, proving that Count Sketch compression
preserves Byzantine resilience with controlled degradation bounds where
approximation errors introduce only a $(1+O(\epsilon))$ factor in the effective
threshold parameter. Comprehensive experiments across multiple datasets,
network topologies, and attack scenarios demonstrate that SketchGuard maintains
identical robustness to state-of-the-art methods while reducing computation
time by up to 82% and communication overhead by 50-70% depending on filtering
effectiveness, with benefits scaling multiplicatively with model dimensionality
and network connectivity. These results establish the viability of sketch-based
compression as a fundamental enabler of robust DFL at web scale.

</details>


### [65] [Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers](https://arxiv.org/abs/2510.07924)
*Yongqi Ding,Lin Zuo,Mengmeng Jing,Kunshan Yang,Pei He,Tonglan Xie*

Main category: cs.LG

TL;DR: The paper proposes efficient self-distillation methods for spiking neural networks (SNNs) by treating each timestep as a submodel and leveraging strong-weak relationships between them, achieving improved performance without additional training overhead.


<details>
  <summary>Details</summary>
Motivation: SNNs offer low-power alternatives to ANNs but suffer from performance gaps. Existing knowledge distillation methods rely on large teacher models or introduce extra training costs. The authors aim to develop efficient self-distillation that leverages SNNs' temporal properties.

Method: Deconstruct SNNs into timestep submodels and evaluate their output confidence. Propose two self-distillation schemes: Strong2Weak (stronger timesteps guide weaker ones) and Weak2Strong (weaker timesteps distill strong ones with dark knowledge). Implement flexible distillation approaches including ensemble, simultaneous, and cascade methods.

Result: The method effectively improves SNN discriminability and overall performance while enhancing adversarial robustness. Experiments show significant performance gains benefiting from self-distillation stability.

Conclusion: The approach ingeniously exploits SNNs' temporal properties and provides insights for efficiently training high-performance SNNs without additional training overhead.

Abstract: Brain-inspired spiking neural networks (SNNs) promise to be a low-power
alternative to computationally intensive artificial neural networks (ANNs),
although performance gaps persist. Recent studies have improved the performance
of SNNs through knowledge distillation, but rely on large teacher models or
introduce additional training overhead. In this paper, we show that SNNs can be
naturally deconstructed into multiple submodels for efficient
self-distillation. We treat each timestep instance of the SNN as a submodel and
evaluate its output confidence, thus efficiently identifying the strong and the
weak. Based on this strong and weak relationship, we propose two efficient
self-distillation schemes: (1) \textbf{Strong2Weak}: During training, the
stronger "teacher" guides the weaker "student", effectively improving overall
performance. (2) \textbf{Weak2Strong}: The weak serve as the "teacher",
distilling the strong in reverse with underlying dark knowledge, again yielding
significant performance gains. For both distillation schemes, we offer flexible
implementations such as ensemble, simultaneous, and cascade distillation.
Experiments show that our method effectively improves the discriminability and
overall performance of the SNN, while its adversarial robustness is also
enhanced, benefiting from the stability brought by self-distillation. This
ingeniously exploits the temporal properties of SNNs and provides insight into
how to efficiently train high-performance SNNs.

</details>


### [66] [DISCO: Diversifying Sample Condensation for Efficient Model Evaluation](https://arxiv.org/abs/2510.07959)
*Alexander Rubinstein,Benjamin Raible,Martin Gubri,Seong Joon Oh*

Main category: cs.LG

TL;DR: DISCO is a method for efficient ML model evaluation that selects samples based on model disagreement rather than clustering, achieving state-of-the-art performance prediction with simpler implementation.


<details>
  <summary>Details</summary>
Motivation: Modern ML model evaluation is prohibitively expensive (thousands of GPU hours), reducing inclusivity, slowing innovation, and worsening environmental impact.

Method: DISCO selects top-k samples with greatest model disagreements using greedy, sample-wise statistics instead of global clustering, based on information-theoretically optimal selection rules.

Result: Achieves state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC benchmarks.

Conclusion: Model response diversity is more important than sample diversity for efficient evaluation; DISCO provides a conceptually simpler and more effective approach than clustering-based methods.

Abstract: Evaluating modern machine learning models has become prohibitively expensive.
Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.
Costly evaluation reduces inclusivity, slows the cycle of innovation, and
worsens environmental impact. The typical approach follows two steps. First,
select an anchor subset of data. Second, train a mapping from the accuracy on
this subset to the final test result. The drawback is that anchor selection
depends on clustering, which can be complex and sensitive to design choices. We
argue that promoting diversity among samples is not essential; what matters is
to select samples that $\textit{maximise diversity in model responses}$. Our
method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k
samples with the greatest model disagreements. This uses greedy, sample-wise
statistics rather than global clustering. The approach is conceptually simpler.
From a theoretical view, inter-model disagreement provides an
information-theoretically optimal rule for such greedy selection.
$\textbf{DISCO}$ shows empirical gains over prior methods, achieving
state-of-the-art results in performance prediction across MMLU, Hellaswag,
Winogrande, and ARC. Code is available here:
https://github.com/arubique/disco-public.

</details>


### [67] [PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation](https://arxiv.org/abs/2510.07964)
*Jiabei Cheng,Changxi Chi,Jingbo Zhou,Hongyi Xin,Jun Xia*

Main category: cs.LG

TL;DR: PRESCRIBE is a Bayesian deep learning framework that jointly estimates epistemic and aleatoric uncertainty for single-cell perturbation predictions, enabling confidence scoring that correlates with empirical accuracy and improves prediction reliability.


<details>
  <summary>Details</summary>
Motivation: Single-cell perturbation prediction requires assessing prediction reliability due to the stochastic nature of gene perturbation processes. Current methods lack joint uncertainty quantification for both model (epistemic) and data (aleatoric) uncertainty sources.

Method: Proposed PRESCRIBE - a multivariate deep evidential regression framework that jointly measures epistemic uncertainty (from gene similarity to training data) and aleatoric uncertainty (from training data quality).

Result: PRESCRIBE effectively estimates confidence scores that strongly correlate with empirical accuracy, enabling filtering of untrustworthy predictions and achieving over 3% steady accuracy improvements compared to baselines.

Conclusion: The framework provides reliable uncertainty quantification for single-cell perturbation predictions, improving prediction trustworthiness and enabling practical deployment by filtering unreliable results.

Abstract: In single-cell perturbation prediction, a central task is to forecast the
effects of perturbing a gene unseen in the training data. The efficacy of such
predictions depends on two factors: (1) the similarity of the target gene to
those covered in the training data, which informs model (epistemic)
uncertainty, and (2) the quality of the corresponding training data, which
reflects data (aleatoric) uncertainty. Both factors are critical for
determining the reliability of a prediction, particularly as gene perturbation
is an inherently stochastic biochemical process. In this paper, we propose
PRESCRIBE (PREdicting Single-Cell Response wIth Bayesian Estimation), a
multivariate deep evidential regression framework designed to measure both
sources of uncertainty jointly. Our analysis demonstrates that PRESCRIBE
effectively estimates a confidence score for each prediction, which strongly
correlates with its empirical accuracy. This capability enables the filtering
of untrustworthy results, and in our experiments, it achieves steady accuracy
improvements of over 3% compared to comparable baselines.

</details>


### [68] [Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A Case Study with CICERO-SCM](https://arxiv.org/abs/2510.07971)
*Oskar Bohn Lassen,Serio Angelo Maria Agriesti,Filipe Rodrigues,Francisco Camara Pereira*

Main category: cs.LG

TL;DR: A multi-agent reinforcement learning framework that integrates an efficient climate surrogate model to enable regional agents to learn climate policies under multi-gas dynamics, achieving 1000x faster inference and >100x training acceleration while maintaining policy fidelity.


<details>
  <summary>Details</summary>
Motivation: Climate policy studies need models that capture multiple greenhouse gases' combined effects on global temperature, but existing models are computationally expensive and difficult to embed in reinforcement learning systems.

Method: Developed a recurrent neural network architecture pretrained on 20,000 multi-gas emission pathways to surrogate the CICERO-SCM climate model, integrated within a multi-agent reinforcement learning framework.

Result: The surrogate model achieves near-simulator accuracy with global-mean temperature RMSE ≈ 0.0004K and approximately 1000x faster one-step inference. When used in climate-policy MARL, it accelerates end-to-end training by >100x while converging to the same optimal policies as the original simulator.

Conclusion: The framework bypasses computational bottlenecks without sacrificing policy fidelity, enabling large-scale multi-agent experiments across alternative climate-policy regimes with multi-gas dynamics and high-fidelity climate response.

Abstract: Climate policy studies require models that capture the combined effects of
multiple greenhouse gases on global temperature, but these models are
computationally expensive and difficult to embed in reinforcement learning. We
present a multi-agent reinforcement learning (MARL) framework that integrates a
high-fidelity, highly efficient climate surrogate directly in the environment
loop, enabling regional agents to learn climate policies under multi-gas
dynamics. As a proof of concept, we introduce a recurrent neural network
architecture pretrained on ($20{,}000$) multi-gas emission pathways to
surrogate the climate model CICERO-SCM. The surrogate model attains
near-simulator accuracy with global-mean temperature RMSE $\approx 0.0004
\mathrm{K}$ and approximately $1000\times$ faster one-step inference. When
substituted for the original simulator in a climate-policy MARL setting, it
accelerates end-to-end training by $>\!100\times$. We show that the surrogate
and simulator converge to the same optimal policies and propose a methodology
to assess this property in cases where using the simulator is intractable. Our
work allows to bypass the core computational bottleneck without sacrificing
policy fidelity, enabling large-scale multi-agent experiments across
alternative climate-policy regimes with multi-gas dynamics and high-fidelity
climate response.

</details>


### [69] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: Multi-Gossip Steps (MGS) bridges decentralized and centralized training, reducing performance gaps. Theoretical analysis shows MGS exponentially reduces optimization error but a non-negligible generalization gap remains compared to centralized training.


<details>
  <summary>Details</summary>
Motivation: Decentralized training is communication-efficient but suffers from degraded performance compared to centralized training. MGS helps bridge this gap, but the theoretical reasons for its effectiveness and whether the gap can be fully eliminated remained unknown.

Method: The authors use stability analysis to derive upper bounds on generalization error and excess error of MGS. They provide unified analysis of factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact on generalization under non-convex settings without bounded gradients assumption.

Result: 1) MGS reduces optimization error bound at exponential rate, tightening generalization error bound and enabling convergence to better solutions. 2) Even with infinite MGS, a non-negligible generalization gap remains compared to centralized mini-batch SGD. Experiments on CIFAR datasets support theoretical findings.

Conclusion: MGS significantly improves decentralized training performance but cannot fully eliminate the gap to centralized training. The theoretical analysis provides systematic understanding of MGS effectiveness and limitations, filling a critical gap in decentralized training theory.

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


### [70] [Fewer Weights, More Problems: A Practical Attack on LLM Pruning](https://arxiv.org/abs/2510.07985)
*Kazuki Egashira,Robin Staab,Thibaud Gloaguen,Mark Vero,Martin Vechev*

Main category: cs.LG

TL;DR: This paper reveals a novel security vulnerability in LLM pruning where adversaries can create models that appear benign but exhibit malicious behaviors after pruning, achieving high success rates in various attack scenarios.


<details>
  <summary>Details</summary>
Motivation: While LLM pruning has become popular for reducing memory footprint during inference, its security implications remain underexplored. The authors aim to investigate whether modern pruning methods can be maliciously exploited.

Method: The attack method involves computing a proxy metric to estimate parameter pruning likelihood, injecting malicious behavior into unlikely-to-be-pruned parameters, and then repairing the model using likely-to-be-pruned parameters to cancel out the behavior in unpruned models.

Result: Extensive evaluation on five models shows consistent malicious behaviors after pruning with vLLM methods (Magnitude, Wanda, SparseGPT), achieving success rates up to 95.7% for jailbreak, 98.7% for benign instruction refusal, and 99.5% for targeted content injection.

Conclusion: The research reveals a critical deployment-time security gap in model compression and underscores the urgent need for stronger security awareness in pruning techniques.

Abstract: Model pruning, i.e., removing a subset of model weights, has become a
prominent approach to reducing the memory footprint of large language models
(LLMs) during inference. Notably, popular inference engines, such as vLLM,
enable users to conveniently prune downloaded models before they are deployed.
While the utility and efficiency of pruning methods have improved
significantly, the security implications of pruning remain underexplored. In
this work, for the first time, we show that modern LLM pruning methods can be
maliciously exploited. In particular, an adversary can construct a model that
appears benign yet, once pruned, exhibits malicious behaviors. Our method is
based on the idea that the adversary can compute a proxy metric that estimates
how likely each parameter is to be pruned. With this information, the adversary
can first inject a malicious behavior into those parameters that are unlikely
to be pruned. Then, they can repair the model by using parameters that are
likely to be pruned, effectively canceling out the injected behavior in the
unpruned model. We demonstrate the severity of our attack through extensive
evaluation on five models; after any of the pruning in vLLM are applied
(Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious
behaviors in a diverse set of attack scenarios (success rates of up to $95.7\%$
for jailbreak, $98.7\%$ for benign instruction refusal, and $99.5\%$ for
targeted content injection). Our results reveal a critical deployment-time
security gap and underscore the urgent need for stronger security awareness in
model compression.

</details>


### [71] [DemandCast: Global hourly electricity demand forecasting](https://arxiv.org/abs/2510.08000)
*Kevin Steijn,Vamsi Priya Goli,Enrico Antonini*

Main category: cs.LG

TL;DR: A machine learning framework using XGBoost for electricity demand forecasting across diverse regions, integrating historical demand, weather, and socioeconomic data.


<details>
  <summary>Details</summary>
Motivation: To provide accurate and scalable electricity demand forecasts to support energy system planners and policymakers during the global energy transition.

Method: Uses XGBoost algorithm with historical electricity demand, weather, and socioeconomic variables; employs temporal data-splitting strategy for robust training and evaluation across multiple years and countries.

Result: The approach delivers accurate demand forecasts and provides valuable insights for energy planning.

Conclusion: The framework successfully enables robust electricity demand forecasting across diverse geographical regions, supporting energy transition challenges.

Abstract: This paper presents a machine learning framework for electricity demand
forecasting across diverse geographical regions using the gradient boosting
algorithm XGBoost. The model integrates historical electricity demand and
comprehensive weather and socioeconomic variables to predict normalized
electricity demand profiles. To enable robust training and evaluation, we
developed a large-scale dataset spanning multiple years and countries, applying
a temporal data-splitting strategy that ensures benchmarking of out-of-sample
performance. Our approach delivers accurate and scalable demand forecasts,
providing valuable insights for energy system planners and policymakers as they
navigate the challenges of the global energy transition.

</details>


### [72] [Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training](https://arxiv.org/abs/2510.08008)
*Ruizhe Wang,Yucheng Ding,Xiao Liu,Yaoxiang Wang,Peng Cheng,Baining Guo,Zhengjun Zha,Yeyun Gong*

Main category: cs.LG

TL;DR: This paper proposes checkpoint recycling - reusing pretrained LLM checkpoints by expanding their parameters (depth and width) and continuing training, achieving 10.66% accuracy gain over training from scratch with same compute budget.


<details>
  <summary>Details</summary>
Motivation: To efficiently reuse the 'sunk' computational cost invested in existing pretrained checkpoints that remain underutilized due to engineering constraints or limited model capacity.

Method: Proposes orthogonal growth method: interpositional layer copying for depth growth and expert duplication with injected noise for width growth in Mixture-of-Experts models. Determines optimal growth timing through scaling experiments.

Result: Scales approach to 70B parameter models with over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget.

Conclusion: Checkpoint recycling establishes a foundation for economically efficient large language model pretraining by leveraging prior computational investments.

Abstract: The rapidly increasing computational cost of pretraining Large Language
Models necessitates more efficient approaches. Numerous computational costs
have been invested in existing well-trained checkpoints, but many of them
remain underutilized due to engineering constraints or limited model capacity.
To efficiently reuse this "sunk" cost, we propose to recycle pretrained
checkpoints by expanding their parameter counts and continuing training. We
propose orthogonal growth method well-suited for converged Mixture-of-Experts
model: interpositional layer copying for depth growth and expert duplication
with injected noise for width growth. To determine the optimal timing for such
growth across checkpoints sequences, we perform comprehensive scaling
experiments revealing that the final accuracy has a strong positive correlation
with the amount of sunk cost, indicating that greater prior investment leads to
better performance. We scale our approach to models with 70B parameters and
over 1T training tokens, achieving 10.66% accuracy gain over training from
scratch under the same additional compute budget. Our checkpoint recycling
approach establishes a foundation for economically efficient large language
model pretraining.

</details>


### [73] [Accelerated Evolving Set Processes for Local PageRank Computation](https://arxiv.org/abs/2510.08010)
*Binbin Huang,Luo Luo,Yanghua Xiao,Deqing Yang,Baojian Zhou*

Main category: cs.LG

TL;DR: A novel framework using nested evolving set processes to accelerate Personalized PageRank computation with localized inexact proximal point iterations, achieving time complexity independent of graph size under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient algorithms for Personalized PageRank computation that can handle large graphs by reducing dependency on graph size through localized methods and nested evolving set processes.

Method: Uses nested evolving set processes with localized inexact proximal point iterations to solve simplified linear systems, requiring only ~O(1/√α) such systems to be solved.

Result: Achieves time complexity of min{~O(R²/ε²), ~O(m)} for ε-approximation, and ~O(R²/(√αε²)) when 1/ε² ≪ m, independent of graph size. Experimental validation shows significant early convergence on real-world graphs.

Conclusion: The framework successfully accelerates PPR computation with graph-size-independent complexity under certain conditions, resolving an open conjecture and demonstrating practical efficiency on real-world data.

Abstract: This work proposes a novel framework based on nested evolving set processes
to accelerate Personalized PageRank (PPR) computation. At each stage of the
process, we employ a localized inexact proximal point iteration to solve a
simplified linear system. We show that the time complexity of such localized
methods is upper bounded by $\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2),
\tilde{\mathcal{O}}(m)\}$ to obtain an $\epsilon$-approximation of the PPR
vector, where $m$ denotes the number of edges in the graph and $R$ is a
constant defined via nested evolving set processes. Furthermore, the algorithms
induced by our framework require solving only
$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$ such linear systems, where $\alpha$ is
the damping factor. When $1/\epsilon^2\ll m$, this implies the existence of an
algorithm that computes an $\ epsilon $-approximation of the PPR vector with an
overall time complexity of $\tilde{\mathcal{O}}\left(R^2 /
(\sqrt{\alpha}\epsilon^2)\right)$, independent of the underlying graph size.
Our result resolves an open conjecture from existing literature. Experimental
results on real-world graphs validate the efficiency of our methods,
demonstrating significant convergence in the early stages.

</details>


### [74] [Unsupervised Radio Map Construction in Mixed LoS/NLoS Indoor Environments](https://arxiv.org/abs/2510.08015)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: Proposes an HMM-based framework to recover data collection trajectories from channel propagation sequences without location calibration, achieving 0.65m localization accuracy in indoor environments.


<details>
  <summary>Details</summary>
Motivation: Eliminate costly calibration processes for radio map construction by recovering trajectories directly from channel propagation sequences, addressing both LOS and NLOS conditions.

Method: HMM-based framework jointly models conditional channel propagation (power, delay, angle) and user trajectory evolution using Gaussian-Markov model, with simultaneous optimization of propagation parameters, mobility model, and LOS/NLOS classification.

Result: Achieves 0.65m average localization accuracy in indoor environments covering both LOS and NLOS regions, outperforming conventional supervised methods like KNN, SVM, and DNN.

Conclusion: The proposed calibration-free method successfully constructs radio maps and enables accurate localization without requiring labeled CSI datasets, demonstrating superior performance over traditional supervised approaches.

Abstract: Radio maps are essential for enhancing wireless communications and
localization. However, existing methods for constructing radio maps typically
require costly calibration pro- cesses to collect location-labeled channel
state information (CSI) datasets. This paper aims to recover the data
collection trajectory directly from the channel propagation sequence,
eliminating the need for location calibration. The key idea is to employ a
hidden Markov model (HMM)-based framework to conditionally model the channel
propagation matrix, while simultaneously modeling the location correlation in
the trajectory. The primary challenges involve modeling the complex
relationship between channel propagation in multiple-input multiple-output
(MIMO) networks and geographical locations, and addressing both line-of-sight
(LOS) and non-line-of-sight (NLOS) indoor conditions. In this paper, we propose
an HMM-based framework that jointly characterizes the conditional propagation
model and the evolution of the user trajectory. Specifically, the channel
propagation in MIMO networks is modeled separately in terms of power, delay,
and angle, with distinct models for LOS and NLOS conditions. The user
trajectory is modeled using a Gaussian-Markov model. The parameters for channel
propagation, the mobility model, and LOS/NLOS classification are optimized
simultaneously. Experimental validation using simulated MIMO-Orthogonal
Frequency-Division Multiplexing (OFDM) networks with a multi-antenna uniform
linear arrays (ULA) configuration demonstrates that the proposed method
achieves an average localization accuracy of 0.65 meters in an indoor
environment, covering both LOS and NLOS regions. Moreover, the constructed
radio map enables localization with a reduced error compared to conventional
supervised methods, such as k-nearest neighbors (KNN), support vector machine
(SVM), and deep neural network (DNN).

</details>


### [75] [Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses](https://arxiv.org/abs/2510.08016)
*Stanisław Pawlak,Jan Dubiński,Daniel Marczak,Bartłomiej Twardowski*

Main category: cs.LG

TL;DR: This paper introduces Backdoor Vector (BV) as a framework for understanding backdoor attacks in model merging, proposes Sparse Backdoor Vector (SBV) to enhance attack effectiveness through merging, and develops Injection BV Subtraction (IBVS) as a lightweight defense against backdoor threats.


<details>
  <summary>Details</summary>
Motivation: Model merging is susceptible to backdoor attacks that can control model outputs, posing significant security risks. The authors aim to better understand these attacks and develop both more effective attacks and defenses.

Method: The paper proposes treating backdoor attacks as task vectors (BVs), calculates BVs as weight differences between backdoored and clean models, introduces SBV to combine multiple attacks, and develops IBVS defense that subtracts injection BVs without requiring assumptions.

Result: SBVs surpass prior backdoor attacks and are the first method to leverage merging to improve backdoor effectiveness. IBVS provides an effective lightweight defense that works even when the backdoor threat is unknown.

Conclusion: The BV framework provides new insights into backdoor attacks, SBV demonstrates that merging can enhance attack effectiveness, and IBVS offers a practical defense solution against backdoor threats in model merging scenarios.

Abstract: Model merging (MM) recently emerged as an effective method for combining
large deep learning models. However, it poses significant security risks.
Recent research shows that it is highly susceptible to backdoor attacks, which
introduce a hidden trigger into a single fine-tuned model instance that allows
the adversary to control the output of the final merged model at inference
time. In this work, we propose a simple framework for understanding backdoor
attacks by treating the attack itself as a task vector. $Backdoor\ Vector\
(BV)$ is calculated as the difference between the weights of a fine-tuned
backdoored model and fine-tuned clean model. BVs reveal new insights into
attacks understanding and a more effective framework to measure their
similarity and transferability. Furthermore, we propose a novel method that
enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\
(SBV)$ that combines multiple attacks into a single one. We identify the core
vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit
adversarial weaknesses in the base model. To counter this, we propose
$Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against
backdoors in MM. Our results show that SBVs surpass prior attacks and is the
first method to leverage merging to improve backdoor effectiveness. At the same
time, IBVS provides a lightweight, general defense that remains effective even
when the backdoor threat is entirely unknown.

</details>


### [76] [Do We Really Need Permutations? Impact of Width Expansion on Linear Mode Connectivity](https://arxiv.org/abs/2510.08023)
*Akira Ito,Masanori Yamada,Daiki Chijiwa,Atsutoshi Kumagai*

Main category: cs.LG

TL;DR: This paper shows that simply widening models is sufficient for achieving linear mode connectivity (LMC) without parameter permutations, using softmax temperature calibration, and explains this through layerwise exponentially weighted connectivity (LEWC).


<details>
  <summary>Details</summary>
Motivation: Prior research suggested that achieving LMC required both permutation search and wide models. This work challenges that by demonstrating that widening alone suffices with proper calibration.

Method: The authors use softmax temperature calibration and analyze intermediate layer outputs, introducing LEWC to explain how merged model outputs can be represented as exponentially weighted sums of original model layers.

Result: Empirical evidence shows that widened models achieve LMC without permutations when using appropriate softmax temperature calibration, contrary to previous beliefs.

Conclusion: Model widening not only facilitates nonlinear mode connectivity but also significantly increases the possibility of achieving linear mode connectivity, with LEWC providing the theoretical explanation.

Abstract: Recently, Ainsworth et al. empirically demonstrated that, given two
independently trained models, applying a parameter permutation that preserves
the input-output behavior allows the two models to be connected by a low-loss
linear path. When such a path exists, the models are said to achieve linear
mode connectivity (LMC). Prior studies, including Ainsworth et al., have
reported that achieving LMC requires not only an appropriate permutation search
but also sufficiently wide models (e.g., a 32 $\times$ width multiplier for
ResNet-20). This is broadly believed to be because increasing the model width
ensures a large enough space of candidate permutations, increasing the chance
of finding one that yields LMC. In this work, we empirically demonstrate that,
even without any permutations, simply widening the models is sufficient for
achieving LMC when using a suitable softmax temperature calibration. We further
explain why this phenomenon arises by analyzing intermediate layer outputs.
Specifically, we introduce layerwise exponentially weighted connectivity
(LEWC), which states that the output of each layer of the merged model can be
represented as an exponentially weighted sum of the outputs of the
corresponding layers of the original models. Consequently the merged model's
output matches that of an ensemble of the original models, which facilitates
LMC. To the best of our knowledge, this work is the first to show that widening
the model not only facilitates nonlinear mode connectivity, as suggested in
prior research, but also significantly increases the possibility of achieving
linear mode connectivity.

</details>


### [77] [Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters](https://arxiv.org/abs/2510.08059)
*Timon Klein,Piotr Minakowski,Sebastian Sager*

Main category: cs.LG

TL;DR: The paper introduces Subject-Conditioned Layer, an adaptive layer that replaces standard layers in neural networks to handle subject-specific distribution shifts in EEG decoding by decomposing weights into shared and personalized components.


<details>
  <summary>Details</summary>
Motivation: Subject-specific distribution shifts are a major challenge for developing foundation models in EEG decoding, as individual differences in brain signals can significantly impact model performance across different subjects.

Method: Proposes Subject-Conditioned Layer as a drop-in replacement for standard layers, decomposing weights into a shared subject-invariant component and lightweight low-rank corrections unique to each subject, separating general knowledge from personalized adaptation.

Result: Models with the proposed layer outperform both subject-agnostic models (shared weights only) and the average of individually trained subject-specific models, demonstrating improved robustness to subject shifts.

Conclusion: The Subject-Conditioned Layer provides a practical and scalable approach for building effective cross-subject foundation models for EEG decoding by enabling adaptive personalization while maintaining shared knowledge.

Abstract: Subject-specific distribution shifts represent an important obstacle to the
development of foundation models for EEG decoding. To address this, we propose
Subject-Conditioned Layer,, an adaptive layer designed as a drop-in replacement
for standard linear or convolutional layers in any neural network architecture.
Our layer captures subject-specific variability by decomposing its weights into
a shared, subject-invariant component and a lightweight, low-rank correction
unique to each subject. This explicit separation of general knowledge from
personalized adaptation allows existing models to become robust to subject
shifts. Empirically, models equipped with our layer outperform both a
shared-weight-only model (subject-agnostic model) and the average of
individually trained subject-specific models. Consequently, the
Subject-Conditioned Layer, offers a practical and scalable path towards
building effective cross-subject foundation models for EEG.

</details>


### [78] [Bayesian Decision Making around Experts](https://arxiv.org/abs/2510.08113)
*Daniel Jarne Ornia,Joel Dyer,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge*

Main category: cs.LG

TL;DR: This paper analyzes how learning agents can optimally incorporate expert data in Bayesian multi-armed bandits, developing information-theoretic algorithms for deciding when to trust and learn from experts.


<details>
  <summary>Details</summary>
Motivation: Complex learning agents are increasingly deployed alongside existing experts, but it remains unclear how learners should optimally incorporate expert data that may differ in structure from their own action-outcome experiences.

Method: The study examines Bayesian multi-armed bandits in offline settings (receiving expert dataset before interaction) and simultaneous settings (choosing whether to update beliefs based on own experience or expert outcomes). It formalizes expert data's influence on posterior beliefs and proposes information-directed rules for data source selection.

Result: The paper proves that pretraining on expert outcomes tightens information-theoretic regret bounds by the mutual information between expert data and optimal action. It develops strategies for inferring when to trust experts and safeguard against ineffective or compromised experts.

Conclusion: By quantifying the value of expert data, the framework provides practical, information-theoretic algorithms for agents to intelligently decide when to learn from others, addressing both offline and simultaneous learning scenarios with experts.

Abstract: Complex learning agents are increasingly deployed alongside existing experts,
such as human operators or previously trained agents. However, it remains
unclear how should learners optimally incorporate certain forms of expert data,
which may differ in structure from the learner's own action-outcome
experiences. We study this problem in the context of Bayesian multi-armed
bandits, considering: (i) offline settings, where the learner receives a
dataset of outcomes from the expert's optimal policy before interaction, and
(ii) simultaneous settings, where the learner must choose at each step whether
to update its beliefs based on its own experience, or based on the outcome
simultaneously achieved by an expert. We formalize how expert data influences
the learner's posterior, and prove that pretraining on expert outcomes tightens
information-theoretic regret bounds by the mutual information between the
expert data and the optimal action. For the simultaneous setting, we propose an
information-directed rule where the learner processes the data source that
maximizes their one-step information gain about the optimal action. Finally, we
propose strategies for how the learner can infer when to trust the expert and
when not to, safeguarding the learner for the cases where the expert is
ineffective or compromised. By quantifying the value of expert data, our
framework provides practical, information-theoretic algorithms for agents to
intelligently decide when to learn from others.

</details>


### [79] [Approximate Domain Unlearning for Vision-Language Models](https://arxiv.org/abs/2510.08132)
*Kodai Kawamura,Yuta Goto,Rintaro Yanagi,Hirokatsu Kataoka,Go Irie*

Main category: cs.LG

TL;DR: This paper introduces Approximate Domain Unlearning (ADU) for Vision-Language Models, which aims to selectively remove knowledge of specific domains (e.g., illustrations) while preserving performance on other domains (e.g., real images), addressing limitations of class-based unlearning methods.


<details>
  <summary>Details</summary>
Motivation: Current VLMs retain unnecessary information beyond downstream task requirements, raising efficiency and security concerns. Class unlearning is insufficient for practical scenarios like autonomous driving where distinguishing between domains (real vs. illustrated cars) is crucial for safety.

Method: The proposed approach explicitly disentangles domain distributions and adaptively captures instance-specific domain information to overcome the challenge of highly entangled domain features in pre-trained VLMs.

Result: Extensive experiments demonstrate that the proposed method outperforms baseline approaches built upon VLM tuning techniques for domain unlearning tasks.

Conclusion: The ADU framework enables practical and fine-grained unlearning in VLMs, paving the way for more selective knowledge removal while maintaining overall model performance across different domains.

Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization
capabilities, enabling them to recognize a wide range of objects across diverse
domains without additional training. However, they often retain irrelevant
information beyond the requirements of specific downstream tasks, raising
concerns about computational efficiency and potential information leakage. This
has motivated growing interest in approximate unlearning, which aims to
selectively remove unnecessary knowledge while preserving overall model
performance. Existing approaches to approximate unlearning have primarily
focused on class unlearning, where a VLM is retrained to fail to recognize
specified object classes while maintaining accuracy for others. However, merely
forgetting object classes is often insufficient in practical applications. For
instance, an autonomous driving system should accurately recognize real cars
while avoiding misrecognition of illustrated cars depicted in roadside
advertisements as real cars, which could be hazardous. In this paper, we
introduce Approximate Domain Unlearning (ADU), a novel problem setting that
requires reducing recognition accuracy for images from specified domains (e.g.,
illustration) while preserving accuracy for other domains (e.g., real). ADU
presents new technical challenges: due to the strong domain generalization
capability of pre-trained VLMs, domain distributions are highly entangled in
the feature space, making naive approaches based on penalizing target domains
ineffective. To tackle this limitation, we propose a novel approach that
explicitly disentangles domain distributions and adaptively captures
instance-specific domain information. Extensive experiments show that our
approach outperforms baselines built upon VLM tuning techniques, paving the way
for practical and fine-grained unlearning in VLMs. Code:
https://kodaikawamura.github.io/Domain_Unlearning/.

</details>


### [80] [Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning](https://arxiv.org/abs/2510.08141)
*Chen Wang,Zhaochun Li,Jionghao Bai,Yuzhi Zhang,Shisheng Cui,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: AEPO solves entropy collapse in RL fine-tuning by replacing entropy bonuses with temperature-adjusted policy gradients, enabling precise entropy control and revealing a non-monotonic relationship between entropy and performance.


<details>
  <summary>Details</summary>
Motivation: Address entropy collapse in GRPO where entropy monotonically decreases, exploration vanishes, and policies converge prematurely, while existing methods only partially alleviate this with bias and instability.

Method: Proposes Arbitrary Entropy Policy Optimization (AEPO) using three key designs: policy gradient as regularization, distribution as regularization, and REINFORCE as regularization, with temperature regulation to stabilize entropy.

Result: AEPO stabilizes entropy at arbitrary target levels, eliminates collapse in GRPO, reveals non-monotonic entropy-performance relationship, and provides a broader RFT paradigm with superior target distributions as regularizers.

Conclusion: AEPO effectively resolves entropy collapse in reinforcement fine-tuning, enables precise entropy control, clarifies the entropy-exploration-performance relationship, and generalizes to broader regularization paradigms.

Abstract: Reinforcement finetuning (RFT) is essential for enhancing the reasoning
capabilities of large language models (LLM), yet the widely adopted Group
Relative Policy Optimization (GRPO) suffers from entropy collapse, where
entropy monotonically decreases, exploration vanishes, and policies converge
prematurely. Existing entropy-regularized methods only partially alleviate this
issue while introducing bias and instability, leaving entropy control
unresolved and the connection between entropy, exploration, and performance
unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which
eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy
gradient on temperature-adjusted distributions and stabilizing entropy through
temperature regulation. AEPO integrates three key designs: policy gradient as
regularization, distribution as regularization, and REINFORCE as
regularization, enabling precise entropy control without distorting
optimization. Experiments demonstrate three major contributions: AEPO (1)
stabilizes entropy at arbitrary target levels, effectively removing collapse in
GRPO; (2) reveals a non-monotonic relation where performance first improves
then declines with increasing entropy, clarifying the link between entropy,
exploration, and reasoning; and (3) generalizes beyond entropy, providing a
broader RFT paradigm where superior target distributions can serve as REINFORCE
regularizers.

</details>


### [81] [Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning](https://arxiv.org/abs/2510.08146)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: An entropy-based framework using Shannon entropy from token-level logprobs enables early stopping in LLM reasoning tasks, achieving 25-50% computational savings while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve token efficiency in large language models during reasoning tasks by exploiting emergent confidence awareness in advanced reasoning models.

Method: Uses Shannon entropy from token-level logprobs as a confidence signal for early stopping, with entropy thresholds calculated using few examples from reasoning datasets.

Result: Achieves 25-50% computational savings while preserving task accuracy, with consistent performance across reasoning-optimized model families.

Conclusion: Entropy-based confidence calibration is an emergent property of advanced post-training optimization in modern reasoning models, distinguishing them from standard instruction-tuned and pre-trained models.

Abstract: We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they've gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.

</details>


### [82] [Unsupervised Multi-Source Federated Domain Adaptation under Domain Diversity through Group-Wise Discrepancy Minimization](https://arxiv.org/abs/2510.08150)
*Larissa Reichart,Cem Ata Baykara,Ali Burak Ünal,Mete Akgün,Harlin Lee*

Main category: cs.LG

TL;DR: GALA is a scalable federated unsupervised multi-source domain adaptation framework that uses inter-group discrepancy minimization and temperature-controlled centroid weighting to handle many heterogeneous source domains efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing distributed UMDA methods don't scale well with many source domains, becoming computationally expensive or unstable when dealing with high domain diversity.

Method: Proposes two key components: (1) inter-group discrepancy minimization that approximates full pairwise domain alignment without quadratic computation, and (2) temperature-controlled centroid-based weighting that dynamically prioritizes source domains based on target alignment.

Result: GALA achieves competitive or state-of-the-art results on standard benchmarks and significantly outperforms prior methods in diverse multi-source settings where others fail to converge. Also introduced Digit-18 benchmark for evaluation.

Conclusion: GALA enables stable and parallelizable training across large numbers of heterogeneous sources, making it practical for real-world UMDA applications with many domains.

Abstract: Unsupervised multi-source domain adaptation (UMDA) aims to learn models that
generalize to an unlabeled target domain by leveraging labeled data from
multiple, diverse source domains. While distributed UMDA methods address
privacy constraints by avoiding raw data sharing, existing approaches typically
assume a small number of sources and fail to scale effectively. Increasing the
number of heterogeneous domains often makes existing methods impractical,
leading to high computational overhead or unstable performance. We propose
GALA, a scalable and robust federated UMDA framework that introduces two key
components: (1) a novel inter-group discrepancy minimization objective that
efficiently approximates full pairwise domain alignment without quadratic
computation; and (2) a temperature-controlled, centroid-based weighting
strategy that dynamically prioritizes source domains based on alignment with
the target. Together, these components enable stable and parallelizable
training across large numbers of heterogeneous sources. To evaluate performance
in high-diversity scenarios, we introduce Digit-18, a new benchmark comprising
18 digit datasets with varied synthetic and real-world domain shifts. Extensive
experiments show that GALA consistently achieves competitive or
state-of-the-art results on standard benchmarks and significantly outperforms
prior methods in diverse multi-source settings where others fail to converge.

</details>


### [83] [Beyond Sub-6 GHz: Leveraging mmWave Wi-Fi for Gait-Based Person Identification](https://arxiv.org/abs/2510.08160)
*Nabeel Nisar Bhat,Maksim Karnaukh,Jakob Struye,Rafael Berkvens,Jeroen Famaey*

Main category: cs.LG

TL;DR: This paper presents the first comparative study between sub-6 GHz and mmWave Wi-Fi signals for person identification using COTS devices, showing mmWave achieves 91.2% accuracy on 20 individuals with 10 Hz sampling.


<details>
  <summary>Details</summary>
Motivation: Person identification is crucial for intelligent human-computer interaction. While existing work focuses on sub-6 GHz Wi-Fi, mmWave offers finer spatial resolution but its comparative advantages for person identification remain unexplored.

Method: Used synchronized measurements from sub-6 GHz and mmWave Wi-Fi bands in indoor environment with identical training pipelines and model configurations. Applied end-to-end deep learning with effective background subtraction.

Result: mmWave Wi-Fi signals achieved 91.2% identification accuracy on 20 individuals even at low sampling rates (10 Hz) when combined with background subtraction.

Conclusion: mmWave Wi-Fi signals demonstrate superior performance for person identification compared to sub-6 GHz frequencies, offering high accuracy with low sampling rates in indoor environments.

Abstract: Person identification plays a vital role in enabling intelligent,
personalized, and secure human-computer interaction. Recent research has
demonstrated the feasibility of leveraging Wi-Fi signals for passive person
identification using a person's unique gait pattern. Although most existing
work focuses on sub-6 GHz frequencies, the emergence of mmWave offers new
opportunities through its finer spatial resolution, though its comparative
advantages for person identification remain unexplored. This work presents the
first comparative study between sub-6 GHz and mmWave Wi-Fi signals for person
identification with commercial off-the-shelf (COTS) Wi-Fi, using a novel
dataset of synchronized measurements from the two frequency bands in an indoor
environment. To ensure a fair comparison, we apply identical training pipelines
and model configurations across both frequency bands. Leveraging end-to-end
deep learning, we show that even at low sampling rates (10 Hz), mmWave Wi-Fi
signals can achieve high identification accuracy (91.2% on 20 individuals) when
combined with effective background subtraction.

</details>


### [84] [Bidirectional Representations Augmented Autoregressive Biological Sequence Generation:Application in De Novo Peptide Sequencing](https://arxiv.org/abs/2510.08169)
*Xiang Zhang,Jiaqi Wei,Zijie Qiu,Sheng Xu,Zhi Jin,ZhiQiang Gao,Nanqing Dong,Siqi Sun*

Main category: cs.LG

TL;DR: A hybrid AR-NAR framework for biological sequence generation that combines autoregressive stability with non-autoregressive bidirectional context, achieving superior performance on de novo peptide sequencing.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models fail to capture bidirectional dependencies in biological sequences, while non-autoregressive models lack generative coherence. A hybrid approach is needed to leverage the strengths of both.

Method: Shared encoder with two decoders: NAR decoder learns bidirectional features, AR decoder generates sequences using cross-decoder attention to integrate bidirectional context. Training uses importance annealing and gradient blocking.

Result: Substantially outperforms AR and NAR baselines on nine-species peptide sequencing benchmark, harmonizing AR stability with NAR contextual awareness.

Conclusion: The hybrid framework advances biological sequence modeling by enhancing AR models with bidirectional understanding, providing a novel architectural paradigm for complex sequence generation.

Abstract: Autoregressive (AR) models, common in sequence generation, are limited in
many biological tasks such as de novo peptide sequencing and protein modeling
by their unidirectional nature, failing to capture crucial global bidirectional
token dependencies. Non-Autoregressive (NAR) models offer holistic,
bidirectional representations but face challenges with generative coherence and
scalability. To transcend this, we propose a hybrid framework enhancing AR
generation by dynamically integrating rich contextual information from
non-autoregressive mechanisms. Our approach couples a shared input encoder with
two decoders: a non-autoregressive one learning latent bidirectional biological
features, and an AR decoder synthesizing the biological sequence by leveraging
these bidirectional features. A novel cross-decoder attention module enables
the AR decoder to iteratively query and integrate these bidirectional features,
enriching its predictions. This synergy is cultivated via a tailored training
strategy with importance annealing for balanced objectives and cross-decoder
gradient blocking for stable, focused learning. Evaluations on a demanding
nine-species benchmark of de novo peptide sequencing show that our model
substantially surpasses AR and NAR baselines. It uniquely harmonizes AR
stability with NAR contextual awareness, delivering robust, superior
performance on diverse downstream data. This research advances biological
sequence modeling techniques and contributes a novel architectural paradigm for
augmenting AR models with enhanced bidirectional understanding for complex
sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.

</details>


### [85] [Long-tailed Recognition with Model Rebalancing](https://arxiv.org/abs/2510.08177)
*Jiaan Luo,Feng Hong,Qiang Hu,Xiaofeng Cao,Feng Liu,Jiangchao Yao*

Main category: cs.LG

TL;DR: MORE is a novel framework that addresses long-tailed recognition by directly rebalancing the model's parameter space through a low-rank parameter component, guided by tailored loss and sinusoidal reweighting, without increasing model complexity or inference costs.


<details>
  <summary>Details</summary>
Motivation: Long-tailed recognition is challenging in deep learning due to skewed class distributions that prevent model generalization to tail classes. Existing methods from data augmentation, loss rebalancing, and decoupled training struggle with consistent improvement in broad scenarios like multi-label long-tailed recognition.

Method: MORE introduces a low-rank parameter component to mediate parameter space allocation, guided by a tailored loss and sinusoidal reweighting schedule. This approach directly rebalances the model's parameter space without increasing overall model complexity or inference costs.

Result: Extensive experiments on diverse long-tailed benchmarks (multi-class and multi-label tasks) show that MORE significantly improves generalization, particularly for tail classes, and effectively complements existing imbalance mitigation methods.

Conclusion: MORE demonstrates potential as a robust plug-and-play module in long-tailed settings, providing consistent improvements across various scenarios without additional computational overhead.

Abstract: Long-tailed recognition is ubiquitous and challenging in deep learning and
even in the downstream finetuning of foundation models, since the skew class
distribution generally prevents the model generalization to the tail classes.
Despite the promise of previous methods from the perspectives of data
augmentation, loss rebalancing and decoupled training etc., consistent
improvement in the broad scenarios like multi-label long-tailed recognition is
difficult. In this study, we dive into the essential model capacity impact
under long-tailed context, and propose a novel framework, Model Rebalancing
(MORE), which mitigates imbalance by directly rebalancing the model's parameter
space. Specifically, MORE introduces a low-rank parameter component to mediate
the parameter space allocation guided by a tailored loss and sinusoidal
reweighting schedule, but without increasing the overall model complexity or
inference costs. Extensive experiments on diverse long-tailed benchmarks,
spanning multi-class and multi-label tasks, demonstrate that MORE significantly
improves generalization, particularly for tail classes, and effectively
complements existing imbalance mitigation methods. These results highlight
MORE's potential as a robust plug-and-play module in long-tailed settings.

</details>


### [86] [Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data](https://arxiv.org/abs/2510.08179)
*Feng Hong,Yu Huang,Zihua Zhao,Zhihan Zhou,Jiangchao Yao,Dongsheng Li,Ya Zhang,Yanfeng Wang*

Main category: cs.LG

TL;DR: D-SINK is a framework that combines 'weak' auxiliary models specialized for class imbalance and label noise through optimal transport-based distillation to achieve dual robustness in learning from long-tailed noisy data.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets often have both class imbalance and label noise, but existing methods for each issue conflict when combined. The paper proposes leveraging complementary strengths of specialized auxiliary models since these issues operate at different granularities.

Method: Dual-granularity Sinkhorn Distillation (D-SINK) uses optimal transport-optimized surrogate label allocation to align the target model's sample-level predictions with a noise-robust auxiliary and class distributions with an imbalance-robust auxiliary.

Result: Extensive experiments on benchmark datasets show that D-SINK significantly improves robustness and achieves strong empirical performance in learning from long-tailed noisy data.

Conclusion: The proposed framework successfully integrates complementary insights from specialized auxiliary models to handle both class imbalance and label noise simultaneously, demonstrating effective dual robustness.

Abstract: Real-world datasets for deep learning frequently suffer from the co-occurring
challenges of class imbalance and label noise, hindering model performance.
While methods exist for each issue, effectively combining them is non-trivial,
as distinguishing genuine tail samples from noisy data proves difficult, often
leading to conflicting optimization strategies. This paper presents a novel
perspective: instead of primarily developing new complex techniques from
scratch, we explore synergistically leveraging well-established, individually
'weak' auxiliary models - specialized for tackling either class imbalance or
label noise but not both. This view is motivated by the insight that class
imbalance (a distributional-level concern) and label noise (a sample-level
concern) operate at different granularities, suggesting that robustness
mechanisms for each can in principle offer complementary strengths without
conflict. We propose Dual-granularity Sinkhorn Distillation (D-SINK), a novel
framework that enhances dual robustness by distilling and integrating
complementary insights from such 'weak', single-purpose auxiliary models.
Specifically, D-SINK uses an optimal transport-optimized surrogate label
allocation to align the target model's sample-level predictions with a
noise-robust auxiliary and its class distributions with an imbalance-robust
one. Extensive experiments on benchmark datasets demonstrate that D-SINK
significantly improves robustness and achieves strong empirical performance in
learning from long-tailed noisy data.

</details>


### [87] [FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption](https://arxiv.org/abs/2510.08217)
*Justus Viga,Penelope Mueck,Alexander Löser,Torben Weis*

Main category: cs.LG

TL;DR: This paper introduces a new dataset for ship fuel consumption prediction, establishes standardized benchmarks for tabular and time-series regression, and explores in-context learning using the TabPFN foundation model, showing improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of ship fuel consumption is crucial for economic efficiency and environmental sustainability in the shipping industry, but current approaches are hindered by heterogeneous methodologies and limited high-quality datasets.

Method: The authors (1) release a new dataset with operational and environmental data from three ships, (2) define standardized benchmarks for tabular and time-series regression, and (3) investigate in-context learning using the TabPFN foundation model for ship consumption modeling.

Result: Results show strong performance across all evaluated models, with models incorporating environmental conditions consistently outperforming simple polynomial baselines. TabPFN slightly outperforms other techniques, and including temporal context improves accuracy.

Conclusion: The study demonstrates the feasibility of onboard, data-driven fuel prediction and highlights the potential of foundation models with in-context learning capabilities for tabular prediction in maritime applications.

Abstract: In the shipping industry, fuel consumption and emissions are critical factors
due to their significant impact on economic efficiency and environmental
sustainability. Accurate prediction of ship fuel consumption is essential for
further optimization of maritime operations. However, heterogeneous
methodologies and limited high-quality datasets hinder direct comparison of
modeling approaches. This paper makes three key contributions: (1) we introduce
and release a new dataset
(https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational
and environmental data from three ships; (2) we define a standardized benchmark
covering tabular regression and time-series regression (3) we investigate the
application of in-context learning for ship consumption modeling using the
TabPFN foundation model - a first in this domain to our knowledge. Our results
demonstrate strong performance across all evaluated models, supporting the
feasibility of onboard, data-driven fuel prediction. Models incorporating
environmental conditions consistently outperform simple polynomial baselines
relying solely on vessel speed. TabPFN slightly outperforms other techniques,
highlighting the potential of foundation models with in-context learning
capabilities for tabular prediction. Furthermore, including temporal context
improves accuracy.

</details>


### [88] [Expressive Value Learning for Scalable Offline Reinforcement Learning](https://arxiv.org/abs/2510.08218)
*Nicolas Espinosa-Dice,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: EVOR is a scalable offline RL method that uses expressive value functions and flow matching to avoid BPTT and policy distillation, enabling efficient inference-time policy extraction via rejection sampling.


<details>
  <summary>Details</summary>
Motivation: Current offline RL methods face scalability issues due to reliance on computationally expensive BPTT or error-prone policy distillation, limiting their application to complex robotics tasks.

Method: EVOR integrates expressive policies and value functions, learning optimal regularized Q-functions via flow matching during training, and performs inference-time policy extraction through rejection sampling against the expressive value function.

Result: Empirical evaluation shows EVOR outperforms baseline methods on diverse offline RL tasks, demonstrating the benefits of expressive value learning integration.

Conclusion: EVOR provides a scalable offline RL approach that avoids computational bottlenecks of existing methods while maintaining performance across various tasks.

Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make
sequences of decisions. However, RL has yet to be fully leveraged in robotics,
principally due to its lack of scalability. Offline RL offers a promising
avenue by training agents on large, diverse datasets, avoiding the costly
real-world interactions of online RL. Scaling offline RL to increasingly
complex datasets requires expressive generative models such as diffusion and
flow matching. However, existing methods typically depend on either
backpropagation through time (BPTT), which is computationally prohibitive, or
policy distillation, which introduces compounding errors and limits scalability
to larger base policies. In this paper, we consider the question of how to
develop a scalable offline RL approach without relying on distillation or
backpropagation through time. We introduce Expressive Value Learning for
Offline Reinforcement Learning (EVOR): a scalable offline RL approach that
integrates both expressive policies and expressive value functions. EVOR learns
an optimal, regularized Q-function via flow matching during training. At
inference-time, EVOR performs inference-time policy extraction via rejection
sampling against the expressive value function, enabling efficient
optimization, regularization, and compute-scalable search without retraining.
Empirically, we show that EVOR outperforms baselines on a diverse set of
offline RL tasks, demonstrating the benefit of integrating expressive value
learning into offline RL.

</details>


### [89] [Post-hoc Stochastic Concept Bottleneck Models](https://arxiv.org/abs/2510.08219)
*Wiktor Jan Hoffmann,Sonia Laguna,Moritz Vandenhirtz,Emanuele Palumbo,Julia E. Vogt*

Main category: cs.LG

TL;DR: PSCBMs enhance pre-trained CBMs by adding a small covariance module to model concept dependencies, improving accuracy and intervention performance without retraining the backbone model.


<details>
  <summary>Details</summary>
Motivation: Existing CBMs that model concept dependencies require full retraining, which is impractical when original data or compute resources are limited.

Method: Add a lightweight covariance-prediction module to pre-trained CBMs to create multivariate normal distributions over concepts, using two training strategies.

Result: PSCBMs consistently match or improve concept and target accuracy over standard CBMs, and perform significantly better under interventions while being more efficient than full retraining.

Conclusion: PSCBMs provide an efficient way to enhance CBMs with concept dependency modeling, improving interpretability and intervention performance without the computational cost of retraining.

Abstract: Concept Bottleneck Models (CBMs) are interpretable models that predict the
target variable through high-level human-understandable concepts, allowing
users to intervene on mispredicted concepts to adjust the final output. While
recent work has shown that modeling dependencies between concepts can improve
CBM performance, especially under interventions, such approaches typically
require retraining the entire model, which may be infeasible when access to the
original data or compute is limited. In this paper, we introduce Post-hoc
Stochastic Concept Bottleneck Models (PSCBMs), a lightweight method that
augments any pre-trained CBM with a multivariate normal distribution over
concepts by adding only a small covariance-prediction module, without
retraining the backbone model. We propose two training strategies and show on
real-world data that PSCBMs consistently match or improve both concept and
target accuracy over standard CBMs at test time. Furthermore, we show that due
to the modeling of concept dependencies, PSCBMs perform much better than CBMs
under interventions, while remaining far more efficient than retraining a
similar stochastic model from scratch.

</details>


### [90] [Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning](https://arxiv.org/abs/2510.08226)
*Michal Koren,Or Peretz,Tai Dinh,Philip S. Yu*

Main category: cs.LG

TL;DR: The paper introduces UAMDP, a framework combining Bayesian forecasting, posterior-sampling RL, and CVaR-constrained planning for risk-aware sequential decision-making in volatile environments like trading and inventory control.


<details>
  <summary>Details</summary>
Motivation: Sequential decisions in volatile, high-stakes settings require principled uncertainty management beyond just maximizing expected return, addressing structural uncertainty and economic volatility.

Method: UAMDP couples Bayesian forecasting, posterior-sampling reinforcement learning via Thompson sampling, and planning under conditional value-at-risk (CVaR) constraints in a closed loop where the agent updates beliefs over latent dynamics.

Result: UAMDP improves long-horizon forecasting accuracy (RMSE decreases by up to 25% and sMAPE by 32%), increases trading Sharpe ratio from 1.54 to 1.74 while roughly halving maximum drawdown in high-frequency trading and retail inventory control domains.

Conclusion: Integrating calibrated probabilistic modeling, exploration aligned with posterior uncertainty, and risk-aware control yields a robust, generalizable approach to safer and more profitable sequential decision-making.

Abstract: Sequential decisions in volatile, high-stakes settings require more than
maximizing expected return; they require principled uncertainty management.
This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a
unified framework that couples Bayesian forecasting, posterior-sampling
reinforcement learning, and planning under a conditional value-at-risk (CVaR)
constraint. In a closed loop, the agent updates its beliefs over latent
dynamics, samples plausible futures via Thompson sampling, and optimizes
policies subject to preset risk tolerances. We establish regret bounds that
converge to the Bayes-optimal benchmark under standard regularity conditions.
We evaluate UAMDP in two domains-high-frequency equity trading and retail
inventory control-both marked by structural uncertainty and economic
volatility. Relative to strong deep learning baselines, UAMDP improves
long-horizon forecasting accuracy (RMSE decreases by up to 25\% and sMAPE by
32\%), and these gains translate into economic performance: the trading Sharpe
ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These
results show that integrating calibrated probabilistic modeling, exploration
aligned with posterior uncertainty, and risk-aware control yields a robust,
generalizable approach to safer and more profitable sequential decision-making.

</details>


### [91] [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Petr Molodyk,Bo Yuan,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: DMPO is a reinforcement learning method designed specifically for diffusion large language models to enhance reasoning capabilities by matching policy distributions to optimal reward-tilted ones through cross-entropy optimization.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs offer higher inference throughput than autoregressive LLMs but need RL to achieve comparable performance on reasoning tasks. Existing RL algorithms are not well-suited for dLLMs' unique characteristics.

Method: Proposes Distribution Matching Policy Optimization (DMPO) using cross-entropy optimization to match dLLM policy distribution to optimal reward-tilted distribution, with novel weight baseline subtraction technique to address small batch size challenges.

Result: DMPO achieves up to 42.9% accuracy improvement over previous SOTA baselines and 55.8% over base model on multiple reasoning benchmarks without supervised fine-tuning.

Conclusion: The distribution matching framework is highly effective for enhancing dLLMs' reasoning capabilities, with DMPO demonstrating superior performance compared to existing methods.

Abstract: Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.

</details>


### [92] [The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models](https://arxiv.org/abs/2510.08236)
*Konrad Löhr,Shuzhou Yuan,Michael Färber*

Main category: cs.LG

TL;DR: This study investigates political bias and stereotype propagation in eight major LLMs using the Political Compass Test, revealing consistent left-leaning alignment across models and finding that implicit stereotypes through language variation are more pronounced than explicit ones.


<details>
  <summary>Details</summary>
Motivation: Understanding political biases in LLMs is crucial as they increasingly influence information dissemination and decision-making, potentially impacting public opinion and democratic processes.

Method: Used the two-dimensional Political Compass Test (PCT) to assess inherent political leanings, employed persona prompting with PCT to explore explicit stereotypes, and evaluated implicit stereotypes using multilingual PCT versions.

Result: All investigated models showed consistent left-leaning political alignment. Implicit stereotypes elicited through language variation were more pronounced than explicit ones, and most models showed notable alignment between implicit and explicit stereotypes.

Conclusion: The study reveals complex interplay between political bias and stereotypes in LLMs, suggesting models have some awareness of their inherent biases, which underscores the importance of addressing these issues in AI development.

Abstract: Large Language Models (LLMs) are increas- ingly integral to information
dissemination and decision-making processes. Given their grow- ing societal
influence, understanding potential biases, particularly within the political
domain, is crucial to prevent undue influence on public opinion and democratic
processes. This work investigates political bias and stereotype propa- gation
across eight prominent LLMs using the two-dimensional Political Compass Test
(PCT). Initially, the PCT is employed to assess the in- herent political
leanings of these models. Sub- sequently, persona prompting with the PCT is
used to explore explicit stereotypes across vari- ous social dimensions. In a
final step, implicit stereotypes are uncovered by evaluating mod- els with
multilingual versions of the PCT. Key findings reveal a consistent left-leaning
polit- ical alignment across all investigated models. Furthermore, while the
nature and extent of stereotypes vary considerably between models, implicit
stereotypes elicited through language variation are more pronounced than those
iden- tified via explicit persona prompting. Interest- ingly, for most models,
implicit and explicit stereotypes show a notable alignment, suggest- ing a
degree of transparency or "awareness" regarding their inherent biases. This
study un- derscores the complex interplay of political bias and stereotypes in
LLMs.

</details>


### [93] [Opponent Shaping in LLM Agents](https://arxiv.org/abs/2510.08255)
*Marta Emili Garcia Segura,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: This paper investigates whether LLM-based agents can perform opponent shaping - influencing other agents' learning dynamics through interaction alone. The authors introduce ShapeLLM, an adaptation of model-free opponent shaping methods for transformer-based agents, and demonstrate its effectiveness in both competitive and cooperative game environments.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly deployed as autonomous agents in real-world environments, multi-agent interactions become inevitable. Understanding strategic behavior in such systems is crucial, particularly whether LLM agents can shape others' learning dynamics through interaction alone, similar to reinforcement learning agents.

Method: The authors introduce ShapeLLM, an adaptation of model-free opponent shaping methods specifically tailored for transformer-based LLM agents. They test this approach across diverse game-theoretic environments including competitive games (Iterated Prisoner's Dilemma, Matching Pennies, Chicken) and cooperative games (Iterated Stag Hunt, cooperative Prisoner's Dilemma).

Result: LLM agents successfully guided opponents toward exploitable equilibria in competitive games and promoted coordination while improving collective welfare in cooperative games. The findings demonstrate that LLM agents can both shape and be shaped through interaction.

Conclusion: Opponent shaping is established as a key dimension of multi-agent LLM research, showing that LLM agents can effectively influence co-players' learning dynamics across diverse strategic environments, bridging capabilities previously associated mainly with reinforcement learning agents.

Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous
agents in real-world environments. As these deployments scale, multi-agent
interactions become inevitable, making it essential to understand strategic
behavior in such systems. A central open question is whether LLM agents, like
reinforcement learning agents, can shape the learning dynamics and influence
the behavior of others through interaction alone. In this paper, we present the
first investigation of opponent shaping (OS) with LLM-based agents. Existing OS
algorithms cannot be directly applied to LLMs, as they require higher-order
derivatives, face scalability constraints, or depend on architectural
components that are absent in transformers. To address this gap, we introduce
ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based
agents. Using ShapeLLM, we examine whether LLM agents can influence co-players'
learning dynamics across diverse game-theoretic environments. We demonstrate
that LLM agents can successfully guide opponents toward exploitable equilibria
in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and
Chicken) and promote coordination and improve collective welfare in cooperative
games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).
Our findings show that LLM agents can both shape and be shaped through
interaction, establishing opponent shaping as a key dimension of multi-agent
LLM research.

</details>


### [94] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: Mix- and MoE-DPO extends Direct Preference Optimization (DPO) with mixture models and mixture-of-experts architectures using stochastic variational inference, enabling more expressive and adaptable preference alignment for large language models.


<details>
  <summary>Details</summary>
Motivation: Standard DPO relies on a single monolithic model, which limits expressivity in multi-task settings and adaptability to diverse preference distributions. There's a need for more flexible approaches that can handle heterogeneous preferences and enable specialization.

Method: Proposes Mix- and MoE-DPO framework using soft mixture models and mixture-of-experts architectures with stochastic variational inference. Introduces latent-variable model over expert assignments and optimizes variational evidence lower bound (ELBO) for stable learning of specialized expert policies from preference data.

Result: The framework provides three key advantages: generalization via universal function approximation through mixtures, reward and policy specialization through expert components tailored to distinct preference modes, and contextual alignment through input-dependent soft gating. Supports both shared base architectures with expert-specific policy heads and fully independent expert models.

Conclusion: Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment, validated on various model sizes and multi-preference datasets, providing flexible trade-offs between parameter efficiency and specialization.

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [95] [Bridging the Physics-Data Gap with FNO-Guided Conditional Flow Matching: Designing Inductive Bias through Hierarchical Physical Constraints](https://arxiv.org/abs/2510.08295)
*Tsuyoshi Okita*

Main category: cs.LG

TL;DR: A hierarchical framework that embeds physical laws directly into deep generative models, combining Fourier Neural Operators for learning physical operators with Conditional Flow Matching for probabilistic generation, achieving superior generation quality and physical consistency.


<details>
  <summary>Details</summary>
Motivation: Conventional time-series generation often ignores domain-specific physical constraints, limiting statistical and physical consistency, creating a need for methods that can incorporate physical laws directly into generative models.

Method: Proposes a hierarchical framework that embeds physical laws (conservation, dynamics, boundary, and empirical relations) using Fourier Neural Operators (FNOs) for learning physical operators and Conditional Flow Matching (CFM) for probabilistic generation, integrated via time-dependent hierarchical constraints and FNO-guided corrections.

Result: Experiments on harmonic oscillators, human activity recognition, and lithium-ion battery degradation show 16.3% higher generation quality, 46% fewer physics violations, and 18.5% improved predictive accuracy over baselines.

Conclusion: The proposed physics-informed inductive bias framework successfully integrates physical constraints into deep generative models, achieving both statistical and physical consistency in time-series generation across multiple domains.

Abstract: Conventional time-series generation often ignores domain-specific physical
constraints, limiting statistical and physical consistency. We propose a
hierarchical framework that embeds the inherent hierarchy of physical
laws-conservation, dynamics, boundary, and empirical relations-directly into
deep generative models, introducing a new paradigm of physics-informed
inductive bias. Our method combines Fourier Neural Operators (FNOs) for
learning physical operators with Conditional Flow Matching (CFM) for
probabilistic generation, integrated via time-dependent hierarchical
constraints and FNO-guided corrections. Experiments on harmonic oscillators,
human activity recognition, and lithium-ion battery degradation show 16.3%
higher generation quality, 46% fewer physics violations, and 18.5% improved
predictive accuracy over baselines.

</details>


### [96] [Dynamic Features Adaptation in Networking: Toward Flexible training and Explainable inference](https://arxiv.org/abs/2510.08303)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Merim Dzaferagic,John D. Kelleher*

Main category: cs.LG

TL;DR: This paper proposes Adaptive Random Forests (ARFs) for dynamic feature adaptation in 6G networks and introduces Drift-Aware Feature Importance (DAFI) for efficient explainable AI, achieving 2x runtime reduction while maintaining stable predictions.


<details>
  <summary>Details</summary>
Motivation: As AI becomes integral to 6G network control, models must adapt to changing conditions from multi-vendor deployments, hardware upgrades, and evolving service requirements in non-stationary environments.

Method: The paper uses Adaptive Random Forests (ARFs) for iterative training with dynamic feature adaptation, and proposes Drift-Aware Feature Importance (DAFI) - an XAI method that uses distributional drift detection to optimize when to apply computationally intensive feature importance methods.

Result: Tests on 3 datasets show that ARFs achieve stable predictions with improving accuracy over time as more features are added, while DAFI reduces runtime by up to 2 times and produces more consistent feature importance values.

Conclusion: ARFs and DAFI together provide a promising framework for building flexible AI methods adapted to 6G network use-cases, addressing the need for adaptable learning in dynamic network environments.

Abstract: As AI becomes a native component of 6G network control, AI models must adapt
to continuously changing conditions, including the introduction of new features
and measurements driven by multi-vendor deployments, hardware upgrades, and
evolving service requirements. To address this growing need for flexible
learning in non-stationary environments, this vision paper highlights Adaptive
Random Forests (ARFs) as a reliable solution for dynamic feature adaptation in
communication network scenarios. We show that iterative training of ARFs can
effectively lead to stable predictions, with accuracy improving over time as
more features are added. In addition, we highlight the importance of
explainability in AI-driven networks, proposing Drift-Aware Feature Importance
(DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a
distributional drift detector to signal when to apply computationally intensive
FI methods instead of lighter alternatives. Our tests on 3 different datasets
indicate that our approach reduces runtime by up to 2 times, while producing
more consistent feature importance values. Together, ARFs and DAFI provide a
promising framework to build flexible AI methods adapted to 6G network
use-cases.

</details>


### [97] [Robust and Efficient Collaborative Learning](https://arxiv.org/abs/2510.08311)
*Abdellah El Mrini,Sadegh Farhadkhan,Rachid Guerraoui*

Main category: cs.LG

TL;DR: RPEL is a decentralized collaborative machine learning approach that uses pull-based epidemic communication to achieve robustness against adversaries while scaling efficiently with O(n log n) communication costs.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative ML approaches either rely on central servers or have high O(n²) communication costs, making them inefficient for large-scale deployments with adversarial nodes.

Method: RPEL uses a pull-based epidemic communication strategy where nodes pull model parameters from small random subsets of other nodes, avoiding all-to-all communication while maintaining convergence guarantees.

Result: Empirical results show RPEL maintains robustness in adversarial settings, achieves accuracy comparable to all-to-all communication, and scales efficiently across large networks.

Conclusion: RPEL provides a scalable, decentralized solution for robust collaborative learning that significantly reduces communication overhead while maintaining performance and security guarantees.

Abstract: Collaborative machine learning is challenged by training-time adversarial
behaviors. Existing approaches to tolerate such behaviors either rely on a
central server or induce high communication costs. We propose Robust Pull-based
Epidemic Learning (RPEL), a novel, scalable collaborative approach to ensure
robust learning despite adversaries. RPEL does not rely on any central server
and, unlike traditional methods, where communication costs grow in
$\mathcal{O}(n^2)$ with the number of nodes $n$, RPEL employs a pull-based
epidemic-based communication strategy that scales in $\mathcal{O}(n \log n)$.
By pulling model parameters from small random subsets of nodes, RPEL
significantly lowers the number of required messages without compromising
convergence guarantees, which hold with high probability. Empirical results
demonstrate that RPEL maintains robustness in adversarial settings, competes
with all-to-all communication accuracy, and scales efficiently across large
networks.

</details>


### [98] [To Ask or Not to Ask: Learning to Require Human Feedback](https://arxiv.org/abs/2510.08314)
*Andrea Pugnana,Giovanni De Toni,Cesare Barbera,Roberto Pellungrini,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: Learning to Ask (LtA) is a new framework that improves human-AI collaboration by determining both when and how to incorporate expert input, using a two-part architecture with standard and enriched ML models.


<details>
  <summary>Details</summary>
Motivation: To address limitations of Learning to Defer (LtD) which treats humans and ML models as mutually exclusive decision-makers, restricting expert contribution to mere predictions.

Method: Two-part architecture with standard ML model and enriched model trained with expert feedback, with optimal strategy for querying enriched model. Two implementations: sequential training and joint optimization with surrogate losses.

Result: Experiments with synthetic and real expert data show LtA provides more flexible and powerful foundation for effective human-AI collaboration.

Conclusion: LtA offers a superior approach to human-AI collaboration by enabling more sophisticated integration of expert input beyond simple deferral mechanisms.

Abstract: Developing decision-support systems that complement human performance in
classification tasks remains an open challenge. A popular approach, Learning to
Defer (LtD), allows a Machine Learning (ML) model to pass difficult cases to a
human expert. However, LtD treats humans and ML models as mutually exclusive
decision-makers, restricting the expert contribution to mere predictions. To
address this limitation, we propose Learning to Ask (LtA), a new framework that
handles both when and how to incorporate expert input in an ML model. LtA is
based on a two-part architecture: a standard ML model and an enriched model
trained with additional expert human feedback, with a formally optimal strategy
for selecting when to query the enriched model. We provide two practical
implementations of LtA: a sequential approach, which trains the models in
stages, and a joint approach, which optimises them simultaneously. For the
latter, we design surrogate losses with realisable-consistency guarantees. Our
experiments with synthetic and real expert data demonstrate that LtA provides a
more flexible and powerful foundation for effective human-AI collaboration.

</details>


### [99] [Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization](https://arxiv.org/abs/2510.08341)
*Pál Zsámboki,Benjamin Levi,David Ansel Josef Smith,Mitansh Kagalwala,Arlington Kell,Samuel Liechty,Cong Wang*

Main category: cs.LG

TL;DR: The paper studies length generalization in transformers using the set complement task, showing theoretical bounds and practical limitations, then proposes and validates dropout and EMA as solutions to improve generalization.


<details>
  <summary>Details</summary>
Motivation: To understand and improve transformers' ability to generalize to longer sequences than seen during training, particularly for tasks like board-game reasoning where predicting absent elements is crucial.

Method: Theoretical analysis of single-layer attention-only transformers, followed by empirical validation using random hyperparameter search on set complement task and testing OthelloGPT with EMA.

Result: Proved tight bounds on embedding/value dimensions; showed balanced logit displacement at short lengths enables longer generalization with reduced precision; validated that dropout counteracts softmax compression and EMA reduces noisy updates.

Conclusion: Length generalization in transformers faces fundamental limitations from softmax compression and noisy training dynamics, but these can be mitigated through dropout and EMA techniques.

Abstract: We study length generalization in transformers through the set complement
task, where a model must predict a uniform distribution over tokens absent from
an input sequence -- an ability central to board-game style reasoning. Our main
theoretical result establishes two statements. First, we prove tight bounds on
embedding and value dimensions for single-layer attention-only transformers.
Second, we show that if such a model achieves balanced logit displacement at
lengths 1 and 2, then it must generalize to longer sequences, though with
reduced precision. A mechanistic reading of the proof explains this limitation:
as more tokens are attended to, softmax compresses logit displacements, eroding
separation between valid and invalid outputs. Training dynamics also suggest a
second obstacle: when many next tokens are possible, updates become noisy. We
hypothesize that dropout can counteract the first effect and Exponential Moving
Average (EMA) the second. We validate these hypotheses through random
hyperparameter search on the set complement task, which confirms both
mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random
Othello moves, and find that EMA again improves length generalization in this
more complex setting.

</details>


### [100] [DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning](https://arxiv.org/abs/2510.08350)
*Daniel Jason Tan,Jiayang Chen,Dilruk Perera,Kay Choong See,Mengling Feng*

Main category: cs.LG

TL;DR: DeepEN is a deep reinforcement learning framework that provides personalized enteral nutrition recommendations for ICU patients, trained on 11,000+ patients from MIMIC-IV database and achieving 3.7 percentage-point mortality reduction.


<details>
  <summary>Details</summary>
Motivation: To improve outcomes for critically ill patients by providing personalized enteral nutrition therapy that goes beyond traditional guideline-based approaches, addressing the need for tailored nutrition management in ICU settings.

Method: Uses dueling double deep Q-network with conservative Q-learning regularization, trained offline on MIMIC-IV data. Integrates clinically informed state space with custom reward function balancing short-term physiological goals and long-term survival outcomes.

Result: Outperforms clinician-derived and guideline-based policies with 3.7±0.17 percentage-point reduction in estimated mortality (18.8% vs 22.5%) and improvements in key nutritional biomarkers.

Conclusion: DeepEN demonstrates the potential of safe, data-driven personalization of enteral nutrition therapy to improve patient outcomes beyond traditional approaches.

Abstract: We introduce DeepEN, a deep reinforcement learning (RL) framework for
personalized enteral nutrition (EN) in critically ill patients. Trained offline
on over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates
4-hourly recommendations for caloric, protein, and fluid intake tailored to
each patient's evolving physiology. The model integrates a curated, clinically
informed state space with a custom reward function that balances short-term
physiological and nutrition-related goals with long-term survival outcomes.
Using a dueling double deep Q-network with conservative Q-learning
regularization, DeepEN learns clinically realistic policies that align with
high-value clinician actions while discouraging unsafe deviations. Across
various qualitative and quantitative metrics, DeepEN outperforms
clinician-derived and guideline-based policies, achieving a 3.7 $\pm$ 0.17
percentage-point reduction in estimated mortality (18.8% vs 22.5%) and
improvements in key nutritional biomarkers. These findings highlight the
potential of safe, data-driven personalization of EN therapy to improve
outcomes beyond traditional guideline- or heuristic-based approaches.

</details>


### [101] [Guided Star-Shaped Masked Diffusion](https://arxiv.org/abs/2510.08369)
*Viacheslav Meshchaninov,Egor Shibaev,Artem Makoian,Ivan Klimov,Danil Sheshenya,Andrei Malinin,Nikita Balagansky,Daniil Gavrilov,Aibek Alanov,Dmitry Vetrov*

Main category: cs.LG

TL;DR: A novel sampling algorithm for pre-trained masked diffusion models that improves sample quality and efficiency through a star-shaped paradigm with learnable re-masking scheduler, enabling error correction especially in low-step generation.


<details>
  <summary>Details</summary>
Motivation: Pre-trained masked diffusion models are constrained by irreversible sampling decisions and struggle with low-step generation, limiting their practical efficiency and quality.

Method: Introduces a star-shaped sampling paradigm with learnable re-masking scheduler that identifies and revises likely errors, requiring only lightweight fine-tuning of a single layer in pre-trained models.

Result: Significant quality improvement particularly in low-step regimes, outperforming or matching existing methods in text and code generation tasks across comprehensive experiments.

Conclusion: The proposed sampling algorithm effectively addresses limitations of traditional diffusion sampling by enabling error correction through intelligent re-masking, making pre-trained models more efficient and higher-quality.

Abstract: The performance of pre-trained masked diffusion models is often constrained
by their sampling procedure, which makes decisions irreversible and struggles
in low-step generation regimes. We introduce a novel sampling algorithm that
works with pre-trained models and, after a lightweight fine-tuning of a single
layer, significantly improves sample quality and efficiency. Our method
reformulates the generation process using a star-shaped paradigm, which
inherently allows for error correction. To make this process effective, we
augment it with a learnable re-masking scheduler that intelligently identifies
and revises likely errors. This approach yields a substantial quality boost,
particularly when using a small number of sampling steps. We extensively ablate
key components of our approach and show its usability in different scenarios.
In comprehensive experiments on text, and code generation, our sampling
algorithm outperforms or matches existing methods.

</details>


### [102] [Contrastive Self-Supervised Learning at the Edge: An Energy Perspective](https://arxiv.org/abs/2510.08374)
*Fernanda Famá,Roberto Pereira,Charalampos Kalalas,Paolo Dini,Lorena Qendro,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: This paper evaluates four contrastive learning frameworks (SimCLR, MoCo, SimSiam, Barlow Twins) for edge/fog deployment, finding SimCLR has lowest energy consumption despite perceived computational costs.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning shows promise for self-supervised representation learning but its deployment on resource-constrained devices is underexplored, with challenges in energy consumption, data availability, and memory usage.

Method: Systematic benchmarking of four CL frameworks with energy profiling and reduced training data conditions, plus evaluation of lightweight neural architectures paired with CL frameworks.

Result: SimCLR demonstrates the lowest energy consumption across various data regimes, contrary to its perceived computational cost.

Conclusion: The study provides insights into resource implications of deploying CL in edge/fog environments and opens research directions for future optimization.

Abstract: While contrastive learning (CL) shows considerable promise in self-supervised
representation learning, its deployment on resource-constrained devices remains
largely underexplored. The substantial computational demands required for
training conventional CL frameworks pose a set of challenges, particularly in
terms of energy consumption, data availability, and memory usage. We conduct an
evaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and Barlow
Twins. We focus on the practical feasibility of these CL frameworks for edge
and fog deployment, and introduce a systematic benchmarking strategy that
includes energy profiling and reduced training data conditions. Our findings
reveal that SimCLR, contrary to its perceived computational cost, demonstrates
the lowest energy consumption across various data regimes. Finally, we also
extend our analysis by evaluating lightweight neural architectures when paired
with CL frameworks. Our study aims to provide insights into the resource
implications of deploying CL in edge/fog environments with limited processing
capabilities and opens several research directions for its future optimization.

</details>


### [103] [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
*Heming Zou,Yunliang Zang,Wutong Xu,Yao Zhu,Xiangyang Ji*

Main category: cs.LG

TL;DR: FlyLoRA is a novel LoRA variant inspired by the fly olfactory circuit that uses implicit mixture-of-experts with rank-wise expert activation and an implicit router using frozen sparse random projection matrices, eliminating explicit routers while mitigating both intra-task and inter-task parameter interference.


<details>
  <summary>Details</summary>
Motivation: Traditional LoRA suffers from parameter interference, and existing MoE-based LoRA variants introduce additional router parameters while remaining ineffective for multi-task model merging due to inter-task interference.

Method: Proposes FlyLoRA with: (1) rank-wise expert activation in up-projection matrix, and (2) implicit router that unifies expert routing and down-projection using frozen sparse random projection matrix instead of dense trainable version.

Result: Extensive experiments across four domains (general knowledge, scientific QA, math reasoning, code generation) show consistent performance improvements over existing methods.

Conclusion: FlyLoRA resolves the trade-off between intra-task decorrelation and computational efficiency while inherently mitigating inter-task interference, demonstrating how biological structures can inspire AI innovations.

Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method for foundation models, but it suffers from parameter interference,
resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based
LoRA variants show promise in mitigating intra-task correlations in single-task
instruction tuning, they introduce additional router parameters and remain
ineffective in multi-task model merging where inter-task interference arises.
Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit
MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the
up-projection matrix, and (2) an implicit router that unifies expert routing
and down-projection, where a frozen sparse random projection matrix replaces
the traditional dense trainable version. This design resolves the trade-off
between intra-task decorrelation and computational efficiency by eliminating
the need for an explicit router, while inherently mitigating inter-task
interference due to the orthogonality property of random matrices. Extensive
experiments across four domains -- general knowledge understanding, scientific
question answering, mathematical reasoning, and code generation -- demonstrate
consistent performance improvements over existing methods. Beyond empirical
gains, FlyLoRA highlights how biological structures can inspire innovations in
AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.

</details>


### [104] [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
*Lauren Anderson,Lucas Chatelain,Nicolas Tremblay,Kathryn Grandfield,David Rousseau,Aurélien Gourrier*

Main category: cs.LG

TL;DR: Deep learning super-resolution models were tested to overcome resolution limitations in dental porosity imaging, but standard image quality metrics failed to match visual perception, requiring biology-driven assessment using segmentation and graph analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of small field of view in high-resolution confocal microscopy of dental porosity by using deep learning super-resolution to restore image quality from faster low-resolution acquisitions.

Method: Tested three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised model (CycleGAN) on paired high- and low-resolution confocal images with pixel size increases of x2, x4, x8. Used segmentation of porosity networks and graph analysis to evaluate 3D connectivity preservation.

Result: Standard image quality assessment metrics yielded inconsistent results that contradicted visual perception. Biology-driven assessment through segmentation and graph analysis revealed differences in model sensitivity to weak intensity features and impact of non-linearity in image generation.

Conclusion: Generic image quality metrics are insufficient for evaluating super-resolution performance in dental porosity imaging. Biology-driven assessment using segmentation and connectivity analysis provides better mechanistic interpretation of model performance.

Abstract: The mechanosensory system of teeth is currently believed to partly rely on
Odontoblast cells stimulation by fluid flow through a porosity network
extending through dentin. Visualizing the smallest sub-microscopic porosity
vessels therefore requires the highest achievable resolution from confocal
fluorescence microscopy, the current gold standard. This considerably limits
the extent of the field of view to very small sample regions. To overcome this
limitation, we tested different deep learning (DL) super-resolution (SR) models
to allow faster experimental acquisitions of lower resolution images and
restore optimal image quality by post-processing. Three supervised 2D SR models
(RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a
unique set of experimentally paired high- and low-resolution confocal images
acquired with different sampling schemes, resulting in a pixel size increase of
x2, x4, x8. Model performance was quantified using a broad set of similarity
and distribution-based image quality assessment (IQA) metrics, which yielded
inconsistent results that mostly contradicted our visual perception. This
raises the question of the relevance of such generic metrics to efficiently
target the specific structure of dental porosity. To resolve this conflicting
information, the generated SR images were segmented taking into account the
specific scales and morphology of the porosity network and analysed by
comparing connected components. Additionally, the capacity of the SR models to
preserve 3D porosity connectivity throughout the confocal image stacks was
evaluated using graph analysis. This biology-driven assessment allowed a far
better mechanistic interpretation of SR performance, highlighting differences
in model sensitivity to weak intensity features and the impact of non-linearity
in image generation, which explains the failure of standard IQA metrics.

</details>


### [105] [Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors](https://arxiv.org/abs/2510.08413)
*David Madras,Joshua Safyan,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: The paper explains the success of prompt engineering through data-dependent perplexity regularization, deriving novel generalization bounds that remain non-vacuous even with limited data, and demonstrates practical benefits of perplexity regularization for improving prompt generalization.


<details>
  <summary>Details</summary>
Motivation: To better explain the widespread success of prompt engineering techniques that work well even with small amounts of task-specific data, going beyond existing generalization bounds that only apply in data-rich scenarios.

Method: Derived novel generalization bounds using PAC-Bayes theory with more useful priors, formally analyzing how perplexity regularization tightens bounds by limiting exploration, and conducted empirical evaluation of both bounds effectiveness and practical benefits.

Result: Developed generalization bounds that remain non-vacuous for data-scarce prompt optimization, showing how perplexity regularization acts as an effective prior and improves prompt generalization.

Conclusion: Perplexity regularization provides a formal explanation for prompt engineering success by serving as an effective prior that steers optimization toward more natural prompts for the task, with both theoretical guarantees and practical benefits for generalization.

Abstract: Many prompt engineering techniques have been successful in practice, even
when optimizing over a large prompt space with with a small amount of
task-specific data. Recent work has partially explained this success by showing
generalization bounds which apply PAC-Bayes theory to the discrete prompt
space, but they are non-vacuous only in data-rich scenarios. We argue that such
widespread success can be more fully explained through more carefully
considering data- or distribution-dependent perplexity, which acts as an
effective prior and steers the optimization towards prompts that are more
``natural'' for the task at hand. We derive novel generalization bounds that
are non-vacuous for data-scarce prompt optimization via more useful priors,
formally analyzing how perplexity regularization tightens these bounds by
limiting exploration. Empirically, we explore both the bounds' effectiveness
and the practical benefits of perplexity regularization in improving prompt
generalization.

</details>


### [106] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: DGPO is a new online RL algorithm that enables efficient preference optimization for diffusion models by eliminating the need for stochastic policies, allowing use of deterministic ODE samplers and achieving 20x faster training than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods like GRPO require stochastic policies, but the most efficient diffusion samplers are deterministic ODE-based. Existing solutions use inefficient SDE-based samplers with slow convergence due to model-agnostic Gaussian noise.

Method: DGPO learns directly from group-level preferences using relative information of samples within groups, dispensing with the policy-gradient framework entirely. This eliminates the need for stochastic policies and enables use of efficient deterministic ODE samplers.

Result: DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics.

Conclusion: DGPO resolves the conflict between preference optimization requirements and efficient diffusion sampling by directly learning from group preferences, enabling significantly faster training while maintaining superior performance.

Abstract: While reinforcement learning methods such as Group Relative Preference
Optimization (GRPO) have significantly enhanced Large Language Models, adapting
them to diffusion models remains challenging. In particular, GRPO demands a
stochastic policy, yet the most cost-effective diffusion samplers are based on
deterministic ODEs. Recent work addresses this issue by using inefficient
SDE-based samplers to induce stochasticity, but this reliance on model-agnostic
Gaussian noise leads to slow convergence. To resolve this conflict, we propose
Direct Group Preference Optimization (DGPO), a new online RL algorithm that
dispenses with the policy-gradient framework entirely. DGPO learns directly
from group-level preferences, which utilize relative information of samples
within groups. This design eliminates the need for inefficient stochastic
policies, unlocking the use of efficient deterministic ODE samplers and faster
training. Extensive results show that DGPO trains around 20 times faster than
existing state-of-the-art methods and achieves superior performance on both
in-domain and out-of-domain reward metrics. Code is available at
https://github.com/Luo-Yihong/DGPO.

</details>


### [107] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter is a tool-calling-based routing system that uses reinforcement learning to dynamically route queries between expensive premium LLMs and lightweight models based on cost-performance trade-offs, eliminating the need for hand-engineered rules.


<details>
  <summary>Details</summary>
Motivation: Modern LLM deployments face a cost-performance spectrum where premium models are expensive but strong at reasoning, while lightweight models are economical but brittle on complex tasks. Static escalation rules and keyword heuristics fail to adapt across task types and under-utilize this spectrum.

Method: xRouter uses a learned router trained end-to-end with reinforcement learning using an explicit, cost-aware reward function. The router can either answer queries directly or invoke one or more external models. The implementation includes full RL framework with reward and cost accounting, deployment, and evaluation pipelines.

Result: Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs with substantial cost reductions at comparable task completion rates. The system provides empirical insights into model trainability and the difficulty of eliciting sophisticated orchestration behaviors in small open models.

Conclusion: xRouter serves as a practical substrate for advancing learned, cost-aware LLM orchestration, demonstrating that learned routing can effectively balance cost and performance without hand-engineered rules.

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


### [108] [Synthetic Series-Symbol Data Generation for Time Series Foundation Models](https://arxiv.org/abs/2510.08445)
*Wenxuan Wang,Kai Wu,Yujian Betterest Li,Dan Wang,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: SymTime is a foundation model for time series analysis that uses series-symbol data generation to overcome data scarcity issues, achieving competitive performance across five major TSA tasks.


<details>
  <summary>Details</summary>
Motivation: Foundation models for time series analysis face challenges like training data scarcity and imbalance, which hinder their development and performance.

Method: Developed a series-symbol data generation mechanism inspired by complex dynamic system theories, creating unlimited high-quality time series data with corresponding symbolic expressions. Built SymTime as a pre-trained foundation model that leverages these series-symbol data pairs with strong correlations.

Result: SymTime demonstrates competitive performance across five major time series analysis tasks when fine-tuned, rivaling foundation models pre-trained on real-world datasets.

Conclusion: The series-symbol data generation and pretraining mechanism shows strong potential for overcoming data scarcity and enhancing task performance in time series analysis.

Abstract: Foundation models for time series analysis (TSA) have attracted significant
attention. However, challenges such as training data scarcity and imbalance
continue to hinder their development. Inspired by complex dynamic system
theories, we design a series-symbol data generation mechanism, enabling the
unrestricted creation of high-quality time series data paired with
corresponding symbolic expressions. To leverage series-symbol data pairs with
strong correlations, we develop \texttt{SymTime}, a pre-trained foundation
model for enhancing time series representation using symbolic information.
\texttt{SymTime} demonstrates competitive performance across five major TSA
tasks when fine-tunes with downstream tasks, rivaling foundation models
pre-trained on real-world datasets. This approach underscores the potential of
series-symbol data generation and pretraining mechanisms in overcoming data
scarcity and enhancing task performance. The code is available at
https://github.com/wwhenxuan/SymTime.

</details>


### [109] [Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning](https://arxiv.org/abs/2510.08456)
*Ankur Mali,Lawrence Hall,Jake Williams,Gordon Richards*

Main category: cs.LG

TL;DR: A rigorous framework for classifying activation functions using a nine-dimensional integral signature that combines Gaussian propagation statistics, asymptotic slopes, and regularity measures.


<details>
  <summary>Details</summary>
Motivation: Existing comparisons of activation functions are largely heuristic, lacking rigorous classification methods to guide principled design choices for neural networks.

Method: Proposed a nine-dimensional integral signature S_sigma(phi) combining Gaussian propagation statistics (m1, g1, g2, m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures (TV(phi'), C(phi)). Applied dynamical analysis with Lyapunov theorems and kernel perspective analysis.

Result: Classified eight standard activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU) with sharp distinctions between saturating, linear-growth, and smooth families. Numerical validation confirmed theoretical predictions.

Conclusion: The framework provides principled design guidance, moving activation function choice from trial-and-error to provable stability and kernel conditioning based on rigorous mathematical foundations.

Abstract: Activation functions govern the expressivity and stability of neural
networks, yet existing comparisons remain largely heuristic. We propose a
rigorous framework for their classification via a nine-dimensional integral
signature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2,
m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures
(TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine
reparameterization laws with bias, and closure under bounded slope variation.
Dynamical analysis yields Lyapunov theorems with explicit descent constants and
identifies variance stability regions through (m2', g2). From a kernel
perspective, we derive dimension-free Hessian bounds and connect smoothness to
bounded variation of phi'. Applying the framework, we classify eight standard
activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving
sharp distinctions between saturating, linear-growth, and smooth families.
Numerical Gauss-Hermite and Monte Carlo validation confirms theoretical
predictions. Our framework provides principled design guidance, moving
activation choice from trial-and-error to provable stability and kernel
conditioning.

</details>


### [110] [SummDiff: Generative Modeling of Video Summarization with Diffusion](https://arxiv.org/abs/2510.08458)
*Kwanseok Kim,Jaehoon Hahm,Sumin Kim,Jinhwan Sul,Byunghak Kim,Joonseok Lee*

Main category: cs.LG

TL;DR: SummDiff introduces video summarization as a conditional generation task using diffusion models, enabling generation of multiple plausible summaries that reflect human subjectivity rather than deterministic averaging.


<details>
  <summary>Details</summary>
Motivation: Previous video summarization methods deterministically regressed to averaged frame scores, ignoring the inherent subjectivity of what constitutes a good summary according to different human perspectives.

Method: Proposes SummDiff, the first diffusion model approach for video summarization, framing it as conditional generation to learn the distribution of good summaries and generate multiple candidate summaries conditioned on input videos.

Result: Achieves state-of-the-art performance on various benchmarks, produces summaries that closely align with individual annotator preferences, and provides novel insights through knapsack analysis metrics.

Conclusion: SummDiff successfully addresses the subjectivity problem in video summarization by generating multiple plausible summaries through diffusion modeling, better reflecting varying human perspectives while maintaining high performance.

Abstract: Video summarization is a task of shortening a video by choosing a subset of
frames while preserving its essential moments. Despite the innate subjectivity
of the task, previous works have deterministically regressed to an averaged
frame score over multiple raters, ignoring the inherent subjectivity of what
constitutes a good summary. We propose a novel problem formulation by framing
video summarization as a conditional generation task, allowing a model to learn
the distribution of good summaries and to generate multiple plausible summaries
that better reflect varying human perspectives. Adopting diffusion models for
the first time in video summarization, our proposed method, SummDiff,
dynamically adapts to visual contexts and generates multiple candidate
summaries conditioned on the input video. Extensive experiments demonstrate
that SummDiff not only achieves the state-of-the-art performance on various
benchmarks but also produces summaries that closely align with individual
annotator preferences. Moreover, we provide a deeper insight with novel metrics
from an analysis of the knapsack, which is an important last step of generating
summaries but has been overlooked in evaluation.

</details>


### [111] [In-Context Clustering with Large Language Models](https://arxiv.org/abs/2510.08466)
*Ying Wang,Mengye Ren,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: ICC is an LLM-based clustering method that uses attention mechanisms instead of predefined similarity measures, showing strong zero-shot clustering on text-encoded data and enabling text-conditioned image clustering through flexible prompting.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering algorithms are constrained by predefined similarity measures and cannot capture complex relationships or perform text-conditioned clustering, which LLMs can address through their flexible attention mechanisms.

Method: Uses LLM attention matrices for spectral clustering, fine-tunes LLMs with Next Token Prediction loss on numeric and image data, and leverages flexible prompting for text-conditioned image clustering.

Result: Pretrained LLMs demonstrate impressive zero-shot clustering on text-encoded numeric data, with spectral clustering using attention matrices showing competitive performance. Fine-tuning further enhances clustering capabilities.

Conclusion: ICC extends in-context learning to unsupervised settings, showcasing LLMs' effectiveness and flexibility for clustering tasks, including capabilities that classical methods lack like text-conditioned image clustering.

Abstract: We propose In-Context Clustering (ICC), a flexible LLM-based procedure for
clustering data from diverse distributions. Unlike traditional clustering
algorithms constrained by predefined similarity measures, ICC flexibly captures
complex relationships among inputs through an attention mechanism. We show that
pretrained LLMs exhibit impressive zero-shot clustering capabilities on
text-encoded numeric data, with attention matrices showing salient cluster
patterns. Spectral clustering using attention matrices offers surprisingly
competitive performance. We further enhance the clustering capabilities of LLMs
on numeric and image data through fine-tuning using the Next Token Prediction
(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned
image clustering, a capability that classical clustering methods lack. Our work
extends in-context learning to an unsupervised setting, showcasing the
effectiveness and flexibility of LLMs for clustering. Our code is available at
https://agenticlearning.ai/icc.

</details>


### [112] [Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models](https://arxiv.org/abs/2510.08492)
*Sharut Gupta,Shobhita Sundaram,Chenyu Wang,Stefanie Jegelka,Phillip Isola*

Main category: cs.LG

TL;DR: UML is a modality-agnostic training paradigm that leverages unpaired multimodal data to enhance representation learning in target modalities without requiring paired datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional multimodal learners rely heavily on paired datasets, but there's potential to leverage auxiliary unpaired multimodal data to directly enhance representation learning in target modalities.

Method: UML uses a single model that alternately processes inputs from different modalities while sharing parameters across them, exploiting the assumption that different modalities are projections of a shared underlying reality.

Result: Theoretically, unpaired auxiliary data yields more informative representations than unimodal training. Empirically, using unpaired data from auxiliary modalities consistently improves downstream performance across diverse unimodal targets.

Conclusion: UML demonstrates that unpaired multimodal data can effectively enhance representation learning across different modalities without requiring explicit paired datasets.

Abstract: Traditional multimodal learners find unified representations for tasks like
visual question answering, but rely heavily on paired datasets. However, an
overlooked yet potentially powerful question is: can one leverage auxiliary
unpaired multimodal data to directly enhance representation learning in a
target modality? We introduce UML: Unpaired Multimodal Learner, a
modality-agnostic training paradigm in which a single model alternately
processes inputs from different modalities while sharing parameters across
them. This design exploits the assumption that different modalities are
projections of a shared underlying reality, allowing the model to benefit from
cross-modal structure without requiring explicit pairs. Theoretically, under
linear data-generating assumptions, we show that unpaired auxiliary data can
yield representations strictly more informative about the data-generating
process than unimodal training. Empirically, we show that using unpaired data
from auxiliary modalities -- such as text, audio, or images -- consistently
improves downstream performance across diverse unimodal targets such as image
and audio. Our project page: https://unpaired-multimodal.github.io/

</details>


### [113] [DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems](https://arxiv.org/abs/2510.08522)
*Yuanjun Dai,Keqiang He,An Wang*

Main category: cs.LG

TL;DR: DYNAMIX is a reinforcement learning framework using PPO to dynamically optimize batch sizes in distributed ML, achieving up to 6.3% accuracy improvement and 46% training time reduction.


<details>
  <summary>Details</summary>
Motivation: Existing batch size selection methods use static allocation or simplistic heuristics that fail to adapt to heterogeneous, dynamic computing environments.

Method: Formulates batch size optimization as sequential decision-making using PPO with multi-dimensional state representation including network metrics, system resource utilization, and training efficiency indicators.

Result: Achieves up to 6.3% improvement in final model accuracy and 46% reduction in total training time across diverse workloads, hardware configurations, and network conditions.

Conclusion: DYNAMIX eliminates need for explicit system modeling, integrates with existing frameworks, scales well to 32 nodes, and learned policies generalize effectively across related model architectures.

Abstract: Existing batch size selection approaches in dis- tributed machine learning
rely on static allocation or simplistic heuristics that fail to adapt to
heterogeneous, dynamic computing environments. We present DYNAMIX, a
reinforcement learning framework that formulates batch size optimization as a
sequen- tial decision-making problem using Proximal Policy Optimiza- tion
(PPO). Our approach employs a multi-dimensional state representation
encompassing network-level metrics, system-level resource utilization, and
training statistical efficiency indicators to enable informed decision-making
across diverse computational resources. Our approach eliminates the need for
explicit system modeling while integrating seamlessly with existing distributed
training frameworks. Through evaluations across diverse work- loads, hardware
configurations, and network conditions, DY- NAMIX achieves up to 6.3%
improvement in the final model accuracy and 46% reduction in the total training
time. Our scalability experiments demonstrate that DYNAMIX maintains the best
performance as cluster size increases to 32 nodes, while policy transfer
experiments show that learned policies generalize effectively across related
model architectures.

</details>


### [114] [Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning](https://arxiv.org/abs/2510.08526)
*Yash Jhaveri,Harley Wiltzer,Patrick Shafto,Marc G. Bellemare,David Meger*

Main category: cs.LG

TL;DR: This paper presents a theoretical framework for policy optimization in reinforcement learning that guarantees convergence to interpretable, diversity-preserving optimal policies through vanishing entropy regularization and temperature decoupling.


<details>
  <summary>Details</summary>
Motivation: Standard RL methods focus only on expected return and ignore policy properties, making it difficult to characterize which policies will be learned and what they will do. The authors aim to develop methods that produce interpretable and diversity-preserving optimal policies.

Method: The approach uses vanishing entropy regularization and a temperature decoupling gambit to ensure convergence to particular optimal policies. The framework guarantees convergence of policy-derived objects like value functions and return distributions.

Result: The method realizes interpretable, diversity-preserving optimal policies as regularization temperature vanishes. In one instance, the policy samples all optimal actions uniformly. An algorithm is presented that can estimate the return distribution associated with these interpretable policies to arbitrary accuracy.

Conclusion: The proposed framework provides theoretical guarantees for convergence to interpretable optimal policies while preserving diversity, addressing limitations of traditional RL methods that ignore policy properties beyond expected return.

Abstract: In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.

</details>


### [115] [Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints](https://arxiv.org/abs/2510.08549)
*Zilin Kang,Chonghua Liao,Tingqiang Xu,Huazhe Xu*

Main category: cs.LG

TL;DR: ERA is a new paradigm that constrains sampling entropy above given thresholds using specially designed activations, achieving significant performance improvements across LLMs, RL agents, and image classification with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To develop a simpler and more robust approach for entropy control in various machine learning domains, addressing the need for effective sampling strategies without complex algorithmic modifications.

Method: Apply specially designed output activations to constrain sampling entropy above given thresholds, maintaining computational efficiency with less than 7% overhead.

Result: Achieved 37.4% boost in AIME 2025 score for Qwen2.5-Math-7B, over 30% performance improvement on HumanoidBench for RL agents, and 0.69% ImageNet top-1 accuracy improvement for ResNet-50.

Conclusion: Output activation serves as a powerful tool for entropy control, opening new directions for designing simpler and more robust algorithms across multiple machine learning domains.

Abstract: We propose ERA, a new paradigm that constrains the sampling entropy above
given thresholds by applying specially designed activations to the outputs of
models. Our approach demonstrates broad effectiveness across different domains:
1) for large language models(LLMs), boosting the AIME 2025 score for
Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning
agents, improving performance by more than 30% over strong baselines such as
SAC on the challenging HumanoidBench; 3) for image classification, enhancing
ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a
computational overhead of less than 7%. Our work validates output activation as
a powerful tool for entropy control, opening a new direction for designing
simpler and more robust algorithms.

</details>


### [116] [Who Said Neural Networks Aren't Linear?](https://arxiv.org/abs/2510.08570)
*Nimrod Berman,Assaf Hallak,Assaf Shocher*

Main category: cs.LG

TL;DR: This paper introduces Linearizer, a method that makes nonlinear neural networks linear by sandwiching a linear operator between two invertible neural networks, enabling the application of linear algebra tools to nonlinear mappings.


<details>
  <summary>Details</summary>
Motivation: To identify non-standard vector spaces where conventionally nonlinear functions become linear, making linear algebra tools applicable to neural networks.

Method: Sandwich a linear operator A between two invertible neural networks: f(x)=g_y^{-1}(A g_x(x)), creating vector spaces with new addition and scaling operations derived from g_x and g_y.

Result: Enables application of SVD, pseudo-inverse, orthogonal projection to nonlinear mappings; allows composition of Linearizers; collapses hundreds of diffusion sampling steps into one; enables globally projective generative models and modular style transfer.

Conclusion: The Linearizer framework successfully makes nonlinear mappings linear in constructed vector spaces, enabling powerful linear algebra techniques to be applied to neural networks with practical benefits in diffusion models and generative modeling.

Abstract: Neural networks are famously nonlinear. However, linearity is defined
relative to a pair of vector spaces, $f$$:$$X$$\to$$Y$. Is it possible to
identify a pair of non-standard vector spaces for which a conventionally
nonlinear function is, in fact, linear? This paper introduces a method that
makes such vector spaces explicit by construction. We find that if we sandwich
a linear operator $A$ between two invertible neural networks, $f(x)=g_y^{-1}(A
g_x(x))$, then the corresponding vector spaces $X$ and $Y$ are induced by newly
defined addition and scaling actions derived from $g_x$ and $g_y$. We term this
kind of architecture a Linearizer. This framework makes the entire arsenal of
linear algebra, including SVD, pseudo-inverse, orthogonal projection and more,
applicable to nonlinear mappings. Furthermore, we show that the composition of
two Linearizers that share a neural network is also a Linearizer. We leverage
this property and demonstrate that training diffusion models using our
architecture makes the hundreds of sampling steps collapse into a single step.
We further utilize our framework to enforce idempotency (i.e. $f(f(x))=f(x)$)
on networks leading to a globally projective generative model and to
demonstrate modular style transfer.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [117] [Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization](https://arxiv.org/abs/2510.07518)
*Blake Hansen,Dafne Zorzetto,Valeria Edefonti,Roberta De Vito*

Main category: stat.ME

TL;DR: A Bayesian NMF model for multi-study dietary data that handles zero inflation, incorporates covariates, and enables cross-study information sharing to identify dietary patterns associated with cancer risk.


<details>
  <summary>Details</summary>
Motivation: Dietary intake data presents statistical challenges: high-dimensional, sparse with excess zeros, and heterogeneous due to individual covariates. Current NMF methods lack flexibility to address these complexities and cannot effectively integrate data across multiple studies.

Method: Novel Bayesian NMF model that jointly models multi-study count data, incorporates mixture component for zero inflation, and uses Bayesian non-parametric priors to characterize heterogeneity in pattern scores from individual covariates.

Result: Extensive simulations show significant improvement in estimation accuracy compared to existing Bayesian NMF methods. Application to case-control studies on upper aero-digestive tract cancers identified nutritionally meaningful dietary patterns.

Conclusion: The proposed model effectively addresses key challenges in dietary pattern analysis and enables better understanding of diet-health associations through improved pattern estimation and cross-study information sharing.

Abstract: Understanding the association between dietary patterns and health outcomes,
such as the cancer risk, is crucial to inform public health guidelines and
shaping future dietary interventions. However, dietary intake data present
several statistical challenges: they are high-dimensional, often sparse with
excess zeros, and exhibit heterogeneity driven by individual-level covariates.
Non-Negative Matrix Factorization (NMF), commonly used to estimate patterns in
high-dimensional count data, typically relies on Poisson assumptions and lacks
the flexibility to fully address these complexities. Additionally, integrating
data across multiple studies, such as case-control studies on cancer risk,
requires models that can share information across sources while preserving
study-specific structure.
  In this paper, we introduce a novel Bayesian NMF model that (i) jointly
models multi-study count data to enable cross-study information sharing, (ii)
incorporate a mixture component to account for zero inflation, and (iii)
leverage flexible Bayesian non-parametric priors for characterizing the
heterogeneity in pattern scores induced by the individual covariates. This
structure allows for clustering of individuals based on dietary profiles,
enabling downstream association analyses with health outcomes. Through
extensive simulation studies, we demonstrate that our model significantly
improves estimation accuracy compared to existing Bayesian NMF methods.
  We further illustrate its utility through an application to multiple
case-control studies on diet and upper aero-digestive tract cancers,
identifying nutritionally meaningful dietary patterns. An R package
implementing our approach is available at
https://github.com/blhansen/ZIMultiStudyNMF.

</details>


### [118] [Integrating smart surveys with traditional surveys](https://arxiv.org/abs/2510.07521)
*Danielle Mccool,Peter Lugtig,Bella Struminskaya*

Main category: stat.ME

TL;DR: This paper compares two approaches for integrating smart surveys with traditional diaries: mixed-mode (prioritizing outcome alignment) and multisource (maintaining mode differences and integrating at modeling stage), proposing a decision framework for selecting the appropriate integration strategy.


<details>
  <summary>Details</summary>
Motivation: Smart surveys using sensors and AI can reduce respondent burden and improve data quality, but integrating them with traditional diaries is challenging due to measurement and representation differences, making it difficult to produce consistent statistics over time or in mixed-source contexts.

Method: The paper distinguishes between mixed-mode approach (minimizing measurement differences for straightforward data merging) and multisource approach (maintaining inherent mode differences and integrating data at modeling stage), using travel surveys as an illustrative example to explore benefits and drawbacks.

Result: The analysis reveals the trade-offs between the two integration approaches and their respective advantages for different statistical contexts and objectives.

Conclusion: A decision framework is proposed to guide researchers in selecting the appropriate integration strategy based on their specific needs, balancing the benefits of outcome alignment versus exploiting the unique strengths of each data source.

Abstract: Smart surveys are surveys that make use of sensors and machine intelligence
to reduce respondent burden and increase data quality. Smart surveys have been
tests as a way to improve diary surveys in official statistics, where data are
collected on topics such as travel, time use and household expenditures. There
are often inherent differences both in measurement and representation between
smart surveys and traditional diaries, which makes it difficult to integrate
both data sources in producing statistics over time, or within a mixed- or
multi-source context. This paper distinguishes two different approaches to
integration: the mixed-mode approach, which prioritizes outcome alignment and
minimizes measurement differences for straightforward data merging, and the
multisource approach, which maintains inherent mode differences and integrates
data at the modeling stage, allowing exploitation of the strengths of each
source. Using travel surveys as an illustrative example, we explore the
benefits and drawbacks of each approach, and propose a decision framework to
guide researchers in selecting the appropriate integration strategy.

</details>


### [119] [Density estimation for compositional data using nonparametric mixtures](https://arxiv.org/abs/2510.07608)
*Jiajin Xie,Yong Wang,Eduardo García-Portugués*

Main category: stat.ME

TL;DR: A nonparametric Dirichlet mixture framework for compositional data density estimation that naturally handles boundary values without transformations, outperforming existing methods in simulations and real applications.


<details>
  <summary>Details</summary>
Motivation: Compositional data with simplex constraints appear in many fields, but existing nonparametric density estimation methods using transformations often introduce bias near boundaries and struggle with zero/near-zero values.

Method: Proposes nonparametric Dirichlet mixtures that naturally accommodate boundary values without transformations or zero-replacement, with bandwidth selection and initialization schemes. Also compares with Gaussian mixtures using log-ratio transformations.

Result: Extensive simulations show the proposed estimators outperform existing approaches. Real applications in GDP analysis, handwritten digit recognition, and skin detection demonstrate practical usefulness.

Conclusion: Nonparametric Dirichlet mixtures provide a reliable framework for compositional data density estimation, effectively handling boundary values and zero/near-zero observations without transformation-induced bias.

Abstract: Compositional data, representing proportions constrained to the simplex,
arise in diverse fields such as geosciences, ecology, genomics, and microbiome
research. Existing nonparametric density estimation methods often rely on
transformations, which may induce substantial bias near the simplex boundary.
We propose a nonparametric mixture-based framework for density estimation on
compositions. Nonparametric Dirichlet mixtures are employed to naturally
accommodate boundary values, thereby avoiding the transformation or
zero-replacement, while also identifying components supported on the boundary,
providing reliable estimates for data with zero or near-zero values. Bandwidth
selection and initialization schemes are addressed. For comparison,
nonparametric Gaussian mixtures, coupled with log-ratio transformations, are
also considered. Extensive simulations show that the proposed estimators
outperform existing approaches. Three real data applications, including GDP
data analysis, handwritten digit recognition, and skin detection, demonstrate
the usefulness of nonparametric Dirichlet mixtures in practice.

</details>


### [120] [Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data](https://arxiv.org/abs/2510.07770)
*Zhi Yang Tho,Raymond Chambers,A. H. Welsh*

Main category: stat.ME

TL;DR: Proposes two new bootstrap methods (proportional and modified random effect block bootstraps) for linear mixed models with highly unbalanced cluster sizes, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional random effect block bootstrap methods are only consistent for balanced cluster sizes, but real-world clustered data often has highly unbalanced cluster sizes, requiring new methods that can handle such cases.

Method: Developed proportional random effect block bootstrap and modified random effect block bootstrap that accommodate general distributions of random effects and error terms, generalizing the original random effect block bootstrap designed for balanced cases.

Result: Both proposed bootstraps achieve Fisher consistency under general cluster sizes, while the original method is only consistent for balanced clusters. Simulations show superior finite sample performance, and application to Oman rainfall data (cluster sizes 1-58) demonstrates improved confidence intervals and statistically significant effect of ionization technology on rainfall.

Conclusion: The proposed bootstrap methods effectively handle highly unbalanced cluster sizes in linear mixed models, providing reliable inference where traditional methods fail, with practical applications in real-world datasets like rainfall enhancement trials.

Abstract: Clustered data arise naturally in many scientific and applied research
settings where units are grouped within clusters. They are commonly analyzed
using linear mixed models to account for within-cluster correlations. This
article focuses on the scenario in which cluster sizes might be highly
unbalanced and proposes a proportional random effect block bootstrap and a
modified random effect block bootstrap, which are applicable in such cases and
accommodate general distributions of random effects and error terms. These
methods generalize the random effect block bootstrap, originally designed for
the balanced case, and can be used for inference on parameters of linear mixed
models or functions thereof. Both proposed bootstraps are shown to enjoy Fisher
consistency under general cluster sizes, while the original random effect block
bootstrap is consistent only for balanced clusters. Simulations demonstrate
strong finite sample inferential performance of the proposed bootstraps
relative to the random effect block bootstrap and other existing bootstrap
methods for clustered data. Application to the Oman rainfall enhancement trial
dataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap
confidence intervals using the proposed bootstraps over the random effect block
bootstrap and a statistically significant effect of the ionization technology
on rainfall.

</details>


### [121] [Detection of mean changes in partially observed functional data](https://arxiv.org/abs/2510.07854)
*Šárka Hudecová,Claudia Kirch*

Main category: stat.ME

TL;DR: A test for detecting mean changes in partially observed functional data, handling both abrupt and gradual changes, using permutation testing with fixed or random sample sizes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting mean changes in functional data that are only partially observed on subsets of the domain, with no information available on the unobserved parts.

Method: Permutation test approach with two variants: classical fixed-number permutation samples and a controlled resampling risk variant using random data-driven number of permutation samples.

Result: The methodology demonstrates good performance in small sample scenarios as shown through Monte Carlo simulation studies and real data applications.

Conclusion: The proposed test effectively detects mean changes in partially observed functional data across both abrupt and gradual change scenarios, with robust performance validated through simulations and real-world applications.

Abstract: We propose a test for a change in the mean for a sequence of functional
observations that are only partially observed on subsets of the domain, with no
information available on the complement. The framework accommodates important
scenarios, including both abrupt and gradual changes. The significance of the
test statistic is assessed via a permutation test. In addition to the classical
permutation approach with a fixed number of permutation samples, we also
discuss a variant with controlled resampling risk that relies on a random
(data-driven) number of permutation samples. The small sample performance of
the proposed methodology is illustrated in a Monte Carlo simulation study and
an application to real data.

</details>


### [122] [Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles](https://arxiv.org/abs/2510.08204)
*Soham Ghosh,Saloni Bhogale,Sameer K. Deshpande*

Main category: stat.ME

TL;DR: SparseVCBART is a Bayesian method for high-dimensional varying-coefficient models that uses regression tree ensembles with sparsity-inducing priors to identify important covariates and effect modifiers while providing accurate predictions and well-calibrated uncertainty intervals.


<details>
  <summary>Details</summary>
Motivation: Existing varying-coefficient models fail in high-dimensional settings where the number of covariates and/or effect modifiers exceeds observations, making it difficult to identify which covariates have non-zero effects and which effect modifiers drive these effects.

Method: A fully Bayesian model that approximates each coefficient function with regression tree ensembles, using global-local shrinkage priors on leaf outputs and hierarchical priors on splitting probabilities to encourage sparsity.

Result: SparseVCBART achieves competitive predictive accuracy, substantially narrower and better-calibrated uncertainty intervals (especially for null effects), and its posterior contracts at near-minimax optimal rates while adapting to unknown sparsity and smoothness.

Conclusion: SparseVCBART provides an effective solution for high-dimensional varying-coefficient modeling, demonstrated through an application investigating how interpersonal conversations affect prejudice based on political and demographic characteristics.

Abstract: By allowing the effects of $p$ covariates in a linear regression model to
vary as functions of $R$ additional effect modifiers, varying-coefficient
models (VCMs) strike a compelling balance between interpretable-but-rigid
parametric models popular in classical statistics and flexible-but-opaque
methods popular in machine learning. But in high-dimensional settings where $p$
and/or $R$ exceed the number of observations, existing approaches to fitting
VCMs fail to identify which covariates have a non-zero effect and which effect
modifiers drive these effects. We propose sparseVCBART, a fully Bayesian model
that approximates each coefficient function in a VCM with a regression tree
ensemble and encourages sparsity with a global--local shrinkage prior on the
regression tree leaf outputs and a hierarchical prior on the splitting
probabilities of each tree. We show that the sparseVCBART posterior contracts
at a near-minimax optimal rate, automatically adapting to the unknown sparsity
structure and smoothness of the true coefficient functions. Compared to
existing state-of-the-art methods, sparseVCBART achieved competitive predictive
accuracy and substantially narrower and better-calibrated uncertainty
intervals, especially for null covariate effects. We use sparseVCBART to
investigate how the effects of interpersonal conversations on prejudice could
vary according to the political and demographic characteristics of the
respondents.

</details>


### [123] [Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data](https://arxiv.org/abs/2510.08304)
*Matteo Amestoy,Mark van de Wiel,Jeroen Lakerveld,Wessel van Wieringen*

Main category: stat.ME

TL;DR: A novel Bayesian profile regression framework that integrates linear mixed models to analyze longitudinal exposome data, addressing collinearity, repeated measurements, and interactions with individual characteristics.


<details>
  <summary>Details</summary>
Motivation: The exposome (diverse non-genetic factors) is crucial for health outcomes but presents methodological challenges including high collinearity among exposures, longitudinal data structure, and complex interactions with individual characteristics.

Method: Extends Bayesian profile regression by integrating it with linear mixed models (LMM) to create a profile-LMM approach that clusters exposures into latent profiles while accounting for within-person variability over time and incorporating interactions with individual characteristics.

Result: Validated using simulated data showing accurate parameter identification and recovery of true latent exposure cluster structure. Applied to Lifelines cohort data to identify exposure combinations significantly associated with diastolic blood pressure.

Conclusion: The proposed profile-LMM framework effectively addresses key challenges in exposome analysis by combining the strengths of profile regression for handling collinearity with LMM's ability to model longitudinal data and interactions.

Abstract: Exposure to diverse non-genetic factors, known as the exposome, is a critical
determinant of health outcomes. However, analyzing the exposome presents
significant methodological challenges, including: high collinearity among
exposures, the longitudinal nature of repeated measurements, and potential
complex interactions with individual characteristics. In this paper, we address
these challenges by proposing a novel statistical framework that extends
Bayesian profile regression. Our method integrates profile regression, which
handles collinearity by clustering exposures into latent profiles, into a
linear mixed model (LMM), a framework for longitudinal data analysis. This
profile-LMM approach effectively accounts for within-person variability over
time while also incorporating interactions between the latent exposure clusters
and individual characteristics. We validate our method using simulated data,
demonstrating its ability to accurately identify model parameters and recover
the true latent exposure cluster structure. Finally, we apply this approach to
a large longitudinal data set from the Lifelines cohort to identify
combinations of exposures that are significantly associated with diastolic
blood pressure.

</details>


### [124] [Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials](https://arxiv.org/abs/2510.08359)
*Jinho Cha,Eunchan Cha*

Main category: stat.ME

TL;DR: Proposes a doubly robust estimated mean excursion effect (DR-EMEE) estimator for micro-randomized trials with binary outcomes, combining inverse probability weighting and outcome regression with stabilized weights to improve efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Standard IPW estimators are unbiased but unstable in small samples or under extreme randomization, while EMEE improves efficiency but lacks double robustness, creating a need for more robust and efficient estimators.

Method: Developed DR-EMEE with stabilized and truncated weights that combines per-decision inverse probability weighting and outcome regression, with theoretical guarantees of double robustness and asymptotic efficiency, plus finite-sample variance corrections.

Result: Simulations show DR-EMEE reduces root mean squared error, improves coverage, and achieves up to twofold efficiency gains over IPW and 5-10% gains over EMEE. Applications to real datasets confirm stable and efficient inference across randomized and observational settings.

Conclusion: DR-EMEE provides a doubly robust, efficient, and stable estimator for micro-randomized trials that outperforms existing methods and works well in both randomized and observational contexts.

Abstract: Micro-randomized trials (MRTs) are increasingly used to evaluate mobile
health interventions with binary proximal outcomes. Standard inverse
probability weighting (IPW) estimators are unbiased but unstable in small
samples or under extreme randomization. Estimated mean excursion effect (EMEE)
improves efficiency but lacks double robustness. We propose a doubly robust
EMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision
IPW and outcome regression. We prove double robustness, asymptotic efficiency,
and provide finite-sample variance corrections, with extensions to machine
learning nuisance estimators. In simulations, DR-EMEE reduces root mean squared
error, improves coverage, and achieves up to twofold efficiency gains over IPW
and five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and
mHealth datasets confirm stable and efficient inference across both randomized
and observational settings.

</details>


### [125] [Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes](https://arxiv.org/abs/2510.08438)
*Xi Fang,Bingkai Wang,Liangyuan Hu,Fan Li*

Main category: stat.ME

TL;DR: This paper proposes doubly robust estimators for cluster-level and individual-level treatment effects in cluster-randomized trials with right-censored survival outcomes, ensuring consistency when either the censoring model or outcome model is correctly specified.


<details>
  <summary>Details</summary>
Motivation: Cluster-randomized trials require distinguishing between treatment effects at cluster and individual levels, particularly for survival outcomes where censoring complicates analysis. Existing methods need robust approaches that handle covariate-dependent censoring.

Method: The authors formally characterize treatment effect estimands under potential outcomes framework, propose doubly robust estimators that work when either censoring model or outcome model is correct, explore modeling options for censoring/survival distributions, and use deletion-based jackknife for variance estimation.

Result: Extensive simulations show the proposed methods perform adequately in finite samples. The method is demonstrated through analysis of a completed cluster-randomized trial with survival endpoints.

Conclusion: The proposed doubly robust estimators provide reliable estimation of treatment effects in cluster-randomized trials with survival outcomes, offering protection against model misspecification through their robustness properties.

Abstract: Cluster-randomized trials (CRTs) are experimental designs where groups or
clusters of participants, rather than the individual participants themselves,
are randomized to intervention groups. Analyzing CRT requires distinguishing
between treatment effects at the cluster level and the individual level, which
requires a clear definition of the estimands under the potential outcomes
framework. For analyzing survival outcomes, it is common to assess the
treatment effect by comparing survival functions or restricted mean survival
times between treatment groups. In this article, we formally characterize
cluster-level and individual-level treatment effect estimands with
right-censored survival outcomes in CRTs and propose doubly robust estimators
for targeting such estimands. Under covariate-dependent censoring, our
estimators ensure consistency when either the censoring model or the outcome
model is correctly specified, but not necessarily both. We explore different
modeling options for the censoring and outcome models to estimate the censoring
and survival distributions, and investigate a deletion-based jackknife method
for variance and interval estimation. Extensive simulations demonstrate that
the proposed methods perform adequately in finite samples. Finally, we
illustrate our method by analyzing a completed CRT with survival endpoints.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [126] [Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death](https://arxiv.org/abs/2510.07501)
*Sihyung Park,Wenbin Lu,Shu Yang*

Main category: stat.ML

TL;DR: A principal stratification method for evaluating dynamic treatment regimes in critical care settings where truncation by death makes traditional methods inapplicable.


<details>
  <summary>Details</summary>
Motivation: Traditional DTR evaluation methods fail when patients die during treatment (truncation by death), making potential outcomes ill-defined and requiring new approaches.

Method: Principal stratification focusing on always-survivor value function with semiparametrically efficient, multiply robust estimator for multi-stage DTRs.

Result: The proposed estimator demonstrates robustness and efficiency in empirical validation and electronic health records application.

Conclusion: The method provides utility for personalized treatment optimization in critical care settings affected by truncation by death.

Abstract: Truncation by death, a prevalent challenge in critical care, renders
traditional dynamic treatment regime (DTR) evaluation inapplicable due to
ill-defined potential outcomes. We introduce a principal stratification-based
method, focusing on the always-survivor value function. We derive a
semiparametrically efficient, multiply robust estimator for multi-stage DTRs,
demonstrating its robustness and efficiency. Empirical validation and an
application to electronic health records showcase its utility for personalized
treatment optimization.

</details>


### [127] [From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation](https://arxiv.org/abs/2510.07624)
*Abdelhakim Benechehab,Gabriel Singer,Corentin Léger,Youssef Attia El Hili,Giuseppe Paolo,Albert Thomas,Maurizio Filippone,Balázs Kégl*

Main category: stat.ML

TL;DR: The paper proposes a bilevel optimization framework to align generative models using only high-quality datasets, addressing limitations of Maximum Likelihood Estimation and Reinforcement Learning approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional Maximum Likelihood Estimation has limitations in generalization and susceptibility to catastrophic forgetting, while Reinforcement Learning methods require explicit reward signals that are often unavailable in practice.

Method: A Bilevel Optimization framework where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level.

Result: Theoretical analysis in a tractable setting provides insights that generalize to applications such as tabular classification and model-based reinforcement learning.

Conclusion: The proposed framework enables alignment of generative models using only high-quality datasets without requiring explicit reward signals, bridging the gap between Maximum Likelihood Estimation and Reinforcement Learning approaches.

Abstract: Generative models form the backbone of modern machine learning, underpinning
state-of-the-art systems in text, vision, and multimodal applications. While
Maximum Likelihood Estimation has traditionally served as the dominant training
paradigm, recent work have highlighted its limitations, particularly in
generalization and susceptibility to catastrophic forgetting compared to
Reinforcement Learning techniques, such as Policy Gradient methods. However,
these approaches depend on explicit reward signals, which are often unavailable
in practice, leaving open the fundamental problem of how to align generative
models when only high-quality datasets are accessible. In this work, we address
this challenge via a Bilevel Optimization framework, where the reward function
is treated as the optimization variable of an outer-level problem, while a
policy gradient objective defines the inner-level. We then conduct a
theoretical analysis of this optimization problem in a tractable setting and
extract insights that, as we demonstrate, generalize to applications such as
tabular classification and model-based reinforcement learning. We release the
code at https://github.com/abenechehab/nll_to_po .

</details>


### [128] [On the Optimality of the Median-of-Means Estimator under Adversarial Contamination](https://arxiv.org/abs/2510.07867)
*Xabier de Juan,Santiago Mazuelas*

Main category: stat.ML

TL;DR: This paper analyzes the Median-of-Means (MoM) estimator's optimality under adversarial contamination, showing it's minimax optimal for distributions with finite variance and infinite variance with finite absolute (1+r)-th moment, but sub-optimal for light-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: While MoM is known to be optimal for i.i.d. samples, its performance and optimality under adversarial contamination scenarios remain unknown beyond Gaussian cases, particularly for various distribution classes.

Method: The authors present theoretical analysis with upper and lower bounds for MoM's error under adversarial contamination, examining multiple distribution classes including finite variance distributions and infinite variance distributions with finite absolute moments.

Result: MoM is proven to be minimax optimal for distributions with finite variance and for distributions with infinite variance but finite absolute (1+r)-th moment. However, it is shown to be sub-optimal for light-tailed distributions.

Conclusion: The Median-of-Means estimator achieves minimax optimality under adversarial contamination for broad classes of heavy-tailed distributions, but has limitations for light-tailed distributions where it performs sub-optimally.

Abstract: The Median-of-Means (MoM) is a robust estimator widely used in machine
learning that is known to be (minimax) optimal in scenarios where samples are
i.i.d. In more grave scenarios, samples are contaminated by an adversary that
can inspect and modify the data. Previous work has theoretically shown the
suitability of the MoM estimator in certain contaminated settings. However, the
(minimax) optimality of MoM and its limitations under adversarial contamination
remain unknown beyond the Gaussian case. In this paper, we present upper and
lower bounds for the error of MoM under adversarial contamination for multiple
classes of distributions. In particular, we show that MoM is (minimax) optimal
in the class of distributions with finite variance, as well as in the class of
distributions with infinite variance and finite absolute $(1+r)$-th moment. We
also provide lower bounds for MoM's error that match the order of the presented
upper bounds, and show that MoM is sub-optimal for light-tailed distributions.

</details>


### [129] [A Honest Cross-Validation Estimator for Prediction Performance](https://arxiv.org/abs/2510.07649)
*Tianyu Pan,Vincent Z. Yu,Viswanath Devanarayan,Lu Tian*

Main category: stat.ML

TL;DR: The paper proposes new estimators that improve cross-validation by better estimating performance of models trained on specific training sets, using random-effects models to enhance naive single-split estimators.


<details>
  <summary>Details</summary>
Motivation: Standard cross-validation doesn't directly estimate performance of the specific model recommended for future use, as it averages across different data splits rather than focusing on the particular model trained on a specific training set.

Method: Developed two estimators: hierarchical Bayesian estimator and empirical Bayes estimator that use cross-validation results from other random splits to improve naive single-split estimators within a random-effects model framework.

Result: The proposed estimators perform similarly to or better than both conventional cross-validation and naive single-split estimators, with superior performance demonstrated through simulations and real-data examples.

Conclusion: The new random-effects model framework provides improved performance estimation for specific trained models compared to traditional cross-validation methods.

Abstract: Cross-validation is a standard tool for obtaining a honest assessment of the
performance of a prediction model. The commonly used version repeatedly splits
data, trains the prediction model on the training set, evaluates the model
performance on the test set, and averages the model performance across
different data splits. A well-known criticism is that such cross-validation
procedure does not directly estimate the performance of the particular model
recommended for future use. In this paper, we propose a new method to estimate
the performance of a model trained on a specific (random) training set. A naive
estimator can be obtained by applying the model to a disjoint testing set.
Surprisingly, cross-validation estimators computed from other random splits can
be used to improve this naive estimator within a random-effects model
framework. We develop two estimators -- a hierarchical Bayesian estimator and
an empirical Bayes estimator -- that perform similarly to or better than both
the conventional cross-validation estimator and the naive single-split
estimator. Simulations and a real-data example demonstrate the superior
performance of the proposed method.

</details>


### [130] [When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making](https://arxiv.org/abs/2510.07750)
*Wenbin Zhou,Shixiang Zhu*

Main category: stat.ML

TL;DR: A new framework for robust optimization that provides finite-sample guarantees on both miscoverage and regret, enabling data-driven calibration of robustness levels according to cost-risk preferences.


<details>
  <summary>Details</summary>
Motivation: Traditional robust optimization requires prespecified robustness levels chosen ad hoc, leading to either insufficient protection or overly conservative solutions. Existing conformal prediction methods still fix coverage targets a priori without guidance for selecting appropriate robustness levels.

Method: Proposes a framework that constructs valid estimators to trace out the miscoverage-regret Pareto frontier, allowing decision-makers to evaluate and calibrate robustness levels based on their cost-risk preferences. The method is simple to implement and broadly applicable across classical optimization formulations.

Result: The framework achieves sharper finite-sample performance than existing approaches and provides the first principled data-driven methodology for guiding robustness selection.

Conclusion: This approach empowers practitioners to balance robustness and conservativeness in high-stakes decision-making by offering distribution-free, finite-sample guarantees on both miscoverage and regret for any family of robust predict-then-optimize policies.

Abstract: Robust optimization safeguards decisions against uncertainty by optimizing
against worst-case scenarios, yet their effectiveness hinges on a prespecified
robustness level that is often chosen ad hoc, leading to either insufficient
protection or overly conservative and costly solutions. Recent approaches using
conformal prediction construct data-driven uncertainty sets with finite-sample
coverage guarantees, but they still fix coverage targets a priori and offer
little guidance for selecting robustness levels. We propose a new framework
that provides distribution-free, finite-sample guarantees on both miscoverage
and regret for any family of robust predict-then-optimize policies. Our method
constructs valid estimators that trace out the miscoverage-regret Pareto
frontier, enabling decision-makers to reliably evaluate and calibrate
robustness levels according to their cost-risk preferences. The framework is
simple to implement, broadly applicable across classical optimization
formulations, and achieves sharper finite-sample performance than existing
approaches. These results offer the first principled data-driven methodology
for guiding robustness selection and empower practitioners to balance
robustness and conservativeness in high-stakes decision-making.

</details>


### [131] [Surrogate Graph Partitioning for Spatial Prediction](https://arxiv.org/abs/2510.07832)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: The paper proposes a graph partitioning approach to create spatial segments that minimize within-segment prediction variances, addressing interpretability needs in spatial prediction.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between advanced spatial prediction models and practical industry needs for interpretability, using surrogate models to explain black-box predictors.

Method: Formulates spatial segmentation as a mixed-integer quadratic programming problem and develops an approximation scheme leveraging graph partitioning structural properties to handle computational complexity.

Result: Experimental results show the approximation scheme achieves computational efficiency in identifying spatial segments while maintaining effectiveness.

Conclusion: The proposed graph partitioning approach provides an efficient and interpretable solution for spatial segmentation, making complex spatial prediction models more accessible for practical applications.

Abstract: Spatial prediction refers to the estimation of unobserved values from
spatially distributed observations. Although recent advances have improved the
capacity to model diverse observation types, adoption in practice remains
limited in industries that demand interpretability. To mitigate this gap,
surrogate models that explain black-box predictors provide a promising path
toward interpretable decision making. In this study, we propose a graph
partitioning problem to construct spatial segments that minimize the sum of
within-segment variances of individual predictions. The assignment of data
points to segments can be formulated as a mixed-integer quadratic programming
problem. While this formulation potentially enables the identification of exact
segments, its computational complexity becomes prohibitive as the number of
data points increases. Motivated by this challenge, we develop an approximation
scheme that leverages the structural properties of graph partitioning.
Experimental results demonstrate the computational efficiency of this
approximation in identifying spatial segments.

</details>


### [132] [On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses](https://arxiv.org/abs/2510.07862)
*Sanghwa Kim,Dohyun Ahn,Seungki Min*

Main category: stat.ML

TL;DR: The paper proposes an adaptive testing algorithm that uses Fisher information to select questions and a method-of-moments approach to estimate ability parameters, achieving optimal performance in both fixed-confidence and fixed-budget settings.


<details>
  <summary>Details</summary>
Motivation: To develop efficient adaptive testing procedures that can certify estimation accuracy with minimal queries, addressing challenges in adaptive testing and online preference learning where binary responses are used to estimate continuous ability parameters.

Method: Proposes a Fisher-tracking algorithm that adaptively selects questions to maximize Fisher information, uses method-of-moments for estimation updates, and employs a novel test statistic to determine when estimates reach desired accuracy.

Result: The algorithm achieves optimal performance in both fixed-confidence and fixed-budget regimes, overcoming technical challenges related to dependence between evolving estimates and query distributions through structural symmetry analysis and large deviation tools.

Conclusion: The Fisher-tracking strategy provides rigorous theoretical support for simple and efficient adaptive testing procedures, with proven optimality in standard performance regimes.

Abstract: We study the problem of estimating a continuous ability parameter from
sequential binary responses by actively asking questions with varying
difficulties, a setting that arises naturally in adaptive testing and online
preference learning. Our goal is to certify that the estimate lies within a
desired margin of error, using as few queries as possible. We propose a simple
algorithm that adaptively selects questions to maximize Fisher information and
updates the estimate using a method-of-moments approach, paired with a novel
test statistic to decide when the estimate is accurate enough. We prove that
this Fisher-tracking strategy achieves optimal performance in both
fixed-confidence and fixed-budget regimes, which are commonly invested in the
best-arm identification literature. Our analysis overcomes a key technical
challenge in the fixed-budget setting -- handling the dependence between the
evolving estimate and the query distribution -- by exploiting a structural
symmetry in the model and combining large deviation tools with Ville's
inequality. Our results provide rigorous theoretical support for simple and
efficient adaptive testing procedures.

</details>


### [133] [Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference](https://arxiv.org/abs/2510.07965)
*Seungsu Han,Juyoung Hwang,Won Chang*

Main category: stat.ML

TL;DR: StiCTAF is a novel normalizing flow method that uses a stick-breaking mixture base with tail adaptation to better approximate complex posterior distributions with multimodality and heavy tails.


<details>
  <summary>Details</summary>
Motivation: Traditional normalizing flows with Gaussian bases struggle to capture complex posterior distributions that exhibit multimodality and heavy tails, limiting their effectiveness in Bayesian inference.

Method: The method learns a flexible mixture base to address mode-seeking bias through weighted component-wise ELBOs, estimates local tail indices of unnormalized densities, and refines mixture components using shared backbone with component-specific tail transforms calibrated by estimated indices.

Result: Experiments on synthetic posteriors show improved tail recovery and better coverage of multiple modes compared to benchmark models, with practical benefits demonstrated in real-data analysis.

Conclusion: StiCTAF enables accurate mode coverage and anisotropic tail modeling while maintaining exact density evaluation and stable optimization, providing superior posterior approximation for complex distributions.

Abstract: Normalizing flows with a Gaussian base provide a computationally efficient
way to approximate posterior distributions in Bayesian inference, but they
often struggle to capture complex posteriors with multimodality and heavy
tails. We propose a stick-breaking mixture base with component-wise tail
adaptation (StiCTAF) for posterior approximation. The method first learns a
flexible mixture base to mitigate the mode-seeking bias of reverse KL
divergence through a weighted average of component-wise ELBOs. It then
estimates local tail indices of unnormalized densities and finally refines each
mixture component using a shared backbone combined with component-specific tail
transforms calibrated by the estimated indices. This design enables accurate
mode coverage and anisotropic tail modeling while retaining exact density
evaluation and stable optimization. Experiments on synthetic posteriors
demonstrate improved tail recovery and better coverage of multiple modes
compared to benchmark models. We also present a real-data analysis illustrating
the practical benefits of our approach for posterior inference.

</details>


### [134] [Beyond Real Data: Synthetic Data through the Lens of Regularization](https://arxiv.org/abs/2510.08095)
*Amitis Shidani,Tyler Farghly,Yang Sun,Habib Ganjgahi,George Deligiannidis*

Main category: stat.ML

TL;DR: A learning-theoretic framework that quantifies the optimal synthetic-to-real data ratio using algorithmic stability and Wasserstein distance, showing a U-shaped test error curve with respect to synthetic data proportion.


<details>
  <summary>Details</summary>
Motivation: Synthetic data can improve generalization when real data is scarce, but excessive reliance may introduce distributional mismatches that degrade performance, necessitating a principled approach to balance synthetic and real data usage.

Method: Leverages algorithmic stability to derive generalization error bounds in kernel ridge regression with mixed data, characterizing the optimal synthetic-to-real data ratio as a function of Wasserstein distance between distributions.

Result: Theory predicts existence of optimal synthetic-to-real ratio leading to U-shaped test error behavior, empirically validated on CIFAR-10 and clinical brain MRI datasets. Framework extends to domain adaptation showing synthetic target data can mitigate domain shift.

Conclusion: Provides practical guidance for applying optimal synthetic-to-real data ratios in both in-domain and out-of-domain scenarios, demonstrating that carefully blending synthetic and real data can enhance generalization while avoiding distributional mismatches.

Abstract: Synthetic data can improve generalization when real data is scarce, but
excessive reliance may introduce distributional mismatches that degrade
performance. In this paper, we present a learning-theoretic framework to
quantify the trade-off between synthetic and real data. Our approach leverages
algorithmic stability to derive generalization error bounds, characterizing the
optimal synthetic-to-real data ratio that minimizes expected test error as a
function of the Wasserstein distance between the real and synthetic
distributions. We motivate our framework in the setting of kernel ridge
regression with mixed data, offering a detailed analysis that may be of
independent interest. Our theory predicts the existence of an optimal ratio,
leading to a U-shaped behavior of test error with respect to the proportion of
synthetic data. Empirically, we validate this prediction on CIFAR-10 and a
clinical brain MRI dataset. Our theory extends to the important scenario of
domain adaptation, showing that carefully blending synthetic target data with
limited source data can mitigate domain shift and enhance generalization. We
conclude with practical guidance for applying our results to both in-domain and
out-of-domain scenarios.

</details>


### [135] [High-dimensional Analysis of Synthetic Data Selection](https://arxiv.org/abs/2510.08123)
*Parham Rezaei,Filip Kovacevic,Francesco Locatello,Marco Mondelli*

Main category: stat.ML

TL;DR: The paper analyzes how synthetic data affects classifier performance, showing that covariance shift (not mean shift) impacts generalization error in linear models, and this insight extends to deep neural networks and generative models.


<details>
  <summary>Details</summary>
Motivation: To understand which specific properties of synthetic data affect generalization error, as current heuristic principles like 'synthetic data should be close to real data distribution' are insufficient.

Method: Theoretical analysis through high-dimensional regression lens for linear models, followed by empirical validation with deep neural networks and generative models across various architectures, datasets, and training paradigms.

Result: Covariance shift between target distribution and synthetic data affects generalization error, while mean shift does not. Covariance matching procedure outperforms recent synthetic data selection approaches.

Conclusion: Matching the covariance of synthetic data with target distribution is optimal in some settings, and theoretical insights from linear models generalize to deep neural networks and generative models.

Abstract: Despite the progress in the development of generative models, their
usefulness in creating synthetic data that improve prediction performance of
classifiers has been put into question. Besides heuristic principles such as
"synthetic data should be close to the real data distribution", it is actually
not clear which specific properties affect the generalization error. Our paper
addresses this question through the lens of high-dimensional regression.
Theoretically, we show that, for linear models, the covariance shift between
the target distribution and the distribution of the synthetic data affects the
generalization error but, surprisingly, the mean shift does not. Furthermore we
prove that, in some settings, matching the covariance of the target
distribution is optimal. Remarkably, the theoretical insights from linear
models carry over to deep neural networks and generative models. We empirically
demonstrate that the covariance matching procedure (matching the covariance of
the synthetic data with that of the data coming from the target distribution)
performs well against several recent approaches for synthetic data selection,
across training paradigms, architectures, datasets and generative models used
for augmentation.

</details>


### [136] [PAC Learnability in the Presence of Performativity](https://arxiv.org/abs/2510.08335)
*Ivan Kirev,Lyuben Baltadzhiev,Nikola Konstantinov*

Main category: stat.ML

TL;DR: The paper studies whether performative binary classification problems (where models cause distribution shifts) are PAC-learnable. It introduces a performative empirical risk function that uses only original distribution data and accounts for performative effects, showing that PAC-learnable hypothesis spaces remain learnable under performative shifts.


<details>
  <summary>Details</summary>
Motivation: Machine learning models deployed in real-world applications often cause performative shifts in test distributions, leading to decreased performance. Since models are trained on original distributions, this creates a need to understand when such performative classification problems remain learnable.

Method: The authors construct a performative empirical risk function that depends only on original distribution data and the type of performative effect. This function provides an unbiased estimate of the true risk on the shifted distribution. They consider scenarios including linear shifts in label distribution and more general changes in both labels and features.

Result: The paper shows that any PAC-learnable hypothesis space in standard binary classification remains PAC-learnable for the considered performative scenarios. Experimental evaluation on synthetic and real data demonstrates the benefits of their performative risk minimization approach.

Conclusion: Performative binary classification problems can be PAC-learnable when using the proposed performative empirical risk function, which enables learning from original distribution data while accounting for performative effects on the test distribution.

Abstract: Following the wide-spread adoption of machine learning models in real-world
applications, the phenomenon of performativity, i.e. model-dependent shifts in
the test distribution, becomes increasingly prevalent. Unfortunately, since
models are usually trained solely based on samples from the original
(unshifted) distribution, this performative shift may lead to decreased
test-time performance. In this paper, we study the question of whether and when
performative binary classification problems are learnable, via the lens of the
classic PAC (Probably Approximately Correct) learning framework. We motivate
several performative scenarios, accounting in particular for linear shifts in
the label distribution, as well as for more general changes in both the labels
and the features. We construct a performative empirical risk function, which
depends only on data from the original distribution and on the type
performative effect, and is yet an unbiased estimate of the true risk of a
classifier on the shifted distribution. Minimizing this notion of performative
risk allows us to show that any PAC-learnable hypothesis space in the standard
binary classification setting remains PAC-learnable for the considered
performative scenarios. We also conduct an extensive experimental evaluation of
our performative risk minimization method and showcase benefits on synthetic
and real data.

</details>


### [137] [Optimal Stopping in Latent Diffusion Models](https://arxiv.org/abs/2510.08409)
*Yu-Han Wu,Quentin Berthet,Gérard Biau,Claire Boyer,Romuald Elie,Pierre Marion*

Main category: stat.ML

TL;DR: The paper identifies that early stopping in Latent Diffusion Models (LDMs) can improve sample quality, contrary to conventional wisdom. This phenomenon is intrinsic to dimensionality reduction in LDMs, where lower-dimensional latent spaces benefit from earlier termination while higher-dimensional ones require later stopping.


<details>
  <summary>Details</summary>
Motivation: To understand why the final steps of diffusion in LDMs can degrade sample quality, and to provide a theoretical explanation for the interaction between latent dimension and stopping time that challenges conventional arguments about early stopping.

Method: The authors analyze the phenomenon using a Gaussian framework with linear autoencoders, characterizing conditions under which early stopping minimizes distance between generated and target distributions. They establish mathematical relationships between latent dimension, stopping time, and other hyperparameters like score matching constraints.

Result: Theoretical analysis shows that lower-dimensional representations benefit from earlier termination, while higher-dimensional latent spaces require later stopping time. Experiments on synthetic and real datasets confirm these properties, demonstrating that early stopping can improve generative quality in LDMs.

Conclusion: The study provides a theoretical foundation for understanding how latent dimension influences sample quality in LDMs, highlighting stopping time as a crucial hyperparameter that should be carefully tuned based on the latent space dimensionality.

Abstract: We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.

</details>


### [138] [Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models](https://arxiv.org/abs/2510.08465)
*Chih-Yu Chang,Ming-Chung Chang*

Main category: stat.ML

TL;DR: A2D2E is a new estimator for explaining black-box models that uses accelerated aggregated D-optimal designs to address limitations of existing methods like poor scalability, sensitivity to out-of-distribution sampling, and instability under correlated features.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for explaining black-box models have key limitations including poor scalability, sensitivity to out-of-distribution sampling, and instability under correlated features, which motivated the development of a more robust and efficient method.

Method: A2D2E uses accelerated aggregated D-optimal designs based on principled experimental design to improve efficiency and robustness in main effect estimation for explaining black-box models.

Result: The method establishes theoretical guarantees including convergence and variance reduction, and is validated through extensive simulations. It also shows potential in real-world applications including language models.

Conclusion: A2D2E provides an effective solution for explaining black-box models with improved efficiency, robustness, and theoretical guarantees, demonstrating practical value in real data applications and language models.

Abstract: Recent advances in supervised learning have driven growing interest in
explaining black-box models, particularly by estimating the effects of input
variables on model predictions. However, existing approaches often face key
limitations, including poor scalability, sensitivity to out-of-distribution
sampling, and instability under correlated features. To address these issues,
we propose A2D2E, an $\textbf{E}$stimator based on $\textbf{A}$ccelerated
$\textbf{A}$ggregated $\textbf{D}$-Optimal $\textbf{D}$esigns. Our method
leverages principled experimental design to improve efficiency and robustness
in main effect estimation. We establish theoretical guarantees, including
convergence and variance reduction, and validate A2D2E through extensive
simulations. We further provide the potential of the proposed method with a
case study on real data and applications in language models. The code to
reproduce the results can be found at https://github.com/cchihyu/A2D2E.

</details>


### [139] [Permutation-Invariant Spectral Learning via Dyson Diffusion](https://arxiv.org/abs/2510.08535)
*Tassilo Schwarz,Cai Dieball,Constantin Kogler,Kevin Lam,Renaud Lambiotte,Arnaud Doucet,Aljaž Godec,George Deligiannidis*

Main category: stat.ML

TL;DR: The paper introduces Dyson Diffusion Model, a novel graph diffusion approach that uses Dyson's Brownian Motion to capture spectral dynamics, overcoming limitations of existing models in distinguishing graph families without requiring ad hoc feature augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing graph diffusion models struggle to distinguish certain graph families unless augmented with additional features, due to enforcing inductive bias in the learning architecture rather than the diffusion dynamics.

Method: Leverages random matrix theory to analytically extract spectral properties of diffusion process, uses Dyson's Brownian Motion to capture spectral dynamics of Ornstein-Uhlenbeck process on adjacency matrix while retaining non-spectral information.

Result: The Dyson Diffusion Model learns graph spectra accurately and outperforms existing graph diffusion models in performance.

Conclusion: By pushing inductive bias from architecture into dynamics through spectral analysis, the proposed model overcomes limitations of existing approaches and achieves superior performance in graph generation tasks.

Abstract: Diffusion models are central to generative modeling and have been adapted to
graphs by diffusing adjacency matrix representations. The challenge of having
up to $n!$ such representations for graphs with $n$ nodes is only partially
mitigated by using permutation-equivariant learning architectures. Despite
their computational efficiency, existing graph diffusion models struggle to
distinguish certain graph families, unless graph data are augmented with ad hoc
features. This shortcoming stems from enforcing the inductive bias within the
learning architecture. In this work, we leverage random matrix theory to
analytically extract the spectral properties of the diffusion process, allowing
us to push the inductive bias from the architecture into the dynamics. Building
on this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian
Motion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the
adjacency matrix while retaining all non-spectral information. We demonstrate
that the Dyson Diffusion Model learns graph spectra accurately and outperforms
existing graph diffusion models.

</details>
