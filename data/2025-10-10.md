<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 9]
- [stat.ML](#stat.ML) [Total: 14]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Zero-Inflated Bayesian Multi-Study Infinite Non-Negative Matrix Factorization](https://arxiv.org/abs/2510.07518)
*Blake Hansen,Dafne Zorzetto,Valeria Edefonti,Roberta De Vito*

Main category: stat.ME

TL;DR: A novel Bayesian Non-Negative Matrix Factorization model that handles multi-study dietary data with zero inflation and covariate-driven heterogeneity, enabling better dietary pattern estimation and cancer risk analysis.


<details>
  <summary>Details</summary>
Motivation: Dietary intake data presents statistical challenges including high dimensionality, sparsity with excess zeros, and heterogeneity from individual covariates. Existing NMF methods using Poisson assumptions lack flexibility to address these complexities and cannot effectively integrate data across multiple studies.

Method: Developed a Bayesian NMF model that jointly models multi-study count data for cross-study information sharing, incorporates a mixture component for zero inflation, and uses flexible Bayesian non-parametric priors to characterize heterogeneity in pattern scores from individual covariates.

Result: Simulation studies show significant improvement in estimation accuracy compared to existing Bayesian NMF methods. Application to case-control studies on diet and upper aero-digestive tract cancers successfully identified nutritionally meaningful dietary patterns.

Conclusion: The proposed Bayesian NMF model effectively addresses key challenges in dietary data analysis, enabling better dietary pattern estimation and facilitating downstream association analyses with health outcomes like cancer risk.

Abstract: Understanding the association between dietary patterns and health outcomes,
such as the cancer risk, is crucial to inform public health guidelines and
shaping future dietary interventions. However, dietary intake data present
several statistical challenges: they are high-dimensional, often sparse with
excess zeros, and exhibit heterogeneity driven by individual-level covariates.
Non-Negative Matrix Factorization (NMF), commonly used to estimate patterns in
high-dimensional count data, typically relies on Poisson assumptions and lacks
the flexibility to fully address these complexities. Additionally, integrating
data across multiple studies, such as case-control studies on cancer risk,
requires models that can share information across sources while preserving
study-specific structure.
  In this paper, we introduce a novel Bayesian NMF model that (i) jointly
models multi-study count data to enable cross-study information sharing, (ii)
incorporate a mixture component to account for zero inflation, and (iii)
leverage flexible Bayesian non-parametric priors for characterizing the
heterogeneity in pattern scores induced by the individual covariates. This
structure allows for clustering of individuals based on dietary profiles,
enabling downstream association analyses with health outcomes. Through
extensive simulation studies, we demonstrate that our model significantly
improves estimation accuracy compared to existing Bayesian NMF methods.
  We further illustrate its utility through an application to multiple
case-control studies on diet and upper aero-digestive tract cancers,
identifying nutritionally meaningful dietary patterns. An R package
implementing our approach is available at
https://github.com/blhansen/ZIMultiStudyNMF.

</details>


### [2] [Integrating smart surveys with traditional surveys](https://arxiv.org/abs/2510.07521)
*Danielle Mccool,Peter Lugtig,Bella Struminskaya*

Main category: stat.ME

TL;DR: This paper analyzes two approaches for integrating smart surveys with traditional diaries in official statistics: mixed-mode (aligning outcomes for direct merging) and multisource (preserving mode differences and integrating through modeling).


<details>
  <summary>Details</summary>
Motivation: Smart surveys using sensors and AI can reduce respondent burden and improve data quality, but integrating them with traditional diaries is challenging due to measurement and representation differences.

Method: The paper distinguishes between mixed-mode and multisource integration approaches, using travel surveys as an illustrative example to explore benefits and drawbacks of each method.

Result: The analysis reveals trade-offs between the two approaches - mixed-mode enables straightforward data merging but may lose unique strengths of each source, while multisource preserves mode differences but requires more complex modeling.

Conclusion: The authors propose a decision framework to help researchers select the appropriate integration strategy based on their specific needs and context.

Abstract: Smart surveys are surveys that make use of sensors and machine intelligence
to reduce respondent burden and increase data quality. Smart surveys have been
tests as a way to improve diary surveys in official statistics, where data are
collected on topics such as travel, time use and household expenditures. There
are often inherent differences both in measurement and representation between
smart surveys and traditional diaries, which makes it difficult to integrate
both data sources in producing statistics over time, or within a mixed- or
multi-source context. This paper distinguishes two different approaches to
integration: the mixed-mode approach, which prioritizes outcome alignment and
minimizes measurement differences for straightforward data merging, and the
multisource approach, which maintains inherent mode differences and integrates
data at the modeling stage, allowing exploitation of the strengths of each
source. Using travel surveys as an illustrative example, we explore the
benefits and drawbacks of each approach, and propose a decision framework to
guide researchers in selecting the appropriate integration strategy.

</details>


### [3] [Density estimation for compositional data using nonparametric mixtures](https://arxiv.org/abs/2510.07608)
*Jiajin Xie,Yong Wang,Eduardo García-Portugués*

Main category: stat.ME

TL;DR: Proposes nonparametric Dirichlet mixtures for compositional data density estimation to handle boundary values without transformations, outperforming existing methods in simulations and real applications.


<details>
  <summary>Details</summary>
Motivation: Existing density estimation methods for compositional data often use transformations that cause bias near simplex boundaries, especially for data with zero or near-zero values.

Method: Uses nonparametric Dirichlet mixtures that naturally accommodate boundary values without transformations or zero-replacement, with bandwidth selection and initialization schemes.

Result: Extensive simulations show proposed estimators outperform existing approaches. Real applications in GDP analysis, digit recognition, and skin detection demonstrate practical usefulness.

Conclusion: Nonparametric Dirichlet mixtures provide reliable density estimation for compositional data with boundary values, avoiding transformation-induced bias and handling zero/near-zero values effectively.

Abstract: Compositional data, representing proportions constrained to the simplex,
arise in diverse fields such as geosciences, ecology, genomics, and microbiome
research. Existing nonparametric density estimation methods often rely on
transformations, which may induce substantial bias near the simplex boundary.
We propose a nonparametric mixture-based framework for density estimation on
compositions. Nonparametric Dirichlet mixtures are employed to naturally
accommodate boundary values, thereby avoiding the transformation or
zero-replacement, while also identifying components supported on the boundary,
providing reliable estimates for data with zero or near-zero values. Bandwidth
selection and initialization schemes are addressed. For comparison,
nonparametric Gaussian mixtures, coupled with log-ratio transformations, are
also considered. Extensive simulations show that the proposed estimators
outperform existing approaches. Three real data applications, including GDP
data analysis, handwritten digit recognition, and skin detection, demonstrate
the usefulness of nonparametric Dirichlet mixtures in practice.

</details>


### [4] [Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data](https://arxiv.org/abs/2510.07770)
*Zhi Yang Tho,Raymond Chambers,A. H. Welsh*

Main category: stat.ME

TL;DR: Proposes two new bootstrap methods (proportional and modified random effect block bootstraps) for linear mixed models with highly unbalanced cluster sizes, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Clustered data with highly unbalanced cluster sizes are common in research, but existing bootstrap methods like the random effect block bootstrap were designed for balanced cases and perform poorly with unbalanced clusters.

Method: Developed proportional random effect block bootstrap and modified random effect block bootstrap that accommodate general distributions of random effects and error terms, generalizing the original balanced-case method to handle highly unbalanced cluster sizes.

Result: Both proposed methods achieve Fisher consistency under general cluster sizes, while the original method only works for balanced clusters. Simulations show superior finite sample performance compared to existing bootstrap methods. Application to Oman rainfall data (cluster sizes 1-58) shows improved confidence intervals and statistically significant effect of ionization technology on rainfall.

Conclusion: The proposed bootstrap methods provide reliable inference for linear mixed models with highly unbalanced cluster sizes, outperforming existing methods and enabling proper statistical analysis in real-world applications with unbalanced clustered data.

Abstract: Clustered data arise naturally in many scientific and applied research
settings where units are grouped within clusters. They are commonly analyzed
using linear mixed models to account for within-cluster correlations. This
article focuses on the scenario in which cluster sizes might be highly
unbalanced and proposes a proportional random effect block bootstrap and a
modified random effect block bootstrap, which are applicable in such cases and
accommodate general distributions of random effects and error terms. These
methods generalize the random effect block bootstrap, originally designed for
the balanced case, and can be used for inference on parameters of linear mixed
models or functions thereof. Both proposed bootstraps are shown to enjoy Fisher
consistency under general cluster sizes, while the original random effect block
bootstrap is consistent only for balanced clusters. Simulations demonstrate
strong finite sample inferential performance of the proposed bootstraps
relative to the random effect block bootstrap and other existing bootstrap
methods for clustered data. Application to the Oman rainfall enhancement trial
dataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap
confidence intervals using the proposed bootstraps over the random effect block
bootstrap and a statistically significant effect of the ionization technology
on rainfall.

</details>


### [5] [Detection of mean changes in partially observed functional data](https://arxiv.org/abs/2510.07854)
*Šárka Hudecová,Claudia Kirch*

Main category: stat.ME

TL;DR: A test for detecting mean changes in partially observed functional data using permutation methods with both fixed and data-driven sample sizes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting mean changes in functional observations that are only partially observed on subsets of the domain, with no information available on the complement, accommodating both abrupt and gradual changes.

Method: The test uses a permutation approach with two variants: classical permutation with fixed samples and a controlled resampling risk variant with random, data-driven permutation samples.

Result: The methodology's small sample performance is evaluated through Monte Carlo simulations and real data applications.

Conclusion: The proposed test effectively detects mean changes in partially observed functional data using permutation methods, with variants for different sampling approaches.

Abstract: We propose a test for a change in the mean for a sequence of functional
observations that are only partially observed on subsets of the domain, with no
information available on the complement. The framework accommodates important
scenarios, including both abrupt and gradual changes. The significance of the
test statistic is assessed via a permutation test. In addition to the classical
permutation approach with a fixed number of permutation samples, we also
discuss a variant with controlled resampling risk that relies on a random
(data-driven) number of permutation samples. The small sample performance of
the proposed methodology is illustrated in a Monte Carlo simulation study and
an application to real data.

</details>


### [6] [Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles](https://arxiv.org/abs/2510.08204)
*Soham Ghosh,Saloni Bhogale,Sameer K. Deshpande*

Main category: stat.ME

TL;DR: SparseVCBART is a Bayesian varying-coefficient model that uses regression tree ensembles and sparsity-inducing priors to handle high-dimensional settings where the number of covariates and effect modifiers exceeds observations.


<details>
  <summary>Details</summary>
Motivation: Existing varying-coefficient models fail in high-dimensional settings to identify which covariates have non-zero effects and which effect modifiers drive these effects, creating a need for methods that balance interpretability and flexibility.

Method: The method uses regression tree ensembles to approximate coefficient functions, with global-local shrinkage priors on leaf outputs and hierarchical priors on tree splitting probabilities to encourage sparsity.

Result: SparseVCBART achieved competitive predictive accuracy with substantially narrower and better-calibrated uncertainty intervals compared to state-of-the-art methods, especially for null covariate effects.

Conclusion: The posterior contracts at near-minimax optimal rate, automatically adapting to unknown sparsity and smoothness, making sparseVCBART effective for high-dimensional varying-coefficient modeling with demonstrated application in social science research.

Abstract: By allowing the effects of $p$ covariates in a linear regression model to
vary as functions of $R$ additional effect modifiers, varying-coefficient
models (VCMs) strike a compelling balance between interpretable-but-rigid
parametric models popular in classical statistics and flexible-but-opaque
methods popular in machine learning. But in high-dimensional settings where $p$
and/or $R$ exceed the number of observations, existing approaches to fitting
VCMs fail to identify which covariates have a non-zero effect and which effect
modifiers drive these effects. We propose sparseVCBART, a fully Bayesian model
that approximates each coefficient function in a VCM with a regression tree
ensemble and encourages sparsity with a global--local shrinkage prior on the
regression tree leaf outputs and a hierarchical prior on the splitting
probabilities of each tree. We show that the sparseVCBART posterior contracts
at a near-minimax optimal rate, automatically adapting to the unknown sparsity
structure and smoothness of the true coefficient functions. Compared to
existing state-of-the-art methods, sparseVCBART achieved competitive predictive
accuracy and substantially narrower and better-calibrated uncertainty
intervals, especially for null covariate effects. We use sparseVCBART to
investigate how the effects of interpersonal conversations on prejudice could
vary according to the political and demographic characteristics of the
respondents.

</details>


### [7] [Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data](https://arxiv.org/abs/2510.08304)
*Matteo Amestoy,Mark van de Wiel,Jeroen Lakerveld,Wessel van Wieringen*

Main category: stat.ME

TL;DR: A novel Bayesian statistical framework combining profile regression with linear mixed models to analyze longitudinal exposome data, addressing collinearity, repeated measurements, and interactions.


<details>
  <summary>Details</summary>
Motivation: To overcome methodological challenges in exposome analysis including high collinearity among exposures, longitudinal nature of repeated measurements, and complex interactions with individual characteristics.

Method: Extends Bayesian profile regression by integrating it into a linear mixed model framework, creating a profile-LMM approach that clusters exposures into latent profiles while accounting for within-person variability over time.

Result: Validated on simulated data showing accurate parameter identification and recovery of true latent exposure cluster structure. Applied to Lifelines cohort data to identify exposure combinations significantly associated with diastolic blood pressure.

Conclusion: The proposed profile-LMM framework effectively addresses key challenges in exposome analysis and provides a robust statistical approach for identifying exposure-health outcome relationships in longitudinal studies.

Abstract: Exposure to diverse non-genetic factors, known as the exposome, is a critical
determinant of health outcomes. However, analyzing the exposome presents
significant methodological challenges, including: high collinearity among
exposures, the longitudinal nature of repeated measurements, and potential
complex interactions with individual characteristics. In this paper, we address
these challenges by proposing a novel statistical framework that extends
Bayesian profile regression. Our method integrates profile regression, which
handles collinearity by clustering exposures into latent profiles, into a
linear mixed model (LMM), a framework for longitudinal data analysis. This
profile-LMM approach effectively accounts for within-person variability over
time while also incorporating interactions between the latent exposure clusters
and individual characteristics. We validate our method using simulated data,
demonstrating its ability to accurately identify model parameters and recover
the true latent exposure cluster structure. Finally, we apply this approach to
a large longitudinal data set from the Lifelines cohort to identify
combinations of exposures that are significantly associated with diastolic
blood pressure.

</details>


### [8] [Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials](https://arxiv.org/abs/2510.08359)
*Jinho Cha,Eunchan Cha*

Main category: stat.ME

TL;DR: Proposes a doubly robust estimated mean excursion effect (DR-EMEE) estimator with stabilized weights for micro-randomized trials, combining inverse probability weighting and outcome regression to improve efficiency and stability over existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard IPW estimators are unbiased but unstable in small samples or under extreme randomization, while EMEE improves efficiency but lacks double robustness, creating a need for more robust estimators in micro-randomized trials.

Method: Developed DR-EMEE with stabilized and truncated weights that combines per-decision inverse probability weighting and outcome regression, with extensions to machine learning nuisance estimators and finite-sample variance corrections.

Result: Simulations show DR-EMEE reduces root mean squared error, improves coverage, achieves up to twofold efficiency gains over IPW and 5-10% over EMEE, with stable performance across HeartSteps, PAMAP2, and mHealth datasets.

Conclusion: DR-EMEE provides a doubly robust, asymptotically efficient estimator that maintains stability and efficiency across both randomized and observational settings in micro-randomized trials.

Abstract: Micro-randomized trials (MRTs) are increasingly used to evaluate mobile
health interventions with binary proximal outcomes. Standard inverse
probability weighting (IPW) estimators are unbiased but unstable in small
samples or under extreme randomization. Estimated mean excursion effect (EMEE)
improves efficiency but lacks double robustness. We propose a doubly robust
EMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision
IPW and outcome regression. We prove double robustness, asymptotic efficiency,
and provide finite-sample variance corrections, with extensions to machine
learning nuisance estimators. In simulations, DR-EMEE reduces root mean squared
error, improves coverage, and achieves up to twofold efficiency gains over IPW
and five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and
mHealth datasets confirm stable and efficient inference across both randomized
and observational settings.

</details>


### [9] [Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes](https://arxiv.org/abs/2510.08438)
*Xi Fang,Bingkai Wang,Liangyuan Hu,Fan Li*

Main category: stat.ME

TL;DR: This paper proposes doubly robust estimators for cluster-level and individual-level treatment effects in cluster-randomized trials with survival outcomes, addressing right-censoring through flexible modeling approaches.


<details>
  <summary>Details</summary>
Motivation: Cluster-randomized trials require distinguishing between cluster-level and individual-level treatment effects, particularly for survival outcomes with right-censoring, where existing methods may not adequately handle covariate-dependent censoring.

Method: The authors develop doubly robust estimators that ensure consistency when either the censoring model or outcome model is correctly specified, using various modeling approaches for censoring and survival distributions, along with a deletion-based jackknife method for variance estimation.

Result: Extensive simulations show the proposed methods perform well in finite samples, demonstrating adequate performance for estimating treatment effects in cluster-randomized trials with survival endpoints.

Conclusion: The proposed doubly robust estimators provide reliable estimation of treatment effects in cluster-randomized trials with survival outcomes, effectively handling right-censoring and offering practical utility as demonstrated through application to a completed trial.

Abstract: Cluster-randomized trials (CRTs) are experimental designs where groups or
clusters of participants, rather than the individual participants themselves,
are randomized to intervention groups. Analyzing CRT requires distinguishing
between treatment effects at the cluster level and the individual level, which
requires a clear definition of the estimands under the potential outcomes
framework. For analyzing survival outcomes, it is common to assess the
treatment effect by comparing survival functions or restricted mean survival
times between treatment groups. In this article, we formally characterize
cluster-level and individual-level treatment effect estimands with
right-censored survival outcomes in CRTs and propose doubly robust estimators
for targeting such estimands. Under covariate-dependent censoring, our
estimators ensure consistency when either the censoring model or the outcome
model is correctly specified, but not necessarily both. We explore different
modeling options for the censoring and outcome models to estimate the censoring
and survival distributions, and investigate a deletion-based jackknife method
for variance and interval estimation. Extensive simulations demonstrate that
the proposed methods perform adequately in finite samples. Finally, we
illustrate our method by analyzing a completed CRT with survival endpoints.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [10] [Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death](https://arxiv.org/abs/2510.07501)
*Sihyung Park,Wenbin Lu,Shu Yang*

Main category: stat.ML

TL;DR: A principal stratification method for evaluating dynamic treatment regimes in critical care settings where truncation by death makes traditional methods inapplicable, with a semiparametrically efficient, multiply robust estimator for multi-stage DTRs.


<details>
  <summary>Details</summary>
Motivation: Truncation by death is a common problem in critical care that makes traditional dynamic treatment regime evaluation methods inapplicable because potential outcomes become ill-defined when patients die before treatment completion.

Method: Principal stratification-based approach focusing on the always-survivor value function, with derivation of a semiparametrically efficient, multiply robust estimator for multi-stage dynamic treatment regimes.

Result: The proposed estimator demonstrates robustness and efficiency, with empirical validation and application to electronic health records showing its utility for personalized treatment optimization.

Conclusion: The method provides a viable solution for evaluating dynamic treatment regimes in critical care settings affected by truncation by death, enabling personalized treatment optimization through robust statistical estimation.

Abstract: Truncation by death, a prevalent challenge in critical care, renders
traditional dynamic treatment regime (DTR) evaluation inapplicable due to
ill-defined potential outcomes. We introduce a principal stratification-based
method, focusing on the always-survivor value function. We derive a
semiparametrically efficient, multiply robust estimator for multi-stage DTRs,
demonstrating its robustness and efficiency. Empirical validation and an
application to electronic health records showcase its utility for personalized
treatment optimization.

</details>


### [11] [From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation](https://arxiv.org/abs/2510.07624)
*Abdelhakim Benechehab,Gabriel Singer,Corentin Léger,Youssef Attia El Hili,Giuseppe Paolo,Albert Thomas,Maurizio Filippone,Balázs Kégl*

Main category: stat.ML

TL;DR: This paper proposes a bilevel optimization framework to align generative models when only high-quality datasets are available, addressing limitations of Maximum Likelihood Estimation and the dependency on explicit reward signals in Reinforcement Learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Maximum Likelihood Estimation has limitations in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning, but RL methods require explicit reward signals which are often unavailable in practice. The paper aims to solve the fundamental problem of aligning generative models using only high-quality datasets.

Method: The authors introduce a Bilevel Optimization framework where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. They conduct theoretical analysis in a tractable setting.

Result: The extracted insights from the theoretical analysis are demonstrated to generalize to applications such as tabular classification and model-based reinforcement learning.

Conclusion: The proposed bilevel optimization framework provides a solution for aligning generative models when only high-quality datasets are accessible, bridging the gap between Maximum Likelihood Estimation and Reinforcement Learning approaches.

Abstract: Generative models form the backbone of modern machine learning, underpinning
state-of-the-art systems in text, vision, and multimodal applications. While
Maximum Likelihood Estimation has traditionally served as the dominant training
paradigm, recent work have highlighted its limitations, particularly in
generalization and susceptibility to catastrophic forgetting compared to
Reinforcement Learning techniques, such as Policy Gradient methods. However,
these approaches depend on explicit reward signals, which are often unavailable
in practice, leaving open the fundamental problem of how to align generative
models when only high-quality datasets are accessible. In this work, we address
this challenge via a Bilevel Optimization framework, where the reward function
is treated as the optimization variable of an outer-level problem, while a
policy gradient objective defines the inner-level. We then conduct a
theoretical analysis of this optimization problem in a tractable setting and
extract insights that, as we demonstrate, generalize to applications such as
tabular classification and model-based reinforcement learning. We release the
code at https://github.com/abenechehab/nll_to_po .

</details>


### [12] [A Honest Cross-Validation Estimator for Prediction Performance](https://arxiv.org/abs/2510.07649)
*Tianyu Pan,Vincent Z. Yu,Viswanath Devanarayan,Lu Tian*

Main category: stat.ML

TL;DR: The paper proposes a new method to estimate the performance of a model trained on a specific training set, improving upon conventional cross-validation by using information from other random splits within a random-effects model framework.


<details>
  <summary>Details</summary>
Motivation: Standard cross-validation doesn't directly estimate the performance of the particular model recommended for future use, as it averages performance across different data splits rather than focusing on the specific model trained on the available data.

Method: Developed two estimators - a hierarchical Bayesian estimator and an empirical Bayes estimator - that use cross-validation estimators from other random splits to improve the naive single-split estimator within a random-effects model framework.

Result: The proposed estimators perform similarly to or better than both conventional cross-validation and naive single-split estimators in simulations and real-data examples.

Conclusion: The new method provides superior performance for estimating the performance of models trained on specific training sets, offering an improvement over standard cross-validation approaches.

Abstract: Cross-validation is a standard tool for obtaining a honest assessment of the
performance of a prediction model. The commonly used version repeatedly splits
data, trains the prediction model on the training set, evaluates the model
performance on the test set, and averages the model performance across
different data splits. A well-known criticism is that such cross-validation
procedure does not directly estimate the performance of the particular model
recommended for future use. In this paper, we propose a new method to estimate
the performance of a model trained on a specific (random) training set. A naive
estimator can be obtained by applying the model to a disjoint testing set.
Surprisingly, cross-validation estimators computed from other random splits can
be used to improve this naive estimator within a random-effects model
framework. We develop two estimators -- a hierarchical Bayesian estimator and
an empirical Bayes estimator -- that perform similarly to or better than both
the conventional cross-validation estimator and the naive single-split
estimator. Simulations and a real-data example demonstrate the superior
performance of the proposed method.

</details>


### [13] [When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making](https://arxiv.org/abs/2510.07750)
*Wenbin Zhou,Shixiang Zhu*

Main category: stat.ML

TL;DR: A new framework for robust optimization that provides finite-sample guarantees on both miscoverage and regret, enabling decision-makers to calibrate robustness levels based on cost-risk preferences without presetting coverage targets.


<details>
  <summary>Details</summary>
Motivation: Traditional robust optimization relies on ad hoc robustness levels that lead to either insufficient protection or overly conservative solutions, while existing conformal prediction methods still fix coverage targets a priori without guidance for selecting appropriate robustness levels.

Method: Constructs valid estimators that trace out the miscoverage-regret Pareto frontier, providing distribution-free finite-sample guarantees for any family of robust predict-then-optimize policies.

Result: The framework achieves sharper finite-sample performance than existing approaches, is simple to implement, broadly applicable across classical optimization formulations, and enables reliable evaluation and calibration of robustness levels.

Conclusion: This offers the first principled data-driven methodology for guiding robustness selection, empowering practitioners to balance robustness and conservativeness in high-stakes decision-making.

Abstract: Robust optimization safeguards decisions against uncertainty by optimizing
against worst-case scenarios, yet their effectiveness hinges on a prespecified
robustness level that is often chosen ad hoc, leading to either insufficient
protection or overly conservative and costly solutions. Recent approaches using
conformal prediction construct data-driven uncertainty sets with finite-sample
coverage guarantees, but they still fix coverage targets a priori and offer
little guidance for selecting robustness levels. We propose a new framework
that provides distribution-free, finite-sample guarantees on both miscoverage
and regret for any family of robust predict-then-optimize policies. Our method
constructs valid estimators that trace out the miscoverage-regret Pareto
frontier, enabling decision-makers to reliably evaluate and calibrate
robustness levels according to their cost-risk preferences. The framework is
simple to implement, broadly applicable across classical optimization
formulations, and achieves sharper finite-sample performance than existing
approaches. These results offer the first principled data-driven methodology
for guiding robustness selection and empower practitioners to balance
robustness and conservativeness in high-stakes decision-making.

</details>


### [14] [Surrogate Graph Partitioning for Spatial Prediction](https://arxiv.org/abs/2510.07832)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: Proposes a graph partitioning approach for interpretable spatial prediction by creating spatial segments that minimize within-segment variance, with an efficient approximation scheme for large datasets.


<details>
  <summary>Details</summary>
Motivation: Address the gap between advanced spatial prediction models and practical adoption in industries requiring interpretability, by developing surrogate models that explain black-box predictors.

Method: Formulates spatial segmentation as a mixed-integer quadratic programming problem using graph partitioning to minimize within-segment variances, and develops an approximation scheme for computational efficiency with large datasets.

Result: Experimental results show the approximation scheme achieves computational efficiency while effectively identifying spatial segments.

Conclusion: The proposed graph partitioning approach provides an interpretable spatial prediction method with practical computational efficiency through approximation, bridging the gap between advanced models and industry adoption requirements.

Abstract: Spatial prediction refers to the estimation of unobserved values from
spatially distributed observations. Although recent advances have improved the
capacity to model diverse observation types, adoption in practice remains
limited in industries that demand interpretability. To mitigate this gap,
surrogate models that explain black-box predictors provide a promising path
toward interpretable decision making. In this study, we propose a graph
partitioning problem to construct spatial segments that minimize the sum of
within-segment variances of individual predictions. The assignment of data
points to segments can be formulated as a mixed-integer quadratic programming
problem. While this formulation potentially enables the identification of exact
segments, its computational complexity becomes prohibitive as the number of
data points increases. Motivated by this challenge, we develop an approximation
scheme that leverages the structural properties of graph partitioning.
Experimental results demonstrate the computational efficiency of this
approximation in identifying spatial segments.

</details>


### [15] [On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses](https://arxiv.org/abs/2510.07862)
*Sanghwa Kim,Dohyun Ahn,Seungki Min*

Main category: stat.ML

TL;DR: The paper proposes an adaptive testing algorithm that uses Fisher information to select questions and a method-of-moments approach to estimate ability parameters, achieving optimal performance in both fixed-confidence and fixed-budget settings.


<details>
  <summary>Details</summary>
Motivation: To develop efficient adaptive testing procedures that can certify estimation accuracy with minimal queries, addressing challenges in adaptive testing and online preference learning where sequential binary responses are used to estimate continuous ability parameters.

Method: Proposes a Fisher-tracking algorithm that adaptively selects questions to maximize Fisher information, updates estimates using method-of-moments, and uses a novel test statistic to determine when estimates are sufficiently accurate.

Result: The algorithm achieves optimal performance in both fixed-confidence and fixed-budget regimes, overcoming technical challenges in handling dependence between evolving estimates and query distributions through structural symmetry and large deviation tools.

Conclusion: The Fisher-tracking strategy provides rigorous theoretical support for simple and efficient adaptive testing procedures, with proven optimality in common experimental settings.

Abstract: We study the problem of estimating a continuous ability parameter from
sequential binary responses by actively asking questions with varying
difficulties, a setting that arises naturally in adaptive testing and online
preference learning. Our goal is to certify that the estimate lies within a
desired margin of error, using as few queries as possible. We propose a simple
algorithm that adaptively selects questions to maximize Fisher information and
updates the estimate using a method-of-moments approach, paired with a novel
test statistic to decide when the estimate is accurate enough. We prove that
this Fisher-tracking strategy achieves optimal performance in both
fixed-confidence and fixed-budget regimes, which are commonly invested in the
best-arm identification literature. Our analysis overcomes a key technical
challenge in the fixed-budget setting -- handling the dependence between the
evolving estimate and the query distribution -- by exploiting a structural
symmetry in the model and combining large deviation tools with Ville's
inequality. Our results provide rigorous theoretical support for simple and
efficient adaptive testing procedures.

</details>


### [16] [On the Optimality of the Median-of-Means Estimator under Adversarial Contamination](https://arxiv.org/abs/2510.07867)
*Xabier de Juan,Santiago Mazuelas*

Main category: stat.ML

TL;DR: This paper analyzes the Median-of-Means (MoM) estimator's performance under adversarial contamination, establishing its minimax optimality for distributions with finite variance and infinite variance with finite absolute (1+r)-th moment, while showing sub-optimality for light-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: While MoM is known to be optimal for i.i.d. samples, its performance and optimality under adversarial contamination scenarios where samples can be inspected and modified by adversaries remains unknown beyond Gaussian cases.

Method: The authors present theoretical analysis with upper and lower bounds for MoM's error under adversarial contamination across multiple distribution classes, including finite variance distributions and infinite variance distributions with finite absolute (1+r)-th moment.

Result: MoM is proven to be minimax optimal for distributions with finite variance and for infinite variance distributions with finite absolute (1+r)-th moment. Lower bounds matching the order of upper bounds are provided, and MoM is shown to be sub-optimal for light-tailed distributions.

Conclusion: The study establishes the theoretical foundations of MoM's robustness under adversarial contamination, characterizing its optimality conditions across different distribution classes and revealing its limitations for light-tailed distributions.

Abstract: The Median-of-Means (MoM) is a robust estimator widely used in machine
learning that is known to be (minimax) optimal in scenarios where samples are
i.i.d. In more grave scenarios, samples are contaminated by an adversary that
can inspect and modify the data. Previous work has theoretically shown the
suitability of the MoM estimator in certain contaminated settings. However, the
(minimax) optimality of MoM and its limitations under adversarial contamination
remain unknown beyond the Gaussian case. In this paper, we present upper and
lower bounds for the error of MoM under adversarial contamination for multiple
classes of distributions. In particular, we show that MoM is (minimax) optimal
in the class of distributions with finite variance, as well as in the class of
distributions with infinite variance and finite absolute $(1+r)$-th moment. We
also provide lower bounds for MoM's error that match the order of the presented
upper bounds, and show that MoM is sub-optimal for light-tailed distributions.

</details>


### [17] [Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference](https://arxiv.org/abs/2510.07965)
*Seungsu Han,Juyoung Hwang,Won Chang*

Main category: stat.ML

TL;DR: The paper proposes StiCTAF, a stick-breaking mixture base with component-wise tail adaptation for posterior approximation, addressing limitations of Gaussian-based normalizing flows in capturing multimodal and heavy-tailed posteriors.


<details>
  <summary>Details</summary>
Motivation: Normalizing flows with Gaussian bases struggle to capture complex posterior distributions with multimodality and heavy tails, which limits their effectiveness in Bayesian inference applications.

Method: The method learns a flexible mixture base using weighted average of component-wise ELBOs, estimates local tail indices of unnormalized densities, and refines mixture components using shared backbone with component-specific tail transforms calibrated by estimated indices.

Result: Experiments on synthetic posteriors show improved tail recovery and better coverage of multiple modes compared to benchmark models, with practical benefits demonstrated in real-data analysis.

Conclusion: StiCTAF enables accurate mode coverage and anisotropic tail modeling while maintaining exact density evaluation and stable optimization, providing superior posterior approximation for complex distributions.

Abstract: Normalizing flows with a Gaussian base provide a computationally efficient
way to approximate posterior distributions in Bayesian inference, but they
often struggle to capture complex posteriors with multimodality and heavy
tails. We propose a stick-breaking mixture base with component-wise tail
adaptation (StiCTAF) for posterior approximation. The method first learns a
flexible mixture base to mitigate the mode-seeking bias of reverse KL
divergence through a weighted average of component-wise ELBOs. It then
estimates local tail indices of unnormalized densities and finally refines each
mixture component using a shared backbone combined with component-specific tail
transforms calibrated by the estimated indices. This design enables accurate
mode coverage and anisotropic tail modeling while retaining exact density
evaluation and stable optimization. Experiments on synthetic posteriors
demonstrate improved tail recovery and better coverage of multiple modes
compared to benchmark models. We also present a real-data analysis illustrating
the practical benefits of our approach for posterior inference.

</details>


### [18] [Beyond Real Data: Synthetic Data through the Lens of Regularization](https://arxiv.org/abs/2510.08095)
*Amitis Shidani,Tyler Farghly,Yang Sun,Habib Ganjgahi,George Deligiannidis*

Main category: stat.ML

TL;DR: A learning-theoretic framework that quantifies the optimal synthetic-to-real data ratio using algorithmic stability and Wasserstein distance, predicting U-shaped test error behavior with empirical validation on CIFAR-10 and clinical MRI data.


<details>
  <summary>Details</summary>
Motivation: Synthetic data can improve generalization when real data is scarce, but excessive reliance may introduce distributional mismatches that degrade performance. The paper aims to quantify the trade-off between synthetic and real data.

Method: Leverages algorithmic stability to derive generalization error bounds, characterizing the optimal synthetic-to-real data ratio as a function of Wasserstein distance between distributions. Analyzes kernel ridge regression with mixed data.

Result: Theory predicts existence of an optimal ratio leading to U-shaped test error behavior. Empirical validation on CIFAR-10 and clinical brain MRI dataset confirms predictions. Framework extends to domain adaptation scenarios.

Conclusion: Provides practical guidance for applying optimal data blending in both in-domain and out-of-domain scenarios, showing that carefully blending synthetic target data with limited source data can mitigate domain shift and enhance generalization.

Abstract: Synthetic data can improve generalization when real data is scarce, but
excessive reliance may introduce distributional mismatches that degrade
performance. In this paper, we present a learning-theoretic framework to
quantify the trade-off between synthetic and real data. Our approach leverages
algorithmic stability to derive generalization error bounds, characterizing the
optimal synthetic-to-real data ratio that minimizes expected test error as a
function of the Wasserstein distance between the real and synthetic
distributions. We motivate our framework in the setting of kernel ridge
regression with mixed data, offering a detailed analysis that may be of
independent interest. Our theory predicts the existence of an optimal ratio,
leading to a U-shaped behavior of test error with respect to the proportion of
synthetic data. Empirically, we validate this prediction on CIFAR-10 and a
clinical brain MRI dataset. Our theory extends to the important scenario of
domain adaptation, showing that carefully blending synthetic target data with
limited source data can mitigate domain shift and enhance generalization. We
conclude with practical guidance for applying our results to both in-domain and
out-of-domain scenarios.

</details>


### [19] [High-dimensional Analysis of Synthetic Data Selection](https://arxiv.org/abs/2510.08123)
*Parham Rezaei,Filip Kovacevic,Francesco Locatello,Marco Mondelli*

Main category: stat.ML

TL;DR: The paper shows that matching the covariance of synthetic data to the target distribution improves classifier performance, while mean shift has no effect on generalization error.


<details>
  <summary>Details</summary>
Motivation: To understand which specific properties of synthetic data affect generalization error, beyond heuristic principles like closeness to real data distribution.

Method: Theoretical analysis through high-dimensional regression and linear models, then empirical validation with deep neural networks and generative models using covariance matching procedure.

Result: Covariance shift affects generalization error but mean shift does not; covariance matching performs better than recent synthetic data selection approaches across various training paradigms, architectures, datasets and generative models.

Conclusion: Matching the covariance of synthetic data with the target distribution is optimal for improving classifier performance, and theoretical insights from linear models generalize to deep neural networks and generative models.

Abstract: Despite the progress in the development of generative models, their
usefulness in creating synthetic data that improve prediction performance of
classifiers has been put into question. Besides heuristic principles such as
"synthetic data should be close to the real data distribution", it is actually
not clear which specific properties affect the generalization error. Our paper
addresses this question through the lens of high-dimensional regression.
Theoretically, we show that, for linear models, the covariance shift between
the target distribution and the distribution of the synthetic data affects the
generalization error but, surprisingly, the mean shift does not. Furthermore we
prove that, in some settings, matching the covariance of the target
distribution is optimal. Remarkably, the theoretical insights from linear
models carry over to deep neural networks and generative models. We empirically
demonstrate that the covariance matching procedure (matching the covariance of
the synthetic data with that of the data coming from the target distribution)
performs well against several recent approaches for synthetic data selection,
across training paradigms, architectures, datasets and generative models used
for augmentation.

</details>


### [20] [PAC Learnability in the Presence of Performativity](https://arxiv.org/abs/2510.08335)
*Ivan Kirev,Lyuben Baltadzhiev,Nikola Konstantinov*

Main category: stat.ML

TL;DR: The paper studies PAC learnability of performative binary classification, where model predictions shift the test distribution. It proposes performative empirical risk minimization using only original distribution data and proves PAC learnability remains for linear label shifts and general feature/label changes.


<details>
  <summary>Details</summary>
Motivation: Machine learning models deployed in real-world applications cause performative shifts where predictions change the test distribution, leading to decreased performance. Models are typically trained on unshifted data, creating a need to understand when such problems remain learnable.

Method: Constructs a performative empirical risk function using only original distribution data and knowledge of the performative effect type. This provides an unbiased estimate of true risk on shifted distribution. Proves PAC learnability through performative risk minimization.

Result: Shows that any PAC-learnable hypothesis space in standard binary classification remains PAC-learnable for performative scenarios with linear label shifts and general feature/label changes. Experimental evaluation demonstrates benefits on synthetic and real data.

Conclusion: Performative binary classification problems remain PAC-learnable under various shift scenarios. The proposed performative risk minimization approach enables learning from original distribution data while accounting for performative effects, maintaining theoretical guarantees.

Abstract: Following the wide-spread adoption of machine learning models in real-world
applications, the phenomenon of performativity, i.e. model-dependent shifts in
the test distribution, becomes increasingly prevalent. Unfortunately, since
models are usually trained solely based on samples from the original
(unshifted) distribution, this performative shift may lead to decreased
test-time performance. In this paper, we study the question of whether and when
performative binary classification problems are learnable, via the lens of the
classic PAC (Probably Approximately Correct) learning framework. We motivate
several performative scenarios, accounting in particular for linear shifts in
the label distribution, as well as for more general changes in both the labels
and the features. We construct a performative empirical risk function, which
depends only on data from the original distribution and on the type
performative effect, and is yet an unbiased estimate of the true risk of a
classifier on the shifted distribution. Minimizing this notion of performative
risk allows us to show that any PAC-learnable hypothesis space in the standard
binary classification setting remains PAC-learnable for the considered
performative scenarios. We also conduct an extensive experimental evaluation of
our performative risk minimization method and showcase benefits on synthetic
and real data.

</details>


### [21] [Optimal Stopping in Latent Diffusion Models](https://arxiv.org/abs/2510.08409)
*Yu-Han Wu,Quentin Berthet,Gérard Biau,Claire Boyer,Romuald Elie,Pierre Marion*

Main category: stat.ML

TL;DR: Latent Diffusion Models (LDMs) exhibit a phenomenon where final diffusion steps degrade sample quality, which is intrinsic to dimensionality reduction rather than numerical stability. The study shows that lower-dimensional latent spaces benefit from earlier stopping, while higher-dimensional ones require later stopping.


<details>
  <summary>Details</summary>
Motivation: To understand why early stopping improves sample quality in LDMs, challenging conventional explanations based on numerical stability and revealing the fundamental role of latent dimension in this phenomenon.

Method: Theoretical analysis using Gaussian framework with linear autoencoders to characterize conditions for optimal stopping time, examining interaction between latent dimension and stopping time, and experiments on synthetic and real datasets.

Result: Lower-dimensional latent representations require earlier termination to minimize distribution distance, while higher-dimensional spaces need later stopping. Latent dimension interacts with other hyperparameters like score matching constraints.

Conclusion: Early stopping is a crucial hyperparameter in LDMs that significantly impacts generative quality, with optimal stopping time depending on latent dimension. This provides theoretical foundation for understanding how dimensionality affects sample quality.

Abstract: We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.

</details>


### [22] [Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models](https://arxiv.org/abs/2510.08465)
*Chih-Yu Chang,Ming-Chung Chang*

Main category: stat.ML

TL;DR: A2D2E is an estimator using Accelerated Aggregated D-Optimal Designs to address limitations in explaining black-box models, improving efficiency and robustness in main effect estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for explaining black-box models suffer from poor scalability, sensitivity to out-of-distribution sampling, and instability under correlated features.

Method: Leverages principled experimental design through Accelerated Aggregated D-Optimal Designs to improve efficiency and robustness.

Result: Established theoretical guarantees including convergence and variance reduction, validated through extensive simulations and a real data case study.

Conclusion: A2D2E shows potential for improving model explanation methods, with applications demonstrated in language models.

Abstract: Recent advances in supervised learning have driven growing interest in
explaining black-box models, particularly by estimating the effects of input
variables on model predictions. However, existing approaches often face key
limitations, including poor scalability, sensitivity to out-of-distribution
sampling, and instability under correlated features. To address these issues,
we propose A2D2E, an $\textbf{E}$stimator based on $\textbf{A}$ccelerated
$\textbf{A}$ggregated $\textbf{D}$-Optimal $\textbf{D}$esigns. Our method
leverages principled experimental design to improve efficiency and robustness
in main effect estimation. We establish theoretical guarantees, including
convergence and variance reduction, and validate A2D2E through extensive
simulations. We further provide the potential of the proposed method with a
case study on real data and applications in language models. The code to
reproduce the results can be found at https://github.com/cchihyu/A2D2E.

</details>


### [23] [Permutation-Invariant Spectral Learning via Dyson Diffusion](https://arxiv.org/abs/2510.08535)
*Tassilo Schwarz,Cai Dieball,Constantin Kogler,Kevin Lam,Renaud Lambiotte,Arnaud Doucet,Aljaž Godec,George Deligiannidis*

Main category: stat.ML

TL;DR: The paper introduces Dyson Diffusion Model, a novel graph diffusion approach that uses Dyson's Brownian Motion to capture spectral dynamics, overcoming limitations of existing methods that struggle with graph family discrimination and require ad hoc feature augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing graph diffusion models face challenges in distinguishing certain graph families and often require additional feature augmentation, primarily due to enforcing inductive bias within the learning architecture rather than the diffusion dynamics.

Method: Leverages random matrix theory to analytically extract spectral properties of diffusion process, pushing inductive bias from architecture to dynamics. Uses Dyson's Brownian Motion to capture spectral dynamics of Ornstein-Uhlenbeck process on adjacency matrix while preserving non-spectral information.

Result: The Dyson Diffusion Model learns graph spectra accurately and outperforms existing graph diffusion models in performance.

Conclusion: By shifting the inductive bias from the learning architecture to the diffusion dynamics through spectral analysis, the proposed Dyson Diffusion Model provides a more effective approach for graph generation that better captures graph structural properties.

Abstract: Diffusion models are central to generative modeling and have been adapted to
graphs by diffusing adjacency matrix representations. The challenge of having
up to $n!$ such representations for graphs with $n$ nodes is only partially
mitigated by using permutation-equivariant learning architectures. Despite
their computational efficiency, existing graph diffusion models struggle to
distinguish certain graph families, unless graph data are augmented with ad hoc
features. This shortcoming stems from enforcing the inductive bias within the
learning architecture. In this work, we leverage random matrix theory to
analytically extract the spectral properties of the diffusion process, allowing
us to push the inductive bias from the architecture into the dynamics. Building
on this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian
Motion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the
adjacency matrix while retaining all non-spectral information. We demonstrate
that the Dyson Diffusion Model learns graph spectra accurately and outperforms
existing graph diffusion models.

</details>
