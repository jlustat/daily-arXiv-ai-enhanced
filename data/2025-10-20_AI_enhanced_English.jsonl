{"id": "2510.14983", "categories": ["cs.LG", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14983", "abs": "https://arxiv.org/abs/2510.14983", "authors": ["Oskar Triebe", "Fletcher Passow", "Simon Wittner", "Leonie Wagner", "Julio Arend", "Tao Sun", "Chad Zanocco", "Marek Miltner", "Arezou Ghesmati", "Chen-Hao Tsai", "Christoph Bergmeir", "Ram Rajagopal"], "title": "Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators", "comment": "Collaborative Research, Stanford University and Midcontinent\n  Independent System Operator", "summary": "The reliability of local power grid infrastructure is challenged by\nsustainable energy developments increasing electric load uncertainty.\nTransmission System Operators (TSOs) need load forecasts of higher spatial\nresolution, extending current forecasting operations from zonal aggregates to\nindividual nodes. However, nodal loads are less accurate to forecast and\nrequire a large number of individual forecasts, which are hard to manage for\nthe human experts assessing risks in the control room's daily operations\n(operator). In collaboration with a TSO, we design a multi-level system that\nmeets the needs of operators for hourly day-ahead load forecasting. Utilizing a\nuniquely extensive dataset of zonal and nodal net loads, we experimentally\nevaluate our system components. First, we develop an interpretable and scalable\nforecasting model that allows for TSOs to gradually extend zonal operations to\ninclude nodal forecasts. Second, we evaluate solutions to address the\nheterogeneity and volatility of nodal load, subject to a trade-off. Third, our\nsystem is manageable with a fully parallelized single-model forecasting\nworkflow. Our results show accuracy and interpretability improvements for zonal\nforecasts, and substantial improvements for nodal forecasts. In practice, our\nmulti-level forecasting system allows operators to adjust forecasts with\nunprecedented confidence and accuracy, and to diagnose otherwise opaque errors\nprecisely.", "AI": {"tldr": "A multi-level forecasting system for power grid operators that improves nodal load forecast accuracy and interpretability while maintaining scalability.", "motivation": "Sustainable energy developments increase electric load uncertainty, requiring higher spatial resolution forecasts from zonal aggregates to individual nodes, but nodal loads are harder to forecast accurately and manage in daily operations.", "method": "Developed an interpretable and scalable forecasting model using extensive zonal and nodal net load data, with fully parallelized single-model workflow and solutions addressing nodal load heterogeneity and volatility.", "result": "Showed accuracy and interpretability improvements for zonal forecasts, substantial improvements for nodal forecasts, and enabled operators to adjust forecasts with unprecedented confidence and diagnose errors precisely.", "conclusion": "The multi-level forecasting system successfully addresses the challenges of nodal load forecasting, allowing transmission system operators to extend operations from zonal to nodal level with improved manageability and reliability."}}
{"id": "2510.15005", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15005", "abs": "https://arxiv.org/abs/2510.15005", "authors": ["Allen Daniel Sunny"], "title": "TangledFeatures: Robust Feature Selection in Highly Correlated Spaces", "comment": "Accepted for poster presentation at the Machine Learning for\n  Structural Biology (MLSB) Workshop @ NeurIPS 2025, co-located with NeurIPS\n  2025 (San Diego, USA). Non-archival", "summary": "Feature selection is a fundamental step in model development, shaping both\npredictive performance and interpretability. Yet, most widely used methods\nfocus on predictive accuracy, and their performance degrades in the presence of\ncorrelated predictors. To address this gap, we introduce TangledFeatures, a\nframework for feature selection in correlated feature spaces. It identifies\nrepresentative features from groups of entangled predictors, reducing\nredundancy while retaining explanatory power. The resulting feature subset can\nbe directly applied in downstream models, offering a more interpretable and\nstable basis for analysis compared to traditional selection techniques. We\ndemonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying\nit to the prediction of backbone torsional angles and show that the selected\nfeatures correspond to structurally meaningful intra-atomic distances that\nexplain variation in these angles.", "AI": {"tldr": "TangledFeatures is a feature selection framework that identifies representative features from correlated predictor groups, reducing redundancy while maintaining explanatory power for more interpretable and stable analysis.", "motivation": "Traditional feature selection methods focus on predictive accuracy but degrade with correlated predictors, creating a need for approaches that handle feature entanglement while maintaining interpretability.", "method": "The framework identifies representative features from groups of entangled predictors, selecting features that reduce redundancy while retaining explanatory power for downstream modeling.", "result": "Applied to Alanine Dipeptide, TangledFeatures successfully identified structurally meaningful intra-atomic distances that explain variation in backbone torsional angles.", "conclusion": "TangledFeatures provides a more interpretable and stable feature selection approach for correlated feature spaces compared to traditional techniques, with demonstrated effectiveness in molecular analysis applications."}}
{"id": "2510.15006", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15006", "abs": "https://arxiv.org/abs/2510.15006", "authors": ["Rijul Tandon", "Peter Vamplew", "Cameron Foale"], "title": "ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm", "comment": null, "summary": "In most value-based reinforcement learning (RL) algorithms, the agent\nestimates only the expected reward for each action and selects the action with\nthe highest reward. In contrast, Distributional Reinforcement Learning (DRL)\nestimates the entire probability distribution of possible rewards, providing\nricher information about uncertainty and variability. C51 is a popular DRL\nalgorithm for discrete action spaces. It uses a Q-learning approach, where the\ndistribution is learned using a greedy Bellman update. However, this can cause\nproblems if multiple actions at a state have similar expected reward but with\ndifferent distributions, as the algorithm may not learn a stable distribution.\nThis study presents a modified version of C51 (ES-C51) that replaces the greedy\nQ-learning update with an Expected Sarsa update, which uses a softmax\ncalculation to combine information from all possible actions at a state rather\nthan relying on a single best action. This reduces instability when actions\nhave similar expected rewards and allows the agent to learn higher-performing\npolicies. This approach is evaluated on classic control environments from Gym,\nand Atari-10 games. For a fair comparison, we modify the standard C51's\nexploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-\nLearning based C51). The results demonstrate that ES-C51 outperforms QL-C51\nacross many environments.", "AI": {"tldr": "This paper presents ES-C51, a modified version of the C51 distributional RL algorithm that replaces greedy Q-learning updates with Expected Sarsa updates using softmax, improving stability and performance when actions have similar expected rewards.", "motivation": "Standard C51 uses greedy Q-learning updates that can cause instability when multiple actions have similar expected rewards but different distributions, preventing stable distribution learning.", "method": "ES-C51 replaces the greedy Bellman update in C51 with an Expected Sarsa update that uses softmax to combine information from all possible actions rather than relying on a single best action.", "result": "ES-C51 outperforms the modified QL-C51 (which uses softmax exploration instead of e-greedy) across many classic control environments from Gym and Atari-10 games.", "conclusion": "Using Expected Sarsa updates in distributional RL improves stability and performance compared to greedy Q-learning approaches when actions have similar expected rewards."}}
{"id": "2510.15012", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15012", "abs": "https://arxiv.org/abs/2510.15012", "authors": ["Yi-Shan Chu", "Yueh-Cheng Kuo"], "title": "From Universal Approximation Theorem to Tropical Geometry of Multi-Layer Perceptrons", "comment": null, "summary": "We revisit the Universal Approximation Theorem(UAT) through the lens of the\ntropical geometry of neural networks and introduce a constructive,\ngeometry-aware initialization for sigmoidal multi-layer perceptrons (MLPs).\nTropical geometry shows that Rectified Linear Unit (ReLU) networks admit\ndecision functions with a combinatorial structure often described as a tropical\nrational, namely a difference of tropical polynomials. Focusing on planar\nbinary classification, we design purely sigmoidal MLPs that adhere to the\nfinite-sum format of UAT: a finite linear combination of shifted and scaled\nsigmoids of affine functions. The resulting models yield decision boundaries\nthat already align with prescribed shapes at initialization and can be refined\nby standard training if desired. This provides a practical bridge between the\ntropical perspective and smooth MLPs, enabling interpretable, shape-driven\ninitialization without resorting to ReLU architectures. We focus on the\nconstruction and empirical demonstrations in two dimensions; theoretical\nanalysis and higher-dimensional extensions are left for future work.", "AI": {"tldr": "This paper introduces a geometry-aware initialization method for sigmoidal MLPs using tropical geometry, enabling decision boundaries to align with prescribed shapes at initialization without needing ReLU networks.", "motivation": "To bridge the gap between tropical geometry insights (which show ReLU networks have combinatorial decision functions) and smooth sigmoidal MLPs, enabling interpretable, shape-driven initialization while maintaining the finite-sum format of the Universal Approximation Theorem.", "method": "Using tropical geometry principles to design purely sigmoidal MLPs that adhere to the finite-sum format of UAT, creating decision boundaries that match prescribed shapes at initialization through constructive, geometry-aware initialization.", "result": "The method produces sigmoidal MLPs with decision boundaries that already align with desired shapes at initialization, which can then be refined through standard training if needed.", "conclusion": "This work provides a practical connection between tropical geometry and smooth MLPs, enabling interpretable initialization without requiring ReLU architectures, though currently limited to 2D with theoretical extensions left for future work."}}
{"id": "2510.15003", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15003", "abs": "https://arxiv.org/abs/2510.15003", "authors": ["Mingao Yuan"], "title": "Asymptotic distribution of the global clustering coefficient in a random annulus graph", "comment": null, "summary": "The global clustering coefficient is an effective measure for analyzing and\ncomparing the structures of complex networks. The random annulus graph is a\nmodified version of the well-known Erd\\H{o}s-R\\'{e}nyi random graph. It has\nbeen recently proposed in modeling network communities. This paper investigates\nthe asymptotic distribution of the global clustering coefficient in a random\nannulus graph. It is demonstrated that the standardized global clustering\ncoefficient converges in law to the standard normal distribution. The result is\nestablished using the asymptotic theory of degenerate U-statistics with a\nsample-size dependent kernel. As far as we know, this method is different from\nestablished approaches for deriving asymptotic distributions of network\nstatistics. Moreover, we get the explicit expression of the limit of the global\nclustering coefficient.", "AI": {"tldr": "The paper proves that the standardized global clustering coefficient in random annulus graphs converges to a standard normal distribution using degenerate U-statistics theory.", "motivation": "To analyze the asymptotic distribution of global clustering coefficient in random annulus graphs, which are used for modeling network communities, and to develop new methods for deriving asymptotic distributions of network statistics.", "method": "Used the asymptotic theory of degenerate U-statistics with a sample-size dependent kernel, which differs from established approaches for deriving asymptotic distributions of network statistics.", "result": "Demonstrated that the standardized global clustering coefficient converges in law to the standard normal distribution, and obtained the explicit expression of the limit of the global clustering coefficient.", "conclusion": "The global clustering coefficient in random annulus graphs follows a normal distribution asymptotically, and the method using degenerate U-statistics provides a novel approach for analyzing network statistics."}}
{"id": "2510.15010", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15010", "abs": "https://arxiv.org/abs/2510.15010", "authors": ["Rekha R Nair", "Tina Babu", "Alavikunhu Panthakkan", "Balamurugan Balusamy", "Wathiq Mansoor"], "title": "Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines", "comment": null, "summary": "Wind turbine reliability is critical to the growing renewable energy sector,\nwhere early fault detection significantly reduces downtime and maintenance\ncosts. This paper introduces a novel ensemble-based deep learning framework for\nunsupervised anomaly detection in wind turbines. The method integrates\nVariational Autoencoders (VAE), LSTM Autoencoders, and Transformer\narchitectures, each capturing different temporal and contextual patterns from\nhigh-dimensional SCADA data. A unique feature engineering pipeline extracts\ntemporal, statistical, and frequency-domain indicators, which are then\nprocessed by the deep models. Ensemble scoring combines model predictions,\nfollowed by adaptive thresholding to detect operational anomalies without\nrequiring labeled fault data. Evaluated on the CARE dataset containing 89 years\nof real-world turbine data across three wind farms, the proposed method\nachieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to\nfailure. This approach offers significant societal value by enabling predictive\nmaintenance, reducing turbine failures, and enhancing operational efficiency in\nlarge-scale wind energy deployments.", "AI": {"tldr": "Novel ensemble deep learning framework for unsupervised anomaly detection in wind turbines using VAE, LSTM Autoencoders, and Transformers on SCADA data, achieving 0.947 AUC-ROC and 48-hour early fault detection.", "motivation": "Wind turbine reliability is critical for renewable energy sector growth, where early fault detection reduces downtime and maintenance costs significantly.", "method": "Ensemble framework integrating VAE, LSTM Autoencoders, and Transformers with feature engineering pipeline extracting temporal, statistical, and frequency-domain indicators from SCADA data, using ensemble scoring and adaptive thresholding.", "result": "Achieved 0.947 AUC-ROC on CARE dataset with 89 years of real-world turbine data, detecting faults up to 48 hours before failure.", "conclusion": "The approach enables predictive maintenance, reduces turbine failures, and enhances operational efficiency in large-scale wind energy deployments."}}
{"id": "2510.15013", "categories": ["stat.ML", "cs.LG", "physics.data-an", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15013", "abs": "https://arxiv.org/abs/2510.15013", "authors": ["Magnus Neuman", "Jelena Smiljani\u0107", "Martin Rosvall"], "title": "Reliable data clustering with Bayesian community detection", "comment": null, "summary": "From neuroscience and genomics to systems biology and ecology, researchers\nrely on clustering similarity data to uncover modular structure. Yet widely\nused clustering methods, such as hierarchical clustering, k-means, and WGCNA,\nlack principled model selection, leaving them susceptible to noise. A common\nworkaround sparsifies a correlation matrix representation to remove noise\nbefore clustering, but this extra step introduces arbitrary thresholds that can\ndistort the structure and lead to unreliable results. To detect reliable\nclusters, we capitalize on recent advances in network science to unite\nsparsification and clustering with principled model selection. We test two\nBayesian community detection methods, the Degree-Corrected Stochastic Block\nModel and the Regularized Map Equation, both grounded in the Minimum\nDescription Length principle for model selection. In synthetic data, they\noutperform traditional approaches, detecting planted clusters under high-noise\nconditions and with fewer samples. Compared to WGCNA on gene co-expression\ndata, the Regularized Map Equation identifies more robust and functionally\ncoherent gene modules. Our results establish Bayesian community detection as a\nprincipled and noise-resistant framework for uncovering modular structure in\nhigh-dimensional data across fields.", "AI": {"tldr": "The paper introduces Bayesian community detection methods as a principled alternative to traditional clustering approaches, showing superior performance in detecting reliable clusters under high-noise conditions and with fewer samples.", "motivation": "Traditional clustering methods lack principled model selection and are susceptible to noise, while common workarounds like correlation matrix sparsification introduce arbitrary thresholds that distort structure and produce unreliable results.", "method": "The authors test two Bayesian community detection methods: Degree-Corrected Stochastic Block Model and Regularized Map Equation, both based on Minimum Description Length principle for model selection. These methods unite sparsification and clustering with principled model selection.", "result": "In synthetic data, the Bayesian methods outperform traditional approaches, detecting planted clusters under high-noise conditions and with fewer samples. On gene co-expression data, Regularized Map Equation identifies more robust and functionally coherent gene modules compared to WGCNA.", "conclusion": "Bayesian community detection provides a principled and noise-resistant framework for uncovering modular structure in high-dimensional data across various scientific fields."}}
{"id": "2510.15020", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15020", "abs": "https://arxiv.org/abs/2510.15020", "authors": ["Fan Chen", "Audrey Huang", "Noah Golowich", "Sadhika Malladi", "Adam Block", "Jordan T. Ash", "Akshay Krishnamurthy", "Dylan J. Foster"], "title": "The Coverage Principle: How Pre-training Enables Post-Training", "comment": null, "summary": "Language models demonstrate remarkable abilities when pre-trained on large\ntext corpora and fine-tuned for specific tasks, but how and why pre-training\nshapes the success of the final model remains poorly understood. Notably,\nalthough pre-training success is often quantified by cross entropy loss,\ncross-entropy can be a poor predictor of downstream performance. Instead, we\nprovide a theoretical perspective on this relationship through the lens of\n\\emph{coverage}, which quantifies the probability mass the pre-trained model\nplaces on high-quality responses and which is necessary and sufficient for\npost-training and test-time scaling methods such as Best-of-N to succeed. Our\nmain results develop an understanding of \\emph{the coverage principle}, a\nphenomenon whereby next-token prediction implicitly optimizes toward a model\nwith good coverage. In particular, we uncover a mechanism that explains the\npower of coverage in predicting downstream performance: \\emph{coverage\ngeneralizes faster than cross entropy}, avoiding spurious dependence on\nproblem-dependent parameters such as the sequence length. We also study\npractical algorithmic interventions with provable benefits for improving\ncoverage, including (i) model/checkpoint selection procedures, (ii) gradient\nnormalization schemes, and (iii) test-time decoding strategies.", "AI": {"tldr": "The paper introduces 'coverage' as a better predictor of downstream performance than cross-entropy loss, showing how next-token prediction implicitly optimizes for coverage, which generalizes faster and avoids spurious dependencies on sequence length.", "motivation": "Current understanding of why pre-training works is limited, as cross-entropy loss often fails to predict downstream performance. The authors aim to provide theoretical insights into this relationship through the concept of coverage.", "method": "Theoretical analysis of coverage principle, studying how next-token prediction implicitly optimizes for coverage. Also examines practical interventions: model selection procedures, gradient normalization schemes, and test-time decoding strategies with provable benefits.", "result": "Coverage generalizes faster than cross-entropy and avoids spurious dependence on problem-dependent parameters like sequence length. Coverage is necessary and sufficient for post-training methods like Best-of-N to succeed.", "conclusion": "Coverage provides a more reliable predictor of downstream performance than cross-entropy loss, with theoretical mechanisms explaining its effectiveness and practical methods for improving coverage."}}
{"id": "2510.15000", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15000", "abs": "https://arxiv.org/abs/2510.15000", "authors": ["Yixin Fang", "Man Jin"], "title": "Estimand framework and intercurrent events handling for clinical trials with time-to-event outcomes", "comment": null, "summary": "The ICH E9(R1) guideline presents a framework of estimand for clinical\ntrials, proposes five strategies for handling intercurrent events (ICEs), and\nprovides a comprehensive discussion and many real-life clinical examples for\nquantitative outcomes and categorical outcomes. However, in ICH E9(R1) the\ndiscussion is lacking for time-to-event (TTE) outcomes. In this paper, we\ndiscuss how to define estimands and how to handle ICEs for clinical trials with\nTTE outcomes. Specifically, we discuss six ICE handling strategies, including\nthose five strategies proposed by ICH E9(R1) and a new strategy, the\ncompeting-risk strategy. Compared with ICH E9(R1), the novelty of this paper is\nthree-fold: (1) the estimands are defined in terms of potential outcomes, (2)\nthe methods can utilize time-dependent covariates straightforwardly, and (3)\nthe efficient estimators are discussed accordingly.", "AI": {"tldr": "This paper extends the ICH E9(R1) estimand framework to time-to-event outcomes, proposing six strategies for handling intercurrent events including a new competing-risk strategy, with innovations in potential outcomes definition, time-dependent covariate utilization, and efficient estimators.", "motivation": "The ICH E9(R1) guideline provides comprehensive estimand framework for clinical trials but lacks discussion for time-to-event outcomes, creating a gap that this paper aims to address.", "method": "The authors define estimands using potential outcomes framework, propose six strategies for handling intercurrent events (including five from ICH E9(R1) plus a new competing-risk strategy), and discuss methods that can utilize time-dependent covariates straightforwardly.", "result": "The paper provides a comprehensive framework for defining estimands and handling intercurrent events specifically for time-to-event outcomes in clinical trials, extending beyond the ICH E9(R1) guideline.", "conclusion": "This work fills the gap in ICH E9(R1) for time-to-event outcomes by providing novel approaches for estimand definition and intercurrent event handling, with practical applications for clinical trial design and analysis."}}
{"id": "2510.15038", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15038", "abs": "https://arxiv.org/abs/2510.15038", "authors": ["Lingkai Kong", "Molei Tao", "Yang Liu", "Bryan Wang", "Jinmiao Fu", "Chien-Chih Wang", "Huidong Liu"], "title": "AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport", "comment": "Submitted for peer review on Sep 24, 2025. Note: chairs and reviewers\n  can see and bid on our submission since Sep 28, 2025", "summary": "Flow-based Generative Models (FGMs) effectively transform noise into complex\ndata distributions. Incorporating Optimal Transport (OT) to couple noise and\ndata during FGM training has been shown to improve the straightness of flow\ntrajectories, enabling more effective inference. However, existing OT-based\nmethods estimate the OT plan using (mini-)batches of sampled noise and data\npoints, which limits their scalability to large and high-dimensional datasets\nin FGMs. This paper introduces AlignFlow, a novel approach that leverages\nSemi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by\nestablishing an explicit, optimal alignment between noise distribution and data\npoints with guaranteed convergence. SDOT computes a transport map by\npartitioning the noise space into Laguerre cells, each mapped to a\ncorresponding data point. During FGM training, i.i.d. noise samples are paired\nwith data points via the SDOT map. AlignFlow scales well to large datasets and\nmodel architectures with negligible computational overhead. Experimental\nresults show that AlignFlow improves the performance of a wide range of\nstate-of-the-art FGM algorithms and can be integrated as a plug-and-play\ncomponent. Code is available at: https://github.com/konglk1203/AlignFlow.", "AI": {"tldr": "AlignFlow introduces Semi-Discrete Optimal Transport (SDOT) to improve Flow-based Generative Models by creating optimal noise-data pairings through Laguerre cell partitioning, enabling better scalability and performance with minimal computational overhead.", "motivation": "Existing OT-based methods in FGMs use mini-batch sampling for optimal transport estimation, which limits scalability to large and high-dimensional datasets. There's a need for more efficient and scalable approaches.", "method": "Leverages Semi-Discrete Optimal Transport (SDOT) to establish explicit optimal alignment between noise distribution and data points. SDOT partitions noise space into Laguerre cells, each mapped to a data point, creating optimal pairings during training.", "result": "AlignFlow scales well to large datasets and model architectures with negligible computational overhead. It improves performance of various state-of-the-art FGM algorithms and can be integrated as a plug-and-play component.", "conclusion": "AlignFlow provides an effective and scalable solution for enhancing FGM training through SDOT-based noise-data alignment, offering guaranteed convergence and improved performance across different model architectures."}}
{"id": "2510.15014", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.15014", "abs": "https://arxiv.org/abs/2510.15014", "authors": ["Jack Kendrick"], "title": "The Tree-SNE Tree Exists", "comment": null, "summary": "The clustering and visualisation of high-dimensional data is a ubiquitous\ntask in modern data science. Popular techniques include nonlinear\ndimensionality reduction methods like t-SNE or UMAP. These methods face the\n`scale-problem' of clustering: when dealing with the MNIST dataset, do we want\nto distinguish different digits or do we want to distinguish different ways of\nwriting the digits? The answer is task dependent and depends on scale. We\nrevisit an idea of Robinson & Pierce-Hoffman that exploits an underlying\nscaling symmetry in t-SNE to replace 2-dimensional with (2+1)-dimensional\nembeddings where the additional parameter accounts for scale. This gives rise\nto the t-SNE tree (short: tree-SNE). We prove that the optimal embedding\ndepends continuously on the scaling parameter for all initial conditions\noutside a set of measure 0: the tree-SNE tree exists. This idea conceivably\nextends to other attraction-repulsion methods and is illustrated on several\nexamples.", "AI": {"tldr": "The paper introduces tree-SNE, a (2+1)-dimensional extension of t-SNE that addresses the scale-problem in clustering by incorporating an additional scaling parameter to handle different levels of detail in high-dimensional data visualization.", "motivation": "To solve the 'scale-problem' in clustering where traditional methods like t-SNE and UMAP struggle to determine whether to distinguish between different categories (e.g., digits) or different variations within categories (e.g., writing styles), which depends on the task and scale.", "method": "Extends t-SNE by exploiting its underlying scaling symmetry to create (2+1)-dimensional embeddings where the additional parameter accounts for scale, resulting in tree-SNE. Proves continuous dependence of optimal embeddings on scaling parameter for almost all initial conditions.", "result": "Demonstrates that tree-SNE trees exist and can handle different clustering scales effectively. The method is illustrated on several examples and potentially extends to other attraction-repulsion dimensionality reduction techniques.", "conclusion": "Tree-SNE provides a principled solution to the scale-problem in clustering by incorporating scale as an explicit parameter, enabling continuous exploration of clustering at different levels of granularity while maintaining mathematical guarantees of continuity."}}
{"id": "2510.15058", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62C20 (Primary) 46E22, 62B10 (Secondary)", "G.3; H.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15058", "abs": "https://arxiv.org/abs/2510.15058", "authors": ["Jose Cribeiro-Ramallo", "Agnideep Aich", "Florian Kalinke", "Ashit Baran Aich", "Zolt\u00e1n Szab\u00f3"], "title": "The Minimax Lower Bound of Kernel Stein Discrepancy Estimation", "comment": null, "summary": "Kernel Stein discrepancies (KSDs) have emerged as a powerful tool for\nquantifying goodness-of-fit over the last decade, featuring numerous successful\napplications. To the best of our knowledge, all existing KSD estimators with\nknown rate achieve $\\sqrt n$-convergence. In this work, we present two\ncomplementary results (with different proof strategies), establishing that the\nminimax lower bound of KSD estimation is $n^{-1/2}$ and settling the optimality\nof these estimators. Our first result focuses on KSD estimation on $\\mathbb\nR^d$ with the Langevin-Stein operator; our explicit constant for the Gaussian\nkernel indicates that the difficulty of KSD estimation may increase\nexponentially with the dimensionality $d$. Our second result settles the\nminimax lower bound for KSD estimation on general domains.", "AI": {"tldr": "The paper establishes that the minimax lower bound for Kernel Stein Discrepancy (KSD) estimation is n^{-1/2}, confirming the optimality of existing estimators that achieve \u221an-convergence.", "motivation": "Kernel Stein discrepancies have become important for goodness-of-fit testing, but it was unknown whether existing estimators achieving \u221an-convergence were optimal or if faster rates were possible.", "method": "The authors present two complementary proof strategies: one for KSD estimation on \u211d^d with Langevin-Stein operator and Gaussian kernel, and another for general domains.", "result": "The minimax lower bound for KSD estimation is proven to be n^{-1/2}, with the Gaussian kernel case showing that estimation difficulty may increase exponentially with dimensionality d.", "conclusion": "Existing KSD estimators with \u221an-convergence are indeed minimax optimal, settling the fundamental limits of KSD estimation rates."}}
{"id": "2510.15044", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.15044", "abs": "https://arxiv.org/abs/2510.15044", "authors": ["Abdul Samad Khan", "Nouhaila Innan", "Aeysha Khalique", "Muhammad Shafique"], "title": "IQNN-CS: Interpretable Quantum Neural Network for Credit Scoring", "comment": "Accepted for oral presentation at QUEST-IS'25. To appear in Springer\n  proceedings", "summary": "Credit scoring is a high-stakes task in financial services, where model\ndecisions directly impact individuals' access to credit and are subject to\nstrict regulatory scrutiny. While Quantum Machine Learning (QML) offers new\ncomputational capabilities, its black-box nature poses challenges for adoption\nin domains that demand transparency and trust. In this work, we present\nIQNN-CS, an interpretable quantum neural network framework designed for\nmulticlass credit risk classification. The architecture combines a variational\nQNN with a suite of post-hoc explanation techniques tailored for structured\ndata. To address the lack of structured interpretability in QML, we introduce\nInter-Class Attribution Alignment (ICAA), a novel metric that quantifies\nattribution divergence across predicted classes, revealing how the model\ndistinguishes between credit risk categories. Evaluated on two real-world\ncredit datasets, IQNN-CS demonstrates stable training dynamics, competitive\npredictive performance, and enhanced interpretability. Our results highlight a\npractical path toward transparent and accountable QML models for financial\ndecision-making.", "AI": {"tldr": "IQNN-CS is an interpretable quantum neural network framework for multiclass credit risk classification that combines variational QNN with post-hoc explanation techniques and introduces Inter-Class Attribution Alignment (ICAA) metric to enhance transparency in financial decision-making.", "motivation": "Credit scoring requires transparency and trust due to regulatory scrutiny and impact on individuals' access to credit, but current Quantum Machine Learning approaches are black-box models that lack interpretability needed for financial services.", "method": "Developed IQNN-CS framework combining variational quantum neural network with post-hoc explanation techniques for structured data, and introduced Inter-Class Attribution Alignment (ICAA) metric to quantify attribution divergence across predicted classes.", "result": "IQNN-CS demonstrated stable training dynamics, competitive predictive performance, and enhanced interpretability when evaluated on two real-world credit datasets.", "conclusion": "The framework provides a practical path toward transparent and accountable QML models for financial decision-making, addressing the interpretability gap in quantum machine learning for high-stakes applications."}}
{"id": "2510.15222", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15222", "abs": "https://arxiv.org/abs/2510.15222", "authors": ["Gabriel Nixon Raj"], "title": "Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent", "comment": null, "summary": "We study sequential decision-making under distribution drift. We propose\nentropy-regularized trust-decay, which injects stress-aware exponential tilting\ninto both belief updates and mirror-descent decisions. On the simplex, a\nFenchel-dual equivalence shows that belief tilt and decision tilt coincide. We\nformalize robustness via fragility (worst-case excess risk in a KL ball),\nbelief bandwidth (radius sustaining a target excess), and a decision-space\nFragility Index (drift tolerated at $O(\\sqrt{T})$ regret). We prove\nhigh-probability sensitivity bounds and establish dynamic-regret guarantees of\n$\\tilde{O}(\\sqrt{T})$ under KL-drift path length $S_T = \\sum_{t\\ge2}\\sqrt{{\\rm\nKL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch\nregret, while stress-free updates incur $\\Omega(1)$ tails. A parameter-free\nhedge adapts the tilt to unknown drift, whereas persistent over-tilting yields\nan $\\Omega(\\lambda^2 T)$ stationary penalty. We further obtain\ncalibrated-stress bounds and extensions to second-order updates, bandit\nfeedback, outliers, stress variation, distributed optimization, and plug-in\nKL-drift estimation. The framework unifies dynamic-regret analysis,\ndistributionally robust objectives, and KL-regularized control within a single\nstress-adaptive update.", "AI": {"tldr": "This paper proposes entropy-regularized trust-decay for sequential decision-making under distribution drift, using stress-aware exponential tilting in belief updates and mirror-descent decisions. It establishes robustness measures, proves sensitivity bounds, and achieves O(\u221aT) dynamic regret under KL-drift path length.", "motivation": "To address sequential decision-making problems where the underlying data distribution changes over time (distribution drift), requiring adaptive methods that can handle uncertainty and maintain performance under changing conditions.", "method": "Proposes entropy-regularized trust-decay framework with stress-aware exponential tilting applied to both belief updates and mirror-descent decisions. Uses Fenchel-dual equivalence on simplex, formalizes robustness via fragility measures, and develops parameter-free hedge adaptation.", "result": "Achieves dynamic-regret guarantees of O\u0303(\u221aT) under KL-drift path length, O(1) per-switch regret, high-probability sensitivity bounds, and extensions to various settings including bandit feedback, distributed optimization, and KL-drift estimation.", "conclusion": "The framework unifies dynamic-regret analysis, distributionally robust objectives, and KL-regularized control within a single stress-adaptive update, providing a comprehensive approach to handling distribution drift in sequential decision-making."}}
{"id": "2510.15203", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15203", "abs": "https://arxiv.org/abs/2510.15203", "authors": ["Mauricio Tejo", "Cristian Meza", "Fernando Marmolejo-Ramos"], "title": "Conditional GLMMs for reaction times in choice tasks", "comment": null, "summary": "This study connects two methods for modeling reaction times (RTs) in choice\ntasks: (1) the first-hitting time of a simple diffusion model with a single\nbarrier, representing the cognitive process leading to a response, and (2)\nGeneralized Linear Mixed Models (GLMMs). We achieve this by analyzing RT\ndistributions conditioned on each response alternative. Because certain\ndiffusion model variants yield Inverse Gaussian (IG) and Gamma distributions\nfor first-hitting times, we can justify using these distributions in RT models.\nConversely, employing IG and Gamma distributions within GLMMs allows us to\ninfer the underlying cognitive processes. We demonstrate this concept through\nsimulations and apply it to previously published real-world data. Finally, we\ndiscuss the scope and potential extensions of our approach.", "AI": {"tldr": "This paper connects diffusion models and GLMMs for reaction time analysis by showing how Inverse Gaussian and Gamma distributions from diffusion models can be used in GLMMs to infer cognitive processes.", "motivation": "To bridge the gap between two established methods for modeling reaction times: diffusion models that describe cognitive processes and Generalized Linear Mixed Models (GLMMs) that are commonly used in statistical analysis.", "method": "Analyzing RT distributions conditioned on response alternatives, leveraging that certain diffusion model variants yield Inverse Gaussian and Gamma distributions for first-hitting times, and incorporating these distributions into GLMMs.", "result": "The approach successfully connects diffusion models and GLMMs, demonstrated through simulations and application to real-world data, allowing inference of underlying cognitive processes from GLMMs.", "conclusion": "The proposed method provides a unified framework for reaction time analysis that combines the cognitive interpretability of diffusion models with the statistical flexibility of GLMMs, with potential for further extensions."}}
{"id": "2510.15047", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15047", "abs": "https://arxiv.org/abs/2510.15047", "authors": ["Shiqi Chen", "Tongyao Zhu", "Zian Wang", "Jinghan Zhang", "Kangrui Wang", "Siyang Gao", "Teng Xiao", "Yee Whye Teh", "Junxian He", "Manling Li"], "title": "Internalizing World Models via Self-Play Finetuning for Agentic RL", "comment": null, "summary": "Large Language Models (LLMs) as agents often struggle in out-of-distribution\n(OOD) scenarios. Real-world environments are complex and dynamic, governed by\ntask-specific rules and stochasticity, which makes it difficult for LLMs to\nground their internal knowledge in those dynamics. Under such OOD conditions,\nvanilla RL training often fails to scale; we observe Pass@k--the probability\nthat at least one of (k) sampled trajectories succeeds--drops markedly across\ntraining steps, indicating brittle exploration and limited generalization.\nInspired by model-based reinforcement learning, we hypothesize that equipping\nLLM agents with an internal world model can better align reasoning with\nenvironmental dynamics and improve decision-making. We show how to encode this\nworld model by decomposing it into two components: state representation and\ntransition modeling. Building on this, we introduce SPA, a simple reinforcement\nlearning framework that cold-starts the policy via a Self-Play supervised\nfinetuning (SFT) stage to learn the world model by interacting with the\nenvironment, then uses it to simulate future states prior to policy\noptimization. This simple initialization outperforms the online world-modeling\nbaseline and greatly boosts the RL-based agent training performance.\nExperiments across diverse environments like Sokoban, FrozenLake, and Sudoku\nshow that our approach significantly improves performance. For example, SPA\nboosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake\nscore from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.", "AI": {"tldr": "SPA framework improves LLM agent performance in out-of-distribution scenarios by incorporating an internal world model through self-play supervised finetuning before reinforcement learning.", "motivation": "LLM agents struggle in out-of-distribution scenarios due to difficulty grounding internal knowledge in complex environmental dynamics, leading to brittle exploration and limited generalization.", "method": "SPA framework decomposes world model into state representation and transition modeling, uses self-play supervised finetuning to learn the world model, then applies it to simulate future states before policy optimization.", "result": "Significant performance improvements across diverse environments: Sokoban success rate increased from 25.6% to 59.8%, FrozenLake score from 22.1% to 70.9% for Qwen2.5-1.5B-Instruct model.", "conclusion": "Equipping LLM agents with an internal world model through SPA framework better aligns reasoning with environmental dynamics and substantially improves decision-making in out-of-distribution scenarios."}}
{"id": "2510.15273", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15273", "abs": "https://arxiv.org/abs/2510.15273", "authors": ["Liner Xiang", "Jiayi Wang", "Hengrui Cai"], "title": "Foresighted Online Policy Optimization with Interference", "comment": null, "summary": "Contextual bandits, which leverage the baseline features of sequentially\narriving individuals to optimize cumulative rewards while balancing exploration\nand exploitation, are critical for online decision-making. Existing approaches\ntypically assume no interference, where each individual's action affects only\ntheir own reward. Yet, such an assumption can be violated in many practical\nscenarios, and the oversight of interference can lead to short-sighted policies\nthat focus solely on maximizing the immediate outcomes for individuals, which\nfurther results in suboptimal decisions and potentially increased regret over\ntime. To address this significant gap, we introduce the foresighted online\npolicy with interference (FRONT) that innovatively considers the long-term\nimpact of the current decision on subsequent decisions and rewards. The\nproposed FRONT method employs a sequence of exploratory and exploitative\nstrategies to manage the intricacies of interference, ensuring robust parameter\ninference and regret minimization. Theoretically, we establish a tail bound for\nthe online estimator and derive the asymptotic distribution of the parameters\nof interest under suitable conditions on the interference network. We further\nshow that FRONT attains sublinear regret under two distinct definitions,\ncapturing both the immediate and consequential impacts of decisions, and we\nestablish these results with and without statistical inference. The\neffectiveness of FRONT is further demonstrated through extensive simulations\nand a real-world application to urban hotel profits.", "AI": {"tldr": "FRONT is a foresighted contextual bandit method that addresses interference in online decision-making by considering long-term impacts of current decisions on subsequent rewards, achieving sublinear regret through exploratory and exploitative strategies.", "motivation": "Existing contextual bandit approaches assume no interference, but this assumption is often violated in practice, leading to short-sighted policies that maximize only immediate outcomes and result in suboptimal decisions over time.", "method": "FRONT employs a sequence of exploratory and exploitative strategies to manage interference complexities, ensuring robust parameter inference and regret minimization while considering long-term impacts of current decisions.", "result": "Theoretical analysis establishes tail bounds for online estimators, asymptotic parameter distributions under interference network conditions, and demonstrates sublinear regret under two definitions capturing immediate and consequential decision impacts.", "conclusion": "FRONT effectively addresses interference in contextual bandits, providing theoretically sound performance guarantees and demonstrating practical effectiveness through simulations and real-world hotel profit applications."}}
{"id": "2510.15272", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15272", "abs": "https://arxiv.org/abs/2510.15272", "authors": ["Atsushi Senda", "Yuki Takatsu", "Ryokan Ikebe", "Hiroshi Suginaka", "Koji Morishita", "Akira Endo"], "title": "Bayesian Sequential Modeling of Time-to-Urination for Dynamic ED Triage", "comment": null, "summary": "Triage tools in routine emergency care are largely static, failing to exploit\nsimple behavioral cues clinicians notice in real time. Here, we developed a\nBayesian, sequentially updating framework that integrates incoming cues to\nproduce calibrated, time-consistent risk. Using a prospective single-center\ncohort of ambulance arrivals in Japan (February-August 2025; n=2,221), we\nevaluated time to first urination (TTU) as a proof-of-concept bedside cue for\npredicting hospital admission. Population-level fit to the cumulative admission\ncurve was excellent (integrated squared error 0.002; RMSE 0.003;\nKolmogorov-Smirnov 0.008; coverage 0.98). At the patient level, performance\nimproved markedly with age/sex adjustment (AUC[t] 0.70 vs. 0.50 unadjusted),\nwith lower Brier scores and positive calibration slopes. Platt recalibration\nrefined probability scaling without altering discrimination, and decision-curve\nanalysis showed small, favorable net benefit at common thresholds. This\nframework is readily extensible to multimodal inputs and external validation\nand is designed to complement, not replace, existing triage systems.", "AI": {"tldr": "A Bayesian framework that dynamically updates risk predictions using real-time behavioral cues like time to first urination (TTU) to improve emergency triage accuracy.", "motivation": "Current triage tools are static and miss valuable behavioral cues that clinicians observe in real-time, limiting their effectiveness in emergency care settings.", "method": "Developed a Bayesian sequentially updating framework that integrates incoming cues, tested using a prospective single-center cohort of ambulance arrivals in Japan (n=2,221) with time to first urination as a proof-of-concept cue.", "result": "Excellent population-level fit to admission curve (integrated squared error 0.002), improved patient-level performance with age/sex adjustment (AUC 0.70 vs 0.50 unadjusted), better calibration, and favorable net benefit at common thresholds.", "conclusion": "The framework effectively integrates real-time behavioral cues to enhance triage predictions, is extensible to multimodal inputs, and designed to complement rather than replace existing triage systems."}}
{"id": "2510.15056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15056", "abs": "https://arxiv.org/abs/2510.15056", "authors": ["Ziqing Lu", "Babak Hassibi", "Lifeng Lai", "Weiyu Xu"], "title": "Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions", "comment": null, "summary": "Reinforcement learning usually assumes a given or sometimes even fixed\nenvironment in which an agent seeks an optimal policy to maximize its long-term\ndiscounted reward. In contrast, we consider agents that are not limited to\npassive adaptations: they instead have model-changing actions that actively\nmodify the RL model of world dynamics itself. Reconfiguring the underlying\ntransition processes can potentially increase the agents' rewards. Motivated by\nthis setting, we introduce the multi-layer configurable time-varying Markov\ndecision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a\nnon-stationary transition function that is configurable through upper-level\nmodel-changing actions. The agent's objective consists of two parts: Optimize\nthe configuration policies in the upper-level MDP and optimize the primitive\naction policies in the lower-level MDP to jointly improve its expected\nlong-term reward.", "AI": {"tldr": "The paper introduces MCTVMDP, a framework where agents can actively modify their environment's dynamics through model-changing actions, rather than just adapting to a fixed environment.", "motivation": "Traditional RL assumes passive adaptation to fixed environments, but agents could potentially increase rewards by actively reconfiguring the environment dynamics themselves.", "method": "Proposes multi-layer configurable time-varying Markov decision process (MCTVMDP) with upper-level model-changing actions that configure the non-stationary transition function of the lower-level MDP.", "result": "A framework that enables joint optimization of both configuration policies (upper-level) and primitive action policies (lower-level) to maximize long-term reward.", "conclusion": "Active environment modification through model-changing actions provides a new dimension for RL agents to improve performance beyond traditional passive adaptation approaches."}}
{"id": "2510.15141", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15141", "abs": "https://arxiv.org/abs/2510.15141", "authors": ["Zelong Bi", "Pierre Lafaye de Micheaux"], "title": "Beyond PCA: Manifold Dimension Estimation via Local Graph Structure", "comment": null, "summary": "Local principal component analysis (Local PCA) has proven to be an effective\ntool for estimating the intrinsic dimension of a manifold. More recently,\ncurvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly\naccounting for the curvature of the underlying manifold, rather than assuming\nlocal flatness. Building on these insights, we propose a general framework for\nmanifold dimension estimation that captures the manifold's local graph\nstructure by integrating PCA with regression-based techniques. Within this\nframework, we introduce two representative estimators: quadratic embedding (QE)\nand total least squares (TLS). Experiments on both synthetic and real-world\ndatasets demonstrate that these methods perform competitively with, and often\noutperform, state-of-the-art alternatives.", "AI": {"tldr": "A general framework for manifold dimension estimation that combines PCA with regression techniques to capture local graph structure, introducing QE and TLS estimators that outperform state-of-the-art methods.", "motivation": "To improve upon existing Local PCA and CA-PCA methods by developing a more comprehensive framework that better captures the manifold's local graph structure rather than assuming local flatness.", "method": "Proposed a general framework integrating PCA with regression-based techniques, specifically introducing quadratic embedding (QE) and total least squares (TLS) estimators to model the manifold's local graph structure.", "result": "Experiments on synthetic and real-world datasets show that the proposed QE and TLS methods perform competitively with and often outperform state-of-the-art dimension estimation alternatives.", "conclusion": "The proposed framework successfully advances manifold dimension estimation by effectively capturing local graph structure through PCA-regression integration, with QE and TLS proving to be superior estimators."}}
{"id": "2510.15284", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15284", "abs": "https://arxiv.org/abs/2510.15284", "authors": ["Zhilin Li", "Yao Zhou", "Xianglong Li", "Zeng Liu", "Zhaokuan Lu", "Shanlin Xu", "Seungnam Kim", "Guangyao Wang"], "title": "Small Ensemble-based Data Assimilation: A Machine Learning-Enhanced Data Assimilation Method with Limited Ensemble Size", "comment": null, "summary": "Ensemble-based data assimilation (DA) methods have become increasingly\npopular due to their inherent ability to address nonlinear dynamic problems.\nHowever, these methods often face a trade-off between analysis accuracy and\ncomputational efficiency, as larger ensemble sizes required for higher accuracy\nalso lead to greater computational cost. In this study, we propose a novel\nmachine learning-based data assimilation approach that combines the traditional\nensemble Kalman filter (EnKF) with a fully connected neural network (FCNN).\nSpecifically, our method uses a relatively small ensemble size to generate\npreliminary yet suboptimal analysis states via EnKF. A FCNN is then employed to\nlearn and predict correction terms for these states, thereby mitigating the\nperformance degradation induced by the limited ensemble size. We evaluate the\nperformance of our proposed EnKF-FCNN method through numerical experiments\ninvolving Lorenz systems and nonlinear ocean wave field simulations. The\nresults consistently demonstrate that the new method achieves higher accuracy\nthan traditional EnKF with the same ensemble size, while incurring negligible\nadditional computational cost. Moreover, the EnKF-FCNN method is adaptable to\ndiverse applications through coupling with different models and the use of\nalternative ensemble-based DA methods.", "AI": {"tldr": "A novel machine learning-based data assimilation method combining ensemble Kalman filter with fully connected neural network to improve accuracy without significant computational cost increase.", "motivation": "To address the trade-off between analysis accuracy and computational efficiency in ensemble-based data assimilation methods, where larger ensemble sizes needed for accuracy lead to higher computational costs.", "method": "Combine traditional ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN), using small ensemble size for preliminary analysis via EnKF, then FCNN learns and predicts correction terms to compensate for limited ensemble size effects.", "result": "Numerical experiments with Lorenz systems and nonlinear ocean wave field simulations show EnKF-FCNN achieves higher accuracy than traditional EnKF with same ensemble size, with negligible additional computational cost.", "conclusion": "The EnKF-FCNN method effectively improves data assimilation accuracy while maintaining computational efficiency, and is adaptable to diverse applications through coupling with different models and alternative ensemble-based DA methods."}}
{"id": "2510.15381", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.15381", "abs": "https://arxiv.org/abs/2510.15381", "authors": ["Christian H. Wei\u00df", "Philipp Ad\u00e4mmer"], "title": "Nonparametric Testing of Spatial Dependence in 2D and 3D Random Fields", "comment": null, "summary": "We propose a flexible and robust nonparametric framework for testing spatial\ndependence in two- and three-dimensional random fields. Our approach involves\nconverting spatial data into one-dimensional time series using space-filling\nHilbert curves. We then apply ordinal pattern-based tests for serial dependence\nto this series. Because Hilbert curves preserve spatial locality, spatial\ndependence in the original field manifests as serial dependence in the\ntransformed sequence. The approach is easy to implement, accommodates arbitrary\ngrid sizes through generalized Hilbert (``gilbert'') curves, and naturally\nextends beyond three dimensions. This provides a practical and general\nalternative to existing methods based on spatial ordinal patterns, which are\ntypically limited to two-dimensional settings.", "AI": {"tldr": "A nonparametric framework for testing spatial dependence in 2D/3D random fields using Hilbert curves to convert spatial data into time series, then applying ordinal pattern tests for serial dependence.", "motivation": "To provide a flexible and robust alternative to existing spatial dependence testing methods that are typically limited to two-dimensional settings.", "method": "Convert spatial data into 1D time series using space-filling Hilbert curves, then apply ordinal pattern-based tests for serial dependence to the transformed sequence.", "result": "The approach preserves spatial locality through Hilbert curves, making spatial dependence manifest as serial dependence in the transformed sequence.", "conclusion": "This method provides a practical and general framework that accommodates arbitrary grid sizes, extends beyond three dimensions, and is easy to implement."}}
{"id": "2510.15337", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15337", "abs": "https://arxiv.org/abs/2510.15337", "authors": ["Yeichan Kim", "Ilmun Kim", "Seyoung Park"], "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression", "comment": "42 pages, 4 figures, 2 tables, 1 algorithm; camera-ready version\n  accepted at NeurIPS 2025 (Spotlight)", "summary": "Transfer learning is a key component of modern machine learning, enhancing\nthe performance of target tasks by leveraging diverse data sources.\nSimultaneously, overparameterized models such as the minimum-$\\ell_2$-norm\ninterpolator (MNI) in high-dimensional linear regression have garnered\nsignificant attention for their remarkable generalization capabilities, a\nproperty known as benign overfitting. Despite their individual importance, the\nintersection of transfer learning and MNI remains largely unexplored. Our\nresearch bridges this gap by proposing a novel two-step Transfer MNI approach\nand analyzing its trade-offs. We characterize its non-asymptotic excess risk\nand identify conditions under which it outperforms the target-only MNI. Our\nanalysis reveals free-lunch covariate shift regimes, where leveraging\nheterogeneous data yields the benefit of knowledge transfer at limited cost. To\noperationalize our findings, we develop a data-driven procedure to detect\ninformative sources and introduce an ensemble method incorporating multiple\ninformative Transfer MNIs. Finite-sample experiments demonstrate the robustness\nof our methods to model and data heterogeneity, confirming their advantage.", "AI": {"tldr": "The paper proposes a Transfer MNI approach that combines transfer learning with minimum-\u2113\u2082-norm interpolators, analyzing its excess risk and identifying conditions where it outperforms target-only methods, with practical detection and ensemble methods.", "motivation": "To bridge the gap between transfer learning and minimum-\u2113\u2082-norm interpolators (MNI), exploring their intersection for enhanced generalization in high-dimensional linear regression.", "method": "A two-step Transfer MNI approach with theoretical analysis of excess risk, plus data-driven source detection and ensemble methods incorporating multiple informative Transfer MNIs.", "result": "Identifies free-lunch covariate shift regimes where heterogeneous data provides knowledge transfer benefits with limited cost, and demonstrates robustness to model and data heterogeneity in finite-sample experiments.", "conclusion": "The Transfer MNI approach effectively leverages diverse data sources to improve performance over target-only methods, with practical detection and ensemble techniques that maintain robustness across heterogeneous settings."}}
{"id": "2510.15618", "categories": ["stat.ME", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.15618", "abs": "https://arxiv.org/abs/2510.15618", "authors": ["Abdul-Nasah Soale", "Adewale Lukman"], "title": "Adaptive Influence Diagnostics in High-Dimensional Regression", "comment": null, "summary": "An adaptive Cook's distance (ACD) for diagnosing influential observations in\nhigh-dimensional single-index models with multicollinearity and outlier\ncontamination is proposed. ACD is a model-free technique built on sparse local\nlinear gradients to temper leverage effects. In simulations spanning low- and\nhigh-dimensional design settings with strong correlation, ACD based on LASSO\n(ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative\nto classical Cook's distance and local influence as well as the DF-Model and\nCase-Weight adjusted solution for LASSO. Trimming points flagged by ACD\nstabilizes variable selection while preserving core signals. Applications to\ntwo datasets--the 1960 US cities pollution study and a high-dimensional\nriboflavin genomics experiment show consistent gains in selection stability and\ninterpretability.", "AI": {"tldr": "Proposes an adaptive Cook's distance (ACD) method for detecting influential observations in high-dimensional single-index models, addressing multicollinearity and outlier contamination through sparse local linear gradients.", "motivation": "To develop a model-free diagnostic tool that can effectively identify influential observations in high-dimensional settings with multicollinearity and outliers, overcoming limitations of classical methods like Cook's distance and local influence.", "method": "ACD uses sparse local linear gradients to mitigate leverage effects, with implementations based on LASSO (ACD-LASSO) and SCAD (ACD-SCAD) penalties. The method is tested in simulations with varying dimensional designs and strong correlation.", "result": "ACD-LASSO and ACD-SCAD significantly reduced masking and swamping effects compared to classical Cook's distance, local influence, DF-Model, and Case-Weight adjusted LASSO. Trimming points identified by ACD stabilized variable selection while maintaining core signals.", "conclusion": "The proposed ACD method provides consistent improvements in selection stability and interpretability, as demonstrated in applications to US cities pollution data and riboflavin genomics experiments, making it a robust diagnostic tool for high-dimensional models with multicollinearity and outliers."}}
{"id": "2510.15075", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15075", "abs": "https://arxiv.org/abs/2510.15075", "authors": ["Sixian Jia", "Zhiqiao Dong", "Chenhui Shao"], "title": "Physics-informed data-driven machine health monitoring for two-photon lithography", "comment": null, "summary": "Two-photon lithography (TPL) is a sophisticated additive manufacturing\ntechnology for creating three-dimensional (3D) micro- and nano-structures.\nMaintaining the health of TPL systems is critical for ensuring consistent\nfabrication quality. Current maintenance practices often rely on experience\nrather than informed monitoring of machine health, resulting in either untimely\nmaintenance that causes machine downtime and poor-quality fabrication, or\nunnecessary maintenance that leads to inefficiencies and avoidable downtime. To\naddress this gap, this paper presents three methods for accurate and timely\nmonitoring of TPL machine health. Through integrating physics-informed\ndata-driven predictive models for structure dimensions with statistical\napproaches, the proposed methods are able to handle increasingly complex\nscenarios featuring different levels of generalizability. A comprehensive\nexperimental dataset that encompasses six process parameter combinations and\nsix structure dimensions under two machine health conditions was collected to\nevaluate the effectiveness of the proposed approaches. Across all test\nscenarios, the approaches are shown to achieve high accuracies, demonstrating\nexcellent effectiveness, robustness, and generalizability. These results\nrepresent a significant step toward condition-based maintenance for TPL\nsystems.", "AI": {"tldr": "This paper presents three methods for monitoring two-photon lithography (TPL) machine health using physics-informed data-driven predictive models combined with statistical approaches to enable timely maintenance and prevent unnecessary downtime.", "motivation": "Current TPL maintenance practices rely on experience rather than informed monitoring, leading to either untimely maintenance causing machine downtime and poor fabrication quality, or unnecessary maintenance resulting in inefficiencies and avoidable downtime.", "method": "Three methods integrating physics-informed data-driven predictive models for structure dimensions with statistical approaches, designed to handle increasingly complex scenarios with different levels of generalizability.", "result": "The approaches achieved high accuracies across all test scenarios using a comprehensive experimental dataset with six process parameter combinations and six structure dimensions under two machine health conditions, demonstrating excellent effectiveness, robustness, and generalizability.", "conclusion": "The results represent a significant step toward condition-based maintenance for TPL systems, enabling more informed and timely maintenance decisions."}}
{"id": "2510.15632", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.15632", "abs": "https://arxiv.org/abs/2510.15632", "authors": ["Max Welz"], "title": "Robust Estimation of Polyserial Correlation", "comment": "64 pages (30 main text), 16 figures and 5 tables in total", "summary": "The association between a continuous and an ordinal variable is commonly\nmodeled through the polyserial correlation model. However, this model, which is\nbased on a partially-latent normality assumption, may be misspecified in\npractice, due to, for example (but not limited to), outliers or careless\nresponses. We demonstrate that the typically used maximum likelihood (ML)\nestimator is highly susceptible to such misspecification: One single\nobservation not generated by partially-latent normality can suffice to produce\narbitrarily poor estimates. As a remedy, we propose a novel estimator of the\npolyserial correlation model designed to be robust against the adverse effects\nof observations discrepant to that model. The estimator achieves robustness by\nimplicitly downweighting such observations; the ensuing weights constitute a\nuseful tool for pinpointing potential sources of model misspecification. We\nshow that the proposed estimator generalizes ML and is consistent as well as\nasymptotically Gaussian. As price for robustness, some efficiency must be\nsacrificed, but substantial robustness can be gained while maintaining more\nthan 98% of ML efficiency. We demonstrate our estimator's robustness and\npractical usefulness in simulation experiments and an empirical application in\npersonality psychology where our estimator helps identify outliers. Finally,\nthe proposed methodology is implemented in free open-source software.", "AI": {"tldr": "The paper proposes a robust estimator for polyserial correlation models to handle outliers and model misspecification, maintaining high efficiency while providing outlier detection capabilities.", "motivation": "Standard maximum likelihood estimation for polyserial correlation is highly sensitive to outliers and model misspecification, where even one problematic observation can severely degrade estimates.", "method": "A novel robust estimator that implicitly downweights observations discrepant from the partially-latent normality assumption, generalizing maximum likelihood while providing outlier detection through weight analysis.", "result": "The estimator is consistent, asymptotically Gaussian, and achieves substantial robustness while maintaining over 98% of ML efficiency. Simulation and empirical applications demonstrate effective outlier identification.", "conclusion": "The proposed robust estimator provides a practical solution for polyserial correlation modeling in the presence of outliers, with implementation available in open-source software."}}
{"id": "2510.15076", "categories": ["cs.LG", "cs.DM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.15076", "abs": "https://arxiv.org/abs/2510.15076", "authors": ["Sami Davies", "Benjamin Moseley", "Heather Newman"], "title": "Online Correlation Clustering: Simultaneously Optimizing All $\\ell_p$-norms", "comment": "66 pages", "summary": "The $\\ell_p$-norm objectives for correlation clustering present a fundamental\ntrade-off between minimizing total disagreements (the $\\ell_1$-norm) and\nensuring fairness to individual nodes (the $\\ell_\\infty$-norm). Surprisingly,\nin the offline setting it is possible to simultaneously approximate all\n$\\ell_p$-norms with a single clustering. Can this powerful guarantee be\nachieved in an online setting? This paper provides the first affirmative\nanswer. We present a single algorithm for the online-with-a-sample (AOS) model\nthat, given a small constant fraction of the input as a sample, produces one\nclustering that is simultaneously $O(\\log^4 n)$-competitive for all\n$\\ell_p$-norms with high probability, $O(\\log n)$-competitive for the\n$\\ell_\\infty$-norm with high probability, and $O(1)$-competitive for the\n$\\ell_1$-norm in expectation. This work successfully translates the offline\n\"all-norms\" guarantee to the online world.\n  Our setting is motivated by a new hardness result that demonstrates a\nfundamental separation between these objectives in the standard random-order\n(RO) online model. Namely, while the $\\ell_1$-norm is trivially\n$O(1)$-approximable in the RO model, we prove that any algorithm in the RO\nmodel for the fairness-promoting $\\ell_\\infty$-norm must have a competitive\nratio of at least $\\Omega(n^{1/3})$. This highlights the necessity of a\ndifferent beyond-worst-case model. We complement our algorithm with lower\nbounds, showing our competitive ratios for the $\\ell_1$- and $\\ell_\\infty$-\nnorms are nearly tight in the AOS model.", "AI": {"tldr": "This paper presents the first online algorithm that simultaneously approximates all \u2113p-norms for correlation clustering using a small sample, achieving O(log\u2074 n) competitiveness for all norms, O(log n) for \u2113\u221e-norm, and O(1) for \u2113\u2081-norm.", "motivation": "The paper addresses the fundamental trade-off between minimizing total disagreements (\u2113\u2081-norm) and ensuring fairness to individual nodes (\u2113\u221e-norm) in correlation clustering. While offline algorithms can simultaneously approximate all \u2113p-norms, it was unknown if this was possible in online settings. The authors also discovered a fundamental separation between these objectives in standard online models.", "method": "The authors develop a single algorithm for the online-with-a-sample (AOS) model that uses a small constant fraction of the input as a sample. This algorithm produces one clustering that achieves competitive guarantees for all \u2113p-norms simultaneously.", "result": "The algorithm achieves: O(log\u2074 n)-competitive for all \u2113p-norms with high probability, O(log n)-competitive for \u2113\u221e-norm with high probability, and O(1)-competitive for \u2113\u2081-norm in expectation. The paper also proves a hardness result showing that in the standard random-order online model, any algorithm for \u2113\u221e-norm must have competitive ratio at least \u03a9(n\u00b9/\u00b3).", "conclusion": "This work successfully translates the offline \"all-norms\" guarantee to the online world using the AOS model, demonstrating that simultaneous approximation of all \u2113p-norms is possible in online settings with a small sample. The results show that the AOS model is necessary to overcome the fundamental limitations of standard online models for fairness objectives."}}
{"id": "2510.15362", "categories": ["stat.ML", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15362", "abs": "https://arxiv.org/abs/2510.15362", "authors": ["Zixun Wang", "Ben Dai"], "title": "RankSEG-RMA: An Efficient Segmentation Algorithm via Reciprocal Moment Approximation", "comment": null, "summary": "Semantic segmentation labels each pixel in an image with its corresponding\nclass, and is typically evaluated using the Intersection over Union (IoU) and\nDice metrics to quantify the overlap between predicted and ground-truth\nsegmentation masks. In the literature, most existing methods estimate\npixel-wise class probabilities, then apply argmax or thresholding to obtain the\nfinal prediction. These methods have been shown to generally lead to\ninconsistent or suboptimal results, as they do not directly maximize\nsegmentation metrics. To address this issue, a novel consistent segmentation\nframework, RankSEG, has been proposed, which includes RankDice and RankIoU\nspecifically designed to optimize the Dice and IoU metrics, respectively.\nAlthough RankSEG almost guarantees improved performance, it suffers from two\nmajor drawbacks. First, it is its computational expense-RankDice has a\ncomplexity of O(d log d) with a substantial constant factor (where d represents\nthe number of pixels), while RankIoU exhibits even higher complexity O(d^2),\nthus limiting its practical application. For instance, in LiTS, prediction with\nRankSEG takes 16.33 seconds compared to just 0.01 seconds with the argmax rule.\nSecond, RankSEG is only applicable to overlapping segmentation settings, where\nmultiple classes can occupy the same pixel, which contrasts with standard\nbenchmarks that typically assume non-overlapping segmentation. In this paper,\nwe overcome these two drawbacks via a reciprocal moment approximation (RMA) of\nRankSEG with the following contributions: (i) we improve RankSEG using RMA,\nnamely RankSEG-RMA, reduces the complexity of both algorithms to O(d) while\nmaintaining comparable performance; (ii) inspired by RMA, we develop a\npixel-wise score function that allows efficient implementation for\nnon-overlapping segmentation settings.", "AI": {"tldr": "RankSEG-RMA improves upon RankSEG by reducing computational complexity from O(d log d) or O(d\u00b2) to O(d) while maintaining performance, and extends applicability to non-overlapping segmentation settings.", "motivation": "Existing segmentation methods using argmax or thresholding don't directly optimize segmentation metrics like IoU and Dice, leading to suboptimal results. RankSEG addresses this but has high computational cost and limited applicability to overlapping segmentation only.", "method": "Proposed reciprocal moment approximation (RMA) of RankSEG, called RankSEG-RMA, which reduces complexity while maintaining performance. Also developed a pixel-wise score function for efficient implementation in non-overlapping segmentation settings.", "result": "RankSEG-RMA reduces computational complexity significantly - from 16.33 seconds to comparable performance with O(d) complexity, while extending applicability to standard non-overlapping segmentation benchmarks.", "conclusion": "The proposed RankSEG-RMA overcomes the computational and applicability limitations of RankSEG, making consistent segmentation metric optimization practical for real-world applications."}}
{"id": "2510.15664", "categories": ["stat.ME", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.15664", "abs": "https://arxiv.org/abs/2510.15664", "authors": ["Lucas Amoudruz", "Sergey Litvinov", "Costas Papadimitriou", "Petros Koumoutsakos"], "title": "Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss", "comment": null, "summary": "Inverse problems are crucial for many applications in science, engineering\nand medicine that involve data assimilation, design, and imaging. Their\nsolution infers the parameters or latent states of a complex system from noisy\ndata and partially observable processes. When measurements are an incomplete or\nindirect view of the system, additional knowledge is required to accurately\nsolve the inverse problem. Adopting a physical model of the system in the form\nof partial differential equations (PDEs) is a potent method to close this gap.\nIn particular, the method of optimizing a discrete loss (ODIL) has shown great\npotential in terms of robustness and computational cost. In this work, we\nintroduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of\nODIL as prior knowledge and combines it with a likelihood describing the data.\nB-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer\nsolutions with quantified uncertainties. We demonstrate the capabilities of\nB-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and\nthree dimensions. We showcase the application of B-ODIL in estimating tumor\nconcentration and its uncertainty in a patient's brain from MRI scans using a\nthree-dimensional tumor growth model.", "AI": {"tldr": "B-ODIL is a Bayesian extension of the ODIL method that integrates PDE loss as prior knowledge with data likelihood for solving inverse problems with quantified uncertainties.", "motivation": "Inverse problems require additional knowledge when measurements are incomplete or indirect. PDE-based models can close this gap, and extending ODIL to Bayesian framework enables uncertainty quantification.", "method": "B-ODIL combines PDE loss from ODIL as prior knowledge with data likelihood in a Bayesian formulation, using variational inference to solve PDE-based inverse problems.", "result": "The method successfully demonstrated capabilities in synthetic benchmarks across 1D, 2D, and 3D PDEs, and applied to estimate tumor concentration and uncertainty in brain MRI using 3D tumor growth model.", "conclusion": "B-ODIL provides an effective Bayesian framework for PDE-based inverse problems that quantifies uncertainties while maintaining the computational advantages of ODIL."}}
{"id": "2510.15101", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15101", "abs": "https://arxiv.org/abs/2510.15101", "authors": ["Yolanne Yi Ran Lee", "Kyriakos Flouris"], "title": "Operator Flow Matching for Timeseries Forecasting", "comment": "Preprint", "summary": "Forecasting high-dimensional, PDE-governed dynamics remains a core challenge\nfor generative modeling. Existing autoregressive and diffusion-based approaches\noften suffer cumulative errors and discretisation artifacts that limit long,\nphysically consistent forecasts. Flow matching offers a natural alternative,\nenabling efficient, deterministic sampling. We prove an upper bound on FNO\napproximation error and propose TempO, a latent flow matching model leveraging\nsparse conditioning with channel folding to efficiently process 3D\nspatiotemporal fields using time-conditioned Fourier layers to capture\nmulti-scale modes with high fidelity. TempO outperforms state-of-the-art\nbaselines across three benchmark PDE datasets, and spectral analysis further\ndemonstrates superior recovery of multi-scale dynamics, while efficiency\nstudies highlight its parameter- and memory-light design compared to\nattention-based or convolutional regressors.", "AI": {"tldr": "TempO is a latent flow matching model for forecasting high-dimensional PDE-governed dynamics that uses sparse conditioning with channel folding and time-conditioned Fourier layers to efficiently capture multi-scale modes, outperforming state-of-the-art baselines.", "motivation": "Existing autoregressive and diffusion-based approaches for forecasting high-dimensional PDE dynamics suffer from cumulative errors and discretization artifacts that limit long, physically consistent forecasts. Flow matching offers a natural alternative for efficient deterministic sampling.", "method": "TempO uses latent flow matching with sparse conditioning and channel folding to efficiently process 3D spatiotemporal fields. It employs time-conditioned Fourier layers to capture multi-scale modes with high fidelity, providing a parameter- and memory-light design compared to attention-based or convolutional regressors.", "result": "TempO outperforms state-of-the-art baselines across three benchmark PDE datasets. Spectral analysis demonstrates superior recovery of multi-scale dynamics, and efficiency studies highlight its parameter- and memory-light design.", "conclusion": "Flow matching with TempO's architecture provides an effective solution for high-dimensional PDE forecasting, addressing limitations of existing approaches while maintaining efficiency and physical consistency in long-term forecasts."}}
{"id": "2510.15363", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15363", "abs": "https://arxiv.org/abs/2510.15363", "authors": ["Dechen Zhang", "Zhenmei Shi", "Yi Zhang", "Yingyu Liang", "Difan Zou"], "title": "Kernel Regression in Structured Non-IID Settings: Theory and Implications for Denoising Score Learning", "comment": null, "summary": "Kernel ridge regression (KRR) is a foundational tool in machine learning,\nwith recent work emphasizing its connections to neural networks. However,\nexisting theory primarily addresses the i.i.d. setting, while real-world data\noften exhibits structured dependencies - particularly in applications like\ndenoising score learning where multiple noisy observations derive from shared\nunderlying signals. We present the first systematic study of KRR generalization\nfor non-i.i.d. data with signal-noise causal structure, where observations\nrepresent different noisy views of common signals. By developing a novel\nblockwise decomposition method that enables precise concentration analysis for\ndependent data, we derive excess risk bounds for KRR that explicitly depend on:\n(1) the kernel spectrum, (2) causal structure parameters, and (3) sampling\nmechanisms (including relative sample sizes for signals and noises). We further\napply our results to denoising score learning, establishing generalization\nguarantees and providing principled guidance for sampling noisy data points.\nThis work advances KRR theory while providing practical tools for analyzing\ndependent data in modern machine learning applications.", "AI": {"tldr": "First systematic study of kernel ridge regression generalization for non-i.i.d. data with signal-noise causal structure, developing novel blockwise decomposition method for dependent data analysis.", "motivation": "Existing KRR theory primarily addresses i.i.d. settings, but real-world data often has structured dependencies, especially in applications like denoising score learning where multiple noisy observations come from shared underlying signals.", "method": "Developed a novel blockwise decomposition method enabling precise concentration analysis for dependent data, deriving excess risk bounds that depend on kernel spectrum, causal structure parameters, and sampling mechanisms.", "result": "Established generalization guarantees for KRR with non-i.i.d. data, with explicit dependence on kernel spectrum, causal structure parameters, and sampling mechanisms (including relative sample sizes for signals and noises).", "conclusion": "This work advances KRR theory while providing practical tools for analyzing dependent data in modern machine learning applications, with specific applications to denoising score learning and principled guidance for sampling noisy data points."}}
{"id": "2510.15670", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15670", "abs": "https://arxiv.org/abs/2510.15670", "authors": ["Paolo Giudici", "Rosa C. Rosciano", "Johanna Schrader", "Delf-Magnus Kummerfeld"], "title": "A Multiclass ROC Curve", "comment": null, "summary": "This paper introduces a novel methodology for constructing multiclass ROC\ncurves using the multidimensional Gini index. The proposed methodology\nleverages the established relationship between the Gini coefficient and the ROC\nCurve and extends it to multiclass settings through the multidimensional Gini\nindex. The framework is validated by means of two comprehensive case studies in\nhealth care and finance. The paper provides a theoretically grounded solution\nto multiclass performance evaluation, particularly valuable for imbalanced\ndatasets, for which a prudential assessment should take precedence over class\nfrequency considerations.", "AI": {"tldr": "A novel method for constructing multiclass ROC curves using the multidimensional Gini index, extending the Gini-ROC relationship to multiclass settings with validation in healthcare and finance applications.", "motivation": "To address the need for theoretically grounded multiclass performance evaluation methods, particularly for imbalanced datasets where assessment should prioritize prudence over class frequency.", "method": "Leverages the relationship between Gini coefficient and ROC curve, extending it to multiclass settings through multidimensional Gini index. Validated with two comprehensive case studies in healthcare and finance.", "result": "Successfully developed a framework for multiclass ROC curve construction using multidimensional Gini index, providing a theoretically sound approach for performance evaluation in multiclass classification problems.", "conclusion": "The proposed methodology offers a valuable solution for multiclass performance evaluation, especially beneficial for imbalanced datasets where careful assessment is more important than considering class frequencies."}}
{"id": "2510.15110", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15110", "abs": "https://arxiv.org/abs/2510.15110", "authors": ["Shih-Yang Liu", "Xin Dong", "Ximing Lu", "Shizhe Diao", "Mingjie Liu", "Min-Hung Chen", "Hongxu Yin", "Yu-Chiang Frank Wang", "Kwang-Ting Cheng", "Yejin Choi", "Jan Kautz", "Pavlo Molchanov"], "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning", "comment": "NVIDIA-Tech Report", "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward\nsignal. We address them with Doing Length pEnalty Right (DLER), a training\nrecipe combining batch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduce Difficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nan update-selective merging method that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce.", "AI": {"tldr": "DLER is a reinforcement learning training method that uses simple truncation penalties to reduce reasoning model output length by over 70% while maintaining or improving accuracy, addressing optimization challenges like bias in advantage estimation, entropy collapse, and sparse rewards.", "motivation": "Current reasoning language models generate unnecessarily long outputs, wasting computational resources and reducing efficiency. The goal is to maximize intelligence per token - achieving high accuracy relative to response length - which remains an unsolved problem.", "method": "DLER combines batch-wise reward normalization, higher clipping, dynamic sampling, and simple truncation length penalty to address three key RL optimization challenges: large bias in advantage estimation, entropy collapse, and sparse reward signals. Also introduces Difficulty-Aware DLER for adaptive truncation and update-selective merging for preserving baseline accuracy.", "result": "DLER achieves state-of-the-art accuracy-efficiency trade-offs, cutting output length by over 70% while surpassing all previous baseline accuracy. DLER-7B generates multiple concise responses with 28% higher accuracy and lower latency compared to DeepSeek-R1-7B. Difficulty-Aware DLER provides additional efficiency gains.", "conclusion": "Simple truncation penalties can be highly effective when combined with proper RL optimization techniques. DLER demonstrates that sophisticated length penalties are not necessary - the key lies in addressing fundamental RL optimization challenges to achieve optimal accuracy-efficiency balance in reasoning models."}}
{"id": "2510.15127", "categories": ["cs.LG", "math.OC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.15127", "abs": "https://arxiv.org/abs/2510.15127", "authors": ["David J. Albers", "Tell D. Bennett", "Jana de Wiljes", "Bradford J. Smith", "Peter D. Sottile", "J. N. Stroh"], "title": "Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework", "comment": null, "summary": "Identifying the effects of mechanical ventilation strategies and protocols in\ncritical care requires analyzing data from heterogeneous patient-ventilator\nsystems within the context of the clinical decision-making environment. This\nresearch develops a framework to help understand the consequences of mechanical\nventilation (MV) and adjunct care decisions on patient outcome from\nobservations of critical care patients receiving MV. Developing an\nunderstanding of and improving critical care respiratory management requires\nthe analysis of existing secondary-use clinical data to generate hypotheses\nabout advantageous variations and adaptations of current care. This work\nintroduces a perspective of the joint patient-ventilator-care systems\n(so-called J6) to develop a scalable method for analyzing data and trajectories\nof these complex systems. To that end, breath behaviors are analyzed using\nevolutionary game theory (EGT), which generates the necessary quantitative\nprecursors for deeper analysis through probabilistic and stochastic machinery\nsuch as reinforcement learning. This result is one step along the pathway\ntoward MV optimization and personalization. The EGT-based process is\nanalytically validated on synthetic data to reveal potential caveats before\nproceeding to real-world ICU data applications that expose complexities of the\ndata-generating process J6. The discussion includes potential developments\ntoward a state transition model for the simulating effects of MV decision using\nempirical and game-theoretic elements.", "AI": {"tldr": "A framework using evolutionary game theory to analyze mechanical ventilation data from ICU patients, aiming to optimize and personalize ventilation strategies by understanding patient-ventilator-care system interactions.", "motivation": "To understand the effects of mechanical ventilation strategies on patient outcomes by analyzing heterogeneous patient-ventilator systems in critical care environments, enabling hypothesis generation for improved respiratory management.", "method": "Uses evolutionary game theory (EGT) to analyze breath behaviors in mechanical ventilation data, creating quantitative precursors for deeper probabilistic analysis. The approach is validated on synthetic data before applying to real ICU data from joint patient-ventilator-care systems (J6).", "result": "Developed a scalable analytical method for complex patient-ventilator systems that can identify advantageous care variations. The EGT-based process successfully handles the complexities of real-world ICU data and provides foundations for state transition modeling.", "conclusion": "The framework represents a significant step toward mechanical ventilation optimization and personalization, with potential for developing state transition models that simulate MV decision effects using empirical and game-theoretic approaches."}}
{"id": "2510.15422", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15422", "abs": "https://arxiv.org/abs/2510.15422", "authors": ["Lin Wang"], "title": "Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction", "comment": null, "summary": "Open world Machine Learning (OWML) aims to develop intelligent systems\ncapable of recognizing known categories, rejecting unknown samples, and\ncontinually learning from novel information. Despite significant progress in\nopen set recognition, novelty detection, and continual learning, the field\nstill lacks a unified theoretical foundation that can quantify uncertainty,\ncharacterize information transfer, and explain learning adaptability in\ndynamic, nonstationary environments. This paper presents a comprehensive review\nof information theoretic approaches in open world machine learning, emphasizing\nhow core concepts such as entropy, mutual information, and Kullback Leibler\ndivergence provide a mathematical language for describing knowledge\nacquisition, uncertainty suppression, and risk control under open world\nconditions. We synthesize recent studies into three major research axes:\ninformation theoretic open set recognition enabling safe rejection of unknowns,\ninformation driven novelty discovery guiding new concept formation, and\ninformation retentive continual learning ensuring stable long term adaptation.\nFurthermore, we discuss theoretical connections between information theory and\nprovable learning frameworks, including PAC Bayes bounds, open-space risk\ntheory, and causal information flow, to establish a pathway toward provable and\ntrustworthy open world intelligence. Finally, the review identifies key open\nproblems and future research directions, such as the quantification of\ninformation risk, development of dynamic mutual information bounds, multimodal\ninformation fusion, and integration of information theory with causal reasoning\nand world model learning.", "AI": {"tldr": "This paper provides a comprehensive review of information theoretic approaches in open world machine learning, synthesizing research into open set recognition, novelty discovery, and continual learning, while establishing theoretical connections to provable learning frameworks.", "motivation": "Open world machine learning lacks a unified theoretical foundation to quantify uncertainty, characterize information transfer, and explain learning adaptability in dynamic, nonstationary environments.", "method": "The paper reviews and synthesizes information theoretic approaches using core concepts like entropy, mutual information, and Kullback-Leibler divergence to describe knowledge acquisition, uncertainty suppression, and risk control in open world conditions.", "result": "The review organizes recent studies into three major research axes: information theoretic open set recognition, information driven novelty discovery, and information retentive continual learning, while establishing theoretical connections to provable learning frameworks.", "conclusion": "The paper identifies key open problems and future directions including quantification of information risk, development of dynamic mutual information bounds, multimodal information fusion, and integration of information theory with causal reasoning and world model learning."}}
{"id": "2510.15132", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15132", "abs": "https://arxiv.org/abs/2510.15132", "authors": ["Alex Shtoff"], "title": "A Simple Method for PMF Estimation on Large Supports", "comment": null, "summary": "We study nonparametric estimation of a probability mass function (PMF) on a\nlarge discrete support, where the PMF is multi-modal and heavy-tailed. The core\nidea is to treat the empirical PMF as a signal on a line graph and apply a\ndata-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal\noperator, the path graph Laplacian perturbed with a diagonal matrix built from\nthe empirical PMF, then compute the eigenvectors, corresponding to the smallest\nfeq eigenvalues. Projecting the empirical PMF onto this low dimensional\nsubspace produces a smooth, multi-modal estimate that preserves coarse\nstructure while suppressing noise. A light post-processing step of clipping and\nre-normalizing yields a valid PMF.\n  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the\ncomputation is reliable and runs time and memory proportional to the support\ntimes the dimension of the desired low-dimensional supspace. We also provide a\npractical, data-driven rule for selecting the dimension based on an\northogonal-series risk estimate, so the method \"just works\" with minimal\ntuning. On synthetic and real heavy-tailed examples, the approach preserves\ncoarse structure while suppressing sampling noise, compares favorably to\nlogspline and Gaussian-KDE baselines in the intended regimes. However, it has\nknown failure modes (e.g., abrupt discontinuities). The method is short to\nimplement, robust across sample sizes, and suitable for automated pipelines and\nexploratory analysis at scale because of its reliability and speed.", "AI": {"tldr": "A nonparametric method for estimating multi-modal, heavy-tailed PMFs using graph Laplacian filtering and eigenvector projection, with automated dimension selection.", "motivation": "To estimate probability mass functions on large discrete supports when the PMF is multi-modal and heavy-tailed, preserving coarse structure while suppressing sampling noise.", "method": "Treat empirical PMF as signal on line graph, apply data-dependent low-pass filter using path graph Laplacian perturbed with empirical PMF diagonal matrix, project onto smallest eigenvector subspace, then clip and re-normalize.", "result": "Method preserves coarse structure while suppressing noise, compares favorably to logspline and Gaussian-KDE baselines in intended regimes, but has known failure modes like abrupt discontinuities.", "conclusion": "The approach is short to implement, robust across sample sizes, and suitable for automated pipelines and exploratory analysis due to reliability and speed, with practical data-driven dimension selection."}}
{"id": "2510.15458", "categories": ["stat.ML", "cs.AI", "cs.LG", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2510.15458", "abs": "https://arxiv.org/abs/2510.15458", "authors": ["Gabriele Visentin", "Patrick Cheridito"], "title": "Robust Optimization in Causal Models and G-Causal Normalizing Flows", "comment": null, "summary": "In this paper, we show that interventionally robust optimization problems in\ncausal models are continuous under the $G$-causal Wasserstein distance, but may\nbe discontinuous under the standard Wasserstein distance. This highlights the\nimportance of using generative models that respect the causal structure when\naugmenting data for such tasks. To this end, we propose a new normalizing flow\narchitecture that satisfies a universal approximation property for causal\nstructural models and can be efficiently trained to minimize the $G$-causal\nWasserstein distance. Empirically, we demonstrate that our model outperforms\nstandard (non-causal) generative models in data augmentation for causal\nregression and mean-variance portfolio optimization in causal factor models.", "AI": {"tldr": "The paper shows that interventionally robust optimization problems in causal models are continuous under the G-causal Wasserstein distance but discontinuous under standard Wasserstein distance, highlighting the need for causal-aware generative models. The authors propose a new normalizing flow architecture with universal approximation for causal structural models that minimizes G-causal Wasserstein distance, outperforming standard generative models in causal regression and portfolio optimization tasks.", "motivation": "To address the discontinuity of interventionally robust optimization problems under standard Wasserstein distance and emphasize the importance of using generative models that respect causal structure for data augmentation in causal tasks.", "method": "Proposed a new normalizing flow architecture that satisfies universal approximation property for causal structural models and can be efficiently trained to minimize the G-causal Wasserstein distance.", "result": "Empirical demonstrations show the proposed model outperforms standard (non-causal) generative models in data augmentation for causal regression and mean-variance portfolio optimization in causal factor models.", "conclusion": "Causal-aware generative models that respect the causal structure are crucial for interventionally robust optimization problems, and the proposed normalizing flow architecture effectively addresses this need by minimizing G-causal Wasserstein distance."}}
{"id": "2510.15136", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15136", "abs": "https://arxiv.org/abs/2510.15136", "authors": ["Oluwasegun Adegoke"], "title": "Predicting the Unpredictable: Reproducible BiLSTM Forecasting of Incident Counts in the Global Terrorism Database (GTD)", "comment": "12 pages, 5 figures, 2 tables. Code & reproducibility:\n  https://github.com/Davidavid45/Deep-Learning-in-Counterterrorism Data/ethics:\n  GTD used under research-only terms; no raw GTD is redistributed", "summary": "We study short-horizon forecasting of weekly terrorism incident counts using\nthe Global Terrorism Database (GTD, 1970--2016). We build a reproducible\npipeline with fixed time-based splits and evaluate a Bidirectional LSTM\n(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a\ndeep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE\n6.38, outperforming LSTM-Attention (9.19; +30.6\\%) and a linear lag-regression\nbaseline (+35.4\\% RMSE gain), with parallel improvements in MAE and MAPE.\nAblations varying temporal memory, training-history length, spatial grain,\nlookback size, and feature groups show that models trained on long historical\ndata generalize best; a moderate lookback (20--30 weeks) provides strong\ncontext; and bidirectional encoding is critical for capturing both build-up and\naftermath patterns within the window. Feature-group analysis indicates that\nshort-horizon structure (lagged counts and rolling statistics) contributes\nmost, with geographic and casualty features adding incremental lift. We release\ncode, configs, and compact result tables, and provide a data/ethics statement\ndocumenting GTD licensing and research-only use. Overall, the study offers a\ntransparent, baseline-beating reference for GTD incident forecasting.", "AI": {"tldr": "A BiLSTM model outperforms classical and deep learning baselines for weekly terrorism incident forecasting using GTD data, with key findings on optimal training history, lookback windows, and feature importance.", "motivation": "To develop a reproducible pipeline for short-horizon forecasting of weekly terrorism incident counts and establish transparent baseline-beating references for GTD incident forecasting.", "method": "Built a reproducible pipeline with fixed time-based splits, evaluated BiLSTM against seasonal-naive, linear/ARIMA baselines and LSTM-Attention, with ablations on temporal memory, training-history length, spatial grain, lookback size, and feature groups.", "result": "BiLSTM achieved RMSE 6.38 on held-out test set, outperforming LSTM-Attention (9.19; +30.6%) and linear lag-regression baseline (+35.4% RMSE gain), with parallel improvements in MAE and MAPE.", "conclusion": "Models trained on long historical data generalize best, moderate lookback (20-30 weeks) provides strong context, bidirectional encoding is critical for capturing patterns, and short-horizon structure features contribute most to forecasting performance."}}
{"id": "2510.15483", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15483", "abs": "https://arxiv.org/abs/2510.15483", "authors": ["Samuel Girard", "Aur\u00e9lien Bibaut", "Houssam Zenati"], "title": "Online Policy Learning via a Self-Normalized Maximal Inequality", "comment": null, "summary": "Adaptive experiments produce dependent data that break i.i.d. assumptions\nthat underlie classical concentration bounds and invalidate standard learning\nguarantees. In this paper, we develop a self-normalized maximal inequality for\nmartingale empirical processes. Building on this, we first propose an adaptive\nsample-variance penalization procedure which balances empirical loss and sample\nvariance, valid for general dependent data. Next, this allows us to derive a\nnew variance-regularized pessimistic off-policy learning objective, for which\nwe establish excess-risk guarantees. Subsequently, we show that, when combined\nwith sequential updates and under standard complexity and margin conditions,\nthe resulting estimator achieves fast convergence rates in both parametric and\nnonparametric regimes, improving over the usual $1/\\sqrt{n}$\n  baseline. We complement our theoretical findings with numerical simulations\nthat illustrate the practical gains of our approach.", "AI": {"tldr": "Developed a self-normalized maximal inequality for martingale empirical processes to address dependent data in adaptive experiments, enabling improved convergence rates for off-policy learning.", "motivation": "Adaptive experiments produce dependent data that violate i.i.d. assumptions, breaking classical concentration bounds and invalidating standard learning guarantees.", "method": "Proposed adaptive sample-variance penalization for general dependent data, then derived a variance-regularized pessimistic off-policy learning objective with sequential updates.", "result": "Achieved fast convergence rates in both parametric and nonparametric regimes, improving over the usual 1/\u221an baseline, with numerical simulations confirming practical gains.", "conclusion": "The approach successfully handles dependent data in adaptive experiments through martingale-based analysis and variance regularization, providing improved learning guarantees and convergence rates."}}
{"id": "2510.15165", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.15165", "abs": "https://arxiv.org/abs/2510.15165", "authors": ["Xin Guo", "Zijiu Lyu"], "title": "Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization", "comment": null, "summary": "Reinforcement Learning (RL) enables agents to learn optimal decision-making\nstrategies through interaction with an environment, yet training from scratch\non complex tasks can be highly inefficient. Transfer learning (TL), widely\nsuccessful in large language models (LLMs), offers a promising direction for\nenhancing RL efficiency by leveraging pre-trained models.\n  This paper investigates policy transfer, a TL approach that initializes\nlearning in a target RL task using a policy from a related source task, in the\ncontext of continuous-time linear quadratic regulators (LQRs) with entropy\nregularization. We provide the first theoretical proof of policy transfer for\ncontinuous-time RL, proving that a policy optimal for one LQR serves as a\nnear-optimal initialization for closely related LQRs, while preserving the\noriginal algorithm's convergence rate. Furthermore, we introduce a novel policy\nlearning algorithm for continuous-time LQRs that achieves global linear and\nlocal super-linear convergence. Our results demonstrate both theoretical\nguarantees and algorithmic benefits of transfer learning in continuous-time RL,\naddressing a gap in existing literature and extending prior work from discrete\nto continuous time settings.\n  As a byproduct of our analysis, we derive the stability of a class of\ncontinuous-time score-based diffusion models via their connection with LQRs.", "AI": {"tldr": "This paper provides the first theoretical proof of policy transfer for continuous-time RL, showing that optimal policies from one LQR can serve as near-optimal initializations for related LQRs while maintaining convergence rates. It also introduces a new policy learning algorithm with global linear and local super-linear convergence.", "motivation": "Training RL agents from scratch on complex tasks is inefficient. Transfer learning, successful in LLMs, offers potential to enhance RL efficiency by leveraging pre-trained models, but theoretical foundations for continuous-time RL were lacking.", "method": "The study investigates policy transfer in continuous-time linear quadratic regulators (LQRs) with entropy regularization. It provides theoretical analysis and introduces a novel policy learning algorithm for continuous-time LQRs.", "result": "The paper proves that a policy optimal for one LQR serves as near-optimal initialization for related LQRs while preserving original convergence rates. The new algorithm achieves global linear and local super-linear convergence.", "conclusion": "The work demonstrates theoretical guarantees and algorithmic benefits of transfer learning in continuous-time RL, bridging a gap in literature and extending prior work from discrete to continuous time. As a byproduct, it derives stability for continuous-time score-based diffusion models via LQR connections."}}
{"id": "2510.15548", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15548", "abs": "https://arxiv.org/abs/2510.15548", "authors": ["Sushil Bohara", "Amedeo Roberto Esposito"], "title": "Geometric Convergence Analysis of Variational Inference via Bregman Divergences", "comment": "14 pages, 4 figures", "summary": "Variational Inference (VI) provides a scalable framework for Bayesian\ninference by optimizing the Evidence Lower Bound (ELBO), but convergence\nanalysis remains challenging due to the objective's non-convexity and\nnon-smoothness in Euclidean space. We establish a novel theoretical framework\nfor analyzing VI convergence by exploiting the exponential family structure of\ndistributions. We express negative ELBO as a Bregman divergence with respect to\nthe log-partition function, enabling a geometric analysis of the optimization\nlandscape. We show that this Bregman representation admits a weak monotonicity\nproperty that, while weaker than convexity, provides sufficient structure for\nrigorous convergence analysis. By deriving bounds on the objective function\nalong rays in parameter space, we establish properties governed by the spectral\ncharacteristics of the Fisher information matrix. Under this geometric\nframework, we prove non-asymptotic convergence rates for gradient descent\nalgorithms with both constant and diminishing step sizes.", "AI": {"tldr": "This paper establishes a geometric framework for analyzing VI convergence by expressing negative ELBO as a Bregman divergence, proving non-asymptotic convergence rates for gradient descent algorithms.", "motivation": "Convergence analysis of Variational Inference (VI) is challenging due to the non-convexity and non-smoothness of the ELBO objective in Euclidean space, requiring new theoretical approaches.", "method": "Exploit exponential family structure to express negative ELBO as a Bregman divergence with respect to the log-partition function, enabling geometric analysis of the optimization landscape and establishing weak monotonicity properties.", "result": "Derived bounds on the objective function along parameter space rays governed by Fisher information matrix spectral characteristics, and proved non-asymptotic convergence rates for gradient descent with constant and diminishing step sizes.", "conclusion": "The Bregman divergence representation provides sufficient structure for rigorous convergence analysis of VI despite weaker than convexity properties, enabling geometric understanding and convergence rate guarantees."}}
{"id": "2510.15174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15174", "abs": "https://arxiv.org/abs/2510.15174", "authors": ["Niclas G\u00f6ring", "Chris Mingard", "Yoonsoo Nam", "Ard Louis"], "title": "A simple mean field model of feature learning", "comment": null, "summary": "Feature learning (FL), where neural networks adapt their internal\nrepresentations during training, remains poorly understood. Using methods from\nstatistical physics, we derive a tractable, self-consistent mean-field (MF)\ntheory for the Bayesian posterior of two-layer non-linear networks trained with\nstochastic gradient Langevin dynamics (SGLD). At infinite width, this theory\nreduces to kernel ridge regression, but at finite width it predicts a symmetry\nbreaking phase transition where networks abruptly align with target functions.\nWhile the basic MF theory provides theoretical insight into the emergence of FL\nin the finite-width regime, semi-quantitatively predicting the onset of FL with\nnoise or sample size, it substantially underestimates the improvements in\ngeneralisation after the transition. We trace this discrepancy to a key\nmechanism absent from the plain MF description: \\textit{self-reinforcing input\nfeature selection}. Incorporating this mechanism into the MF theory allows us\nto quantitatively match the learning curves of SGLD-trained networks and\nprovides mechanistic insight into FL.", "AI": {"tldr": "The paper develops a mean-field theory for feature learning in finite-width neural networks, revealing a symmetry breaking phase transition where networks align with target functions, and identifies self-reinforcing input feature selection as a key mechanism missing from basic mean-field descriptions.", "motivation": "To better understand feature learning in neural networks, which remains poorly understood despite being fundamental to their success, by developing a theoretical framework that can capture the emergence of feature learning in finite-width networks.", "method": "Using statistical physics methods to derive a self-consistent mean-field theory for the Bayesian posterior of two-layer non-linear networks trained with stochastic gradient Langevin dynamics (SGLD), and extending it to incorporate self-reinforcing input feature selection.", "result": "The basic mean-field theory predicts a symmetry breaking phase transition where networks abruptly align with target functions and semi-quantitatively predicts the onset of feature learning with noise or sample size, but underestimates generalization improvements. The extended theory with self-reinforcing feature selection quantitatively matches SGLD learning curves.", "conclusion": "The mean-field framework provides mechanistic insight into feature learning, with self-reinforcing input feature selection being a crucial mechanism that explains the gap between basic mean-field predictions and actual network performance, offering a quantitative understanding of feature learning dynamics."}}
{"id": "2510.15601", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15601", "abs": "https://arxiv.org/abs/2510.15601", "authors": ["Pierre Glaser", "Steffanie Paul", "Alissa M. Hummer", "Charlotte M. Deane", "Debora S. Marks", "Alan N. Amin"], "title": "Kernel-Based Evaluation of Conditional Biological Sequence Models", "comment": "29 pages", "summary": "We propose a set of kernel-based tools to evaluate the designs and tune the\nhyperparameters of conditional sequence models, with a focus on problems in\ncomputational biology. The backbone of our tools is a new measure of\ndiscrepancy between the true conditional distribution and the model's estimate,\ncalled the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided\nthat the model can be sampled from, the ACMMD can be estimated unbiasedly from\ndata to quantify absolute model fit, integrated within hypothesis tests, and\nused to evaluate model reliability. We demonstrate the utility of our approach\nby analyzing a popular protein design model, ProteinMPNN. We are able to reject\nthe hypothesis that ProteinMPNN fits its data for various protein families, and\ntune the model's temperature hyperparameter to achieve a better fit.", "AI": {"tldr": "The paper introduces kernel-based tools for evaluating conditional sequence models, featuring a new discrepancy measure called ACMMD that enables unbiased estimation of model fit, hypothesis testing, and hyperparameter tuning.", "motivation": "To address the need for better evaluation and tuning methods for conditional sequence models in computational biology, particularly for assessing model fit and reliability.", "method": "Proposes Augmented Conditional Maximum Mean Discrepancy (ACMMD) - a kernel-based discrepancy measure between true conditional distributions and model estimates. The method allows unbiased estimation from data and can be used for hypothesis testing and hyperparameter optimization.", "result": "Applied to ProteinMPNN protein design model, the approach successfully rejected the hypothesis that ProteinMPNN fits its data for various protein families and enabled temperature hyperparameter tuning to achieve better model fit.", "conclusion": "The ACMMD-based toolkit provides effective methods for evaluating conditional sequence models, quantifying model fit, conducting hypothesis tests, and optimizing hyperparameters in computational biology applications."}}
{"id": "2510.15177", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15177", "abs": "https://arxiv.org/abs/2510.15177", "authors": ["Conor Rowan"], "title": "Finding geodesics with the Deep Ritz method", "comment": null, "summary": "Geodesic problems involve computing trajectories between prescribed initial\nand final states to minimize a user-defined measure of distance, cost, or\nenergy. They arise throughout physics and engineering -- for instance, in\ndetermining optimal paths through complex environments, modeling light\npropagation in refractive media, and the study of spacetime trajectories in\ncontrol theory and general relativity. Despite their ubiquity, the scientific\nmachine learning (SciML) community has given relatively little attention to\ninvestigating its methods in the context of these problems. In this work, we\nargue that given their simple geometry, variational structure, and natural\nnonlinearity, geodesic problems are particularly well-suited for the Deep Ritz\nmethod. We substantiate this claim with three numerical examples drawn from\npath planning, optics, and solid mechanics. Our goal is not to provide an\nexhaustive study of geodesic problems, but rather to identify a promising\napplication of the Deep Ritz method and a fruitful direction for future SciML\nresearch.", "AI": {"tldr": "The paper argues that geodesic problems are well-suited for the Deep Ritz method due to their simple geometry, variational structure, and natural nonlinearity, and demonstrates this with three numerical examples from path planning, optics, and solid mechanics.", "motivation": "Geodesic problems are ubiquitous in physics and engineering but have received little attention from the scientific machine learning community. The authors aim to identify a promising application for the Deep Ritz method in this domain.", "method": "The authors apply the Deep Ritz method to geodesic problems, leveraging its suitability for problems with variational structure. They demonstrate the approach through three numerical examples from different domains.", "result": "The paper presents successful applications of the Deep Ritz method to geodesic problems in path planning, optics, and solid mechanics, showing its effectiveness for this class of problems.", "conclusion": "Geodesic problems represent a promising application area for the Deep Ritz method and a fruitful direction for future scientific machine learning research, given their favorable characteristics for this approach."}}
{"id": "2510.15669", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15669", "abs": "https://arxiv.org/abs/2510.15669", "authors": ["Veranika Boukun", "J\u00f6rg L\u00fccke"], "title": "Disentanglement of Sources in a Multi-Stream Variational Autoencoder", "comment": null, "summary": "Variational autoencoders (VAEs) are a leading approach to address the problem\nof learning disentangled representations. Typically a single VAE is used and\ndisentangled representations are sought in its continuous latent space. Here we\nexplore a different approach by using discrete latents to combine\nVAE-representations of individual sources. The combination is done based on an\nexplicit model for source combination, and we here use a linear combination\nmodel which is well suited, e.g., for acoustic data. We formally define such a\nmulti-stream VAE (MS-VAE) approach, derive its inference and learning\nequations, and we numerically investigate its principled functionality. The\nMS-VAE is domain-agnostic, and we here explore its ability to separate sources\ninto different streams using superimposed hand-written digits, and mixed\nacoustic sources in a speaker diarization task. We observe a clear separation\nof digits, and on speaker diarization we observe an especially low rate of\nmissed speakers. Numerical experiments further highlight the flexibility of the\napproach across varying amounts of supervision and training data.", "AI": {"tldr": "The paper introduces a multi-stream VAE (MS-VAE) that uses discrete latents to combine VAE representations of individual sources via a linear combination model, demonstrating effective source separation on hand-written digits and acoustic data.", "motivation": "To address the problem of learning disentangled representations by exploring an alternative approach using discrete latents to combine VAE representations of individual sources, particularly suited for applications like acoustic data.", "method": "Proposes a multi-stream VAE (MS-VAE) approach with discrete latents that combines VAE representations based on an explicit linear combination model, deriving inference and learning equations for the framework.", "result": "Clear separation of superimposed hand-written digits and low missed speaker rate in speaker diarization tasks, with numerical experiments showing flexibility across varying supervision levels and training data amounts.", "conclusion": "The MS-VAE approach is domain-agnostic and effectively separates sources into different streams, demonstrating principled functionality and practical utility in source separation tasks."}}
{"id": "2510.15179", "categories": ["cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.15179", "abs": "https://arxiv.org/abs/2510.15179", "authors": ["Shuo Sun", "Meiling Zhou", "Chen Zhao", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Hui Shen", "Hong-Wen Deng", "Kui Zhang", "Weihua Zhou"], "title": "An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets", "comment": "38 pages, 3 figures, 8 tables. This is a preprint version of the\n  manuscript titled \"An Advanced Two-Stage Model with High Sensitivity and\n  Generalizability for Prediction of Hip Fracture Risk Using Multiple\n  Datasets.\" The paper is currently under journal submission", "summary": "Hip fractures are a major cause of disability, mortality, and healthcare\nburden in older adults, underscoring the need for early risk assessment.\nHowever, commonly used tools such as the DXA T-score and FRAX often lack\nsensitivity and miss individuals at high risk, particularly those without prior\nfractures or with osteopenia. To address this limitation, we propose a\nsequential two-stage model that integrates clinical and imaging information to\nimprove prediction accuracy. Using data from the Osteoporotic Fractures in Men\nStudy (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,\nStage 1 (Screening) employs clinical, demographic, and functional variables to\nestimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived\nfeatures for refinement. The model was rigorously validated through internal\nand external testing, showing consistent performance and adaptability across\ncohorts. Compared to T-score and FRAX, the two-stage framework achieved higher\nsensitivity and reduced missed cases, offering a cost-effective and\npersonalized approach for early hip fracture risk assessment.\n  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,\nFRAX", "AI": {"tldr": "A two-stage model combining clinical data and DXA imaging improves hip fracture risk prediction, outperforming traditional methods like T-score and FRAX in sensitivity and reducing missed high-risk cases.", "motivation": "Current hip fracture risk assessment tools (DXA T-score and FRAX) lack sensitivity and miss high-risk individuals, especially those without prior fractures or with osteopenia, creating a need for more accurate prediction methods.", "method": "Sequential two-stage model: Stage 1 uses clinical, demographic, and functional variables for baseline risk screening; Stage 2 incorporates DXA-derived imaging features for refinement. Validated using data from MrOS, SOF, and UK Biobank cohorts.", "result": "The two-stage framework achieved higher sensitivity and reduced missed cases compared to T-score and FRAX, with consistent performance across different cohorts through internal and external validation.", "conclusion": "The proposed two-stage model provides a cost-effective and personalized approach for early hip fracture risk assessment, offering improved accuracy over traditional methods."}}
{"id": "2510.15814", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15814", "abs": "https://arxiv.org/abs/2510.15814", "authors": ["Marco Pacini", "Mircea Petrache", "Bruno Lepri", "Shubhendu Trivedi", "Robin Walters"], "title": "On Universality of Deep Equivariant Networks", "comment": "Preprint. 22 pages", "summary": "Universality results for equivariant neural networks remain rare. Those that\ndo exist typically hold only in restrictive settings: either they rely on\nregular or higher-order tensor representations, leading to impractically\nhigh-dimensional hidden spaces, or they target specialized architectures, often\nconfined to the invariant setting. This work develops a more general account.\nFor invariant networks, we establish a universality theorem under separation\nconstraints, showing that the addition of a fully connected readout layer\nsecures approximation within the class of separation-constrained continuous\nfunctions. For equivariant networks, where results are even scarcer, we\ndemonstrate that standard separability notions are inadequate and introduce the\nsharper criterion of $\\textit{entry-wise separability}$. We show that with\nsufficient depth or with the addition of appropriate readout layers,\nequivariant networks attain universality within the entry-wise separable\nregime. Together with prior results showing the failure of universality for\nshallow models, our findings identify depth and readout layers as a decisive\nmechanism for universality, additionally offering a unified perspective that\nsubsumes and extends earlier specialized results.", "AI": {"tldr": "This paper establishes universality theorems for equivariant neural networks, showing that depth and readout layers enable approximation of separation-constrained continuous functions for invariant networks and entry-wise separable functions for equivariant networks.", "motivation": "Existing universality results for equivariant neural networks are limited to restrictive settings - either requiring impractical high-dimensional representations or specialized architectures confined to invariant settings. The paper aims to develop a more general theoretical foundation.", "method": "For invariant networks: establish universality under separation constraints with fully connected readout layers. For equivariant networks: introduce the concept of 'entry-wise separability' as a sharper criterion and show that sufficient depth or appropriate readout layers achieve universality within this regime.", "result": "The paper demonstrates that with sufficient depth or readout layers, equivariant networks can achieve universality for entry-wise separable functions, while invariant networks can approximate separation-constrained continuous functions.", "conclusion": "Depth and readout layers are identified as decisive mechanisms for universality in equivariant neural networks, providing a unified perspective that extends earlier specialized results and addresses the limitations of shallow models."}}
{"id": "2510.15201", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.app-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.15201", "abs": "https://arxiv.org/abs/2510.15201", "authors": ["Mohammad Amin Nabian", "Sudeep Chavare", "Deepak Akhare", "Rishikesh Ranade", "Ram Cherukuri", "Srinivas Tadepalli"], "title": "Automotive Crash Dynamics Modeling Accelerated with Machine Learning", "comment": null, "summary": "Crashworthiness assessment is a critical aspect of automotive design,\ntraditionally relying on high-fidelity finite element (FE) simulations that are\ncomputationally expensive and time-consuming. This work presents an exploratory\ncomparative study on developing machine learning-based surrogate models for\nefficient prediction of structural deformation in crash scenarios using the\nNVIDIA PhysicsNeMo framework. Given the limited prior work applying machine\nlearning to structural crash dynamics, the primary contribution lies in\ndemonstrating the feasibility and engineering utility of the various modeling\napproaches explored in this work. We investigate two state-of-the-art neural\nnetwork architectures for modeling crash dynamics: MeshGraphNet, and\nTransolver. Additionally, we examine three strategies for modeling transient\ndynamics: time-conditional, the standard Autoregressive approach, and a\nstability-enhanced Autoregressive scheme incorporating rollout-based training.\nThe models are evaluated on a comprehensive Body-in-White (BIW) crash dataset\ncomprising 150 detailed FE simulations using LS-DYNA. The dataset represents a\nstructurally rich vehicle assembly with over 200 components, including 38 key\ncomponents featuring variable thickness distributions to capture realistic\nmanufacturing variability. Each model utilizes the undeformed mesh geometry and\ncomponent characteristics as inputs to predict the spatiotemporal evolution of\nthe deformed mesh during the crash sequence. Evaluation results show that the\nmodels capture the overall deformation trends with reasonable fidelity,\ndemonstrating the feasibility of applying machine learning to structural crash\ndynamics. Although not yet matching full FE accuracy, the models achieve\norders-of-magnitude reductions in computational cost, enabling rapid design\nexploration and early-stage optimization in crashworthiness evaluation.", "AI": {"tldr": "This paper presents a comparative study of machine learning surrogate models for predicting structural deformation in automotive crash scenarios using NVIDIA PhysicsNeMo framework, demonstrating feasibility and computational efficiency gains over traditional finite element simulations.", "motivation": "Traditional finite element simulations for crashworthiness assessment are computationally expensive and time-consuming, creating a need for faster alternatives to enable rapid design exploration and optimization.", "method": "The study investigates two neural network architectures (MeshGraphNet and Transolver) and three transient dynamics modeling strategies (time-conditional, standard Autoregressive, and stability-enhanced Autoregressive with rollout-based training) on a comprehensive Body-in-White crash dataset of 150 detailed FE simulations.", "result": "The models captured overall deformation trends with reasonable fidelity and achieved orders-of-magnitude reductions in computational cost compared to full FE simulations, though they did not yet match full FE accuracy.", "conclusion": "Machine learning approaches show feasibility for structural crash dynamics prediction, enabling rapid design exploration and early-stage optimization in crashworthiness evaluation despite not yet achieving full FE simulation accuracy."}}
{"id": "2510.15817", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15817", "abs": "https://arxiv.org/abs/2510.15817", "authors": ["Camille Touron", "Gabriel V. Cardoso", "Julyan Arbel", "Pedro L. C. Rodrigues"], "title": "Error analysis of a compositional score-based algorithm for simulation-based inference", "comment": null, "summary": "Simulation-based inference (SBI) has become a widely used framework in\napplied sciences for estimating the parameters of stochastic models that best\nexplain experimental observations. A central question in this setting is how to\neffectively combine multiple observations in order to improve parameter\ninference and obtain sharper posterior distributions. Recent advances in\nscore-based diffusion methods address this problem by constructing a\ncompositional score, obtained by aggregating individual posterior scores within\nthe diffusion process. While it is natural to suspect that the accumulation of\nindividual errors may significantly degrade sampling quality as the number of\nobservations grows, this important theoretical issue has so far remained\nunexplored. In this paper, we study the compositional score produced by the\nGAUSS algorithm of Linhart et al. (2024) and establish an upper bound on its\nmean squared error in terms of both the individual score errors and the number\nof observations. We illustrate our theoretical findings on a Gaussian example,\nwhere all analytical expressions can be derived in a closed form.", "AI": {"tldr": "This paper analyzes the compositional score method in simulation-based inference (SBI), specifically examining how error accumulates when combining multiple observations. It provides theoretical bounds on mean squared error for the GAUSS algorithm.", "motivation": "To understand how effectively combining multiple observations improves parameter inference in SBI, and to address the unexplored theoretical issue of error accumulation in compositional score methods as the number of observations increases.", "method": "Theoretical analysis of the compositional score produced by the GAUSS algorithm, establishing upper bounds on mean squared error in terms of individual score errors and number of observations. Validation using a Gaussian example with closed-form analytical expressions.", "result": "Established an upper bound on the mean squared error of compositional scores, showing how error scales with both individual score errors and the number of observations. Demonstrated theoretical findings on a Gaussian example.", "conclusion": "The study provides important theoretical insights into error accumulation in compositional score methods for SBI, offering bounds that help understand sampling quality degradation as more observations are combined."}}
{"id": "2510.15202", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15202", "abs": "https://arxiv.org/abs/2510.15202", "authors": ["Denis Janiak", "Jakub Binkowski", "Tomasz Kajdanowicz"], "title": "Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for the reliable deployment\nof deep learning models. hile Mahalanobis distance methods are widely used, the\nimpact of representation geometry and normalization on their performance is not\nfully understood, which may limit their downstream application. To address this\ngap, we conducted a comprehensive empirical study across diverse image\nfoundation models, datasets, and distance normalization schemes. First, our\nanalysis shows that Mahalanobis-based methods aren't universally reliable.\nSecond, we define the ideal geometry for data representations and demonstrate\nthat spectral and intrinsic-dimensionality metrics can accurately predict a\nmodel's OOD performance. Finally, we analyze how normalization impacts OOD\nperformance. Building upon these studies, we propose radially scaled $\\ell_2$\nnormalization, a method that generalizes the standard $\\ell_2$ normalization\nrecently applied to Mahalanobis-based OOD detection. Our approach introduces a\ntunable parameter to directly control the radial geometry of the feature space,\nsystematically contracting or expanding representations to significantly\nimprove OOD detection performance. By bridging the gap between representation\ngeometry, normalization, and OOD performance, our findings offer new insights\ninto the design of more effective and reliable deep learning models.", "AI": {"tldr": "This paper analyzes how representation geometry and normalization affect Mahalanobis distance-based OOD detection, showing these methods aren't universally reliable and proposing radially scaled \u2113\u2082 normalization to improve performance by controlling feature space geometry.", "motivation": "The impact of representation geometry and normalization on Mahalanobis distance methods for OOD detection is not fully understood, limiting their practical application. The authors aim to bridge this gap through empirical analysis.", "method": "Conducted comprehensive empirical study across diverse image foundation models, datasets, and distance normalization schemes. Analyzed spectral and intrinsic-dimensionality metrics, and proposed radially scaled \u2113\u2082 normalization with a tunable parameter to control feature space geometry.", "result": "Mahalanobis-based methods aren't universally reliable. Spectral and intrinsic-dimensionality metrics can predict OOD performance. Radially scaled \u2113\u2082 normalization significantly improves OOD detection by systematically contracting or expanding representations.", "conclusion": "The study bridges representation geometry, normalization, and OOD performance, offering insights for designing more effective and reliable deep learning models through controlled feature space geometry."}}
{"id": "2510.15824", "categories": ["stat.ML", "cs.LG", "91A26 62G08"], "pdf": "https://arxiv.org/pdf/2510.15824", "abs": "https://arxiv.org/abs/2510.15824", "authors": ["Guillaume Principato", "Gilles Stoltz"], "title": "Blackwell's Approachability for Sequential Conformal Inference", "comment": "25 pages, 0 figures", "summary": "We study conformal inference in non-exchangeable environments through the\nlens of Blackwell's theory of approachability. We first recast adaptive\nconformal inference (ACI, Gibbs and Cand\\`es, 2021) as a repeated two-player\nvector-valued finite game and characterize attainable coverage--efficiency\ntradeoffs. We then construct coverage and efficiency objectives under potential\nrestrictions on the adversary's play, and design a calibration-based\napproachability strategy to achieve these goals. The resulting algorithm enjoys\nstrong theoretical guarantees and provides practical insights, though its\ncomputational burden may limit deployment in practice.", "AI": {"tldr": "The paper analyzes conformal inference in non-exchangeable settings using Blackwell's approachability theory, recasting adaptive conformal inference as a game and developing calibration-based strategies with strong theoretical guarantees.", "motivation": "To address conformal inference in non-exchangeable environments where traditional exchangeability assumptions don't hold, requiring new theoretical frameworks and practical methods.", "method": "Recast adaptive conformal inference as a repeated two-player vector-valued finite game, characterize coverage-efficiency tradeoffs, construct objectives under adversary restrictions, and design calibration-based approachability strategies.", "result": "Developed an algorithm with strong theoretical guarantees that provides practical insights into coverage-efficiency tradeoffs in non-exchangeable conformal inference settings.", "conclusion": "The proposed approach successfully extends conformal inference to non-exchangeable environments using Blackwell's theory, though computational complexity may limit practical deployment."}}
{"id": "2510.15211", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15211", "abs": "https://arxiv.org/abs/2510.15211", "authors": ["Yongchan Kwon", "Shang Zhu", "Federico Bianchi", "Kaitlyn Zhou", "James Zou"], "title": "ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning", "comment": null, "summary": "The ability of large language models (LLMs) to follow user instructions is\ncentral to their reliability, safety, and usefulness. While prior studies\nassess instruction adherence in the model's main responses, we argue that it is\nalso critical for large reasoning models (LRMs) to follow user instructions\nthroughout their reasoning process. Reasoning instruction following makes LRMs\nmore controllable and transparent, while reducing risks of undesirable\nshortcuts, hallucinations, or reward hacking within reasoning traces. To\nevaluate this dimension, we introduce ReasonIF, a systematic benchmark for\nassessing reasoning instruction following. ReasonIF includes six categories of\ninstruction prompts, spanning multilingual reasoning, formatting and length\ncontrol. Across many open-source LRMs including GPT-OSS, Qwen3, and\nDeepSeek-R1, we find substantial failures in reasoning instruction adherence:\nthe highest instruction following score (IFS) remains below 0.25, meaning that\nfewer than $25\\%$ of reasoning traces comply with the given instructions.\nNotably, as task difficulty increases, reasoning instruction following degrades\nfurther. We also explore two strategies to enhance reasoning instruction\nfidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning\n(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to\n0.27, indicating measurable progress but leaving ample room for improvement.", "AI": {"tldr": "This paper introduces ReasonIF, a benchmark to evaluate how well large reasoning models (LRMs) follow user instructions during their reasoning process, not just in final responses. The study finds significant failures in instruction adherence across major LRMs and proposes two improvement strategies.", "motivation": "While prior work focused on instruction following in final responses, this paper argues it's critical for LRMs to follow instructions throughout their reasoning process to improve controllability, transparency, and reduce risks like hallucinations and reward hacking.", "method": "The authors developed ReasonIF benchmark with six categories of instruction prompts covering multilingual reasoning, formatting, and length control. They evaluated multiple open-source LRMs and explored two enhancement strategies: multi-turn reasoning and Reasoning Instruction Finetuning (RIF) using synthetic data.", "result": "Substantial failures in reasoning instruction adherence were found - the highest instruction following score (IFS) remained below 0.25 across models. Performance degraded further with increased task difficulty. RIF improved GPT-OSS-20B's IFS from 0.11 to 0.27.", "conclusion": "Current LRMs have significant room for improvement in reasoning instruction following. The proposed RIF method shows measurable progress but substantial challenges remain in ensuring LRMs properly follow instructions throughout their reasoning processes."}}
{"id": "2510.15216", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15216", "abs": "https://arxiv.org/abs/2510.15216", "authors": ["Xuansheng Wu", "Xiaoman Pan", "Wenlin Yao", "Jianshu Chen"], "title": "Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential", "comment": "Pre-print", "summary": "Reinforcement learning with verifiable rewards (RLVR) can elicit strong\nreasoning in large language models (LLMs), while their performance after RLVR\nvaries dramatically across different base models. This raises a fundamental\nquestion: what microscopic property of pre-trained models leads to this\nvariation? To investigate, we formalize reasoning as chains of Horn clauses\n(\"if-then\" rules) built from features extracted from the LLM's latent space via\ncross-layer sparse autoencoders (SAEs). We estimate the transition\nprobabilities between its features, and further categorize each rule by its\nsemantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key\ndiscovery is that high-potential models are inherently soundness-aware: their\ninternal probability distributions systematically shift across rules' soundness\nlevels, becoming highly distinct for \"strict\" versus \"noisy\" rules. In\ncontrast, weaker models are soundness-agnostic, collapsing to one distribution\nregardless of soundness levels. To quantify this, we introduce the\nSoundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon\nDivergence to measure the separation between these distributions. We show that\nSAL's predictions of post-RLVR reasoning performance follow a precise empirical\nlaw (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)\nand scales (0.5B-14B). This reveals that a model's reasoning potential is tied\nto its intrinsic, pre-trained ability to distinguish sound knowledge from\nunsound ones. These findings underscore the critical role of model pre-training\nin shaping reasoning and offer a practical metric grounded in the model's\ninternal mechanisms for selecting/designing stronger base models.", "AI": {"tldr": "The paper investigates why different base models show varying performance after reinforcement learning with verifiable rewards (RLVR). It discovers that high-performing models inherently distinguish between sound and unsound reasoning rules, while weaker models don't, and introduces a metric called Soundness-Aware Level (SAL) to quantify this property.", "motivation": "To understand why RLVR performance varies dramatically across different base models and identify the microscopic properties of pre-trained models that lead to this variation.", "method": "Formalize reasoning as chains of Horn clauses built from features extracted via cross-layer sparse autoencoders (SAEs), estimate transition probabilities between features, categorize rules by semantic soundness levels, and introduce SAL metric using Jensen-Shannon Divergence to measure distribution separation.", "result": "High-potential models are soundness-aware with distinct internal probability distributions for different soundness levels, while weaker models are soundness-agnostic. SAL predicts post-RLVR reasoning performance with high accuracy (R^2=0.87) across diverse model families and scales.", "conclusion": "A model's reasoning potential is tied to its intrinsic, pre-trained ability to distinguish sound knowledge from unsound ones, highlighting the critical role of model pre-training in shaping reasoning capabilities and providing a practical metric for selecting stronger base models."}}
{"id": "2510.15217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15217", "abs": "https://arxiv.org/abs/2510.15217", "authors": ["Emily Alsentzer", "Marie-Laure Charpignon", "Bill Chen", "Niharika D'Souza", "Jason Fries", "Yixing Jiang", "Aparajita Kashyap", "Chanwoo Kim", "Simon Lee", "Aishwarya Mandyam", "Ashery Christopher Mbilinyi", "Nikita Mehandru", "Nitish Nagesh", "Brighton Nuwagira", "Emma Pierson", "Arvind Pillai", "Akane Sano", "Tanveer Syeda-Mahmood", "Shashank Yadav", "Elias Adhanom", "Muhammad Umar Afza", "Amelia Archer", "Suhana Bedi", "Vasiliki Bikia", "Trenton Chang", "George H. Chen", "Winston Chen", "Erica Chiang", "Edward Choi", "Octavia Ciora", "Paz Dozie-Nnamah", "Shaza Elsharief", "Matthew Engelhard", "Ali Eshragh", "Jean Feng", "Josh Fessel", "Scott Fleming", "Kei Sen Fong", "Thomas Frost", "Soham Gadgil", "Judy Gichoya", "Leeor Hershkovich", "Sujeong Im", "Bhavya Jain", "Vincent Jeanselme", "Furong Jia", "Qixuan", "Jin", "Yuxuan Jin", "Daniel Kapash", "Geetika Kapoor", "Behdokht Kiafar", "Matthias Kleiner", "Stefan Kraft", "Annika Kumar", "Daeun Kyung", "Zhongyuan Liang", "Joanna Lin", "Qianchu", "Liu", "Chang Liu", "Hongzhou Luan", "Chris Lunt", "Leopoldo Jul\u00edan Lechuga L\u00f3pez", "Matthew B. A. McDermott", "Shahriar Noroozizadeh", "Connor O'Brien", "YongKyung Oh", "Mixail Ota", "Stephen Pfohl", "Meagan Pi", "Tanmoy Sarkar Pias", "Emma Rocheteau", "Avishaan Sethi", "Toru Shirakawa", "Anita Silver", "Neha Simha", "Kamile Stankeviciute", "Max Sunog", "Peter Szolovits", "Shengpu Tang", "Jialu Tang", "Aaron Tierney", "John Valdovinos", "Byron Wallace", "Will Ke Wang", "Peter Washington", "Jeremy Weiss", "Daniel Wolfe", "Emily Wong", "Hye Sun Yun", "Xiaoman Zhang", "Xiao Yu Cindy Zhang", "Hayoung Jeong", "Kaveri A. Thakoor"], "title": "Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025", "comment": null, "summary": "The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),\nhosted by the Association for Health Learning and Inference (AHLI), was held in\nperson on June 25-27, 2025, at the University of California, Berkeley, in\nBerkeley, California, USA. As part of this year's program, we hosted Research\nRoundtables to catalyze collaborative, small-group dialogue around critical,\ntimely topics at the intersection of machine learning and healthcare. Each\nroundtable was moderated by a team of senior and junior chairs who fostered\nopen exchange, intellectual curiosity, and inclusive engagement. The sessions\nemphasized rigorous discussion of key challenges, exploration of emerging\nopportunities, and collective ideation toward actionable directions in the\nfield. In total, eight roundtables were held by 19 roundtable chairs on topics\nof \"Explainability, Interpretability, and Transparency,\" \"Uncertainty, Bias,\nand Fairness,\" \"Causality,\" \"Domain Adaptation,\" \"Foundation Models,\" \"Learning\nfrom Small Medical Data,\" \"Multimodal Methods,\" and \"Scalable, Translational\nHealthcare Solutions.\"", "AI": {"tldr": "CHIL 2025 conference featured Research Roundtables with 8 sessions on key ML-healthcare topics, fostering collaborative dialogue among 19 chairs.", "motivation": "To catalyze collaborative discussions on critical topics at the intersection of machine learning and healthcare through small-group dialogue.", "method": "Hosted Research Roundtables moderated by senior and junior chairs, emphasizing open exchange, intellectual curiosity, and inclusive engagement in small-group settings.", "result": "Successfully conducted 8 roundtables on topics including Explainability, Uncertainty/Bias/Fairness, Causality, Domain Adaptation, Foundation Models, Learning from Small Medical Data, Multimodal Methods, and Scalable Healthcare Solutions.", "conclusion": "The roundtables effectively fostered rigorous discussion of key challenges and collective ideation toward actionable directions in ML-healthcare research."}}
{"id": "2510.15218", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15218", "abs": "https://arxiv.org/abs/2510.15218", "authors": ["Han Ouyang", "Jesse Hamilton", "Saeed Amal"], "title": "Machine Learning for Early Detection of Meningitis: Stacked Ensemble Learning with EHR data", "comment": null, "summary": "We utilized a cohort of 214 meningitis patients and 46,303 non-meningitis\npatients from the MIMIC-III database. After extensive data preprocessing, which\nincluded ICD-based cohort selection, one-hot encoding of coding, and a\ntwo-stage feature selection process (for both the training set and the testing\nsets), clinically relevant features such as gender and high-risk ICD codes\n(including subarachnoid hemorrhage, secondary malignant neoplasm of the brain,\nand generalized epilepsy) are selected. Overall, these clinically reasonable\nand temporally adherent features provided excellent modeling performance. Three\nmodels (Random Forest, LightGBM, and Deep Neural Networks (DNN) are trained as\nbase models for Ensemble Learning. Base model outputs are aggregated and\nstacked into a meta model (Logistic Regression) that uses the base model\noutputs as input values in training. Ultimately, soldier outputs (AUC of\nTesting Set 1: 0.9637, AUC of Testing Set 2: 0.9472) are obtained through\nensemble learning.\n  We created a challenging condition for diagnosing meningitis, simulating a\nreal-world ER (Emergency Room) scenario to enhance clinical use in real-world\napplications. While directly deploying a diagnostic tool that clinicians can\nuse is challenging, this paper paves the way for a potential future AI-driven\ndiagnostic approach for meningitis using Ensemble Learning.", "AI": {"tldr": "This paper develops an ensemble learning approach using Random Forest, LightGBM, and DNN models to diagnose meningitis, achieving high AUC scores (0.9637 and 0.9472) on test sets by simulating real-world ER scenarios.", "motivation": "To create an AI-driven diagnostic tool for meningitis that can be used in real-world emergency room settings, addressing the challenge of accurate and timely diagnosis.", "method": "Used MIMIC-III database with 214 meningitis and 46,303 non-meningitis patients. Applied extensive preprocessing including ICD-based cohort selection, one-hot encoding, and two-stage feature selection. Trained three base models (Random Forest, LightGBM, DNN) and aggregated their outputs through ensemble learning with logistic regression as meta model.", "result": "Achieved excellent performance with AUC of 0.9637 on Testing Set 1 and 0.9472 on Testing Set 2. Selected clinically relevant features including gender, subarachnoid hemorrhage, secondary malignant neoplasm of the brain, and generalized epilepsy.", "conclusion": "The ensemble learning approach provides a promising foundation for future AI-driven diagnostic tools for meningitis, though direct clinical deployment remains challenging. The study successfully simulated real-world ER scenarios to enhance clinical applicability."}}
{"id": "2510.15219", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15219", "abs": "https://arxiv.org/abs/2510.15219", "authors": ["Patricia Medina", "Rasika Karkare"], "title": "Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)", "comment": "16 pages, 6 figures, 5 tables", "summary": "This work extends our previous study on enhancing 3D LiDAR point-cloud\nclassification with product coefficients\n\\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic\ndescriptors that complement the original spatial Lidar features. Here, we show\nthat combining product coefficients with an autoencoder representation and a\nKNN classifier delivers consistent performance gains over both PCA-based\nbaselines and our earlier framework. We also investigate the effect of adding\nproduct coefficients level by level, revealing a clear trend: richer sets of\ncoefficients systematically improve class separability and overall accuracy.\nThe results highlight the value of combining hierarchical product-coefficient\nfeatures with autoencoders to push LiDAR classification performance further.", "AI": {"tldr": "This paper extends previous work by combining product coefficients with autoencoder representations and KNN classifiers, showing consistent performance improvements over PCA baselines and earlier frameworks.", "motivation": "To enhance 3D LiDAR point-cloud classification by integrating product coefficients with autoencoder representations, building on previous research that showed product coefficients complement original spatial LiDAR features.", "method": "Combined product coefficients with autoencoder representation and KNN classifier, investigated adding product coefficients level by level to analyze their impact on performance.", "result": "The combination delivered consistent performance gains over PCA-based baselines and earlier frameworks, with richer sets of coefficients systematically improving class separability and overall accuracy.", "conclusion": "Hierarchical product-coefficient features combined with autoencoders significantly enhance LiDAR classification performance, demonstrating the value of this integrated approach."}}
{"id": "2510.15262", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15262", "abs": "https://arxiv.org/abs/2510.15262", "authors": ["Zhiyuan Fan", "Yifeng Liu", "Qingyue Zhao", "Angela Yuan", "Quanquan Gu"], "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning", "comment": null, "summary": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization ($\\mu$P) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading $\\mu$P transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\n$\\sqrt{\\eta/\\lambda}$ with an approximately invariant shape; under width\nscaling $d$, we observe that the top singular value scales approximately as\n$\\sqrt{\\eta/\\lambda}\\cdot d^{0.75}$. Combining this observation with the $\\mu$P\nlearning-rate rule $\\eta_2\\propto d^{-1}$ for matrix-like parameters implies an\nempirical weight-decay scaling rule $\\lambda_2\\propto \\sqrt{d}$ that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at $\\eta_1=\\Theta_d(1)$ and $\\lambda_1=0$, this yields\n\\emph{zero-shot} transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend $\\mu$P beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW.", "AI": {"tldr": "The paper introduces a weight-decay scaling rule for AdamW that enables zero-shot transfer of learning rates and weight decay across different model widths, extending \u03bcP beyond the initial training phase by controlling optimizer-governed steady-state scales.", "motivation": "Modern scale-invariant architectures quickly enter an optimizer-governed steady state where normalization layers create backward scale sensitivity, making effective learning rate width-dependent and degrading \u03bcP transfer performance.", "method": "Proposes a weight-decay scaling rule \u03bb\u2082 \u221d \u221ad for matrix-like parameters combined with \u03bcP learning-rate rule \u03b7\u2082 \u221d d\u207b\u00b9, while keeping vector-like parameters at \u03b7\u2081 = \u0398_d(1) and \u03bb\u2081 = 0, to maintain sublayer gain invariance across widths.", "result": "Validated on LLaMA-style Transformers and minimal synthetic settings, the method enables zero-shot hyperparameter transfer without per-width sweeps and provides a diagnostic for checking sublayer-gain invariance.", "conclusion": "The approach extends \u03bcP beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW."}}
{"id": "2510.15447", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15447", "abs": "https://arxiv.org/abs/2510.15447", "authors": ["Shiqin Tang", "Shuxin Zhuang", "Rong Feng", "Runsheng Yu", "Hongzong Li", "Youzhi Zhang"], "title": "Particle Dynamics for Latent-Variable Energy-Based Models", "comment": null, "summary": "Latent-variable energy-based models (LVEBMs) assign a single normalized\nenergy to joint pairs of observed data and latent variables, offering\nexpressive generative modeling while capturing hidden structure. We recast\nmaximum-likelihood training as a saddle problem over distributions on the\nlatent and joint manifolds and view the inner updates as coupled Wasserstein\ngradient flows. The resulting algorithm alternates overdamped Langevin updates\nfor a joint negative pool and for conditional latent particles with stochastic\nparameter ascent, requiring no discriminator or auxiliary networks. We prove\nexistence and convergence under standard smoothness and dissipativity\nassumptions, with decay rates in KL divergence and Wasserstein-2 distance. The\nsaddle-point view further yields an ELBO strictly tighter than bounds obtained\nwith restricted amortized posteriors. Our method is evaluated on numerical\napproximations of physical systems and performs competitively against\ncomparable approaches.", "AI": {"tldr": "The paper presents a novel training method for latent-variable energy-based models (LVEBMs) by reformulating maximum-likelihood training as a saddle-point problem over distributions, using coupled Wasserstein gradient flows without requiring discriminators or auxiliary networks.", "motivation": "To develop an expressive generative modeling approach for latent-variable energy-based models that captures hidden structure while avoiding the need for discriminators or auxiliary networks, providing theoretical guarantees and improved performance.", "method": "Recast maximum-likelihood training as a saddle problem over distributions on latent and joint manifolds, using coupled Wasserstein gradient flows with alternating overdamped Langevin updates for joint negative pool and conditional latent particles with stochastic parameter ascent.", "result": "The method achieves competitive performance on numerical approximations of physical systems, with proven existence and convergence under standard assumptions, and provides an ELBO strictly tighter than bounds from restricted amortized posteriors.", "conclusion": "The saddle-point formulation enables effective training of LVEBMs with theoretical guarantees, demonstrating competitive performance while eliminating the need for discriminators or auxiliary networks."}}
{"id": "2510.15232", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15232", "abs": "https://arxiv.org/abs/2510.15232", "authors": ["Tiansheng Hu", "Tongyan Hu", "Liuyang Bai", "Yilun Zhao", "Arman Cohan", "Chen Zhao"], "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain", "comment": "EMNLP 2025 Main", "summary": "Recent LLMs have demonstrated promising ability in solving finance related\nproblems. However, applying LLMs in real-world finance application remains\nchallenging due to its high risk and high stakes property. This paper\nintroduces FinTrust, a comprehensive benchmark specifically designed for\nevaluating the trustworthiness of LLMs in finance applications. Our benchmark\nfocuses on a wide range of alignment issues based on practical context and\nfeatures fine-grained tasks for each dimension of trustworthiness evaluation.\nWe assess eleven LLMs on FinTrust and find that proprietary models like o4-mini\noutperforms in most tasks such as safety while open-source models like\nDeepSeek-V3 have advantage in specific areas like industry-level fairness. For\nchallenging task like fiduciary alignment and disclosure, all LLMs fall short,\nshowing a significant gap in legal awareness. We believe that FinTrust can be a\nvaluable benchmark for LLMs' trustworthiness evaluation in finance domain.", "AI": {"tldr": "FinTrust is a comprehensive benchmark for evaluating LLM trustworthiness in finance applications, assessing 11 LLMs across various alignment dimensions and finding proprietary models generally outperform but open-source models excel in specific areas like fairness, while all models struggle with legal awareness tasks.", "motivation": "Applying LLMs in real-world finance applications is challenging due to high risks and stakes, necessitating a specialized benchmark to evaluate trustworthiness across practical contexts and alignment issues.", "method": "Developed FinTrust benchmark with fine-grained tasks across multiple trustworthiness dimensions, evaluated 11 LLMs including proprietary (o4-mini) and open-source (DeepSeek-V3) models on various finance-related trustworthiness metrics.", "result": "Proprietary models like o4-mini outperformed in most tasks (e.g., safety), open-source models like DeepSeek-V3 had advantages in specific areas (e.g., industry-level fairness), and all LLMs performed poorly on challenging tasks like fiduciary alignment and disclosure, showing significant legal awareness gaps.", "conclusion": "FinTrust serves as a valuable benchmark for evaluating LLM trustworthiness in finance, revealing current limitations in legal awareness and highlighting the need for continued improvement in financial domain alignment."}}
{"id": "2510.15464", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15464", "abs": "https://arxiv.org/abs/2510.15464", "authors": ["Nirmit Joshi", "Gene Li", "Siddharth Bhandari", "Shiva Prasad Kasiviswanathan", "Cong Ma", "Nathan Srebro"], "title": "Learning to Answer from Correct Demonstrations", "comment": "Comments are welcome", "summary": "We study the problem of learning to generate an answer (or completion) to a\nquestion (or prompt), where there could be multiple correct answers, any one of\nwhich is acceptable at test time. Learning is based on demonstrations of some\ncorrect answer to each training question, as in Supervised Fine Tuning (SFT).\nWe formalize the problem as offline imitation learning in contextual bandits,\nwith demonstrations from some optimal policy, without explicitly observed\nrewards. Prior work assumes that the demonstrator belongs to a low-complexity\npolicy class, which motivates maximum likelihood estimation (i.e., log-loss\nminimization). In contrast, we propose relying only on the reward model\n(specifying which answers are correct) being in a low-cardinality class, which\nwe argue is a weaker assumption. We show that likelihood maximization methods\ncan fail in this case, and instead devise an alternative novel approach that\nlearns with sample complexity logarithmic in the cardinality of the reward\nclass. Our work motivates looking beyond likelihood maximization when learning\nfrom correct demonstrations.", "AI": {"tldr": "The paper addresses learning to generate answers from correct demonstrations when multiple valid answers exist, proposing an alternative to maximum likelihood estimation that works with low-cardinality reward models.", "motivation": "Traditional methods assume low-complexity policy classes, but this paper argues that assuming low-cardinality reward classes is a weaker and more realistic assumption for learning from correct demonstrations.", "method": "The authors formalize the problem as offline imitation learning in contextual bandits and devise a novel approach that learns with logarithmic sample complexity in the reward class cardinality, avoiding likelihood maximization failures.", "result": "The proposed method achieves sample complexity logarithmic in the cardinality of the reward class, outperforming maximum likelihood estimation approaches which can fail in this setting.", "conclusion": "The work motivates moving beyond likelihood maximization for learning from correct demonstrations and demonstrates the effectiveness of their alternative approach with provable guarantees."}}
{"id": "2510.15233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15233", "abs": "https://arxiv.org/abs/2510.15233", "authors": ["Amitesh Badkul", "Lei Xie"], "title": "Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction", "comment": null, "summary": "Reliable, informative, and individual uncertainty quantification (UQ) remains\nmissing in current ML community. This hinders the effective application of\nAI/ML to risk-sensitive domains. Most methods either fail to provide coverage\non new data, inflate intervals so broadly that they are not actionable, or\nassign uncertainties that do not track actual error, especially under a\ndistribution shift. In high-stakes drug discovery, protein-ligand affinity\n(PLI) prediction is especially challenging as assay noise is heterogeneous,\nchemical space is imbalanced and large, and practical evaluations routinely\ninvolve distribution shift. In this work, we introduce a novel uncertainty\nquantification method, Trustworthy Expert Split-conformal with Scaled\nEstimation for Efficient Reliable Adaptive intervals (TESSERA), that provides\nper-sample uncertainty with reliable coverage guarantee, informative and\nadaptive prediction interval widths that track the absolute error. We evaluate\non protein-ligand binding affinity prediction under both independent and\nidentically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)\nsplits, comparing against strong UQ baselines. TESSERA attains near-nominal\ncoverage and the best coverage-width trade-off as measured by the\nCoverage-Width Criterion (CWC), while maintaining competitive adaptivity\n(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage\n(SSC) further confirms that intervals are right-sized, indicating width\nincreases when data are scarce or noisy, and remain tight when predictions are\nreliable. By unifying Mixture of Expert (MoE) diversity with conformal\ncalibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties\nthat are well-suited to selective prediction and downstream decision-making in\nthe drug-discovery pipeline and other applications.", "AI": {"tldr": "TESSERA is a novel uncertainty quantification method that provides reliable coverage guarantees, informative prediction intervals, and adaptive interval widths that track actual error, particularly addressing challenges in protein-ligand affinity prediction under distribution shifts.", "motivation": "Current ML methods lack reliable uncertainty quantification, hindering AI/ML application in risk-sensitive domains like drug discovery. Protein-ligand affinity prediction faces challenges with heterogeneous assay noise, imbalanced chemical space, and distribution shifts.", "method": "TESSERA combines Mixture of Expert (MoE) diversity with conformal calibration to provide per-sample uncertainty with reliable coverage guarantees, adaptive prediction intervals, and scaled estimation for efficient reliable adaptive intervals.", "result": "TESSERA achieves near-nominal coverage and the best coverage-width trade-off (measured by CWC), maintains competitive adaptivity (lowest AUSE), and demonstrates right-sized intervals through Size-Stratified Coverage analysis.", "conclusion": "By unifying MoE diversity with conformal calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties suitable for selective prediction and downstream decision-making in drug discovery and other applications."}}
{"id": "2510.15479", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15479", "abs": "https://arxiv.org/abs/2510.15479", "authors": ["Shiqin Tang", "Rong Feng", "Shuxin Zhuang", "Hongzong Li", "Youzhi Zhang"], "title": "Adversary-Free Counterfactual Prediction via Information-Regularized Representations", "comment": null, "summary": "We study counterfactual prediction under assignment bias and propose a\nmathematically grounded, information-theoretic approach that removes\ntreatment-covariate dependence without adversarial training. Starting from a\nbound that links the counterfactual-factual risk gap to mutual information, we\nlearn a stochastic representation Z that is predictive of outcomes while\nminimizing I(Z; T). We derive a tractable variational objective that\nupper-bounds the information term and couples it with a supervised decoder,\nyielding a stable, provably motivated training criterion. The framework extends\nnaturally to dynamic settings by applying the information penalty to sequential\nrepresentations at each decision time. We evaluate the method on controlled\nnumerical simulations and a real-world clinical dataset, comparing against\nrecent state-of-the-art balancing, reweighting, and adversarial baselines.\nAcross metrics of likelihood, counterfactual error, and policy evaluation, our\napproach performs favorably while avoiding the training instabilities and\ntuning burden of adversarial schemes.", "AI": {"tldr": "The paper proposes an information-theoretic approach for counterfactual prediction under assignment bias, using mutual information minimization to remove treatment-covariate dependence without adversarial training.", "motivation": "To address assignment bias in counterfactual prediction while avoiding the training instabilities and tuning difficulties of adversarial methods.", "method": "Learn a stochastic representation Z that predicts outcomes while minimizing I(Z; T) using a tractable variational objective that upper-bounds the information term and couples with a supervised decoder.", "result": "The method performs favorably on controlled simulations and real-world clinical data across likelihood, counterfactual error, and policy evaluation metrics, outperforming balancing, reweighting, and adversarial baselines.", "conclusion": "The information-theoretic framework provides stable, provably motivated training for counterfactual prediction that avoids adversarial training instabilities while maintaining strong performance."}}
{"id": "2510.15508", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15508", "abs": "https://arxiv.org/abs/2510.15508", "authors": ["Naoki Yoshida", "Satoshi Hayakawa", "Yuhta Takida", "Toshimitsu Uesaka", "Hiromi Wakaki", "Yuki Mitsufuji"], "title": "Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity", "comment": null, "summary": "In this study, we propose an enhancement to the similarity computation\nmechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior\ntheoretical research has demonstrated that the optimal similarity metrics\nbetween paired modalities should correspond to the pointwise mutual information\n(PMI) between the two modalities. However, the current implementations of CLIP\nand its variants fail to fully utilize the underlying linear structure of PMI.\nWe therefore propose KME-CLIP, which leverages this structure through the inner\nproduct in a reproducing kernel Hilbert space. We theoretically prove that our\nmethod can approximate PMI with arbitrary accuracy and empirically demonstrate\nthat our approach overall outperforms the standard CLIP formulation across\nseveral retrieval and classification tasks.", "AI": {"tldr": "KME-CLIP enhances CLIP by using kernel methods to better approximate pointwise mutual information (PMI) for similarity computation, improving performance on retrieval and classification tasks.", "motivation": "Current CLIP implementations don't fully utilize the linear structure of PMI, which theory shows should be the optimal similarity metric between paired modalities.", "method": "Proposed KME-CLIP uses inner product in reproducing kernel Hilbert space to leverage PMI's linear structure and approximate it with arbitrary accuracy.", "result": "Empirical results show KME-CLIP outperforms standard CLIP across several retrieval and classification tasks.", "conclusion": "The kernel-based approach effectively captures the optimal PMI structure and improves multi-modal contrastive learning performance."}}
{"id": "2510.15254", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15254", "abs": "https://arxiv.org/abs/2510.15254", "authors": ["Dingya Feng", "Dingyuan Xue"], "title": "Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories", "comment": null, "summary": "Accurate forecasting of avian disease outbreaks is critical for wildlife\nconservation and public health. This study presents a Transformer-based\nframework for predicting the disease risk at the terminal locations of\nmigratory bird trajectories. We integrate multi-source datasets, including GPS\ntracking data from Movebank, outbreak records from the World Organisation for\nAnimal Health (WOAH), and geospatial context from GADM and Natural Earth. The\nraw coordinates are processed using H3 hierarchical geospatial encoding to\ncapture spatial patterns. The model learns spatiotemporal dependencies from\nbird movement sequences to estimate endpoint disease risk. Evaluation on a\nheld-out test set demonstrates strong predictive performance, achieving an\naccuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision\n(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These\nresults highlight the potential of Transformer architectures to support\nearly-warning systems for avian disease surveillance, enabling timely\nintervention and prevention strategies.", "AI": {"tldr": "Transformer-based framework predicts disease risk at migratory bird trajectory endpoints using GPS tracking, outbreak records, and geospatial data, achieving high accuracy (0.9821) and AUC (0.9803).", "motivation": "Accurate forecasting of avian disease outbreaks is critical for wildlife conservation and public health, requiring early-warning systems for timely intervention.", "method": "Integrates multi-source datasets (GPS tracking from Movebank, WOAH outbreak records, geospatial context) with H3 hierarchical geospatial encoding and Transformer architecture to learn spatiotemporal dependencies from bird movement sequences.", "result": "Strong predictive performance on test set: accuracy 0.9821, AUC 0.9803, average precision 0.9299, F1-score 0.8836 at optimal threshold.", "conclusion": "Transformer architectures show potential for supporting early-warning systems in avian disease surveillance, enabling timely intervention and prevention strategies."}}
{"id": "2510.15260", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T50", "I.2.6; I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2510.15260", "abs": "https://arxiv.org/abs/2510.15260", "authors": ["Yangyang Li"], "title": "DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models", "comment": "Preprint. Under review at ICLR 2026. 11 pages, 2 figures", "summary": "Large language models are highly sensitive to prompt wording. However,\npopular automatic prompt search methods, including InstructZero, often degrade\nunder distribution shift and adversarial evaluation because they optimize\nexpected performance under a single evaluation distribution. Consequently,\nprompts that work in one setting frequently fail to transfer. To address this,\nDRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian\noptimization. Specifically, an f-divergence ball defines an ambiguity set\naround the evaluation distribution, and a robust acquisition rule maximizes\nworst-case expected utility while retaining the query efficiency of Bayesian\nsearch. Therefore, the search explicitly targets reliability under distribution\nshift rather than average behavior alone. Experiments follow the\ninstruction-induction protocol with matched query budgets across formality\nrewriting, code debugging, and translation. For example, on BIG-Bench\ninformative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to\napproximately 85-90%, yielding an absolute gain of about 25-30 points.\nMoreover, auto-debugging shows about +25-point gains under domain shift.\nMeanwhile, stable tasks such as cause-and-effect remain above 96%, indicating\nno loss on in-distribution cases. Furthermore, improvements are consistent\nacross divergence choices and decoding temperatures. Overall, DRO-InstructZero\nconnects distributionally robust optimization with prompt learning, offering a\nplug-and-play and general approach for reliable, transferable prompt alignment\nunder real-world uncertainty.", "AI": {"tldr": "DRO-InstructZero introduces robust Bayesian optimization for zero-shot prompt optimization, using f-divergence balls to handle distribution shifts and improve reliability across tasks like formality rewriting and code debugging.", "motivation": "Current prompt search methods like InstructZero often fail under distribution shift and adversarial evaluation because they optimize for expected performance under a single evaluation distribution, leading to poor transferability.", "method": "Formulates zero-shot prompt optimization as robust Bayesian optimization with an f-divergence ball defining an ambiguity set around the evaluation distribution, using a robust acquisition rule to maximize worst-case expected utility while maintaining query efficiency.", "result": "Significant improvements across tasks: BIG-Bench informative-to-formal rewriting accuracy increased from 61.3% to 85-90% (+25-30 points), auto-debugging showed +25-point gains under domain shift, while stable tasks maintained above 96% accuracy with no in-distribution performance loss.", "conclusion": "DRO-InstructZero connects distributionally robust optimization with prompt learning, providing a plug-and-play approach for reliable and transferable prompt alignment under real-world uncertainty, with consistent improvements across divergence choices and decoding temperatures."}}
{"id": "2510.15821", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15821", "abs": "https://arxiv.org/abs/2510.15821", "authors": ["Abdul Fatir Ansari", "Oleksandr Shchur", "Jaris K\u00fcken", "Andreas Auer", "Boran Han", "Pedro Mercado", "Syama Sundar Rangapuram", "Huibin Shen", "Lorenzo Stella", "Xiyuan Zhang", "Mononito Goswami", "Shubham Kapoor", "Danielle C. Maddix", "Pablo Guerron", "Tony Hu", "Junming Yin", "Nick Erickson", "Prateek Mutalik Desai", "Hao Wang", "Huzefa Rangwala", "George Karypis", "Yuyang Wang", "Michael Bohlke-Schneider"], "title": "Chronos-2: From Univariate to Universal Forecasting", "comment": null, "summary": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.", "AI": {"tldr": "Chronos-2 is a pretrained time series model that handles univariate, multivariate, and covariate-informed forecasting tasks in zero-shot manner using group attention mechanism for in-context learning across related time series.", "motivation": "Existing pretrained time series models focus mainly on univariate forecasting, limiting their real-world applicability where multivariate data and covariates are crucial for accurate predictions.", "method": "Chronos-2 employs group attention mechanism for in-context learning across multiple time series within groups, trained on synthetic datasets that impose diverse multivariate structures on univariate series.", "result": "Achieves state-of-the-art performance across three benchmarks (fev-bench, GIFT-Eval, Chronos Benchmark II), with substantial improvements in multivariate and covariate-informed forecasting, consistently outperforming baselines by wide margins.", "conclusion": "Chronos-2's in-context learning capabilities establish it as a general-purpose forecasting model that can be used 'as is' in real-world forecasting pipelines without task-specific training."}}
{"id": "2510.15839", "categories": ["cs.LG", "econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15839", "abs": "https://arxiv.org/abs/2510.15839", "authors": ["Yeshwanth Cherapanamjeri", "Constantinos Daskalakis", "Gabriele Farina", "Sobhan Mohammadpour"], "title": "Learning Correlated Reward Models: Statistical Barriers and Opportunities", "comment": null, "summary": "Random Utility Models (RUMs) are a classical framework for modeling user\npreferences and play a key role in reward modeling for Reinforcement Learning\nfrom Human Feedback (RLHF). However, a crucial shortcoming of many of these\ntechniques is the Independence of Irrelevant Alternatives (IIA) assumption,\nwhich collapses \\emph{all} human preferences to a universal underlying utility\nfunction, yielding a coarse approximation of the range of human preferences. On\nthe other hand, statistical and computational guarantees for models avoiding\nthis assumption are scarce. In this paper, we investigate the statistical and\ncomputational challenges of learning a \\emph{correlated} probit model, a\nfundamental RUM that avoids the IIA assumption. First, we establish that the\nclassical data collection paradigm of pairwise preference data is\n\\emph{fundamentally insufficient} to learn correlational information,\nexplaining the lack of statistical and computational guarantees in this\nsetting. Next, we demonstrate that \\emph{best-of-three} preference data\nprovably overcomes these shortcomings, and devise a statistically and\ncomputationally efficient estimator with near-optimal performance. These\nresults highlight the benefits of higher-order preference data in learning\ncorrelated utilities, allowing for more fine-grained modeling of human\npreferences. Finally, we validate these theoretical guarantees on several\nreal-world datasets, demonstrating improved personalization of human\npreferences.", "AI": {"tldr": "The paper shows that pairwise preference data is insufficient for learning correlated Random Utility Models, but best-of-three preference data enables efficient learning of correlated utilities with improved personalization.", "motivation": "Random Utility Models (RUMs) are crucial for RLHF but suffer from the Independence of Irrelevant Alternatives (IIA) assumption, which oversimplifies human preferences. Existing methods avoiding IIA lack statistical and computational guarantees.", "method": "The authors investigate learning correlated probit models, showing pairwise data is fundamentally insufficient. They propose using best-of-three preference data and develop an efficient estimator with near-optimal performance.", "result": "Theoretical analysis proves best-of-three data overcomes limitations of pairwise data. The proposed estimator achieves statistically and computationally efficient learning of correlated utilities, validated on real-world datasets with improved personalization.", "conclusion": "Higher-order preference data (specifically best-of-three) enables effective learning of correlated utilities, providing more fine-grained modeling of human preferences and better personalization compared to traditional pairwise approaches."}}
{"id": "2510.15265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15265", "abs": "https://arxiv.org/abs/2510.15265", "authors": ["Emam Hossain", "Muhammad Hasan Ferdous", "Devon Dunmire", "Aneesh Subramanian", "Md Osman Gani"], "title": "Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift", "comment": "Accepted as full paper in ICMLA 2025 (Special Session 1: Deep\n  Learning and Applications)", "summary": "Causal modeling offers a principled foundation for uncovering stable,\ninvariant relationships in time-series data, thereby improving robustness and\ngeneralization under distribution shifts. Yet its potential is underutilized in\nspatiotemporal Earth observation, where models often depend on purely\ncorrelational features that fail to transfer across heterogeneous domains. We\npropose RIC-TSC, a regionally-informed causal time-series classification\nframework that embeds lag-aware causal discovery directly into sequence\nmodeling, enabling both predictive accuracy and scientific interpretability.\nUsing multi-modal satellite and reanalysis data-including Sentinel-1 microwave\nbackscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA\nmeteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify\nregion-specific and invariant predictors of supraglacial lake evolution in\nGreenland. Causal graphs are estimated globally and per basin, with validated\npredictors and their time lags supplied to lightweight classifiers. On a\nbalanced benchmark of 1000 manually labeled lakes from two contrasting melt\nseasons (2018-2019), causal models achieve up to 12.59% higher accuracy than\ncorrelation-based baselines under out-of-distribution evaluation. These results\nshow that causal discovery is not only a means of feature selection but also a\npathway to generalizable and mechanistically grounded models of dynamic Earth\nsurface processes.", "AI": {"tldr": "RIC-TSC is a causal time-series classification framework that embeds lag-aware causal discovery into sequence modeling for spatiotemporal Earth observation, achieving 12.59% higher accuracy than correlation-based methods under out-of-distribution evaluation.", "motivation": "Current spatiotemporal Earth observation models rely on correlational features that fail to transfer across heterogeneous domains, while causal modeling offers principled foundations for uncovering stable, invariant relationships that improve robustness and generalization under distribution shifts.", "method": "Proposed RIC-TSC framework uses Joint PCMCI+ (J-PCMCI+) for region-specific and invariant causal discovery of supraglacial lake evolution predictors from multi-modal satellite data (Sentinel-1, Sentinel-2, Landsat-8, CARRA meteorological variables), with validated predictors and time lags supplied to lightweight classifiers.", "result": "On a balanced benchmark of 1000 manually labeled lakes from two contrasting melt seasons (2018-2019), causal models achieved up to 12.59% higher accuracy than correlation-based baselines under out-of-distribution evaluation.", "conclusion": "Causal discovery serves not only as feature selection but also enables generalizable and mechanistically grounded models of dynamic Earth surface processes, demonstrating superior performance over purely correlational approaches."}}
{"id": "2510.15266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15266", "abs": "https://arxiv.org/abs/2510.15266", "authors": ["Xueqing Sun", "Renzhen Wang", "Quanziang Wang", "Yichen Wu", "Xixi Jia", "Deyu Meng"], "title": "Semi-Supervised Regression with Heteroscedastic Pseudo-Labels", "comment": "Accepted by NeurIPS 2025", "summary": "Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet\nits application to semi-supervised regression (SSR) remains relatively\nunder-explored. Unlike classification, where pseudo-labels are discrete and\nconfidence-based filtering is effective, SSR involves continuous outputs with\nheteroscedastic noise, making it challenging to assess pseudo-label\nreliability. As a result, naive pseudo-labeling can lead to error accumulation\nand overfitting to incorrect labels. To address this, we propose an\nuncertainty-aware pseudo-labeling framework that dynamically adjusts\npseudo-label influence from a bi-level optimization perspective. By jointly\nminimizing empirical risk over all data and optimizing uncertainty estimates to\nenhance generalization on labeled data, our method effectively mitigates the\nimpact of unreliable pseudo-labels. We provide theoretical insights and\nextensive experiments to validate our approach across various benchmark SSR\ndatasets, and the results demonstrate superior robustness and performance\ncompared to existing methods. Our code is available at\nhttps://github.com/sxq/Heteroscedastic-Pseudo-Labels.", "AI": {"tldr": "Proposes an uncertainty-aware pseudo-labeling framework for semi-supervised regression that dynamically adjusts pseudo-label influence through bi-level optimization to handle heteroscedastic noise and prevent error accumulation.", "motivation": "Pseudo-labeling is under-explored in semi-supervised regression due to continuous outputs with heteroscedastic noise, making reliability assessment challenging and leading to error accumulation and overfitting to incorrect labels.", "method": "Uncertainty-aware pseudo-labeling framework using bi-level optimization that jointly minimizes empirical risk over all data while optimizing uncertainty estimates to enhance generalization on labeled data.", "result": "Extensive experiments on benchmark SSR datasets demonstrate superior robustness and performance compared to existing methods.", "conclusion": "The proposed framework effectively mitigates the impact of unreliable pseudo-labels in semi-supervised regression through uncertainty-aware dynamic adjustment of pseudo-label influence."}}
{"id": "2510.15280", "categories": ["cs.LG", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.15280", "abs": "https://arxiv.org/abs/2510.15280", "authors": ["Fan Liu", "Jindong Han", "Tengfei Lyu", "Weijia Zhang", "Zhe-Rui Yang", "Lu Dai", "Cancheng Liu", "Hao Liu"], "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition", "comment": "NeurIPS 2025", "summary": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the\nlandscape of scientific research. Beyond accelerating tasks such as hypothesis\ngeneration, experimental design, and result interpretation, they prompt a more\nfundamental question: Are FMs merely enhancing existing scientific\nmethodologies, or are they redefining the way science is conducted? In this\npaper, we argue that FMs are catalyzing a transition toward a new scientific\nparadigm. We introduce a three-stage framework to describe this evolution: (1)\nMeta-Scientific Integration, where FMs enhance workflows within traditional\nparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become active\ncollaborators in problem formulation, reasoning, and discovery; and (3)\nAutonomous Scientific Discovery, where FMs operate as independent agents\ncapable of generating new scientific knowledge with minimal human intervention.\nThrough this lens, we review current applications and emerging capabilities of\nFMs across existing scientific paradigms. We further identify risks and future\ndirections for FM-enabled scientific discovery. This position paper aims to\nsupport the scientific community in understanding the transformative role of\nFMs and to foster reflection on the future of scientific discovery. Our project\nis available at\nhttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.", "AI": {"tldr": "Foundation models are transforming scientific research through a three-stage evolution: from enhancing existing workflows to human-AI collaboration and eventually autonomous scientific discovery.", "motivation": "To examine whether foundation models are merely improving existing scientific methods or fundamentally redefining how science is conducted, and to provide a framework for understanding this transformation.", "method": "Proposes a three-stage framework: (1) Meta-Scientific Integration (enhancing traditional workflows), (2) Hybrid Human-AI Co-Creation (active collaboration in problem-solving), and (3) Autonomous Scientific Discovery (independent knowledge generation). Reviews current applications and emerging capabilities.", "result": "Identifies that foundation models are catalyzing a transition toward a new scientific paradigm, moving beyond mere acceleration of tasks to fundamentally changing how science is conducted.", "conclusion": "Foundation models represent a transformative force in scientific discovery, requiring the community to understand their evolving role and reflect on the future of scientific methodology. The framework helps navigate this transition from enhancement to autonomous discovery."}}
{"id": "2510.15294", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15294", "abs": "https://arxiv.org/abs/2510.15294", "authors": ["Danil Parkhomenko", "Pavel Ovchinnikov", "Konstantin Soldatov", "Vitalii Kapitan", "Gennady Y. Chitov"], "title": "Identifying internal patterns in (1+1)-dimensional directed percolation using neural networks", "comment": "7 pages, 10 figures, 2 tables", "summary": "In this paper we present a neural network-based method for the automatic\ndetection of phase transitions and classification of hidden percolation\npatterns in a (1+1)-dimensional replication process. The proposed network model\nis based on the combination of CNN, TCN and GRU networks, which are trained\ndirectly on raw configurations without any manual feature extraction. The\nnetwork reproduces the phase diagram and assigns phase labels to\nconfigurations. It shows that deep architectures are capable of extracting\nhierarchical structures from the raw data of numerical experiments.", "AI": {"tldr": "Neural network method combining CNN, TCN and GRU for automatic detection of phase transitions and classification of hidden percolation patterns in (1+1)-dimensional replication processes.", "motivation": "To develop an automated approach for detecting phase transitions and classifying hidden percolation patterns without manual feature extraction, leveraging deep learning capabilities.", "method": "Combination of CNN, TCN and GRU networks trained directly on raw configurations without manual feature extraction, applied to (1+1)-dimensional replication processes.", "result": "The network successfully reproduces the phase diagram and assigns phase labels to configurations, demonstrating its capability to extract hierarchical structures from raw numerical experiment data.", "conclusion": "Deep neural architectures are effective for automatically extracting hierarchical structures and detecting phase transitions from raw data in complex physical systems."}}
{"id": "2510.15300", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15300", "abs": "https://arxiv.org/abs/2510.15300", "authors": ["Jonas Kirch", "Sebastian Becker", "Tiago Koketsu Rodrigues", "Stefan Harmeling"], "title": "DFCA: Decentralized Federated Clustering Algorithm", "comment": null, "summary": "Clustered Federated Learning has emerged as an effective approach for\nhandling heterogeneous data across clients by partitioning them into clusters\nwith similar or identical data distributions. However, most existing methods,\nincluding the Iterative Federated Clustering Algorithm (IFCA), rely on a\ncentral server to coordinate model updates, which creates a bottleneck and a\nsingle point of failure, limiting their applicability in more realistic\ndecentralized learning settings. In this work, we introduce DFCA, a fully\ndecentralized clustered FL algorithm that enables clients to collaboratively\ntrain cluster-specific models without central coordination. DFCA uses a\nsequential running average to aggregate models from neighbors as updates\narrive, providing a communication-efficient alternative to batch aggregation\nwhile maintaining clustering performance. Our experiments on various datasets\ndemonstrate that DFCA outperforms other decentralized algorithms and performs\ncomparably to centralized IFCA, even under sparse connectivity, highlighting\nits robustness and practicality for dynamic real-world decentralized networks.", "AI": {"tldr": "DFCA is a fully decentralized clustered federated learning algorithm that eliminates the need for a central server, enabling clients to collaboratively train cluster-specific models through sequential running average aggregation from neighbors.", "motivation": "Existing clustered federated learning methods like IFCA rely on central servers, creating bottlenecks and single points of failure, limiting their applicability in realistic decentralized settings.", "method": "DFCA uses sequential running average to aggregate models from neighbors as updates arrive, providing communication-efficient decentralized clustering without central coordination.", "result": "Experiments show DFCA outperforms other decentralized algorithms and performs comparably to centralized IFCA, even under sparse connectivity conditions.", "conclusion": "DFCA demonstrates robustness and practicality for dynamic real-world decentralized networks by providing effective decentralized clustered federated learning without central coordination bottlenecks."}}
{"id": "2510.15327", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15327", "abs": "https://arxiv.org/abs/2510.15327", "authors": ["Zailin Ma", "Jiansheng Yang", "Yaodong Yang"], "title": "On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions", "comment": null, "summary": "This paper studies the generalization properties of a recently proposed\nkernel method, the Random Feature models with Learnable Activation Functions\n(RFLAF). By applying a data-dependent sampling scheme for generating features,\nwe provide by far the sharpest bounds on the required number of features for\nlearning RFLAF in both the regression and classification tasks. We provide a\nunified theorem that describes the complexity of the feature number $s$, and\ndiscuss the results for the plain sampling scheme and the data-dependent\nleverage weighted scheme. Through weighted sampling, the bound on $s$ in the\nMSE loss case is improved from $\\Omega(1/\\epsilon^2)$ to\n$\\tilde{\\Omega}((1/\\epsilon)^{1/t})$ in general $(t\\geq 1)$, and even to\n$\\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss\ncase, the bound is improved from $\\Omega(1/\\epsilon^2)$ to\n$\\tilde{\\Omega}((1/\\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also\npropose an algorithm to find an approximate kernel and then apply the leverage\nweighted sampling. Empirical results show that the weighted RFLAF achieves the\nsame performances with a significantly fewer number of features compared to the\nplainly sampled RFLAF, validating our theories and the effectiveness of this\nmethod.", "AI": {"tldr": "This paper provides sharp generalization bounds for Random Feature models with Learnable Activation Functions (RFLAF) using data-dependent sampling, significantly reducing the required number of features for learning in regression and classification tasks.", "motivation": "To improve the generalization properties and reduce the computational complexity of kernel methods by developing sharper bounds on the required number of features through data-dependent sampling schemes.", "method": "The authors apply data-dependent sampling schemes (plain sampling and leverage weighted sampling) for generating features in RFLAF models, and propose an algorithm to find approximate kernels for weighted RFLAF learning.", "result": "Through weighted sampling, the bound on required features s improves from \u03a9(1/\u03b5\u00b2) to \u03a9\u0303((1/\u03b5)^{1/t}) for MSE loss, and to \u03a9(1) when the Gram matrix has finite rank. For Lipschitz loss, it improves from \u03a9(1/\u03b5\u00b2) to \u03a9\u0303((1/\u03b5\u00b2)^{1/t}). Empirical results show weighted RFLAF achieves same performance with significantly fewer features.", "conclusion": "Data-dependent sampling schemes, particularly leverage weighted sampling, provide substantially sharper generalization bounds for RFLAF models, enabling efficient learning with fewer features while maintaining performance."}}
{"id": "2510.15333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15333", "abs": "https://arxiv.org/abs/2510.15333", "authors": ["Yuyuan Feng", "Bin Ma", "Enyan Dai"], "title": "Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks", "comment": null, "summary": "Extensive research has highlighted the vulnerability of graph neural networks\n(GNNs) to adversarial attacks, including manipulation, node injection, and the\nrecently emerging threat of backdoor attacks. However, existing defenses\ntypically focus on a single type of attack, lacking a unified approach to\nsimultaneously defend against multiple threats. In this work, we leverage the\nflexibility of the Mixture of Experts (MoE) architecture to design a scalable\nand unified framework for defending against backdoor, edge manipulation, and\nnode injection attacks. Specifically, we propose an MI-based logic diversity\nloss to encourage individual experts to focus on distinct neighborhood\nstructures in their decision processes, thus ensuring a sufficient subset of\nexperts remains unaffected under perturbations in local structures. Moreover,\nwe introduce a robustness-aware router that identifies perturbation patterns\nand adaptively routes perturbed nodes to corresponding robust experts.\nExtensive experiments conducted under various adversarial settings demonstrate\nthat our method consistently achieves superior robustness against multiple\ngraph adversarial attacks.", "AI": {"tldr": "A unified defense framework using Mixture of Experts (MoE) architecture to protect graph neural networks against multiple adversarial attacks including backdoor, edge manipulation, and node injection attacks.", "motivation": "Existing graph neural network defenses focus on single attack types, lacking comprehensive protection against multiple simultaneous threats like backdoor, manipulation, and injection attacks.", "method": "Proposes MI-based logic diversity loss to make experts focus on different neighborhood structures, and a robustness-aware router that detects perturbations and routes nodes to appropriate robust experts.", "result": "Extensive experiments show superior robustness across various adversarial settings compared to existing methods.", "conclusion": "The MoE-based unified framework effectively defends against multiple graph adversarial attacks through expert diversity and adaptive routing."}}
{"id": "2510.15366", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15366", "abs": "https://arxiv.org/abs/2510.15366", "authors": ["Jinwoo Kim", "Max Beier", "Petar Bevanda", "Nayun Kim", "Seunghoon Hong"], "title": "Sequence Modeling with Spectral Mean Flows", "comment": "30 pages, 9 figures", "summary": "A key question in sequence modeling with neural networks is how to represent\nand learn highly nonlinear and probabilistic state dynamics. Operator theory\nviews such dynamics as linear maps on Hilbert spaces containing mean embedding\nvectors of distributions, offering an appealing but currently overlooked\nperspective. We propose a new approach to sequence modeling based on an\noperator-theoretic view of a hidden Markov model (HMM). Instead of\nmaterializing stochastic recurrence, we embed the full sequence distribution as\na tensor in the product Hilbert space. A generative process is then defined as\nmaximum mean discrepancy (MMD) gradient flow in the space of sequences. To\novercome challenges with large tensors and slow sampling convergence, we\nintroduce spectral mean flows, a novel tractable algorithm integrating two core\nconcepts. First, we propose a new neural architecture by leveraging spectral\ndecomposition of linear operators to derive a scalable tensor network\ndecomposition of sequence mean embeddings. Second, we extend MMD gradient flows\nto time-dependent Hilbert spaces and connect them to flow matching via the\ncontinuity equation, enabling simulation-free learning and faster sampling. We\ndemonstrate competitive results on a range of time-series modeling datasets.\nCode is available at https://github.com/jw9730/spectral-mean-flow.", "AI": {"tldr": "The paper proposes Spectral Mean Flows, a novel sequence modeling approach that combines operator theory with maximum mean discrepancy gradient flows to learn nonlinear probabilistic state dynamics through tensor network decompositions and simulation-free learning.", "motivation": "Current sequence modeling approaches struggle with representing and learning highly nonlinear and probabilistic state dynamics. Operator theory offers an appealing perspective by viewing dynamics as linear maps on Hilbert spaces, but this perspective has been overlooked in practical implementations.", "method": "The method uses an operator-theoretic view of hidden Markov models, embedding full sequence distributions as tensors in product Hilbert spaces. It introduces spectral mean flows with two core components: (1) a neural architecture using spectral decomposition for scalable tensor network decomposition of sequence mean embeddings, and (2) extension of MMD gradient flows to time-dependent Hilbert spaces connected to flow matching via continuity equations.", "result": "The approach demonstrates competitive results on various time-series modeling datasets, showing effectiveness in handling nonlinear probabilistic dynamics while overcoming challenges with large tensors and slow sampling convergence.", "conclusion": "Spectral Mean Flows provide a tractable and effective framework for sequence modeling by integrating operator theory with modern gradient flow techniques, enabling scalable representation of complex state dynamics and faster sampling through simulation-free learning."}}
{"id": "2510.15382", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15382", "abs": "https://arxiv.org/abs/2510.15382", "authors": ["Kexin Zheng", "Lauriane Teyssier", "Yinan Zheng", "Yu Luo", "Xiayuan Zhan"], "title": "Towards Robust Zero-Shot Reinforcement Learning", "comment": "Neurips 2025, 36 pages, 18 figures", "summary": "The recent development of zero-shot reinforcement learning (RL) has opened a\nnew avenue for learning pre-trained generalist policies that can adapt to\narbitrary new tasks in a zero-shot manner. While the popular Forward-Backward\nrepresentations (FB) and related methods have shown promise in zero-shot RL, we\nempirically found that their modeling lacks expressivity and that extrapolation\nerrors caused by out-of-distribution (OOD) actions during offline learning\nsometimes lead to biased representations, ultimately resulting in suboptimal\nperformance. To address these issues, we propose Behavior-REgularizEd Zero-shot\nRL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that\nsimultaneously enhances learning stability, policy extraction capability, and\nrepresentation learning quality. BREEZE introduces behavioral regularization in\nzero-shot RL policy learning, transforming policy optimization into a stable\nin-sample learning paradigm. Additionally, BREEZE extracts the policy using a\ntask-conditioned diffusion model, enabling the generation of high-quality and\nmultimodal action distributions in zero-shot RL settings. Moreover, BREEZE\nemploys expressive attention-based architectures for representation modeling to\ncapture the complex relationships between environmental dynamics. Extensive\nexperiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best\nor near-the-best performance while exhibiting superior robustness compared to\nprior offline zero-shot RL methods. The official implementation is available\nat: https://github.com/Whiterrrrr/BREEZE.", "AI": {"tldr": "BREEZE is an improved zero-shot RL framework that addresses expressivity limitations and OOD issues in Forward-Backward representations through behavioral regularization, diffusion-based policy extraction, and attention-based architectures.", "motivation": "Existing Forward-Backward methods in zero-shot RL suffer from limited expressivity and biased representations due to OOD actions during offline learning, leading to suboptimal performance.", "method": "BREEZE introduces behavioral regularization for stable in-sample learning, uses task-conditioned diffusion models for policy extraction, and employs attention-based architectures for expressive representation modeling.", "result": "Extensive experiments on ExORL and D4RL Kitchen show BREEZE achieves best or near-best performance with superior robustness compared to prior offline zero-shot RL methods.", "conclusion": "BREEZE successfully enhances learning stability, policy extraction capability, and representation quality in zero-shot RL, demonstrating improved performance and robustness over existing methods."}}
{"id": "2510.15388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15388", "abs": "https://arxiv.org/abs/2510.15388", "authors": ["Mingyang Sun", "Pengxiang Ding", "Weinan Zhang", "Donglin Wang"], "title": "Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning", "comment": null, "summary": "While behavior cloning with flow/diffusion policies excels at learning\ncomplex skills from demonstrations, it remains vulnerable to distributional\nshift, and standard RL methods struggle to fine-tune these models due to their\niterative inference process and the limitations of existing workarounds. In\nthis work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on\nthe key insight that discretizing the flow matching inference process via a\nfixed-step Euler scheme inherently aligns it with the variational\nJordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP\ndecomposes the global flow into a sequence of small, incremental\ntransformations between proximate distributions. Each step corresponds to a JKO\nupdate, regularizing policy changes to stay near the previous iterate and\nensuring stable online adaptation with entropic regularization. This\ndecomposition yields an efficient algorithm that fine-tunes pre-trained flows\nvia a cascade of small flow blocks, offering significant advantages:\nsimpler/faster training of sub-models, reduced computational/memory costs, and\nprovable stability grounded in Wasserstein trust regions. Comprehensive\nexperiments demonstrate SWFP's enhanced stability, efficiency, and superior\nadaptation performance across diverse robotic control benchmarks.", "AI": {"tldr": "SWFP framework discretizes flow matching inference to align with JKO principle, enabling stable fine-tuning of pre-trained flow policies through incremental transformations with entropic regularization.", "motivation": "Behavior cloning with flow/diffusion policies is vulnerable to distributional shift, and standard RL methods struggle to fine-tune these models due to iterative inference processes and limitations of existing workarounds.", "method": "SWFP decomposes global flow into sequence of small incremental transformations between proximate distributions using fixed-step Euler scheme, with each step corresponding to a JKO update regularized by entropic regularization.", "result": "Comprehensive experiments demonstrate SWFP's enhanced stability, efficiency, and superior adaptation performance across diverse robotic control benchmarks.", "conclusion": "SWFP provides an efficient algorithm for fine-tuning pre-trained flows via cascade of small flow blocks, offering simpler training, reduced computational costs, and provable stability grounded in Wasserstein trust regions."}}
{"id": "2510.15403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15403", "abs": "https://arxiv.org/abs/2510.15403", "authors": ["Anyi Li", "Jiacheng Cen", "Songyou Li", "Mingze Li", "Yang Yu", "Wenbing Huang"], "title": "Geometric Mixture Models for Electrolyte Conductivity Prediction", "comment": null, "summary": "Accurate prediction of ionic conductivity in electrolyte systems is crucial\nfor advancing numerous scientific and technological applications. While\nsignificant progress has been made, current research faces two fundamental\nchallenges: (1) the lack of high-quality standardized benchmarks, and (2)\ninadequate modeling of geometric structure and intermolecular interactions in\nmixture systems. To address these limitations, we first reorganize and enhance\nthe CALiSol and DiffMix electrolyte datasets by incorporating geometric graph\nrepresentations of molecules. We then propose GeoMix, a novel geometry-aware\nframework that preserves Set-SE(3) equivariance-an essential but challenging\nproperty for mixture systems. At the heart of GeoMix lies the Geometric\nInteraction Network (GIN), an equivariant module specifically designed for\nintermolecular geometric message passing. Comprehensive experiments demonstrate\nthat GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,\nand geometric GNNs) across both datasets, validating the importance of\ncross-molecular geometric interactions and equivariant message passing for\naccurate property prediction. This work not only establishes new benchmarks for\nelectrolyte research but also provides a general geometric learning framework\nthat advances modeling of mixture systems in energy materials, pharmaceutical\ndevelopment, and beyond.", "AI": {"tldr": "GeoMix is a geometry-aware framework for predicting ionic conductivity in electrolyte systems that addresses challenges in standardized benchmarks and geometric modeling of mixture systems through equivariant message passing.", "motivation": "Current research faces two key challenges: lack of high-quality standardized benchmarks and inadequate modeling of geometric structure and intermolecular interactions in mixture systems.", "method": "Reorganized CALiSol and DiffMix datasets with geometric graph representations, then proposed GeoMix framework with Geometric Interaction Network (GIN) for equivariant intermolecular geometric message passing that preserves Set-SE(3) equivariance.", "result": "GeoMix consistently outperforms diverse baselines (MLPs, GNNs, and geometric GNNs) across both datasets, validating the importance of cross-molecular geometric interactions and equivariant message passing.", "conclusion": "This work establishes new benchmarks for electrolyte research and provides a general geometric learning framework that advances modeling of mixture systems in energy materials, pharmaceutical development, and beyond."}}
{"id": "2510.15404", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15404", "abs": "https://arxiv.org/abs/2510.15404", "authors": ["Christopher Salazar", "Krithika Manohar", "Ashis G. Banerjee"], "title": "Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing", "comment": null, "summary": "Real-time forecasting from streaming data poses critical challenges: handling\nnon-stationary dynamics, operating under strict computational limits, and\nadapting rapidly without catastrophic forgetting. However, many existing\napproaches face trade-offs between accuracy, adaptability, and efficiency,\nparticularly when deployed in constrained computing environments. We introduce\nWORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method\nthat combines Random Fourier Features with online Dynamic Mode Decomposition to\ncapture nonlinear dynamics through explicit feature mapping, while preserving\nfixed computational cost and competitive predictive accuracy across evolving\ndata. WORK-DMD employs Sherman-Morrison updates within rolling windows,\nenabling continuous adaptation to evolving dynamics from only current data,\neliminating the need for lengthy training or large storage requirements for\nhistorical data. Experiments on benchmark datasets across several domains show\nthat WORK-DMD achieves higher accuracy than several state-of-the-art online\nforecasting methods, while requiring only a single pass through the data and\ndemonstrating particularly strong performance in short-term forecasting. Our\nresults show that combining kernel evaluations with adaptive matrix updates\nachieves strong predictive performance with minimal data requirements. This\nsample efficiency offers a practical alternative to deep learning for streaming\nforecasting applications.", "AI": {"tldr": "WORK-DMD is an online forecasting method that combines Random Fourier Features with Dynamic Mode Decomposition to handle non-stationary streaming data with fixed computational cost and competitive accuracy.", "motivation": "Address challenges in real-time forecasting from streaming data: handling non-stationary dynamics, operating under strict computational limits, and adapting rapidly without catastrophic forgetting, while overcoming trade-offs between accuracy, adaptability, and efficiency in constrained computing environments.", "method": "Combines Random Fourier Features with online Dynamic Mode Decomposition to capture nonlinear dynamics through explicit feature mapping. Uses Sherman-Morrison updates within rolling windows for continuous adaptation from current data only, eliminating need for lengthy training or large historical data storage.", "result": "Achieves higher accuracy than several state-of-the-art online forecasting methods across benchmark datasets, requires only single pass through data, and demonstrates particularly strong performance in short-term forecasting.", "conclusion": "Combining kernel evaluations with adaptive matrix updates achieves strong predictive performance with minimal data requirements, offering a practical alternative to deep learning for streaming forecasting applications."}}
{"id": "2510.15425", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15425", "abs": "https://arxiv.org/abs/2510.15425", "authors": ["Wei Wang", "Xiao-Yong Wei", "Qing Li"], "title": "ParaFormer: Shallow Parallel Transformers with Progressive Approximation", "comment": null, "summary": "The widespread 'deeper is better' philosophy has driven the creation of\narchitectures like ResNet and Transformer, which achieve high performance by\nstacking numerous layers. However, increasing model depth comes with challenges\nsuch as longer training times, higher inference latency, and impracticality on\nresource-constrained devices. To address these issues, we propose ParaFormer, a\nshallow Transformer architecture designed for true parallelism in both\nstructure and computation. By formulating standard Transformers as function\napproximators in closed-form, our theoretical analysis shows that their\nperformance relies on inter-layer collaboration for progressive approximation,\nrather than depth itself. While deep Transformers enforce this collaboration\nthrough sequential designs, we demonstrate that such collaboration is not\ninherently tied to sequential structures. ParaFormer removes the sequential\nconstraint by organizing layers into parallel branches, enforcing inter-layer\ncollaboration algorithmically. Specifically, we implement progressive\napproximation, ensuring that each new branch further reduces the loss from\npreceding branches, enabling faster convergence. Extensive experiments validate\nParaFormer's effectiveness, outperforming standard Transformers like ViT.\nMoreover, ParaFormer supports up to 15.07x model compression and facilitates\nmodel expansion for adaptive continuous learning. Experimental results on\nmulti-GPU deployment demonstrate that ParaFormer is 3.30x faster than widely\nused parallelism solutions such as FairScale. These advancements stem from our\nclosed-form formulation of Transformers based on the Universal Approximation\nTheorem, which not only explains the ``depth belief'' but also opens new\navenues for designing efficient Transformer architectures. Source code:\nhttps://(open-upon-acceptance)", "AI": {"tldr": "ParaFormer is a shallow Transformer architecture that achieves parallelism by organizing layers into parallel branches with algorithmic inter-layer collaboration, enabling faster convergence, model compression, and improved performance compared to standard Transformers.", "motivation": "Address the challenges of deep Transformers including long training times, high inference latency, and impracticality on resource-constrained devices, while challenging the 'deeper is better' philosophy.", "method": "Formulate Transformers as function approximators in closed-form based on Universal Approximation Theorem, then organize layers into parallel branches with progressive approximation that ensures each new branch reduces loss from preceding branches.", "result": "Outperforms standard Transformers like ViT, supports up to 15.07x model compression, enables 3.30x faster parallel deployment than FairScale, and facilitates adaptive continuous learning through model expansion.", "conclusion": "The closed-form formulation of Transformers explains the 'depth belief' and enables efficient parallel architectures like ParaFormer, opening new avenues for designing efficient Transformer models without depth dependency."}}
{"id": "2510.15429", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15429", "abs": "https://arxiv.org/abs/2510.15429", "authors": ["Shashank Gupta"], "title": "Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models", "comment": "PhD Thesis of Shashank Gupta defended at the University of Amsterdam\n  on October 13th 2025", "summary": "This dissertation investigates how reinforcement learning (RL) methods can be\ndesigned to be safe, sample-efficient, and robust. Framed through the unifying\nperspective of contextual-bandit RL, the work addresses two major application\ndomains - ranking and recommendation, and text-to-image diffusion models. The\nfirst part of the thesis develops theory and algorithms for safe deployment in\nranking systems. An exposure-based generalisation bound is derived, leading to\na counterfactual risk-minimisation objective whose solution is guaranteed not\nto underperform the logging policy, even with sparse feedback. This guarantee\nis extended to doubly robust estimators, enabling safety even under adversarial\nor misspecified user models and offering practitioners explicit control over\npermissible utility loss. The second part turns to single-action bandits, where\nvarious off-policy estimators are unified within a baseline-correction\nframework. A closed-form optimal baseline is proposed and shown to minimise\nboth evaluation and policy-gradient variance, thereby improving off-policy\nlearning reliability. The final part examines the trade-offs between efficiency\nand effectiveness in generative RL. A systematic study of PPO and REINFORCE\nmotivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple\ndiffusion trajectories with a REINFORCE-style baseline inside PPO's clipped\nobjective. LOOP achieves PPO-level sample efficiency while producing\ngenerations that align more faithfully with textual attributes.", "AI": {"tldr": "This dissertation develops safe, sample-efficient RL methods for ranking systems and text-to-image diffusion models, featuring exposure-based safety guarantees, optimal baseline corrections, and the LOOP algorithm for improved generative alignment.", "motivation": "To address the need for reinforcement learning methods that are simultaneously safe, sample-efficient, and robust across different application domains including ranking systems and generative models.", "method": "Developed contextual-bandit RL framework with: 1) Exposure-based generalization bounds and counterfactual risk minimization for safe ranking deployment; 2) Baseline-correction framework unifying off-policy estimators with optimal baseline; 3) LOOP algorithm combining PPO and REINFORCE for diffusion models.", "result": "Achieved safety guarantees preventing underperformance of logging policies even with sparse feedback, reduced variance in off-policy learning through optimal baselines, and improved text-to-image alignment with PPO-level efficiency via LOOP algorithm.", "conclusion": "The dissertation successfully demonstrates how RL can be made safe and efficient across diverse applications, providing theoretical guarantees and practical algorithms for real-world deployment in ranking systems and generative modeling."}}
{"id": "2510.15444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15444", "abs": "https://arxiv.org/abs/2510.15444", "authors": ["Zhi Zhou", "Yuhao Tan", "Zenan Li", "Yuan Yao", "Lan-Zhe Guo", "Yu-Feng Li", "Xiaoxing Ma"], "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "comment": "Accepted by NeurIPS 2025", "summary": "Test-time scaling seeks to improve the reasoning performance of large\nlanguage models (LLMs) by adding computational resources. A prevalent approach\nwithin the field is sampling-based test-time scaling methods, which enhance\nreasoning by generating multiple reasoning paths for a given input during\ninference. However, despite its practical success, the theoretical foundations\nremain underexplored. In this paper, we provide the first theoretical framework\nfor analyzing sampling-based test-time scaling methods, grounded in the\nperspective of confidence estimation. Based on the framework, we analyze two\ndominant paradigms: self-consistency and perplexity, and reveal key\nlimitations: self-consistency suffers from high estimation error while\nperplexity exhibits substantial modeling error and possible degradation of the\nestimation error convergence. To address these limitations, we introduce RPC, a\nhybrid method that leverages our theoretical insights through two key\ncomponents: Perplexity Consistency and Reasoning Pruning. Perplexity\nConsistency combines the strengths of self-consistency and perplexity, boosting\nthe convergence rate of estimation error from linear to exponential while\npreserving model error. Reasoning Pruning prevents degradation by eliminating\nlow-probability reasoning paths. Both theoretical analysis and empirical\nresults across seven benchmark datasets demonstrate that RPC has a strong\npotential for reducing reasoning error. Notably, RPC achieves reasoning\nperformance comparable to self-consistency while not only enhancing confidence\nreliability but also reducing sampling costs by 50%. The code and resources are\navailable at https://wnjxyk.github.io/RPC.", "AI": {"tldr": "This paper provides the first theoretical framework for analyzing sampling-based test-time scaling methods in LLMs, identifies limitations in existing approaches (self-consistency and perplexity), and introduces RPC - a hybrid method that combines Perplexity Consistency and Reasoning Pruning to improve reasoning performance while reducing sampling costs.", "motivation": "Despite the practical success of sampling-based test-time scaling methods that generate multiple reasoning paths during inference, their theoretical foundations remain underexplored. The paper aims to establish a theoretical framework to understand and improve these methods.", "method": "The authors introduce RPC (Reasoning Pruning with Perplexity Consistency), a hybrid method with two key components: 1) Perplexity Consistency that combines strengths of self-consistency and perplexity to boost estimation error convergence rate from linear to exponential, and 2) Reasoning Pruning that eliminates low-probability reasoning paths to prevent degradation.", "result": "Empirical results across seven benchmark datasets demonstrate that RPC achieves reasoning performance comparable to self-consistency while enhancing confidence reliability and reducing sampling costs by 50%. The method shows strong potential for reducing reasoning error.", "conclusion": "RPC provides an effective solution to the limitations of existing test-time scaling methods by leveraging theoretical insights to create a hybrid approach that improves reasoning performance while being more computationally efficient."}}
{"id": "2510.15456", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15456", "abs": "https://arxiv.org/abs/2510.15456", "authors": ["Jan Corazza", "Hadi Partovi Aria", "Daniel Neider", "Zhe Xu"], "title": "Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment", "comment": "Please cite the proceedings version. Source code:\n  https://github.com/corazza/tcrl", "summary": "Reinforcement learning (RL) algorithms struggle with learning optimal\npolicies for tasks where reward feedback is sparse and depends on a complex\nsequence of events in the environment. Probabilistic reward machines (PRMs) are\nfinite-state formalisms that can capture temporal dependencies in the reward\nsignal, along with nondeterministic task outcomes. While special RL algorithms\ncan exploit this finite-state structure to expedite learning, PRMs remain\ndifficult to modify and design by hand. This hinders the already difficult\ntasks of utilizing high-level causal knowledge about the environment, and\ntransferring the reward formalism into a new domain with a different causal\nstructure. This paper proposes a novel method to incorporate causal information\nin the form of Temporal Logic-based Causal Diagrams into the reward formalism,\nthereby expediting policy learning and aiding the transfer of task\nspecifications to new environments. Furthermore, we provide a theoretical\nresult about convergence to optimal policy for our method, and demonstrate its\nstrengths empirically.", "AI": {"tldr": "The paper proposes a method to incorporate causal information via Temporal Logic-based Causal Diagrams into probabilistic reward machines to improve reinforcement learning in sparse-reward environments and aid task transfer.", "motivation": "RL algorithms struggle with sparse rewards and complex temporal dependencies. Probabilistic reward machines help but are difficult to design manually and transfer across domains with different causal structures.", "method": "Incorporates causal information using Temporal Logic-based Causal Diagrams into the reward formalism to expedite policy learning and facilitate task specification transfer.", "result": "The method provides theoretical convergence guarantees to optimal policy and demonstrates empirical strengths in improving learning efficiency.", "conclusion": "The proposed approach successfully integrates causal knowledge into reward machines, enhancing RL performance in sparse-reward environments and enabling better transfer of task specifications across domains."}}
{"id": "2510.15495", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15495", "abs": "https://arxiv.org/abs/2510.15495", "authors": ["Woo-Jin Ahn", "Sang-Ryul Baek", "Yong-Jun Lee", "Hyun-Duck Choi", "Myo-Taeg Lim"], "title": "OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning", "comment": null, "summary": "Reinforcement learning algorithms typically utilize an interactive simulator\n(i.e., environment) with a predefined reward function for policy training.\nDeveloping such simulators and manually defining reward functions, however, is\noften time-consuming and labor-intensive. To address this, we propose an\nOffline Simulator (OffSim), a novel model-based offline inverse reinforcement\nlearning (IRL) framework, to emulate environmental dynamics and reward\nstructure directly from expert-generated state-action trajectories. OffSim\njointly optimizes a high-entropy transition model and an IRL-based reward\nfunction to enhance exploration and improve the generalizability of the learned\nreward. Leveraging these learned components, OffSim can subsequently train a\npolicy offline without further interaction with the real environment.\nAdditionally, we introduce OffSim$^+$, an extension that incorporates a\nmarginal reward for multi-dataset settings to enhance exploration. Extensive\nMuJoCo experiments demonstrate that OffSim achieves substantial performance\ngains over existing offline IRL methods, confirming its efficacy and\nrobustness.", "AI": {"tldr": "OffSim is a model-based offline inverse reinforcement learning framework that learns environmental dynamics and reward functions from expert trajectories, enabling offline policy training without real environment interaction.", "motivation": "Traditional RL requires time-consuming simulator development and manual reward function design. OffSim addresses this by learning both dynamics and rewards directly from expert data.", "method": "Jointly optimizes high-entropy transition model and IRL-based reward function from state-action trajectories. OffSim+ extends this with marginal reward for multi-dataset settings.", "result": "Extensive MuJoCo experiments show substantial performance gains over existing offline IRL methods, demonstrating efficacy and robustness.", "conclusion": "OffSim provides an effective framework for learning simulators and rewards from expert data, enabling offline policy training without environment interaction."}}
{"id": "2510.15502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15502", "abs": "https://arxiv.org/abs/2510.15502", "authors": ["Shijia Kang", "Muhan Zhang"], "title": "The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling", "comment": null, "summary": "Reinforcement learning (RL) has been pivotal in enhancing the reasoning\ncapabilities of large language models (LLMs), but it often suffers from limited\nexploration and entropy collapse, where models exploit a narrow set of\nsolutions, leading to a loss of sampling diversity and subsequently preventing\nRL from further improving performance. This issue is exacerbated in parallel\nsampling methods, where multiple outputs are drawn from the same distribution,\npotentially causing the model to converge to similar solutions. We propose\nSESA, a novel SEquential SAmpling framework that mitigates this challenge by\ngenerating diverse solution sketches sequentially before expanding them into\nfull reasoning paths. This approach ensures broader exploration by conditioning\neach new output on previous ones, promoting diversity throughout the process\nand preventing policy collapse. Our experiments on a synthetic task show that\nsequential sampling consistently outperforms traditional RL methods in terms of\npath diversity and recovery from collapse. Further evaluations on real-world\ntasks demonstrate that SESA improves both the exploration of valid strategies\nand the overall performance of LLMs. On three agent benchmarks, SESA lifts\nsuccess rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up\nto an additional $211\\%$ relative improvement over baseline RL), underscoring\nits exploration advantage. This work introduces a structured approach to\nexploration, paving the way for more effective and diverse reasoning in\nRL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.", "AI": {"tldr": "SESA is a sequential sampling framework that addresses exploration limitations in RL-trained LLMs by generating diverse solution sketches sequentially, preventing entropy collapse and improving performance across multiple benchmarks.", "motivation": "Traditional RL methods for LLMs suffer from limited exploration and entropy collapse, where models exploit narrow solution sets, reducing sampling diversity and preventing further performance improvements, especially in parallel sampling scenarios.", "method": "Proposed SESA framework generates diverse solution sketches sequentially before expanding them into full reasoning paths, conditioning each new output on previous ones to ensure broader exploration and prevent policy collapse.", "result": "Experiments show sequential sampling outperforms traditional RL in path diversity and recovery from collapse. On three agent benchmarks, SESA improves success rates by +0.25, +0.42, and +0.07 absolute over base model (up to 211% relative improvement over baseline RL).", "conclusion": "SESA introduces a structured approach to exploration that enables more effective and diverse reasoning in RL-trained LLMs, demonstrating significant improvements in both exploration and overall performance."}}
{"id": "2510.15511", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15511", "abs": "https://arxiv.org/abs/2510.15511", "authors": ["Giorgos Nikolaou", "Tommaso Mencattini", "Donato Crisostomi", "Andrea Santilli", "Yannis Panagakis", "Emanuele Rodola'"], "title": "Language Models are Injective and Hence Invertible", "comment": null, "summary": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.", "AI": {"tldr": "Transformers are proven to be injective (lossless) despite non-linear components, enabling exact input recovery from hidden representations using the SipIt algorithm.", "motivation": "Challenge the view that transformer non-linearities prevent exact input recovery, aiming to establish injectivity as a fundamental property for transparency and interpretability.", "method": "Mathematical proof of injectivity at initialization and during training, empirical collision tests on six language models, and development of SipIt algorithm for exact input reconstruction.", "result": "No collisions observed in billions of tests, SipIt achieves linear-time exact input recovery from hidden activations in practice.", "conclusion": "Injectivity is a fundamental property of language models with implications for transparency, interpretability, and safe deployment."}}
{"id": "2510.15516", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15516", "abs": "https://arxiv.org/abs/2510.15516", "authors": ["Giulia Lanzillotta", "Felix Sarnthein", "Gil Kur", "Thomas Hofmann", "Bobby He"], "title": "Revisiting Knowledge Distillation: The Hidden Role of Dataset Size", "comment": null, "summary": "The concept of knowledge distillation (KD) describes the training of a\nstudent model from a teacher model and is a widely adopted technique in deep\nlearning. However, it is still not clear how and why distillation works.\nPrevious studies focus on two central aspects of distillation: model size, and\ngeneralisation. In this work we study distillation in a third dimension:\ndataset size. We present a suite of experiments across a wide range of\ndatasets, tasks and neural architectures, demonstrating that the effect of\ndistillation is not only preserved but amplified in low-data regimes. We call\nthis newly discovered property the data efficiency of distillation. Equipped\nwith this new perspective, we test the predictive power of existing theories of\nKD as we vary the dataset size. Our results disprove the hypothesis that\ndistillation can be understood as label smoothing, and provide further evidence\nin support of the dark knowledge hypothesis. Finally, we analyse the impact of\nmodelling factors such as the objective, scale and relative number of samples\non the observed phenomenon. Ultimately, this work reveals that the dataset size\nmay be a fundamental but overlooked variable in the mechanisms underpinning\ndistillation.", "AI": {"tldr": "Knowledge distillation's effectiveness increases in low-data regimes, disproving the label smoothing hypothesis and supporting dark knowledge theory.", "motivation": "To understand how knowledge distillation works by examining its relationship with dataset size, as previous studies focused mainly on model size and generalization.", "method": "Conducted extensive experiments across various datasets, tasks, and neural architectures while systematically varying dataset sizes to analyze distillation effects.", "result": "Distillation's benefits are amplified in low-data settings, disproving the label smoothing hypothesis and providing evidence for dark knowledge theory.", "conclusion": "Dataset size is a fundamental but overlooked variable in understanding knowledge distillation mechanisms, with distillation showing enhanced data efficiency in limited-data scenarios."}}
{"id": "2510.15535", "categories": ["cs.LG", "cs.GR", "I.3; I.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.15535", "abs": "https://arxiv.org/abs/2510.15535", "authors": ["Abhay Kumar Dwivedi", "Shanu Saklani", "Soumya Dutta"], "title": "Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation", "comment": "Accepted for publication in 16th Indian Conference on Computer\n  Vision, Graphics and Image Processing (ICVGIP 2025)", "summary": "The extensive adoption of Deep Neural Networks has led to their increased\nutilization in challenging scientific visualization tasks. Recent advancements\nin building compressed data models using implicit neural representations have\nshown promising results for tasks like spatiotemporal volume visualization and\nsuper-resolution. Inspired by these successes, we develop compressed neural\nrepresentations for multivariate datasets containing tens to hundreds of\nvariables. Our approach utilizes a single network to learn representations for\nall data variables simultaneously through parameter sharing. This allows us to\nachieve state-of-the-art data compression. Through comprehensive evaluations,\nwe demonstrate superior performance in terms of reconstructed data quality,\nrendering and visualization quality, preservation of dependency information\namong variables, and storage efficiency.", "AI": {"tldr": "The paper develops compressed neural representations for multivariate datasets using a single network with parameter sharing to achieve state-of-the-art data compression.", "motivation": "The extensive adoption of Deep Neural Networks in scientific visualization tasks and recent successes in compressed data models using implicit neural representations for spatiotemporal volume visualization and super-resolution inspired the development of neural representations for multivariate datasets.", "method": "Utilizes a single network to learn representations for all data variables simultaneously through parameter sharing, enabling compressed neural representations for datasets containing tens to hundreds of variables.", "result": "Achieves state-of-the-art data compression with superior performance in reconstructed data quality, rendering and visualization quality, preservation of dependency information among variables, and storage efficiency.", "conclusion": "The approach demonstrates effective compressed neural representations for multivariate datasets through comprehensive evaluations showing excellent performance across multiple metrics."}}
{"id": "2510.15541", "categories": ["cs.LG", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.15541", "abs": "https://arxiv.org/abs/2510.15541", "authors": ["Saumya B"], "title": "An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation", "comment": "Code and results available at\n  https://github.com/Saumya4321/mc-dropout-boundary-correlation", "summary": "Accurate brain tumor segmentation from MRI is vital for diagnosis and\ntreatment planning. Although Monte Carlo (MC) Dropout is widely used to\nestimate model uncertainty, its effectiveness in identifying segmentation\nerrors -- especially near tumor boundaries -- remains unclear. This study\nempirically examines the relationship between MC Dropout--based uncertainty and\nsegmentation error in 2D brain tumor MRI segmentation using a U-Net trained\nunder four augmentation settings: none, horizontal flip, rotation, and scaling.\nUncertainty was computed from 50 stochastic forward passes and correlated with\npixel-wise errors using Pearson and Spearman coefficients. Results show weak\nglobal correlations ($r \\approx 0.30$--$0.38$) and negligible boundary\ncorrelations ($|r| < 0.05$). Although differences across augmentations were\nstatistically significant ($p < 0.001$), they lacked practical relevance. These\nfindings suggest that MC Dropout uncertainty provides limited cues for boundary\nerror localization, underscoring the need for alternative or hybrid uncertainty\nestimation methods in medical image segmentation.", "AI": {"tldr": "MC Dropout uncertainty shows weak correlation with segmentation errors in brain tumor MRI, especially near boundaries, suggesting limited utility for error localization in medical image segmentation.", "motivation": "To evaluate the effectiveness of MC Dropout-based uncertainty in identifying segmentation errors, particularly near tumor boundaries, for brain tumor MRI segmentation.", "method": "Used U-Net trained under four augmentation settings (none, horizontal flip, rotation, scaling), computed uncertainty from 50 stochastic forward passes, and correlated with pixel-wise errors using Pearson and Spearman coefficients.", "result": "Weak global correlations (r \u2248 0.30-0.38) and negligible boundary correlations (|r| < 0.05). Differences across augmentations were statistically significant but lacked practical relevance.", "conclusion": "MC Dropout uncertainty provides limited cues for boundary error localization, highlighting the need for alternative or hybrid uncertainty estimation methods in medical image segmentation."}}
{"id": "2510.15555", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15555", "abs": "https://arxiv.org/abs/2510.15555", "authors": ["Sibo Xiao"], "title": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems", "comment": null, "summary": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions.", "AI": {"tldr": "SDR is a novel causal inference framework combining strategic equilibrium modeling with doubly robust estimation to handle endogenous treatment from strategic agent behavior, achieving significant bias reduction.", "motivation": "Address endogenous treatment assignment caused by strategic agent behavior in causal inference, where traditional methods fail to account for agents' strategic responses to interventions.", "method": "Integrates strategic equilibrium modeling with doubly robust estimation, maintaining double robustness while incorporating strategic considerations through strategic unconfoundedness assumptions.", "result": "Achieves 7.6%-29.3% bias reduction over baseline methods across varying strategic strengths, maintains robust scalability with agent populations, and demonstrates superior performance in empirical evaluations.", "conclusion": "SDR provides a principled framework for reliable causal inference in strategic environments where agents respond strategically to interventions, with proven theoretical properties and empirical effectiveness."}}
{"id": "2510.15563", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15563", "abs": "https://arxiv.org/abs/2510.15563", "authors": ["Edward Tansley", "Estelle Massart", "Coralia Cartis"], "title": "On the Neural Feature Ansatz for Deep Neural Networks", "comment": null, "summary": "Understanding feature learning is an important open question in establishing\na mathematical foundation for deep neural networks. The Neural Feature Ansatz\n(NFA) states that after training, the Gram matrix of the first-layer weights of\na deep neural network is proportional to some power $\\alpha>0$ of the average\ngradient outer product (AGOP) of this network with respect to its inputs.\nAssuming gradient flow dynamics with balanced weight initialization, the NFA\nwas proven to hold throughout training for two-layer linear networks with\nexponent $\\alpha = 1/2$ (Radhakrishnan et al., 2024). We extend this result to\nnetworks with $L \\geq 2$ layers, showing that the NFA holds with exponent\n$\\alpha = 1/L$, thus demonstrating a depth dependency of the NFA. Furthermore,\nwe prove that for unbalanced initialization, the NFA holds asymptotically\nthrough training if weight decay is applied. We also provide counterexamples\nshowing that the NFA does not hold for some network architectures with\nnonlinear activations, even when these networks fit arbitrarily well the\ntraining data. We thoroughly validate our theoretical results through numerical\nexperiments across a variety of optimization algorithms, weight decay rates and\ninitialization schemes.", "AI": {"tldr": "The paper extends the Neural Feature Ansatz (NFA) to deep linear networks, showing a depth-dependent exponent \u03b1=1/L, and proves asymptotic NFA for unbalanced initialization with weight decay. It also provides counterexamples for nonlinear networks.", "motivation": "To understand feature learning in deep neural networks by extending the Neural Feature Ansatz to multi-layer architectures and investigating its depth dependency and conditions for validity.", "method": "Theoretical analysis using gradient flow dynamics with balanced/unbalanced weight initialization, mathematical proofs for linear networks, counterexamples for nonlinear architectures, and numerical validation across various optimization settings.", "result": "NFA holds with exponent \u03b1=1/L for L-layer linear networks, holds asymptotically for unbalanced initialization with weight decay, but fails for some nonlinear networks even with perfect training fit.", "conclusion": "The NFA exhibits depth dependency in linear networks and requires specific conditions (weight decay) for unbalanced initialization, but may not generalize to nonlinear architectures despite good training performance."}}
{"id": "2510.15583", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15583", "abs": "https://arxiv.org/abs/2510.15583", "authors": ["Jixin Zhang", "Yong Lai"], "title": "Attn-JGNN: Attention Enhanced Join-Graph Neural Networks", "comment": null, "summary": "We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) model\nfor solving #SAT problems, which significantly improves the solving accuracy.\nInspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNN\nuses tree decomposition to encode the CNF formula into a join-graph, then\nperforms iterative message passing on the join-graph, and finally approximates\nthe model number by learning partition functions. In order to further improve\nthe accuracy of the solution, we apply the attention mechanism in and between\nclusters of the join-graphs, which makes Attn-JGNN pay more attention to the\nkey variables and clusters in probabilistic inference, and reduces the\nredundant calculation. Finally, our experiments show that our Attn-JGNN model\nachieves better results than other neural network methods.", "AI": {"tldr": "Attn-JGNN is an attention-enhanced join-graph neural network model that improves #SAT solving accuracy by using tree decomposition, iterative message passing, and attention mechanisms to focus on key variables and clusters.", "motivation": "To improve the solving accuracy of #SAT problems by leveraging neural networks and attention mechanisms to enhance probabilistic inference in constraint satisfaction problems.", "method": "Uses tree decomposition to encode CNF formulas into join-graphs, performs iterative message passing on the graphs, applies attention mechanisms within and between clusters to focus on key variables, and learns partition functions to approximate model counts.", "result": "Attn-JGNN achieves better results than other neural network methods for #SAT problem solving, demonstrating improved accuracy.", "conclusion": "The attention-enhanced join-graph neural network approach effectively improves #SAT solving accuracy by focusing computational resources on critical variables and clusters while reducing redundant calculations."}}
{"id": "2510.15620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15620", "abs": "https://arxiv.org/abs/2510.15620", "authors": ["Jiahao Zhou", "Chengliang Lin", "Dingji Li", "Mingkai Dong", "Haibo Chen"], "title": "GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device", "comment": null, "summary": "Semantic top-K selection with cross-encoder rerankers underpins of on-device\nAI services, such as retrieval-augmented generation, agent memory, and\npersonalized recommendation. However, its latency and memory demands dominate\nend-to-end budgets on edge hardware. Revisiting the objective of top-K\nselection, we reveal that only relative rankings matter, not exact\nper-candidate scores. We further observe sequence-level sparsity: relative\nrankings stabilize early in intermediate layers, allowing pruning opportunities\nprior to completing full inference.\n  Building on this insight, we propose monolithic forwarding and develop a\ntraining-free inference system, GRATING. By maintaining a global view of all\ncandidates, it reduces latency through progressive cluster pruning. It also\nbounds peak memory usage by strategically overlapping I/O with computation via\ndual-layer sliding window and chunked execution. We evaluate GRATING against\nstate-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple\nM2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak\nmemory by up to 94.9% in microbenchmarks, without any loss in precision. Across\nthree real-world on-device AI applications, GRATING lowers latency by\n11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial\nimprovements in efficiency and deployability.", "AI": {"tldr": "GRATING is a training-free inference system that accelerates semantic top-K selection by exploiting sequence-level sparsity and progressive cluster pruning, achieving up to 89% latency reduction and 95% memory savings without precision loss.", "motivation": "Semantic top-K selection with cross-encoder rerankers dominates latency and memory budgets on edge hardware for on-device AI services like retrieval-augmented generation and personalized recommendation.", "method": "Monolithic forwarding with progressive cluster pruning that leverages sequence-level sparsity (relative rankings stabilize early in intermediate layers), plus dual-layer sliding window and chunked execution for memory management.", "result": "GRATING reduces latency by up to 89.0% and peak memory by up to 94.9% in microbenchmarks, and 11.6%-51.0% latency reduction and 18.6%-77.8% memory savings across real-world applications, with no precision loss.", "conclusion": "GRATING demonstrates substantial efficiency improvements for on-device AI services by exploiting relative ranking properties and sequence-level sparsity, enabling more deployable edge AI systems."}}
{"id": "2510.15623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15623", "abs": "https://arxiv.org/abs/2510.15623", "authors": ["Parsa Abbasi", "Stefan Heindorf"], "title": "CQD-SHAP: Explainable Complex Query Answering via Shapley Values", "comment": null, "summary": "Complex query answering (CQA) goes beyond the well-studied link prediction\ntask by addressing more sophisticated queries that require multi-hop reasoning\nover incomplete knowledge graphs (KGs). Research on neural and neurosymbolic\nCQA methods is still an emerging field. Almost all of these methods can be\nregarded as black-box models, which may raise concerns about user trust.\nAlthough neurosymbolic approaches like CQD are slightly more interpretable,\nallowing intermediate results to be tracked, the importance of different parts\nof the query remains unexplained. In this paper, we propose CQD-SHAP, a novel\nframework that computes the contribution of each query part to the ranking of a\nspecific answer. This contribution explains the value of leveraging a neural\npredictor that can infer new knowledge from an incomplete KG, rather than a\nsymbolic approach relying solely on existing facts in the KG. CQD-SHAP is\nformulated based on Shapley values from cooperative game theory and satisfies\nall the fundamental Shapley axioms. Automated evaluation of these explanations\nin terms of necessary and sufficient explanations, and comparisons with various\nbaselines, shows the effectiveness of this approach for most query types.", "AI": {"tldr": "CQD-SHAP is a novel framework that uses Shapley values to explain the contribution of each query part in complex query answering over knowledge graphs, addressing the black-box nature of existing methods.", "motivation": "Current neural and neurosymbolic complex query answering methods are mostly black-box models, raising trust concerns. Even neurosymbolic approaches like CQD lack explanations for the importance of different query parts.", "method": "Proposed CQD-SHAP framework based on Shapley values from cooperative game theory to compute the contribution of each query part to answer ranking, satisfying all fundamental Shapley axioms.", "result": "Automated evaluation shows CQD-SHAP's effectiveness for most query types in terms of necessary and sufficient explanations, outperforming various baselines.", "conclusion": "CQD-SHAP successfully provides interpretable explanations for complex query answering by quantifying the value of neural predictors in inferring new knowledge from incomplete knowledge graphs."}}
{"id": "2510.15644", "categories": ["cs.LG", "eess.SP", "math.OC", "68W10, 68W15, 68W40, 90C06, 90C35, 90C26", "G.1.6; F.2.1; E.4"], "pdf": "https://arxiv.org/pdf/2510.15644", "abs": "https://arxiv.org/abs/2510.15644", "authors": ["Tomas Ortega", "Hamid Jafarkhani"], "title": "Decentralized Parameter-Free Online Learning", "comment": null, "summary": "We propose the first parameter-free decentralized online learning algorithms\nwith network regret guarantees, which achieve sublinear regret without\nrequiring hyperparameter tuning. This family of algorithms connects multi-agent\ncoin-betting and decentralized online learning via gossip steps. To enable our\ndecentralized analysis, we introduce a novel \"betting function\" formulation for\ncoin-betting that simplifies the multi-agent regret analysis. Our analysis\nshows sublinear network regret bounds and is validated through experiments on\nsynthetic and real datasets. This family of algorithms is applicable to\ndistributed sensing, decentralized optimization, and collaborative ML\napplications.", "AI": {"tldr": "First parameter-free decentralized online learning algorithms with sublinear network regret guarantees, connecting multi-agent coin-betting and decentralized learning via gossip steps.", "motivation": "To develop decentralized online learning algorithms that achieve sublinear regret without requiring hyperparameter tuning, addressing the need for practical distributed learning systems.", "method": "Proposes a family of algorithms combining multi-agent coin-betting with gossip steps, introducing a novel \"betting function\" formulation to simplify multi-agent regret analysis.", "result": "Achieves sublinear network regret bounds, validated through experiments on synthetic and real datasets.", "conclusion": "The parameter-free decentralized algorithms are applicable to distributed sensing, decentralized optimization, and collaborative machine learning applications."}}
{"id": "2510.15651", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "AMS Subject Classification: 65M99, 47-08, 68T07, 65D15"], "pdf": "https://arxiv.org/pdf/2510.15651", "abs": "https://arxiv.org/abs/2510.15651", "authors": ["Ziqian Li", "Kang Liu", "Yongcun Song", "Hangrui Yue", "Enrique Zuazua"], "title": "Deep Neural ODE Operator Networks for PDEs", "comment": null, "summary": "Operator learning has emerged as a promising paradigm for developing\nefficient surrogate models to solve partial differential equations (PDEs).\nHowever, existing approaches often overlook the domain knowledge inherent in\nthe underlying PDEs and hence suffer from challenges in capturing temporal\ndynamics and generalization issues beyond training time frames. This paper\nintroduces a deep neural ordinary differential equation (ODE) operator network\nframework, termed NODE-ONet, to alleviate these limitations. The framework\nadopts an encoder-decoder architecture comprising three core components: an\nencoder that spatially discretizes input functions, a neural ODE capturing\nlatent temporal dynamics, and a decoder reconstructing solutions in physical\nspaces. Theoretically, error analysis for the encoder-decoder architecture is\ninvestigated. Computationally, we propose novel physics-encoded neural ODEs to\nincorporate PDE-specific physical properties. Such well-designed neural ODEs\nsignificantly reduce the framework's complexity while enhancing numerical\nefficiency, robustness, applicability, and generalization capacity. Numerical\nexperiments on nonlinear diffusion-reaction and Navier-Stokes equations\ndemonstrate high accuracy, computational efficiency, and prediction\ncapabilities beyond training time frames. Additionally, the framework's\nflexibility to accommodate diverse encoders/decoders and its ability to\ngeneralize across related PDE families further underscore its potential as a\nscalable, physics-encoded tool for scientific machine learning.", "AI": {"tldr": "NODE-ONet is a neural ODE operator network that incorporates PDE physics to improve temporal dynamics modeling and generalization beyond training time frames.", "motivation": "Existing operator learning approaches often ignore domain knowledge from PDEs, leading to poor temporal dynamics capture and generalization issues beyond training periods.", "method": "Encoder-decoder architecture with three components: spatial encoder, neural ODE for latent temporal dynamics, and decoder for physical space reconstruction. Uses physics-encoded neural ODEs to incorporate PDE-specific properties.", "result": "Demonstrates high accuracy, computational efficiency, and prediction capabilities beyond training time frames on nonlinear diffusion-reaction and Navier-Stokes equations. Reduces framework complexity while enhancing numerical efficiency and generalization.", "conclusion": "NODE-ONet provides a scalable, physics-encoded framework for scientific machine learning with flexibility for diverse encoders/decoders and generalization across related PDE families."}}
{"id": "2510.15653", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15653", "abs": "https://arxiv.org/abs/2510.15653", "authors": ["Yefan Zeng", "Shengyu Duan", "Rishad Shafik", "Alex Yakovlev"], "title": "Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization", "comment": null, "summary": "The Tsetlin Machine (TM) offers high-speed inference on resource-constrained\ndevices such as CPUs. Its logic-driven operations naturally lend themselves to\nparallel execution on modern CPU architectures. Motivated by this, we propose\nan efficient software implementation of the TM by leveraging instruction-level\nbitwise operations for compact model representation and accelerated processing.\nTo further improve inference speed, we introduce an early exit mechanism, which\nexploits the TM's AND-based clause evaluation to avoid unnecessary\ncomputations. Building upon this, we propose a literal Reorder strategy\ndesigned to maximize the likelihood of early exits. This strategy is applied\nduring a post-training, pre-inference stage through statistical analysis of all\nliterals and the corresponding actions of their associated Tsetlin Automata\n(TA), introducing negligible runtime overhead. Experimental results using the\ngem5 simulator with an ARM processor show that our optimized implementation\nreduces inference time by up to 96.71% compared to the conventional\ninteger-based TM implementations while maintaining comparable code density.", "AI": {"tldr": "The paper proposes an optimized Tsetlin Machine implementation using bitwise operations and early exit mechanisms to achieve up to 96.71% inference time reduction on ARM processors while maintaining code density.", "motivation": "To leverage the Tsetlin Machine's logic-driven operations for high-speed inference on resource-constrained devices like CPUs, particularly taking advantage of modern CPU architectures' parallel execution capabilities.", "method": "Uses instruction-level bitwise operations for compact model representation and accelerated processing, introduces an early exit mechanism to avoid unnecessary computations, and implements a literal reorder strategy applied during post-training through statistical analysis of literals and Tsetlin Automata actions.", "result": "Experimental results using gem5 simulator with ARM processor show inference time reduction of up to 96.71% compared to conventional integer-based TM implementations while maintaining comparable code density.", "conclusion": "The proposed optimization techniques significantly improve Tsetlin Machine inference speed on CPU architectures through bitwise operations and early exit mechanisms, making it highly suitable for resource-constrained devices."}}
{"id": "2510.15655", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15655", "abs": "https://arxiv.org/abs/2510.15655", "authors": ["Lino Gerlach", "Liv V\u00e5ge", "Thore Gerlach", "Elliott Kauffman"], "title": "WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables", "comment": "Preprint. Under review", "summary": "Fast and efficient machine learning is of growing interest to the scientific\ncommunity and has spurred significant research into novel model architectures\nand hardware-aware design. Recent hard? and software co-design approaches have\ndemonstrated impressive results with entirely multiplication-free models.\nDifferentiable Logic Gate Networks (DLGNs), for instance, provide a\ngradient-based framework for learning optimal combinations of low-level logic\ngates, setting state-of-the-art trade-offs between accuracy, resource usage,\nand latency. However, these models suffer from high computational cost during\ntraining and do not generalize well to logic blocks with more inputs. In this\nwork, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables\n(WARP-LUTs) - a novel gradient-based method that efficiently learns\ncombinations of logic gates with substantially fewer trainable parameters. We\ndemonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10\ncompared to DLGNs, while maintaining comparable accuracy. Furthermore, our\napproach suggests potential for extension to higher-input logic blocks,\nmotivating future research on extremely efficient deployment on modern FPGAs\nand its real-time science applications.", "AI": {"tldr": "WARP-LUTs is a novel gradient-based method that efficiently learns combinations of logic gates with fewer parameters, achieving faster convergence on CIFAR-10 compared to DLGNs while maintaining comparable accuracy.", "motivation": "Existing multiplication-free models like DLGNs suffer from high computational cost during training and poor generalization to logic blocks with more inputs, creating a need for more efficient training methods.", "method": "WARP-LUTs uses Walsh-Assisted Relaxation for Probabilistic Look-Up Tables, a gradient-based approach that learns optimal combinations of logic gates with substantially fewer trainable parameters.", "result": "WARP-LUTs achieve significantly faster convergence on CIFAR-10 compared to DLGNs while maintaining comparable accuracy, suggesting potential for extension to higher-input logic blocks.", "conclusion": "The approach shows promise for extremely efficient deployment on modern FPGAs and real-time science applications, motivating future research in this direction."}}
{"id": "2510.15674", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15674", "abs": "https://arxiv.org/abs/2510.15674", "authors": ["Yung-Chen Tang", "Pin-Yu Chen", "Andrea Cavallaro"], "title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning", "comment": null, "summary": "Allocating more computation during inference time (test-time scaling)\nimproves language model performance, especially for reasoning tasks. However,\npopular methods like Best-of-$N$ sampling often show diminishing returns as $N$\nincreases. To address this inefficiency, we introduce a general test-time\ncalibration framework that adaptively modifies the model toward high-reward\nreasoning paths, with theoretical guarantees of improving the lower bound of\nexpected reward under finite sampling, all without large language model (LLM)\nretraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),\na two-phase method that first explores the solution space and then learns a\ncalibration of the logits via an input-specific temperature $T$ and additive\nshift vector $\\delta$, guiding generation toward more reliable reasoning.\nExperiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,\nwith up to $4\\times$ fewer rollouts to reach the same accuracy, while often\nachieving higher accuracy under fixed budgets. We also analyze the\ncomplementary roles of $T$ and $\\delta$ in balancing output diversity and\ncorrectness, and demonstrate that the framework also generalizes to step-level\nsampling strategies such as beam search. For more information, please refer to\nour project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.", "AI": {"tldr": "CarBoN is a test-time calibration framework that improves reasoning efficiency by adaptively guiding language model generation toward high-reward paths using input-specific temperature and shift parameters, achieving up to 4x fewer rollouts for the same accuracy.", "motivation": "Existing test-time scaling methods like Best-of-N sampling show diminishing returns as N increases, leading to inefficient computation during inference for reasoning tasks.", "method": "A two-phase method that first explores the solution space, then learns calibration of logits via input-specific temperature T and additive shift vector \u03b4 to guide generation toward reliable reasoning paths.", "result": "Experiments on MATH-500 and AIME-2024 show CarBoN improves efficiency with up to 4x fewer rollouts to reach same accuracy, often achieving higher accuracy under fixed budgets.", "conclusion": "The framework provides theoretical guarantees for improving expected reward lower bounds without LLM retraining, and generalizes to step-level sampling strategies like beam search."}}
{"id": "2510.15699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15699", "abs": "https://arxiv.org/abs/2510.15699", "authors": ["Virendra Nishad", "Bhaskar Mukhoty", "Hilal AlQuabeh", "Sandeep K. Shukla", "Sayak Ray Chowdhury"], "title": "Constrained Adversarial Perturbation", "comment": null, "summary": "Deep neural networks have achieved remarkable success in a wide range of\nclassification tasks. However, they remain highly susceptible to adversarial\nexamples - inputs that are subtly perturbed to induce misclassification while\nappearing unchanged to humans. Among various attack strategies, Universal\nAdversarial Perturbations (UAPs) have emerged as a powerful tool for both\nstress testing model robustness and facilitating scalable adversarial training.\nDespite their effectiveness, most existing UAP methods neglect domain specific\nconstraints that govern feature relationships. Violating such constraints, such\nas debt to income ratios in credit scoring or packet flow invariants in network\ncommunication, can render adversarial examples implausible or easily\ndetectable, thereby limiting their real world applicability.\n  In this work, we advance universal adversarial attacks to constrained feature\nspaces by formulating an augmented Lagrangian based min max optimization\nproblem that enforces multiple, potentially complex constraints of varying\nimportance. We propose Constrained Adversarial Perturbation (CAP), an efficient\nalgorithm that solves this problem using a gradient based alternating\noptimization strategy. We evaluate CAP across diverse domains including\nfinance, IT networks, and cyber physical systems, and demonstrate that it\nachieves higher attack success rates while significantly reducing runtime\ncompared to existing baselines. Our approach also generalizes seamlessly to\nindividual adversarial perturbations, where we observe similar strong\nperformance gains. Finally, we introduce a principled procedure for learning\nfeature constraints directly from data, enabling broad applicability across\ndomains with structured input spaces.", "AI": {"tldr": "This paper proposes CAP (Constrained Adversarial Perturbation), a method for generating universal adversarial perturbations that respect domain-specific feature constraints, achieving higher attack success rates with reduced runtime across finance, IT networks, and cyber-physical systems.", "motivation": "Current universal adversarial perturbation methods ignore domain-specific constraints that govern feature relationships, making adversarial examples implausible or easily detectable in real-world applications like credit scoring and network communication.", "method": "The authors formulate an augmented Lagrangian-based min-max optimization problem to enforce multiple complex constraints and propose CAP, a gradient-based alternating optimization algorithm that solves this problem efficiently.", "result": "CAP achieves higher attack success rates while significantly reducing runtime compared to existing baselines across diverse domains including finance, IT networks, and cyber-physical systems. It also generalizes well to individual adversarial perturbations.", "conclusion": "The proposed CAP method effectively generates constrained adversarial perturbations, and the authors introduce a principled procedure for learning feature constraints directly from data, enabling broad applicability across domains with structured input spaces."}}
{"id": "2510.15700", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15700", "abs": "https://arxiv.org/abs/2510.15700", "authors": ["Alex Gu", "Bartosz Piotrowski", "Fabian Gloeckle", "Kaiyu Yang", "Aram H. Markosyan"], "title": "ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations", "comment": "52 pages, 16 figures, website: http://proof-optimizer.github.io/", "summary": "Neural theorem proving has advanced rapidly in the past year, reaching IMO\ngold-medalist capabilities and producing formal proofs that span thousands of\nlines. Although such proofs are mechanically verified by formal systems like\nLean, their excessive length renders them difficult for humans to comprehend\nand limits their usefulness for mathematical insight. Proof simplification is\ntherefore a critical bottleneck. Yet, training data for this task is scarce,\nand existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --\nstruggle with the extremely long proofs generated by RL-trained provers. We\nintroduce ProofOptimizer, the first language model trained to simplify Lean\nproofs without requiring additional human supervision. ProofOptimizer is\ntrained via expert iteration and reinforcement learning, using Lean to verify\nsimplifications and provide training signal. At inference time, it operates\nwithin an iterative proof-shortening workflow, progressively reducing proof\nlength. Experiments show that ProofOptimizer substantially compresses proofs\ngenerated by state-of-the-art RL-trained provers on standard benchmarks,\nreducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on\nSeed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check\nfaster in Lean and further improve downstream prover performance when reused as\ntraining data for supervised finetuning.", "AI": {"tldr": "ProofOptimizer is a language model trained via expert iteration and RL to simplify Lean proofs without human supervision, achieving 49-87% length reduction on benchmarks while maintaining correctness.", "motivation": "Neural theorem provers generate excessively long proofs that are mechanically verified but difficult for humans to comprehend, limiting mathematical insight. Proof simplification is a critical bottleneck with scarce training data.", "method": "Trained via expert iteration and reinforcement learning using Lean to verify simplifications and provide training signal. Operates within an iterative proof-shortening workflow at inference time.", "result": "Substantially compresses proofs: 87% reduction on miniF2F, 57% on PutnamBench, 49% on Seed-Prover's IMO 2025 proofs. Simplified proofs check faster in Lean and improve downstream prover performance when reused as training data.", "conclusion": "ProofOptimizer effectively addresses the proof simplification bottleneck, enabling more comprehensible and efficient formal proofs while improving downstream theorem proving performance."}}
{"id": "2510.15720", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15720", "abs": "https://arxiv.org/abs/2510.15720", "authors": ["Edwin Hamel-De le Court", "Gaspard Ohlmann", "Francesco Belardinelli"], "title": "ProSh: Probabilistic Shielding for Model-free Reinforcement Learning", "comment": null, "summary": "Safety is a major concern in reinforcement learning (RL): we aim at\ndeveloping RL systems that not only perform optimally, but are also safe to\ndeploy by providing formal guarantees about their safety. To this end, we\nintroduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free\nalgorithm for safe reinforcement learning under cost constraints. ProSh\naugments the Constrained MDP state space with a risk budget and enforces safety\nby applying a shield to the agent's policy distribution using a learned cost\ncritic. The shield ensures that all sampled actions remain safe in expectation.\nWe also show that optimality is preserved when the environment is\ndeterministic. Since ProSh is model-free, safety during training depends on the\nknowledge we have acquired about the environment. We provide a tight\nupper-bound on the cost in expectation, depending only on the backup-critic\naccuracy, that is always satisfied during training. Under mild, practically\nachievable assumptions, ProSh guarantees safety even at training time, as shown\nin the experiments.", "AI": {"tldr": "ProSh is a model-free safe RL algorithm that uses risk augmentation and policy shielding to ensure safety under cost constraints while preserving optimality in deterministic environments.", "motivation": "Safety is critical for deploying RL systems, requiring formal guarantees about safety while maintaining optimal performance.", "method": "Augments Constrained MDP state space with risk budget and applies shield to policy distribution using learned cost critic to ensure sampled actions remain safe in expectation.", "result": "Provides tight upper-bound on expected cost depending on backup-critic accuracy, and guarantees safety during training under practical assumptions.", "conclusion": "ProSh enables safe reinforcement learning with formal safety guarantees while preserving optimality in deterministic settings."}}
{"id": "2510.15728", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15728", "abs": "https://arxiv.org/abs/2510.15728", "authors": ["Mahyar Alinejad", "Alvaro Velasquez", "Yue Wang", "George Atia"], "title": "RLAF: Reinforcement Learning from Automaton Feedback", "comment": null, "summary": "Reinforcement Learning (RL) in environments with complex, history-dependent\nreward structures poses significant challenges for traditional methods. In this\nwork, we introduce a novel approach that leverages automaton-based feedback to\nguide the learning process, replacing explicit reward functions with\npreferences derived from a deterministic finite automaton (DFA). Unlike\nconventional approaches that use automata for direct reward specification, our\nmethod employs the structure of the DFA to generate preferences over\ntrajectories that are used to learn a reward function, eliminating the need for\nmanual reward engineering. Our framework introduces a static approach that uses\nthe learned reward function directly for policy optimization and a dynamic\napproach that involves continuous refining of the reward function and policy\nthrough iterative updates until convergence.\n  Our experiments in both discrete and continuous environments demonstrate that\nour approach enables the RL agent to learn effective policies for tasks with\ntemporal dependencies, outperforming traditional reward engineering and\nautomaton-based baselines such as reward machines and LTL-guided methods. Our\nresults highlight the advantages of automaton-based preferences in handling\nnon-Markovian rewards, offering a scalable, efficient, and human-independent\nalternative to traditional reward modeling. We also provide a convergence\nguarantee showing that under standard assumptions our automaton-guided\npreference-based framework learns a policy that is near-optimal with respect to\nthe true non-Markovian objective.", "AI": {"tldr": "A novel RL approach using automaton-based preferences instead of explicit reward functions, with static and dynamic methods for learning policies in environments with complex temporal dependencies.", "motivation": "Traditional RL struggles with complex, history-dependent reward structures that require manual reward engineering. The paper aims to eliminate this need by using automaton-based feedback.", "method": "Leverages deterministic finite automaton (DFA) to generate preferences over trajectories, learning reward functions automatically. Two approaches: static (direct policy optimization) and dynamic (iterative reward and policy refinement).", "result": "Outperforms traditional reward engineering and automaton-based baselines in both discrete and continuous environments, effectively handling temporal dependencies and non-Markovian rewards.", "conclusion": "Automaton-based preferences provide a scalable, efficient, human-independent alternative to traditional reward modeling, with proven convergence guarantees for near-optimal policies."}}
{"id": "2510.15750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15750", "abs": "https://arxiv.org/abs/2510.15750", "authors": ["Nayan Kumar Singh"], "title": "A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis", "comment": "14 pages, 6 figures, 5 tables. Code available\n  at:https://github.com/SinghNayanKumar/DL-surrogate-modelling", "summary": "Although Finite Element Analysis (FEA) is an integral part of the product\ndesign lifecycle, the analysis is computationally expensive, making it\nunsuitable for many design optimization problems. The deep learning models can\nbe a great solution. However, selecting the architecture that emulates the FEA\nwith great accuracy is a challenge. This paper presents a comprehensive\nevaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEA\nof parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)\nframework, governed by the Navier Cauchy equations, to enforce physical laws.\nCrucially, we demonstrate that a curriculum learning strategy, pretraining on\ndata followed by physics informed fine tuning, is essential for stabilizing\ntraining. Our results show that GNNs fundamentally outperform the U-Net. Even\nthe worst performer among GNNs, the GCN framework, achieved a relative L2 error\nof 8.7% while the best framework among U Net, U Net with attention mechanism\ntrained on high resolution data, achieved 13.0% score. Among the graph-based\narchitectures, the Message Passing Neural Networks (MPNN) and Graph\nTransformers achieved the highest accuracy, achieving a relative L2 score of\n3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)\nsignificantly improved the generalization, reducing error by up to 11.3% on\nhigh-signal tasks. While the Graph Transformer is the most accurate model, it\nis more 37.5% slower during inference when compared to second best model, MPNN\nPINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.\nIt offers a good compromise between predictive performance, model size, and\ninference speed.", "AI": {"tldr": "Graph Neural Networks (GNNs) outperform 3D U-Nets as FEA surrogates for parametric I-beams, with Physics-Informed Neural Networks (PINN) enhancing generalization. MPNN PINN offers the best balance of accuracy, speed, and practicality.", "motivation": "FEA is computationally expensive for design optimization, and deep learning models can provide efficient surrogates, but selecting the right architecture is challenging.", "method": "Comprehensive evaluation of GNNs and 3D U-Nets with Physics-Informed Neural Network (PINN) framework using Navier-Cauchy equations, employing curriculum learning with pretraining followed by physics-informed fine-tuning.", "result": "GNNs significantly outperform U-Nets, with MPNN and Graph Transformers achieving lowest errors (3.5% and 2.6% L2). PINN improved generalization by up to 11.3%. MPNN PINN provides best practical balance between performance and speed.", "conclusion": "GNNs are superior FEA surrogates over U-Nets, with PINN enhancing generalization. MPNN PINN offers the most practical solution combining good accuracy, model size, and inference speed."}}
{"id": "2510.15751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15751", "abs": "https://arxiv.org/abs/2510.15751", "authors": ["Trung-Anh Dang", "Vincent Nguyen", "Ngoc-Son Vu", "Christel Vrain"], "title": "SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive Mixup and Neural Collapse", "comment": null, "summary": "While most continual learning methods focus on mitigating forgetting and\nimproving accuracy, they often overlook the critical aspect of network\ncalibration, despite its importance. Neural collapse, a phenomenon where\nlast-layer features collapse to their class means, has demonstrated advantages\nin continual learning by reducing feature-classifier misalignment. Few works\naim to improve the calibration of continual models for more reliable\npredictions. Our work goes a step further by proposing a novel method that not\nonly enhances calibration but also improves performance by reducing\noverconfidence, mitigating forgetting, and increasing accuracy. We introduce\nSphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural\ncollapse-based methods. SAMix adapts the mixing process to the geometric\nproperties of feature spaces under neural collapse, ensuring more robust\nregularization and alignment. Experiments show that SAMix significantly boosts\nperformance, surpassing SOTA methods in continual learning while also improving\nmodel calibration. SAMix enhances both across-task accuracy and the broader\nreliability of predictions, making it a promising advancement for robust\ncontinual learning systems.", "AI": {"tldr": "SAMix is a novel adaptive mixup strategy that enhances neural collapse-based continual learning by improving model calibration, reducing overconfidence, mitigating forgetting, and increasing accuracy through geometric-aware feature space regularization.", "motivation": "Most continual learning methods focus on mitigating forgetting and improving accuracy but overlook network calibration, which is crucial for reliable predictions. Neural collapse has shown benefits in continual learning, but few works address calibration improvement.", "method": "Proposed Sphere-Adaptive Mixup (SAMix) - an adaptive mixup strategy specifically designed for neural collapse-based methods. SAMix adapts the mixing process to the geometric properties of feature spaces under neural collapse to ensure robust regularization and alignment.", "result": "Experiments demonstrate that SAMix significantly boosts performance, surpassing state-of-the-art methods in continual learning while also improving model calibration. It enhances both across-task accuracy and prediction reliability.", "conclusion": "SAMix represents a promising advancement for robust continual learning systems by simultaneously improving performance and calibration through geometric-aware feature space adaptation."}}
{"id": "2510.15757", "categories": ["cs.LG", "cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15757", "abs": "https://arxiv.org/abs/2510.15757", "authors": ["Pieris Panagi", "Savvas Karatsiolis", "Kyriacos Mosphilis", "Nicholas Hadjisavvas", "Andreas Kamilaris", "Nicolas Nicolaou", "Efstathios Stavrakis", "Vassilis Vassiliades"], "title": "Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity", "comment": null, "summary": "Poultry farming faces increasing pressure to meet productivity targets while\nensuring animal welfare and environmental compliance. Yet many small and\nmedium-sized farms lack affordable, integrated tools for continuous monitoring\nand decision-making, relying instead on manual, reactive inspections. This\npaper presents Poultry Farm Intelligence (PoultryFI) - a modular,\ncost-effective platform that integrates six AI-powered modules: Camera\nPlacement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time\nEgg Counting, Production & Profitability Forecasting, and a Recommendation\nModule.\n  Camera layouts are first optimized offline using evolutionary algorithms for\nfull poultry house coverage with minimal hardware. The Audio-Visual Monitoring\nmodule extracts welfare indicators from synchronized video, audio, and feeding\ndata. Analytics & Alerting produces daily summaries and real-time\nnotifications, while Real-Time Egg Counting uses an edge vision model to\nautomate production tracking. Forecasting models predict egg yield and feed\nconsumption up to 10 days in advance, and the Recommendation Module integrates\nforecasts with weather data to guide environmental and operational adjustments.\n  This is among the first systems to combine low-cost sensing, edge analytics,\nand prescriptive AI to continuously monitor flocks, predict production, and\noptimize performance. Field trials demonstrate 100% egg-count accuracy on\nRaspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.\nPoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide\nintelligence, empowering producers to proactively safeguard welfare and\nprofitability.", "AI": {"tldr": "PoultryFI is a modular AI platform that integrates six modules for continuous poultry farm monitoring, including camera placement optimization, audio-visual welfare monitoring, real-time egg counting, production forecasting, and operational recommendations.", "motivation": "Small and medium-sized poultry farms lack affordable, integrated tools for continuous monitoring and decision-making, relying on manual inspections instead of proactive management.", "method": "The system uses evolutionary algorithms for camera placement optimization, synchronized audio-visual data analysis for welfare monitoring, edge vision models for real-time egg counting, forecasting models for production prediction, and integrates weather data for operational recommendations.", "result": "Field trials achieved 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting up to 10 days in advance.", "conclusion": "PoultryFI bridges the gap between isolated pilot tools and scalable farm-wide intelligence, enabling proactive management of animal welfare and profitability through integrated AI-powered monitoring and decision support."}}
{"id": "2510.15796", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15796", "abs": "https://arxiv.org/abs/2510.15796", "authors": ["Anton Raskovalov"], "title": "Cavity Duplexer Tuning with 1d Resnet-like Neural Networks", "comment": null, "summary": "This paper presents machine learning method for tuning of cavity duplexer\nwith a large amount of adjustment screws. After testing we declined\nconventional reinforcement learning approach and reformulated our task in the\nsupervised learning setup. The suggested neural network architecture includes\n1d ResNet-like backbone and processing of some additional information about\nS-parameters, like the shape of curve and peaks positions and amplitudes. This\nneural network with external control algorithm is capable to reach almost the\ntuned state of the duplexer within 4-5 rotations per screw.", "AI": {"tldr": "Machine learning method for tuning cavity duplexers with many adjustment screws using supervised learning with a neural network architecture that includes 1D ResNet-like backbone and processes S-parameter curve characteristics.", "motivation": "To develop an efficient method for tuning cavity duplexers with a large number of adjustment screws, as conventional reinforcement learning approaches were found inadequate for this task.", "method": "Used supervised learning with a neural network architecture featuring a 1D ResNet-like backbone that processes S-parameter characteristics including curve shape, peak positions, and amplitudes, combined with an external control algorithm.", "result": "The neural network with external control algorithm can achieve nearly tuned state of the duplexer within 4-5 rotations per screw.", "conclusion": "The supervised learning approach with specialized neural network architecture is effective for cavity duplexer tuning, significantly outperforming conventional reinforcement learning methods."}}
{"id": "2510.15808", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15808", "abs": "https://arxiv.org/abs/2510.15808", "authors": ["Benedikt Alkin", "Richard Kurle", "Louis Serrano", "Dennis Just", "Johannes Brandstetter"], "title": "AB-UPT for Automotive and Aerospace Applications", "comment": null, "summary": "The recently proposed Anchored-Branched Universal Physics Transformers\n(AB-UPT) shows strong capabilities to replicate automotive computational fluid\ndynamics simulations requiring orders of magnitudes less compute than\ntraditional numerical solvers. In this technical report, we add two new\ndatasets to the body of empirically evaluated use-cases of AB-UPT, combining\nhigh-quality data generation with state-of-the-art neural surrogates. Both\ndatasets were generated with the Luminary Cloud platform containing automotives\n(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data\ngeneration. Next, we show favorable performances of AB-UPT against previous\nstate-of-the-art transformer-based baselines on both datasets, followed by\nextensive qualitative and quantitative evaluations of our best AB-UPT model.\nAB-UPT shows strong performances across the board. Notably, it obtains near\nperfect prediction of integrated aerodynamic forces within seconds from a\nsimple isotopically tesselate geometry representation and is trainable within a\nday on a single GPU, paving the way for industry-scale applications.", "AI": {"tldr": "AB-UPT neural networks show strong performance in automotive and aircraft CFD simulations, achieving near-perfect aerodynamic force predictions with significantly less computation than traditional solvers.", "motivation": "To expand the empirical evaluation of AB-UPT by adding two new automotive and aircraft datasets, demonstrating its capabilities for industry-scale computational fluid dynamics applications.", "method": "Used the Luminary Cloud platform to generate high-quality datasets (SHIFT-SUV for automotive and SHIFT-Wing for aircraft), then trained AB-UPT models on these datasets and compared against previous transformer-based baselines.", "result": "AB-UPT achieved near-perfect prediction of integrated aerodynamic forces within seconds using simple geometry representations, outperforming previous state-of-the-art transformer models on both datasets.", "conclusion": "AB-UPT provides highly efficient CFD simulations that are trainable within a day on a single GPU, making it suitable for industry-scale applications requiring fast and accurate aerodynamic predictions."}}
{"id": "2510.15830", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15830", "abs": "https://arxiv.org/abs/2510.15830", "authors": ["Dominik Kallusky", "Vinay Rao", "Vishal Nandavanam", "Hao-Jun Michael Shi"], "title": "SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients", "comment": null, "summary": "The rapid development of large language models (LLMs) has driven the demand\nfor more efficient optimization techniques. Among these, the Lookahead family\nof optimizers employs a two-loop framework, maintaining fast and slow sets of\nmodel weights. Multiple inner optimizer steps on the fast weights produce a\ntrajectory - the pseudo-gradient - that is used to update the slow weights.\nDiLoCo, a notable example originally designed for distributed training, applies\nNesterov momentum to the averaged pseudo-gradient from multiple workers,\nclaiming to even outperform AdamW in a non-distributed setup. In this paper, we\nempirically show that DiLoCo's surprising effectiveness stems primarily from\napplying Nesterov momentum to the pseudo-gradient, which improves training in a\nnon-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov\nOuter Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains\nof 1.5 - 2.5$\\times$ in a non-distributed setting up to a scale of 1e23\ntraining FLOPs, with improvements that increase with model size. Because of its\nminimal compute and memory overhead and compatibility with model sharding, SNOO\nis a practical enhancement for a variety of inner optimizers, including AdamW\nand Muon.", "AI": {"tldr": "The paper shows that DiLoCo's effectiveness comes from applying Nesterov momentum to pseudo-gradients, and introduces SNOO (Step-K Nesterov Outer Optimizer) which achieves 1.5-2.5\u00d7 compute gains in non-distributed settings with minimal overhead.", "motivation": "To understand why DiLoCo optimizer performs well in non-distributed settings and develop a more efficient optimization technique for large language models.", "method": "Proposed SNOO (Step-K Nesterov Outer Optimizer) which applies Nesterov momentum to pseudo-gradients in a two-loop Lookahead framework, with minimal compute/memory overhead and compatibility with model sharding.", "result": "SNOO achieves compute factor gains of 1.5-2.5\u00d7 in non-distributed settings up to 1e23 training FLOPs, with improvements increasing with model size.", "conclusion": "SNOO is a practical enhancement for various inner optimizers (AdamW, Muon) that provides significant compute efficiency gains with minimal overhead."}}
{"id": "2510.15833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15833", "abs": "https://arxiv.org/abs/2510.15833", "authors": ["Hoang M. Ngo", "Tamer Kahveci", "My T. Thai"], "title": "FIDDLE: Reinforcement Learning for Quantum Fidelity Enhancement", "comment": null, "summary": "Quantum computing has the potential to revolutionize fields like quantum\noptimization and quantum machine learning. However, current quantum devices are\nhindered by noise, reducing their reliability. A key challenge in gate-based\nquantum computing is improving the reliability of quantum circuits, measured by\nprocess fidelity, during the transpilation process, particularly in the routing\nstage. In this paper, we address the Fidelity Maximization in Routing Stage\n(FMRS) problem by introducing FIDDLE, a novel learning framework comprising two\nmodules: a Gaussian Process-based surrogate model to estimate process fidelity\nwith limited training samples and a reinforcement learning module to optimize\nrouting. Our approach is the first to directly maximize process fidelity,\noutperforming traditional methods that rely on indirect metrics such as circuit\ndepth or gate count. We rigorously evaluate FIDDLE by comparing it with\nstate-of-the-art fidelity estimation techniques and routing optimization\nmethods. The results demonstrate that our proposed surrogate model is able to\nprovide a better estimation on the process fidelity compared to existing\nlearning techniques, and our end-to-end framework significantly improves the\nprocess fidelity of quantum circuits across various noise models.", "AI": {"tldr": "FIDDLE is a novel learning framework that directly maximizes quantum circuit process fidelity during routing by combining Gaussian Process-based fidelity estimation with reinforcement learning, outperforming traditional indirect metric approaches.", "motivation": "Current quantum devices suffer from noise that reduces reliability, and existing transpilation methods optimize indirect metrics like circuit depth rather than directly maximizing process fidelity, which is crucial for quantum computing applications.", "method": "FIDDLE uses a two-module approach: a Gaussian Process surrogate model for accurate process fidelity estimation with limited samples, and a reinforcement learning module to optimize routing decisions that directly maximize fidelity.", "result": "The surrogate model provides better fidelity estimation than existing learning techniques, and the end-to-end framework significantly improves process fidelity across various noise models compared to state-of-the-art methods.", "conclusion": "FIDDLE successfully addresses the Fidelity Maximization in Routing Stage problem by directly optimizing process fidelity, demonstrating superior performance over traditional approaches that rely on indirect circuit metrics."}}
{"id": "2510.15837", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15837", "abs": "https://arxiv.org/abs/2510.15837", "authors": ["Vikash Singh"], "title": "Transfer Orthology Networks", "comment": "4 pages", "summary": "We present Transfer Orthology Networks (TRON), a novel neural network\narchitecture designed for cross-species transfer learning. TRON leverages\northologous relationships, represented as a bipartite graph between species, to\nguide knowledge transfer. Specifically, we prepend a learned species conversion\nlayer, whose weights are masked by the biadjacency matrix of this bipartite\ngraph, to a pre-trained feedforward neural network that predicts a phenotype\nfrom gene expression data in a source species. This allows for efficient\ntransfer of knowledge to a target species by learning a linear transformation\nthat maps gene expression from the source to the target species' gene space.\nThe learned weights of this conversion layer offer a potential avenue for\ninterpreting functional orthology, providing insights into how genes across\nspecies contribute to the phenotype of interest. TRON offers a biologically\ngrounded and interpretable approach to cross-species transfer learning, paving\nthe way for more effective utilization of available transcriptomic data. We are\nin the process of collecting cross-species transcriptomic/phenotypic data to\ngain experimental validation of the TRON architecture.", "AI": {"tldr": "TRON is a neural network architecture for cross-species transfer learning that uses orthologous relationships to guide knowledge transfer between species through a species conversion layer.", "motivation": "To enable effective cross-species transfer learning by leveraging biological orthology relationships and provide interpretable insights into functional orthology across species.", "method": "Uses a bipartite graph of orthologous relationships between species, with a learned species conversion layer whose weights are masked by the biadjacency matrix, prepended to a pre-trained feedforward neural network for phenotype prediction.", "result": "The architecture allows efficient knowledge transfer by learning linear transformations that map gene expression from source to target species, with learned weights providing interpretable insights into functional orthology.", "conclusion": "TRON provides a biologically grounded and interpretable approach to cross-species transfer learning, enabling more effective utilization of transcriptomic data across species."}}
{"id": "2510.15850", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.15850", "abs": "https://arxiv.org/abs/2510.15850", "authors": ["Michael Klamkin", "Mathieu Tanneau", "Pascal Van Hentenryck"], "title": "Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch", "comment": null, "summary": "Recent research has shown that optimization proxies can be trained to high\nfidelity, achieving average optimality gaps under 1% for large-scale problems.\nHowever, worst-case analyses show that there exist in-distribution queries that\nresult in orders of magnitude higher optimality gap, making it difficult to\ntrust the predictions in practice. This paper aims at striking a balance\nbetween classical solvers and optimization proxies in order to enable\ntrustworthy deployments with interpretable speed-optimality tradeoffs based on\na user-defined optimality threshold. To this end, the paper proposes a hybrid\nsolver that leverages duality theory to efficiently bound the optimality gap of\npredictions, falling back to a classical solver for queries where optimality\ncannot be certified. To improve the achieved speedup of the hybrid solver, the\npaper proposes an alternative training procedure that combines the primal and\ndual proxy training. Experiments on large-scale transmission systems show that\nthe hybrid solver is highly scalable. The proposed hybrid solver achieves\nspeedups of over 1000x compared to a parallelized simplex-based solver while\nguaranteeing a maximum optimality gap of 2%.", "AI": {"tldr": "A hybrid solver combining optimization proxies with classical solvers to guarantee optimality gaps while achieving significant speedups, using duality theory for certification.", "motivation": "Optimization proxies achieve high average performance but have unreliable worst-case gaps, making them untrustworthy for practical deployment where optimality guarantees are needed.", "method": "Proposes a hybrid solver that uses duality theory to bound optimality gaps of proxy predictions, falling back to classical solvers when optimality cannot be certified. Also introduces combined primal-dual proxy training to improve speedup.", "result": "Achieves over 1000x speedup compared to parallelized simplex-based solver while guaranteeing maximum 2% optimality gap on large-scale transmission systems.", "conclusion": "The hybrid approach enables trustworthy deployment of optimization proxies with interpretable speed-optimality tradeoffs based on user-defined thresholds, balancing the benefits of proxies and classical solvers."}}
